<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-08T08:11:20.855Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>随机森林</title>
    <link href="http://yoursite.com/2019/09/08/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://yoursite.com/2019/09/08/随机森林/</id>
    <published>2019-09-08T08:10:06.000Z</published>
    <updated>2019-09-08T08:11:20.855Z</updated>
    
    <content type="html"><![CDATA[<h2 id="随机森林："><a href="#随机森林：" class="headerlink" title="随机森林："></a>随机森林：</h2><p>随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML38.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;随机森林：&quot;&gt;&lt;a href=&quot;#随机森林：&quot; class=&quot;headerlink&quot; title=&quot;随机森林：&quot;&gt;&lt;/a&gt;随机森林：&lt;/h2&gt;&lt;p&gt;随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。&lt;/p&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="随机森林" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据库的数据模型</title>
    <link href="http://yoursite.com/2019/09/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2019/09/07/数据库的数据模型/</id>
    <published>2019-09-07T11:17:44.000Z</published>
    <updated>2019-09-07T14:10:30.951Z</updated>
    
    <content type="html"><![CDATA[<p>建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。</p><p>由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求：</p><p>1、比较真实地描述现实世界</p><p>2、易为用户所理解</p><p>3、易于在计算机上实现</p><p><strong>为什么需要数据模型？</strong></p><p>由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。</p><p><strong>数据模型含有哪些内容？</strong></p><p>数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。</p><p>​    1、数据结构</p><p>​         用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面</p><p>​    2、数据操作</p><p>​        用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查</p><p>​    3、数据的约束条件</p><p>​        是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。</p><p><strong>实体联系数据模型的地位与作用：</strong></p><p>实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。</p><p>数据模型是用来描述数据的一组概念和定义，是描述数据的手段。</p><p>​    概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。</p><p>​    逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模</p><p>​    物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。</p><p>逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。</p><p>数据模式是对数据结构、联系和约束的描述。<strong>数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。</strong></p><p>信息世界中的基本概念：</p><p>​    (1)  实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。</p><p>​    (2)  属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画</p><p>​    (3)  键(Key)或称为码：唯一标识实体的属性集称为码</p><p>​    (4)  域(Domain)：属性的取值范围称为该属性的域</p><p>​    (5)  实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型</p><p>​    (6)  实体集(Entity Set)：同一类型实体的集合称为实体集</p><p>​    (7)  联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。</p><p>概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。</p><p>​          实体型：用矩形表示，矩形框内写明实体名</p><p>​           属性：用椭圆表示，并用无向边将其与相应的实体连接起来。</p><p>​           联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n）</p><p>​           键：用下划线表示。</p><p>最常用的数据模型</p><p>​    非关系模型</p><p>​        层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。</p><p>​    满足以下两个条件称为层次模型：</p><p>​        (1)  有且仅有一个结点无双亲。这个结点称为“根节点”</p><p>​        (2)  其它节点有且仅有一个双亲，但可以有多个后继</p><p>​        网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。</p><p>​            网状结构特点：</p><p>​                 (1) 允许一个以上的结点无双亲；</p><p>​                 (2)  一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。</p><p>​    关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。</p><p>​        关系数据模型的数据结构：</p><p>​            关系(Relation)：一个关系对应通常说的一张表</p><p>​            元祖(Tuple)：表中的一行即为一个元祖</p><p>​            属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。</p><p>​            主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。</p><p>​            域(Domain)：属性的取值范围</p><p>​            分量：元祖中的一个属性值</p><p>​            关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n)</p><p>​    面向对象模型：</p><p>​    对象关系模型</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。&lt;/p&gt;
&lt;p&gt;由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="数据模型" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>模型的评判和调优</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2019/09/07/模型的评判和调优/</id>
    <published>2019-09-07T08:11:22.000Z</published>
    <updated>2019-09-07T08:16:56.717Z</updated>
    
    <content type="html"><![CDATA[<h2 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt></p><p>分类模型评估API：F1-score,反应了模型的稳健性</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML53.png" alt></p><p>模型选择和调优：1、交叉验证 2、网格搜索</p><p>交叉验证为了让被评估的模型更加准确可信</p><p>网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。</p><p>sklearn.model_Selection.GridSearchCV</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML54.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML55.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;精确率和召回率&quot;&gt;&lt;a href=&quot;#精确率和召回率&quot; class=&quot;headerlink&quot; title=&quot;精确率和召回率&quot;&gt;&lt;/a&gt;精确率和召回率&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bricke
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型的评判和调优" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K-means</title>
    <link href="http://yoursite.com/2019/09/07/K-means/"/>
    <id>http://yoursite.com/2019/09/07/K-means/</id>
    <published>2019-09-07T08:09:47.000Z</published>
    <updated>2019-09-07T08:16:03.722Z</updated>
    
    <content type="html"><![CDATA[<h2 id="k—means"><a href="#k—means" class="headerlink" title="k—means"></a>k—means</h2><p>k代表数据划分为几个类别</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML50.png" alt></p><p>聚类评估标准：轮廓系数，范围在[-1,1]越接近1，效果越好</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML51.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;k—means&quot;&gt;&lt;a href=&quot;#k—means&quot; class=&quot;headerlink&quot; title=&quot;k—means&quot;&gt;&lt;/a&gt;k—means&lt;/h2&gt;&lt;p&gt;k代表数据划分为几个类别&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubus
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="K-means" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/K-means/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/2019/09/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/逻辑回归/</id>
    <published>2019-09-07T08:09:32.000Z</published>
    <updated>2019-09-07T08:15:13.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类算法-逻辑回归"><a href="#分类算法-逻辑回归" class="headerlink" title="分类算法-逻辑回归"></a>分类算法-逻辑回归</h2><p>解决二分类问题</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML48.png" alt></p><p>penalty是正则化的方式是l2，C是正则化力度</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML49.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类算法-逻辑回归&quot;&gt;&lt;a href=&quot;#分类算法-逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;分类算法-逻辑回归&quot;&gt;&lt;/a&gt;分类算法-逻辑回归&lt;/h2&gt;&lt;p&gt;解决二分类问题&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githu
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归</title>
    <link href="http://yoursite.com/2019/09/07/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/线性回归/</id>
    <published>2019-09-07T08:09:19.000Z</published>
    <updated>2019-09-08T05:03:29.070Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h2><p>回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向<br>量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目<br>标根本不是求解出标签，注意加以区别。</p><p>线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这<br>些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决<br>于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型<br>变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归<br>可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png" alt></p><p>线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个<br>有 个特征的样本 而言，它的回归结果可以写作一个几乎人人熟悉的方程 ：</p><p>W被统称为模型的参数。W0被称为截距W1-Wn被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 Xi1-Xin是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML82.png" alt></p><p>我们可以使用矩阵来表示这个方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML83.png" alt></p><p>y=Xw，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中w可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量w 。</p><p>在多元线性回归中，我们在损失函数如下定义：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML84.png" alt></p><p>因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML85.png" alt></p><p><strong>第一次看不明白上面红字，这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数</strong>。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。</p><p>现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对w求导。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML88.png" alt></p><p>我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML89.png" alt></p><h2 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h2><p>sklearn中的线性模型模块是linear_model。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML74.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML90.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML75.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML76.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML77.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML78.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML79.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML80.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML81.png" alt></p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML42.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML91.png" alt></p><h2 id="模型评估指标"><a href="#模型评估指标" class="headerlink" title="模型评估指标"></a>模型评估指标</h2><p>回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等<br>评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算<br>法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不<br>同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。<br>第一，我们是否预测到了正确的数值。<br>第二，我们是否拟合到了足够的信息。 </p><p>sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML43.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML45.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML44.png" alt></p><p>​            <strong>注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue=fch()</span><br><span class="line">xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">std_x=StandardScaler()</span><br><span class="line">xtrain=std_x.fit_transform(xtrain)</span><br><span class="line">xtest=std_x.transform(xtest)</span><br><span class="line">std_y=StandardScaler()</span><br><span class="line">ytrain=std_y.fit_transform(ytrain.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">ytest=std_y.transform(ytest.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">reg=LR().fit(xtrain,ytrain)</span><br><span class="line">ypredict=reg.predict(xtest)</span><br><span class="line">print(std_y.inverse_transform(ypredict))</span><br><span class="line">print(<span class="string">"均方误差："</span>,mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))</span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>这里如果我们运行上面的代码，会在第十九行报错：</p><p>我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评<br>判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，<br>会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，<br>所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是<br>neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助:</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML93.png" alt></p><p>在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。</p><p>R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种<br>是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两<br>种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"R2"</span>,r2_score(ypredict,ytest))</span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML94.png" alt></p><p>????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是<strong>预测值在分子，真实值在分母</strong>。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用r2_score</span></span><br><span class="line">print(<span class="string">"R2"</span>,r2_score(y_true=ytest,y_pred=ypredict))</span><br><span class="line"><span class="comment">#利用接口score</span></span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#EVS的两种调用方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> explained_variance_score <span class="keyword">as</span> evs</span><br><span class="line"><span class="comment">#第一种</span></span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"explained_variance"</span>))</span><br><span class="line"><span class="comment">#第二种</span></span><br><span class="line">print(<span class="string">"evs"</span>,evs(ytest,ypredict))</span><br></pre></td></tr></table></figure><p>过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML46.png" alt></p><p>解决方法：正则化。用岭回归实现</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML47.png" alt></p><p>欠拟合：一个假设在训练数据上不能获得更好的拟合，但是在训练数据外的数据集上也不能很好地拟合数据。此时认为这个假设出现了欠拟合的现象。(模型过于简单) 解决办法是添加数据的特征数量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归原理&quot;&gt;&lt;a href=&quot;#线性回归原理&quot; class=&quot;headerlink&quot; title=&quot;线性回归原理&quot;&gt;&lt;/a&gt;线性回归原理&lt;/h2&gt;&lt;p&gt;回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向&lt;br&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>模型的保存和加载</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/"/>
    <id>http://yoursite.com/2019/09/07/模型的保存和加载/</id>
    <published>2019-09-07T08:09:05.000Z</published>
    <updated>2019-09-07T08:13:11.828Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML40.png" alt></p><p>joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。</p><p>joblib.load()：读取模型。参数是模型的目录</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML56.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;模型的保存和加载&quot;&gt;&lt;a href=&quot;#模型的保存和加载&quot; class=&quot;headerlink&quot; title=&quot;模型的保存和加载&quot;&gt;&lt;/a&gt;模型的保存和加载&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Br
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型的保存和加载" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树和随机森林</title>
    <link href="http://yoursite.com/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://yoursite.com/2019/09/07/决策树和随机森林/</id>
    <published>2019-09-07T08:05:29.000Z</published>
    <updated>2019-09-08T08:39:53.867Z</updated>
    
    <content type="html"><![CDATA[<h2 id="决策树推导"><a href="#决策树推导" class="headerlink" title="决策树推导"></a>决策树推导</h2><p>首先看看下面这组数据集：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML96.png" alt></p><p>得出下面这颗决策树：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML97.png" alt></p><p><strong>关键概念</strong>：</p><p>​    信息熵公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML34.png" alt></p><p>​    信息增益公式：就是熵和特征条件熵的差</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML35.png" alt></p><p>​    随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好</p><p>决策树算法的需要解决的核心问题：</p><p>​    1、如何从数据表中找出最佳节点和最佳分支？</p><p>​    2、如何让决策树停止生长，防止过拟合？</p><p>决策树的基本过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML101.png" alt></p><p>直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。</p><h2 id="决策树五大模块"><a href="#决策树五大模块" class="headerlink" title="决策树五大模块"></a>决策树五大模块</h2><p>sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML98.png" alt></p><h2 id="分类树参数解读"><a href="#分类树参数解读" class="headerlink" title="分类树参数解读"></a>分类树参数解读</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML102.png" alt></p><h4 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h4><p>为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标<br>叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心<br>大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 </p><p>criterion这个参数正是用来决定不纯度的计算方法的：</p><p>​    1、输入”entropy“，使用信息熵</p><p>​    2、输入”gini“，使用基尼系数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML100.png" alt></p><p>其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵<br>时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。</p><h4 id="random-state-amp-splitter"><a href="#random-state-amp-splitter" class="headerlink" title="random_state&amp;splitter"></a>random_state&amp;splitter</h4><p>random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据<br>（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。<br>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会<br>优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在<br>分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这<br>也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能<br>性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 </p><h4 id="max-depth"><a href="#max-depth" class="headerlink" title="max_depth"></a>max_depth</h4><p>限制树的最大深度，超过设定深度的树枝全部剪掉<br>这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所<br>以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效<br>果再决定是否增加设定深度。</p><h4 id="min-samples-leaf-amp-min-samples-split"><a href="#min-samples-leaf-amp-min-samples-split" class="headerlink" title="min_samples_leaf&amp;min_samples_split"></a>min_samples_leaf&amp;min_samples_split</h4><p>min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。<br>min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则<br>分枝就不会发生。 </p><h4 id="max-features-amp-min-impurity-decrease"><a href="#max-features-amp-min-impurity-decrease" class="headerlink" title="max_features&amp;min_impurity_decrease"></a>max_features&amp;min_impurity_decrease</h4><p>max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。<br>min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的<br>功能，在0.19版本之前时使用min_impurity_split。 </p><h4 id="class-weight-amp-min-weight-fraction-leaf"><a href="#class-weight-amp-min-weight-fraction-leaf" class="headerlink" title="class_weight&amp;min_weight_fraction_leaf"></a>class_weight&amp;min_weight_fraction_leaf</h4><p>完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 </p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML37.png" alt></p><h2 id="决策树的本地保存：Graphviz"><a href="#决策树的本地保存：Graphviz" class="headerlink" title="决策树的本地保存：Graphviz"></a>决策树的本地保存：Graphviz</h2><p>windows版本下载地址：<a href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html" target="_blank" rel="noopener">https://graphviz.gitlab.io/_pages/Download/Download_windows.html</a></p><p>双击msi文件，一直next就完事了。</p><p>找到bin文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML103.png" alt></p><p>在下面这张图片的位置加入环境变量</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML106.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML105.png" alt></p><p>用dot -version检查是否安装成功</p><p>将dot文件转为png文件的命令：dot -Tpng *.dot -o *.png</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML107.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">titan=pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line">x=titan[[<span class="string">"pclass"</span>,<span class="string">"age"</span>,<span class="string">"sex"</span>]]</span><br><span class="line">y=titan[<span class="string">"survived"</span>]</span><br><span class="line">x[<span class="string">"age"</span>].fillna(x[<span class="string">"age"</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.25</span>)</span><br><span class="line">dict=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">x_train=dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">x_test=dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">dec=DecisionTreeClassifier()</span><br><span class="line">dec.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line"><span class="comment">#图形化</span></span><br><span class="line">export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">"age"</span>,<span class="string">"pclass=1st"</span>,<span class="string">"pclass=2nd"</span>,<span class="string">"pclass=3rd"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;决策树推导&quot;&gt;&lt;a href=&quot;#决策树推导&quot; class=&quot;headerlink&quot; title=&quot;决策树推导&quot;&gt;&lt;/a&gt;决策树推导&lt;/h2&gt;&lt;p&gt;首先看看下面这组数据集：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubuserconten
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树和随机森林" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="http://yoursite.com/2019/09/07/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://yoursite.com/2019/09/07/朴素贝叶斯/</id>
    <published>2019-09-07T08:05:14.000Z</published>
    <updated>2019-09-07T15:14:14.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类算法—朴素贝叶斯"><a href="#分类算法—朴素贝叶斯" class="headerlink" title="分类算法—朴素贝叶斯"></a>分类算法—朴素贝叶斯</h2><p><strong>关键概念</strong></p><p>​    联合概率：X取值为x和Y取值为y两个事件同时发生的概率，表示为P(X=x,Y=y)</p><p>​    条件概率：在X取值为x的前提下，Y取值为y的概率。表示为P(Y=y|X=x)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML39.png" alt></p><p>这里的C代表类别，W代表特征。</p><p>我们学习的其它分类算法总是有一个特点：这些算法先从训练集中学习，获取某种信息来建立模型，然后用模型去对测试集进行预测。比如逻辑回归，我们要先从训练集中获取让损失函数最小的参数，然后用参数建立模型，再对测试集进行预测。在比如支持向量机，我们要先从训练集中获取让边际最大的决策边界，然后 用决策边界对测试集进行预测。相同的流程在决策树，随机森林中也出现，我们在ﬁt的时候必然已经构造好了能够让对测试集进行判断的模型。而朴素贝叶斯，似乎没有这个过程。这说明，朴素贝叶斯是一个不建模的算法。以往我们学的不建模算法，比如KMeans，比如PCA，都是无监督学习，而朴素贝叶斯是第一个有监督的，不建模的分类算法。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类。</p><p>sklearn中，基于高斯分布、伯努利分布、多项式分布为我们提供了四个朴素贝叶斯的分类器</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML71.png" alt></p><p>sklearnnaive_bayes.GaussianNB(priors=None,var_smoothing=1e-09):高斯朴素贝叶斯，通过假设P(Xi|Y)是服从高斯分布(正态分布)，估计每个特征下每个类别上的条件概率。对于每个特征下的取值，有以下公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML72..png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML73.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">gnb = GaussianNB().fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#查看分数</span></span><br><span class="line">acc_score = gnb.score(Xtest,Ytest)</span><br><span class="line"><span class="comment">#查看预测结果</span></span><br><span class="line">Y_pred = gnb.predict(Xtest)</span><br><span class="line"><span class="comment">#查看预测的概率结果</span></span><br><span class="line">prob = gnb.predict_proba(Xtest)</span><br><span class="line">print(prob)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML33.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML32.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类算法—朴素贝叶斯&quot;&gt;&lt;a href=&quot;#分类算法—朴素贝叶斯&quot; class=&quot;headerlink&quot; title=&quot;分类算法—朴素贝叶斯&quot;&gt;&lt;/a&gt;分类算法—朴素贝叶斯&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;关键概念&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    联合概率
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="朴素贝叶斯" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>k-近邻算法</title>
    <link href="http://yoursite.com/2019/09/07/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/09/07/k-近邻算法/</id>
    <published>2019-09-07T08:03:32.000Z</published>
    <updated>2019-09-07T08:05:39.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类算法-k近邻算法-KNN-："><a href="#分类算法-k近邻算法-KNN-：" class="headerlink" title="分类算法-k近邻算法(KNN)："></a>分类算法-k近邻算法(KNN)：</h2><p>如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><p>如何求距离：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML29.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML30.png" alt></p><p>k值取很小，容易受异常点影响</p><p>k值取很大,容易受k值数量的波动</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML31.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类算法-k近邻算法-KNN-：&quot;&gt;&lt;a href=&quot;#分类算法-k近邻算法-KNN-：&quot; class=&quot;headerlink&quot; title=&quot;分类算法-k近邻算法(KNN)：&quot;&gt;&lt;/a&gt;分类算法-k近邻算法(KNN)：&lt;/h2&gt;&lt;p&gt;如果一个样本在特征空间中的k
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="k-近邻算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读</title>
    <link href="http://yoursite.com/2019/09/05/SVM%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2019/09/05/SVM解读/</id>
    <published>2019-09-05T14:16:50.000Z</published>
    <updated>2019-09-06T14:13:05.314Z</updated>
    
    <content type="html"><![CDATA[<p>​    支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。</p><p>支持向量机原理的三层理解：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM21.png" alt></p><p>支持向量机所做的事情其实非常容易理解，看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM1.png" alt></p><p>上图是一组两种标签的数据，两种标签分别用圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在位置数据集上的分类误差(泛化误差)尽量小。</p><p><strong>关键概念</strong>：</p><p>​    超平面：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。 </p><p>​    决策边界：在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就 说这个超平面是数据的“决策边界”。</p><p>决策边界一侧的所有点在分类为属于一个类，而另一个类的所有店分类属于另一个类。比如上面的数据集，我们很容易的在方块和圆的中间画一条线并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。 所以，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM2.png" alt></p><p>但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有B1和B2两条可能的决策边界。我们可以把决策边界B1向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到b11和b12的距离相等。<strong>在b11和b12中间的距离，叫做 这条决策边界的边际(margin)，通常记作d</strong>。对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM3.png" alt></p><p>接着我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误认成了圆，有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，<strong>拥有更大边际的决策边界在分类中的泛化误差更小</strong>，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM4.png" alt></p><p><strong>结论：支持向量机，就是通过找出边际最多的决策边界，来对数据进行分类的分类器。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM5.png" alt></p><p>我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决 策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM6.png" alt></p><p>我们将此表达式变换一下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM7.png" alt></p><p>其中[a, -1]就是我们的参数向量 ， 就是我们的特征向量， 是我们的截距。</p><p>我们要求解参数向量w和截距b 。如果在决策边界上任意取两个点Xa，Xb，并带入决策边界的表达式，则有：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM.png" alt></p><p>将两式相减，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM8.png" alt></p><p>Xa和Xb是一条直线上的两个点，相减后的得到的向量方向是由Xb指向Xa，所以Xa-Xb的方向是平行于他们所在的直线——我们的决策边界的。而w与Xa-Xb相互垂直，所以参数向量w方向必然是垂直于我们的决策边界。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM9.png" alt></p><p>此时，我们有了我们得决策边界，任意一个紫色得点Xp就可以表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM10.png" alt></p><p>由于紫色的点所代表的标签y是1，所以我们规定，p&gt;0。</p><p>同样的，对于任意一个红色的点Xr而言，我们可以将它表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM11.png" alt></p><p>由于红色点所表示的标签y是-1，所以我们规定，r&lt;0</p><p>由上面得知：如果我们有新的测试数据Xt，则Xt的标签就可以根据以下式子来判定：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM12.png" alt></p><p><strong>注意：p和r的符号是我们人为规定的</strong></p><p>两类数据中距离我们的决策边界 最近的点，这些点就被称为支持向量。</p><p>紫色类的点为Xp ，红色类的点为Xr，则我们可以得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM13.png" alt></p><p>两个式子相减，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM14.png" alt></p><p>如下图所示： (Xp-Xr)可表示为两点之间的连线，而我们的边际d是平行于w的，所以我们现在，相当于是得到 了三角型中的斜边，并且知道一条直角边的方向。在线性代数中，向量有这样的性质：向量a除以向量b的模长||b|| ，可以得到向量a在向量b的方向上的投影的长度。所以，我们另上述式子两边同时除以||w||，则可以得到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM16.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM15.png" alt></p><p>最大边界所对应的决策边界，那问题就简单了，要最大化d，就求解w的最小值。极值问题可以相互转化，我们可以把求解w的最小值转化为，求解以下函数的最小值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM19.png" alt></p><p>之所以要在模长上加上平方，是因为模长的本质是一个距离，所以它是一个带根号的存在，我们对它取平方，是为了消除根号。</p><p>我们得到我们SVM的损失函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM20.png" alt></p><p>至此，SVM的第一层理解就完成了。</p><p>用拉格朗日对偶函数求解线性SVM</p><p>当然，不是所有数据都是线性可分的，不是所有数据我们都能够一眼看出，有一条直线，或一个平面，甚至一个超平面可以将数据完全分开。比如下面的环形数据。对于这样的数据，我们需要对它进行一个升维变化，将数据从原始的空间x投射到新空间F(x)中。升维之后，我们明显可以找出一个平面，能够将数据切分开来。 F是一个映射函数，它代表了某种能够将数据升维的非线性的变换，我们对数据进行这样的变换，确保数据在自己的空间中一定能够线性可分。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM22.png" alt></p><p>这种变换非常巧妙，但也带有一些实现问题。 首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。而解决这些问题的是<strong>核函数</strong>。</p><p><strong>关键概念：</strong>核函数是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。&lt;/p&gt;
&lt;p&gt;支持向量机原理的三层理解：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.gith
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
      <category term="SVM解读" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/SVM%E8%A7%A3%E8%AF%BB/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>数据库概述</title>
    <link href="http://yoursite.com/2019/09/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    <id>http://yoursite.com/2019/09/04/数据库概述/</id>
    <published>2019-09-04T12:02:01.000Z</published>
    <updated>2019-09-04T14:30:34.203Z</updated>
    
    <content type="html"><![CDATA[<p>​    大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。</p><p>数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。</p><p>数据库数据具有永久存储、有组织、可共享三个基本特点。</p><p>数据库系统(DBS)由计算机硬件、数据库、数据库管理系统、应用软件和数据库管理系统组成。</p><p>数据库设计时面向数据模型对象、数据库系统的数据冗余度小、数据共享度高、数据库系统的数据和程序之间具有较高的独立性、数据库中数据的最小存取单位是数据项、数据库系统通过DBMS进行数据安全性、完整性、并发控制和数据恢复控制。</p><p>数据库数据物理独立性高是指当数据的物理结构（存储结构）发生变化时，应用程序不需要修改也可以正常工作</p><p>数据库数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行</p><p>DBMS的功能结构</p><p>​    数据定义功能：能够提供数据定义语言(DDL)和相应的建库机制。用户利用DDL可以方便建立数据库。</p><p>​    数据操纵功能：实现数据的插入、修改、删除、查询、统计等数据存取操作的功能称为数据操纵功能。数据库管理系统通过提供数据操纵语言(DML)实现其数据操纵功能。</p><p>​    运行管理功能：包括并发控制、数据的存取控制、数据完整性条件的检查和执行、数据库内部的维护等</p><p>​    建立和维护功能：指数据的载入、转储、重组织功能及数据库的恢复功能，指数据库结构的修改、变更及扩充功能。</p><p>从数据库管理系统的角度查看，数据库系统通常采用外模式、模式、内模式三级模式结构。从数据库最终用户的角度看，数据库系统的就够分为单用户结构、主从结构、分布式结构、客户/服务器结构。</p><p>外模式：也称为子模式或用户模式。它是数据库用户看见和使用的、对局部数据的逻辑结构和特征的描述，是与某一应用有关的数据的逻辑表示。</p><p>模式：模式是数据库中数据的逻辑结构和特征的描述。它仅仅涉及到<strong>型</strong>的描述，不涉及到具体的值。模式的具体值被称为模式的一个实例。同一模式可以有很多实例。模式是相对稳定的，实例是相对变动的。模式反映的是数据的结构及其关系，而实例反映的是数据某一时刻的状态。</p><p>内模式：也称存储模式，它是对数据的物理结构和存储结构的描述，是数据在数据库内部的表示方式。</p><p>为了能够在内部实现这三个抽象层次的联系和转换，数据库系统在这三级模式之间提供了两层映像：外模式/概念模式映像、概念模式/内模式映像。这两层映像保证了数据库系统中的数据能够具有较高的逻辑独立性和物理独立性。</p><p>外模式/模式映像保证了数据与程序间的逻辑独立性，模式/内模式映像保证了数据的物理独立性。</p><p>逻辑独立性：当数据库的整体逻辑结构发生变化时，通过调整外模式和模式之间的映像，使得外模式中的局部数据及其结构不变，程序不在修改。</p><p>物理独立性：当数据库的存储结构发生变化时，通过调整模式和内模式之间的映像，使得整体模式不变，当然外模式及应用程序不用改变。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。&lt;/p&gt;
&lt;p&gt;数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。&lt;/p&gt;
&lt;p&gt;数据库数据具有永久存储、有组织、可共享三个基本特点。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="数据库概述" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>激活函数以及作用</title>
    <link href="http://yoursite.com/2019/09/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/09/03/激活函数/</id>
    <published>2019-09-03T13:37:07.000Z</published>
    <updated>2019-09-03T14:01:05.002Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客介绍以下激活函数以及激活函数的作用。</p><p>首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL50.png" alt></p><p>激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。</p><p>问题一：为什么我们不能在不激活输入信号的情况下完成此操作呢？</p><p>如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。</p><p>问题二：那么为什么我们需要非线性函数？</p><p>非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。</p><p>接下来介绍一下常用的激活函数：sigmoid、tanh、Relu</p><p>sigmoid：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL45.png" alt></p><p>缺点：当 zz 值<strong>非常大</strong>或者<strong>非常小</strong>时，通过上图我们可以看到，sigmoid函数的导数 g′(z)将接近 00 。这会导致权重 W 的梯度将接近 00 ，使得梯度更新十分缓慢，即<strong>梯度消失</strong></p><p>tanh：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1) 之间，其公式与图形为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL42.png" alt></p><p>tanh函数的缺点同sigmoid函数的第一个缺点一样，当 zz <strong>很大或很小</strong>时，g′(z)接近于 0 ，会导致梯度很小，权重更新非常缓慢，即<strong>梯度消失问题</strong>。</p><p>Relu函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL41.png" alt></p><p>Leaky Relu：这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL43.png" alt></p><p>  其中a取值在（0，1）之间</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客介绍以下激活函数以及激活函数的作用。&lt;/p&gt;
&lt;p&gt;首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="激活函数" scheme="http://yoursite.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>SSD实战-tensorflow实现目标检测</title>
    <link href="http://yoursite.com/2019/09/03/SSD%E5%AE%9E%E6%88%98-tensorflow%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/09/03/SSD实战-tensorflow实现目标检测/</id>
    <published>2019-09-03T11:35:07.000Z</published>
    <updated>2019-09-03T14:05:37.345Z</updated>
    
    <content type="html"><![CDATA[<p>​    因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow&lt;/p&gt;

      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/"/>
    
      <category term="SSD实战" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="SSD实战" scheme="http://yoursite.com/tags/SSD%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>人脸检测数据集构造</title>
    <link href="http://yoursite.com/2019/08/28/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E9%80%A0/"/>
    <id>http://yoursite.com/2019/08/28/人脸检测数据集构造/</id>
    <published>2019-08-28T07:08:15.000Z</published>
    <updated>2019-09-03T11:33:26.161Z</updated>
    
    <content type="html"><![CDATA[<p>在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。</p><p>Caffe-SSD数据集构造流程：</p><p>1、生成VOC个数数据集（图片、XML标注信息文件）</p><p>2、修改Caffe-SSD数据打包脚本相关路径配置</p><p>3、运行Caffe-SSD数据打包脚本</p><p>VOC格式数据集的目录下有三个文件夹：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD6.png" alt></p><p>Annotation中保存的是xml格式的label信息</p><p>ImageSet中Main目录中存放不同照片列表文件</p><p>​    train.txt：训练图片文件名列表</p><p>​    val.txt：验证图片文件名列表</p><p>​    trianval.txt：训练和验证的图片文件名列表</p><p>​    test.txt：测试图片文件名列表</p><p>JPEGImages目录存放所有的图片集。Annotation和JPEGImages一一对应。</p><p>WIDERFace数据集：打开<a href="http://mmlab.ie.cuhk.edu.hk/project/WIDERFace" target="_blank" rel="noopener">人脸检测数据集地址</a>下载数据集。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD7.png" alt></p><p>下载数据的训练集、验证集、测试集和标注信息。</p><p>打开标注信息，可以看到文件夹中有很多txt文件，打开看一下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD9.png" alt>****</p><p>对于VOC格式的数据集，最主要的是生成两个文件。一个是图片的标注数据，另一个是图片数据。对于图片数据，我们只需要将wider_face的图片放到对应目录下，而对于标注数据，我们需要将上面图片的文件进行解析来重新生成针对每张图片的标注信息，这个信息用XML格式存储。</p><p>对于wider_face转为voc，首先创建Annotation、ImageSets、JPEGImages三个文件夹，其中ImageSets中再创建一个Main文件夹。</p><p>将wider_face转为voc的coding：</p><p><code>import os,cv2,sys,shutilfrom xml.dom.minidom import Documentdef writexml(filename,saveing,bboxes,xmlpath):  doc=Document()  annotation=doc.createElement(&quot;annotation&quot;)  doc.appendChild(annotation)  folder=doc.createElement(&quot;folder&quot;)  folder_name=doc.createTextNode(&quot;widerface&quot;)  folder.appendChild(folder_name)  annotation.appendChild(folder)  filenamenode=doc.createElement(&quot;filename&quot;)  filename_name=doc.createTextNode(filename)  filenamenode.appendChild(filename_name)  annotation.appendChild(filenamenode)  source=doc.createElement(&quot;source&quot;)  annotation.appendChild(source)  database=doc.createElement(&quot;database&quot;)  database.appendChild(doc.createTextNode(&quot;wider face database&quot;))  source.appendChild(database)  annotation_s=doc.createElement(&quot;annotation&quot;)  annotation_s.appendChild(doc.createTextNode(&quot;PASCAL VOC2007&quot;))  source.appendChild(annotation_s)  image=doc.createElement(&quot;image&quot;)  image.appendChild(doc.createTextNode(&quot;flickr&quot;))  source.appendChild(image)  flickrid=doc.createElement(&quot;flickrid&quot;)  source.appendChild(doc.createTextNode(&quot;-1&quot;))  source.appendChild(flickrid)  owner=doc.createElement(&quot;owner&quot;)  annotation.appendChild(owner)  flickrid_o=doc.createElement(&quot;flickrid&quot;)  flickrid_o.appendChild(doc.createTextNode(&quot;yanyu&quot;))  owner.appendChild(flickrid_o)  name_o=doc.createElement(&quot;name&quot;)  name_o.appendChild(doc.createTextNode(&quot;yanyu&quot;))  owner.appendChild(name_o)  size=doc.createElement(&quot;size&quot;)  annotation.appendChild(size)  width=doc.createElement(&quot;width&quot;)  width.appendChild(doc.createTextNode(str(saveing.shape[1])))  height=doc.createElement(&quot;height&quot;)  height.appendChild(doc.createTextNode(str(saveing.shape[0])))  depth=doc.createElement(&quot;depth&quot;)  depth.appendChild(doc.createTextNode(str(saveing.shape[2])))  size.appendChild(width)  size.appendChild(height)  size.appendChild(depth)  segmented=doc.createElement(&quot;segmented&quot;)  segmented.appendChild(doc.createTextNode(&quot;0&quot;))  annotation.appendChild(segmented)  for i in range(len(bboxes)):    bbox=bboxes[i]    objects=doc.createElement(&quot;object&quot;)    annotation.appendChild(objects)    object_name=doc.createElement(&quot;name&quot;)    object_name.appendChild(doc.createTextNode(&quot;face&quot;))    objects.appendChild(object_name)    pose=doc.createElement(&quot;pose&quot;)    pose.appendChild(doc.createTextNode(&quot;Unspecified&quot;))    objects.appendChild(pose)    truncated=doc.createElement(&quot;truncated&quot;)    truncated.appendChild(doc.createTextNode(&quot;1&quot;))    objects.appendChild(truncated)    difficult=doc.createElement(&quot;difficult&quot;)    difficult.appendChild(difficult)    difficult.appendChild(doc.createTextNode(&quot;0&quot;))    objects.appendChild(difficult)    bndbox=doc.createElement(&quot;bndbox&quot;)    objects.appendChild(bndbox)    xmin=doc.createElement(&quot;xmin&quot;)    xmin.appendChild(doc.createTextNode(str(bbox[0])))    bndbox.appendChild(xmin)    ymin=doc.createElement(&quot;ymin&quot;)    ymin.appendChild(doc.createTextNode(str(bbox[1])))    bndbox.appendChild(ymin)    xmax=doc.createElement(&quot;xmax&quot;)    xmax.appendChild(doc.createTextNode(str(bbox[0]+bbox[2])))    bndbox.appendChild(xmax)    ymax=doc.createElement(&quot;ymax&quot;)    ymax.appendChild(doc.createTextNode(str(bbox[1]+bbox[3])))    bndbox.appendChild(ymax)  f=open(xmlpath,&quot;w&quot;)  f.write(doc.toprettyxml(indent=&quot; &quot;))  f.close()rootdir=&quot;./wider_face&quot;def convertimgset(img_set):  imgdir=rootdir+&quot;/WIDER_&quot;+img_set+&quot;/images&quot;  gtfilepath=rootdir+&quot;/wider_face_split/wider_face_&quot;+img_set+&quot;_bbx_gt.txt&quot;  fwrite=open(rootdir+&quot;/ImageSets/Main&quot;+img_set+&quot;.txt&quot;,&quot;w&quot;)  index=0  with open(gtfilepath,&quot;r&quot;) as gtfiles:    while(index&lt;1000):      filename=gtfiles.readline()[:-1]      if (filename==&quot;&quot;):        continue      imgpath=imgdir+&quot;/&quot;+filename      img=cv2.imread(imgpath)      if not img.data:        break      numbbox=int(gtfiles.readline())      bboxes=[]      for i in range(numbbox):        line=gtfiles.readline()        lines=line.split()        lines=lines[0:4]        bbox=(int(lines[0]),int(lines[1]),int(lines[2]),int(lines[3]))        bboxes.append(bbox)      filename=filename.replace(&quot;/&quot;,&quot; &quot;)      if len(bboxes)==0:        print(&quot;no face&quot;)      cv2.imwrite(&quot;{}/JPEGImages/{}&quot;.format(rootdir,filename),img)      fwrite.write(filename.split(&quot;.&quot;)[0]+&quot;\n&quot;)      xmlpath=&quot;{}/Annotations/{}&quot;.format(rootdir,filename.split(&quot;.&quot;)[0])      writexml(filename,img,bboxes,xmlpath)      print(&quot;第%d张成功的图片&quot; % index)      index+=1    fwrite.close()if __name__==&quot;__main__&quot;:  img_sets=[&quot;train&quot;,&quot;val&quot;]  for img_set in img_sets:    convertimgset(img_set)  shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;train.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;trainval.txt&quot;)  shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;val.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;test.txt&quot;)</code></p><p>这篇博客废了。。。。caffe的环境太难搭了。。。。</p><p>我将框架换成tensorflow，进行另一个实战</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。&lt;/p&gt;
&lt;p&gt;C
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one—stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/SSD/"/>
    
      <category term="SSD实战" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/SSD/SSD%E5%AE%9E%E6%88%98/"/>
    
      <category term="构造数据集" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/SSD/SSD%E5%AE%9E%E6%88%98/%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SSD实战-人脸检测</title>
    <link href="http://yoursite.com/2019/08/28/SSD%E5%AE%9E%E6%88%98-%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/08/28/SSD实战-人脸检测/</id>
    <published>2019-08-28T01:51:26.000Z</published>
    <updated>2019-09-01T11:56:18.670Z</updated>
    
    <content type="html"><![CDATA[<p>人脸标注方法：矩形标注和椭圆形标注</p><p>矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、椭圆圆心x坐标、椭圆圆心y坐标。</p><p>判断算法性能好坏：</p><p>每一个标记只允许有一个检测与之相对应，也就是说，我们检测出来的每一个人脸图像只能同我们在标注中人脸图像中数据中的一个相对应。如果有多个，就会视为重复检测。重复检测会被视为错误检测。接着我们就可以得出正确率和错误率，可以画出ROC曲线和PR曲线。可以根据曲线看出算法的好坏。</p><p>人脸采集常用方法：</p><p><strong>活体检测</strong>  判断用户是否为正常操作，通过指定用户做随机动作，一搬有张嘴、摇头、点头、凝视、眨眼等等，防止照片攻击。判断用户是否真实在操作，指定用户上下移动手机，防止视频攻击和非正常动作的攻击。</p><p><strong>3D检测</strong>  验证采集到的是否为立体人像，能够防止平面照片、不同弯曲程度的照片等。</p><p><strong>连续检测</strong>  通过连续的检测，验证运动轨迹是否正常，防止跳过活体检测直接替换采集的照片，也能防止中途换人。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;人脸标注方法：矩形标注和椭圆形标注&lt;/p&gt;
&lt;p&gt;矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/"/>
    
      <category term="SSD实战" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%AE%9E%E6%88%98/"/>
    
      <category term="人脸检测综述" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%AE%9E%E6%88%98/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用名词</title>
    <link href="http://yoursite.com/2019/08/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D/"/>
    <id>http://yoursite.com/2019/08/22/目标检测常用名词/</id>
    <published>2019-08-22T01:49:17.000Z</published>
    <updated>2019-09-03T12:17:18.677Z</updated>
    
    <content type="html"><![CDATA[<p>图像分类：一张图像中是否包含某种物体</p><p>物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。</p><p>语义分割：按对象得内容进行图像得分割，分割的依据是内容，即对象类别。</p><p>实例分割：按对象个体进行分割，分割的依据是单个目标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8717.png" alt></p><p><strong>滑动窗口</strong>——为什么要有候选区域？既然目标是在图像中的某一区域，那么最直接的方法就是滑窗法（sliding window approach），就是遍历图像的所有区域，用不同大小的窗口在整个图像上滑动，那么就会产生所有的矩形区域，然后再后续排查，思路很简单，但是开销巨大。</p><p>region proposal（RP）：候选区域</p><p>IOU：region proposal与Ground Truth的窗口的交集比并集的比值，相当于准确率。‘</p><p>SPP：Spatial Pyramid Pooling  空间金字塔采样  在pooing的过程中计算pooling后的结果对应的两个像素点映射到feature map上所占的范围，然后在那个范围中进行max或者average。</p><p>ROI Pooling：就是将一个个大小不同的box矩形框，都映射到大小为w*h的矩形框。</p><p>Anchor：请看<a href="[https://brickexperts.github.io/2019/08/18/Two-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/#more](https://brickexperts.github.io/2019/08/18/Two-stage基本介绍/#more)">Two-stage基本介绍</a></p><p>GT box:Ground Truth box</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8722.png" alt></p><p>如上图所示，绿色的框为飞机的Ground Truth，红色的框是提取的Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU&lt;0.5)，那么这张图相当于没有正确的检测出飞机。如果我们能对红色的框进行微调，使得经过微调后的窗口跟Ground Truth更接近，这样岂不是定位会更准确。</p><p>带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。</p><p>如下图所示，(a)是普通的3×33×3卷积，其视野就是3×33×3，(b)是扩张率为2，此时视野变成7×77×7，(c)扩张率为4时，视野扩大为15×1515×15，但是视野的特征更稀疏了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD10.png" alt></p><p>后面遇见会继续完善。。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;图像分类：一张图像中是否包含某种物体&lt;/p&gt;
&lt;p&gt;物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。&lt;/
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="常用名词解释" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SSD系列算法</title>
    <link href="http://yoursite.com/2019/08/21/SSD%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/08/21/SSD系列算法/</id>
    <published>2019-08-21T01:05:48.000Z</published>
    <updated>2019-09-04T11:54:49.116Z</updated>
    
    <content type="html"><![CDATA[<h2 id="主干网络介绍"><a href="#主干网络介绍" class="headerlink" title="主干网络介绍"></a>主干网络介绍</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8720.png" alt></p><p>主干网络原始作者采用的VGG16，我们也可以将其他神经网络作为主干网络。例如：ResNet、MobileNets等</p><p>输入300<em>300的image，将VGG16网络FC6、FC7换成conv6和conv7，同时将池化层变为stride=1，pool_size=3\</em>3，这样做的目的是为了不减少feature map  size，为了配合这种变化，conv6会使用扩张率为6的带孔卷积。</p><p>​    <strong>带孔卷积</strong>：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。具体看<a href="[https://brickexperts.github.io/2019/08/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D/#more](https://brickexperts.github.io/2019/08/22/目标检测常用名词/#more)">目标检测常用名词</a></p><p>接着然后移除dropout层和fc8层，并新增conv7，conv8，conv9，conv10，conv11，在检测数据集上做finetuing。其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是38×38，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet），以保证和后面的检测层差异不是很大，这个和Batch Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma。</p><h2 id="多尺度feature-map预测"><a href="#多尺度feature-map预测" class="headerlink" title="多尺度feature map预测"></a>多尺度feature map预测</h2><p>多尺度feature map预测，也就是在预测的时候，在接下来预测的时候，针对接下来六个不同的尺寸进行预测。如下图的六条连线，分别是38*38、19*19、10*10、5*5、3*3、1*1。将这六个不同尺度的feature map分别作为检测、预测层的输入，最后通过NMS进行筛选和合并。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8720.png" alt></p><p>对于六种不同尺度的网络，我们通常使用pooling来降采样。对于每一层的feature map，我们输入到相应的预测网络中。而预测网络中，我们会包括Prior box的提取过程。Prior box对应Fast R-CNN中Anchor的概念，也就是说，在Prior box中，feature map上的每一个点都作为一个cell（相当于Anchor）。以这个cell为中心，按照等比的放缩，找到它在原始图片的位置。接着以这个点为中心，提取不同尺度bounding box。而这些不同尺度的bounding box就是Prior box。然后对于每一个Prior box，我们通过和真值比较，就能够拿到它的label。对于每一个Prior box，我们都会分别预测它的类别概率和坐标（x，y，w，h）。也就是说，对于每一个cell，我们会将它对应到不同的Prior box，分别来预测当前这个Prior box所对应当前这个类别的概率分布和坐标。</p><p><strong>对Prior Box的具体定义：</strong></p><p>这里我们假设Prior Box的输入是m*n维的feature map。</p><p>如果每一个点都作为cell，那就会有m<em>n个cell。接着每个cell上生成固定尺寸和不同长宽比例的box。每个cell对应k个bounging box，每个bounding box预测c个类别分数和4个偏移坐标。其中c个类别分数实际上是当前bounding box所对应的不同类别的概率分布。如果输入大小为m\</em>n，那就会输出(c+4)*k*m*n。其中尺寸(scale)和比例(ratio)是超参数。</p><p>接下来我们看看Prior box是怎么生成的：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8721.png" alt></p><p>每个feature map上的点定义了六种长宽比的default box。也就是说，最后对于每一个anchor都会获得六个不同尺寸和长宽比的default box。对于38<em>38层，每个feature map上的点，我们都会提取4个default box作为prior box。对于19\</em>19层、10*10层、5*5，提取6个default box也就是全部都是prior box。而3*3、1*1提取4个default作为prior box。所以最后得到8732个prior box(38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4)。<strong>prior box就是选择的default box</strong>。尺寸和比例都是可以通过SSD的配置文件进行配置，后面实战详解。</p><p>对default box进行筛选成为prior box：每一个feature map cell不是k个default box都取，prior box与GT box(<a href="https://brickexperts.github.io/" target="_blank" rel="noopener">Ground Truth box</a>)做匹配，IOU&gt;阈值为正样本。IOU&lt;阈值 为负样本。介于正样本和负样本中间阈值的default box去掉。</p><h2 id="SSD系列算法优化及扩展"><a href="#SSD系列算法优化及扩展" class="headerlink" title="SSD系列算法优化及扩展"></a>SSD系列算法优化及扩展</h2><p>SSD算法对小目标不够鲁棒，原因最主要是浅层feature map的表征能力不够强。</p><p>DSSD：</p><p>DSSD相当原来的SSD模型主要作了两大更新。一是替换掉VGG，而改用了Resnet-101作为特征提取网络并在对不同尺度feature maps特征进行default boxes检测时使用了更新的检测单元；二则在网络的后端使用了多个deconvolution layers以有效地扩展低维度信息的contextual information，从而有效地提高了小尺度目标的检测。</p><p>下图为DSSD模型与SSD模型的整体网络结构对比：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD4.png" alt></p><p>DSOD：</p><p>SSD+DenseNet=DSOD</p><p>DSOD可以从0开始训练数据，不需要预训练模型。</p><p>FSSD：</p><p>借鉴了FPN的思想，重构了一组pyramid feature map（金字塔特征），使得算法的精度有了明显特征，速度也没有下降很多。具体是把网络中某些feature调整为同一size再contact（连接），得到一个像素层，以此层为base layer来生成pyramid feature map，作者称之为Feature Fusion Module。</p><p>Feature Fusion</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD2.png" alt></p><p>对上面图的解读：</p><p>​    (a) image pyramid </p><p>​    (b) rcnn系列，只在最后一层feature预测 </p><p>​    (c) FPN，语义信息一层传递回去，而且有很多相加的计算 </p><p>​    (d) SSD，在各个level的feature上直接预测，每个level之间没联系 </p><p>​    (e) FSSD的做法，把各个level的feature concat，然后从fusion feature上生成feature pyramid</p><p>FSSD网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD3.png" alt></p><p>RSSD：</p><p>rainbow concatenation方式(pooling加deconvolution)融合不同层的特征，再增加不同层之间feature map关系的同时也增加了不同层的feature map个数。这种融合方式不仅解决了传统SSD算法存在的重复框问题，同时一定程度上解决了small object的检测问题。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD1.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;主干网络介绍&quot;&gt;&lt;a href=&quot;#主干网络介绍&quot; class=&quot;headerlink&quot; title=&quot;主干网络介绍&quot;&gt;&lt;/a&gt;主干网络介绍&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Brickexper
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/"/>
    
      <category term="SSD原理" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>One-stage基本介绍</title>
    <link href="http://yoursite.com/2019/08/20/One-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/08/20/One-stage基本介绍/</id>
    <published>2019-08-20T01:44:40.000Z</published>
    <updated>2019-08-20T09:25:33.086Z</updated>
    
    <content type="html"><![CDATA[<p>​    One-stage也是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。</p><p>One-stage常见算法：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8715.png" alt></p><p>One-stage核心组件：</p><p>CNN网络</p><p>CNN网络设计原则：从简到繁到简的卷积神经网    多尺度特征融合的网络    更轻量级的CNN网络</p><p>回归网络    </p><p><strong>One-stage和Two-stage的区别在于是否存在RPN网络。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    One-stage也是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。&lt;/p&gt;
&lt;p&gt;One-stage常见算法：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.gi
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="One-stage基本介绍" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/One-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="One-stage" scheme="http://yoursite.com/tags/One-stage/"/>
    
  </entry>
  
  <entry>
    <title>Two-stage基本介绍</title>
    <link href="http://yoursite.com/2019/08/18/Two-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/08/18/Two-stage基本介绍/</id>
    <published>2019-08-18T09:09:22.000Z</published>
    <updated>2019-08-22T02:23:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>​    Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。</p><p>Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8714.png" alt></p><p>Two-stage核心组件：</p><p><strong>CNN网络</strong></p><p> CNN网络设计原则：</p><p>从简到繁再到简的卷积神经网</p><p>多尺度特征融合的网络</p><p>更轻量级的CNN网络</p><p><strong>RPN网络</strong></p><p>区域推荐（Anchor机制）：首先我们需要知道anchor的本质是什么，本质是SPP(spatial pyramid pooling)思想的逆向。而SPP本身是做什么的呢，就是将不同尺寸的输入resize成为相同尺寸的输出。所以SPP的逆向就是，将相同尺寸的输出，倒推得到不同尺寸的输入。当前的feature map的大小是n*c*w*h（n是张数，c是层数，w是宽，h是高），对于当前这个feature map上，也就是w<em>h上，选择其中的每一个点作为锚点（也就是候选区域的中心点），我们以每一个点作为中心点去提取候选区域。这样的每一个点都是Anchor。而这个候选区域通常都有比例：对于Fast  R-CNN三个面积尺寸（128^2，256^2，512^2），然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）。一个feature map可以提取w\</em>h*9个候选区域。我们示意图如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8716.png" alt></p><p>ROI Pooling</p><p>分类和回归</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。&lt;/p&gt;
&lt;p&gt;Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Two-stage算法" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/Two-stage%E7%AE%97%E6%B3%95/"/>
    
      <category term="Two-stage基本介绍" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/Two-stage%E7%AE%97%E6%B3%95/Two-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="Two-stage" scheme="http://yoursite.com/tags/Two-stage/"/>
    
  </entry>
  
</feed>
