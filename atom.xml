<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-24T10:28:18.353Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python实现浅层神经网络</title>
    <link href="http://yoursite.com/2019/09/24/Python%E5%AE%9E%E7%8E%B0%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/24/Python实现浅层神经网络/</id>
    <published>2019-09-24T03:06:10.000Z</published>
    <updated>2019-09-24T10:28:18.353Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开始前的准备"><a href="#开始前的准备" class="headerlink" title="开始前的准备"></a>开始前的准备</h2><p>本文用到的库：numpy、sklearn、matplotlib</p><p>另外，我们还要借助一下吴恩达老师的工具函数testCases.py 和 planar_utils.py。<a href="[https://github.com/Brickexperts/superficial-net/tree/master/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C](https://github.com/Brickexperts/superficial-net/tree/master/浅层神经网络)">地址</a></p><p>​    testCases：提供测试样本来评估我们的模型。</p><p>​    planar_utils：提供各种有用的函数。</p><p>这两个文件的函数就不具体阐述了，有兴趣的自己研究去吧。</p><h2 id="导入必要的库"><a href="#导入必要的库" class="headerlink" title="导入必要的库"></a>导入必要的库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br></pre></td></tr></table></figure><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#x是训练集，y是测试集</span></span><br><span class="line">X,<span class="attribute">Y</span>=load_planar_dataset()</span><br></pre></td></tr></table></figure><p>数据集的说明：</p><ul><li>X是维度为(2, 400)的训练集，两个维度分别代表平面的两个坐标。</li><li>Y是为(1, 400)的测试集，每个元素的值是0（代表红色），或者1（代表蓝色）</li></ul><p>利用 matplotlib 进行数据的可视化操作，这是一个由红点和蓝色的点构成的类似花状的图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924125032.png" alt></p><h2 id="构建神经网络模型"><a href="#构建神经网络模型" class="headerlink" title="构建神经网络模型"></a>构建神经网络模型</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130208.png" alt></p><p>对于某个样本x^[i]：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130601.png" alt></p><p>成本函数J：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130811.png" alt></p><p>接下来我们要用代码实现神经网络的结构，步骤大致如下： 1. 定义神经网络结构 2. 随机初始化参数 3. 不断迭代: - 正向传播 - 计算成本函数值 - 利用反向传播得到梯度值 - 更新参数（梯度下降）</p><h3 id="定义神经网络的结构"><a href="#定义神经网络的结构" class="headerlink" title="定义神经网络的结构"></a>定义神经网络的结构</h3><p>主要任务是描述网络各个层的节点个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]  <span class="comment"># 输入层单元数量</span></span><br><span class="line">    n_h = <span class="number">4</span>  <span class="comment"># 隐藏层单元数量，在这个模型中，我们设置成4即可</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]  <span class="comment"># 输出层单元数量</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="随机初始化参数"><a href="#随机初始化参数" class="headerlink" title="随机初始化参数"></a>随机初始化参数</h3><p>参数W的随机初始化是很重要的，如果W和b都初始化成0，那么这一层的所有单元的输出值都一样，导致反向传播时，所有的单元的参数都做出完全相同的调整，这样多个单元的效果和只有一个单元的效果是相同的。那么多层神经网络就没有任何意义。为了避免这样的情况发生，我们会把参数W初始化成非零。</p><p>另外，随机初始化W之后，我们还会乘上一个较小的常数，比如 0.01 ，这样是为了保证输出值 Z 数值都不会太大。为什么这么做？设想我们用的激活函数是 sigmoid 或者 tanh ，那么，太大的 Z 会导致最终 A 会落在函数图像中比较平坦的区域内，这样的地方梯度都接近于0，会降低梯度下降的速度，因此我们会将权重初始化为较小的随机值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">  <span class="comment">#随机初始化</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="循环迭代"><a href="#循环迭代" class="headerlink" title="循环迭代"></a>循环迭代</h3><h4 id="实现正向传播"><a href="#实现正向传播" class="headerlink" title="实现正向传播"></a>实现正向传播</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># 从parameters中取出参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment"># 执行正向传播操作</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">#如果不相等，直接报错</span></span><br><span class="line">    <span class="keyword">assert</span> (A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h4 id="计算成本函数J"><a href="#计算成本函数J" class="headerlink" title="计算成本函数J"></a>计算成本函数J</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130811.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    <span class="comment"># 下一行的结果是 (1, m)的矩阵</span></span><br><span class="line">    logprobs = np.multiply(Y, np.log(A2)) + np.multiply(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A2))</span><br><span class="line">    <span class="comment"># 将 (1, m)的矩阵求和后取平均值</span></span><br><span class="line">    cost = - <span class="number">1</span> / m * np.sum(logprobs)</span><br><span class="line">    <span class="comment"># np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">    <span class="comment"># 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23</span></span><br><span class="line">    <span class="comment"># 目的是确保 cost 是一个浮点数</span></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>利用正向传播函数返回的cache，我们可以实现反向传播了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924133512.png" alt></p><blockquote><p>说明：</p><p>我们使用的 g^[1] () 是 tanh ，并且 A1 = g^[1] ( Z^[1] ) ,那么求导变形后可以得到 g‘ ^[1] ( Z^[1] ) = 1-( A^[1] )^2 用python表示为： 1 - np.power(A1, 2)</p><p>我们使用的 g^[2] () 是 sigmoid ，并且 A2 = g^[2] ( Z^[2] ) ,那么求导变形后可以得到 g‘ ^[2] ( Z^[2] ) =A^2 - Y</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 获取样本的数量</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 从 parameters 和 cache 中取得参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment"># 计算梯度 dW1, db1, dW2, db2.</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span> / m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), <span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><blockquote><p>说明</p><p>1、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。</p><p>2、 keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。</p></blockquote><h4 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h4><p>利用上面反向传播的代码段获得的梯度去更新 (W1, b1, W2, b2)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924134345.png" alt></p><p>式子中 α 是学习率，较小的学习率的值会降低梯度下降的速度，但是可以保证成本函数 J 可以在最小值附近收敛，而较大的学习率让梯度下降速度较快，但是可能会因为下降的步子太大而越过最低点，最终无法在最低点附近出收敛。</p><p>本篇博客选择1.2作为学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    W1 -= learning_rate * dW1</span><br><span class="line">    b1 -= learning_rate * db1</span><br><span class="line">    W2 -= learning_rate * dW2</span><br><span class="line">    b2 -= learning_rate * db2</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="构建完整的神经网络模型"><a href="#构建完整的神经网络模型" class="headerlink" title="构建完整的神经网络模型"></a>构建完整的神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="comment"># 输入层单元数</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>] </span><br><span class="line">    <span class="comment"># 输出层单元数</span></span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 10000次梯度下降的迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播，得到Z1、A1、Z2、A2</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本函数的值</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 反向传播，得到各个参数的梯度值</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate = <span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每迭代1000下就输出一次cost值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回最终训练好的参数</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="对样本进行预测"><a href="#对样本进行预测" class="headerlink" title="对样本进行预测"></a>对样本进行预测</h4><p>我们约定 预测值大于0.5的之后取1，否则取0：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924135028.png" alt></p><p>对一个矩阵M而言，如果你希望它的每个元素 如果大于某个阈值 threshold 就用1表示，小于这个阈值就用 0 表示，那么，在python中可以这么实现：M_new = (M &gt; threshold)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h2 id="利用模型预测"><a href="#利用模型预测" class="headerlink" title="利用模型预测"></a>利用模型预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用含有4个神经元的单隐藏层的神经网络构建分类模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h=<span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 可视化分类结果</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line">print(<span class="string">'准确率: %d'</span> % float((np.dot(Y, predictions.T) + np.dot(<span class="number">1</span> - Y, <span class="number">1</span> - predictions.T)) / float(Y.size) * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h3 id="尝试换一下隐藏层的单元个数"><a href="#尝试换一下隐藏层的单元个数" class="headerlink" title="尝试换一下隐藏层的单元个数"></a>尝试换一下隐藏层的单元个数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><h3 id="换数据集"><a href="#换数据集" class="headerlink" title="换数据集"></a>换数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"><span class="comment"># 在这里选择你要选用的数据集</span></span><br><span class="line">dataset = <span class="string">"noisy_circles"</span></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 测试代码如下：</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 画出边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;开始前的准备&quot;&gt;&lt;a href=&quot;#开始前的准备&quot; class=&quot;headerlink&quot; title=&quot;开始前的准备&quot;&gt;&lt;/a&gt;开始前的准备&lt;/h2&gt;&lt;p&gt;本文用到的库：numpy、sklearn、matplotlib&lt;/p&gt;
&lt;p&gt;另外，我们还要借助一下吴恩达老
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅层神经网络的反向传播</title>
    <link href="http://yoursite.com/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/09/23/浅层神经网络的反向传播/</id>
    <published>2019-09-23T08:11:03.000Z</published>
    <updated>2019-09-25T13:47:05.727Z</updated>
    
    <content type="html"><![CDATA[<h2 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h2><p>首先看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923161512.png" alt></p><p>我们假设n[l] 表示第 l 层的神经元的数量，那么这个单隐层的神经网络的输入层、隐含层和输出层的维度分别是：n[0]= n_x =3、n[1]=4、n[2]=1。</p><p>那么根据<a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的正向传播/#more)">浅层神经网络的正向传播</a>的分析，如果一次性输入带有 m 个样本的矩阵 X 我们可以得到：</p><blockquote><p>W^[1] = (4, 3), Z^[1] = (4, m), A^[1] = (4, m)</p><p>W^[2] = (1, 4), Z^[2] = (1, m), A^[2] = (1, m)</p></blockquote><p>可以总结出如下规律，对第 l 层的单元而言：</p><blockquote><p>W^[l] = (n^[l], n^[l-1]),</p><p>Z^[l] = (n^[l], m),</p><p>A^[1] = (n^[l], m).</p></blockquote><p>另外，还有一个规律：</p><blockquote><p>对任意层而言：</p><p>dW 的维度和 W 的维度相同</p><p>dZ 的维度和 Z 的维度相同</p><p>dA 的维度和 A 的维度相同</p></blockquote><p>注意：以上出现的符号都是将m个样本向量化后的表达。</p><h2 id="单个样本的梯度计算"><a href="#单个样本的梯度计算" class="headerlink" title="单个样本的梯度计算"></a>单个样本的梯度计算</h2><p>讲反向传播前，我们先回顾一下单个样本的正向传播的过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923162211.png" alt></p><p>其中计算z^[1]时用到的 x 可以看做是 a^[0]，σ( ) 是激活函数中一种，一般情况下，我们更习惯用用 g( ) 来表示激活函数。于是将上述式子一般化可以得到第 l 层的公式为：</p><blockquote><p>z^[l] = W^[l]a^[l-1] + b^[l] ——— ①</p><p>a^[l] = g^[l] ( z^[l] ) ——————- ②</p></blockquote><p>由于我们输入的样本只有一个，所以各个向量的维度如下：</p><blockquote><p>W^[l] = (n^[l], n^[l-1]),</p><p>z^[l] = (n^[l], 1),</p><p>a^[1] = (n^[l], 1).</p></blockquote><p>我们现在根据式子 ① 和 ② 来讨论反向传播的过程。</p><p>因为正向传播的时候我们依次计算了z^[1], a^[1], z^[2], a^[2]最终得到了损失函数L。所以反向传播的时候，我们要从L向前计算梯度。</p><p>第一步计算dz^[2]和da^[2]进而算出 dW^[2] 和 db^[2]：</p><blockquote><p>da^[2]=dL/da^[2]=  -y/a^[2]+(1-y)/(1-a^[2])</p><p>dz^[2]= dL/dz^[2] = dL/da^[2]*da^[2]/dz^[2]=-y(1-a^[2])+(1-y)a^[2] = a^[2] - y</p></blockquote><blockquote><p>说明1：</p><p>dz^[2] 的维度和z^[2]相同，da^[2] 的维度和a^[2]相同，为(1, 1)</p><p>g’( z^[2] ) 的维度与 z^[2]维度相同，为(1, 1)</p><p>在第二层中，a^[2] 与 z^[2]的维度也相同，为(1, 1)</p><p>实际上，dz^[2] 应该等于da^[2] 与 g’( z^[2] ) 的内积的结果，理由我们我们先向下看</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203151.png" alt></p><blockquote><p>说明2 ：</p><p>上图中我们可以得到 dW^[2] = dz^[2]乘上 a^[1] 的转置，这里是因为 :</p><p>dW^[2] 和 W^[2] 的形状是一样的(n^[2], n^[1])也就是(1, 4)，</p><p>dz^[2] 和 z^[2] 的形状是一样的(n^[2], 1)也就是(1, 1)，</p><p>a^[1] 的形状是 (n^[1], 1) 也就是(4, 1)，可以得到 a^[1] 的转置 a^( [1]T ) 的形状是 (1, 4)，</p><p>这样 dz^[2] 和 a^[1] 的转置 的乘积的形状才能是 dW^[2] 的形状 (1, 4)；</p><p>因此 dW^[2] = dz^[2]乘上 a^[1] 的转置。</p><p>这里给我们的启发是：<strong>向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行转置操作</strong>。</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203936.png" alt></p><p>第二步计算 da^[1] 和 dz^[1]进而算出 dW^[1] 和 db^[1]</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203959.png" alt></p><blockquote><p>检查矩阵形状是否匹配：（4，1）* （1，1），匹配</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204207.png" alt></p><blockquote><p>说明3：</p><p>dz^[1] 的维度和z^[1]相同，da^[1] 的维度和a^[1]相同，均为(4, 1)</p><p>W^[2]T 的维度是 (4, 1)，dz^[2] 的维度是 (1, 1) ，二者的乘积与da^[1] 维度相同。</p><p>g’( z^[1] ) 的维度与 z^[1]维度相同，也与da^[1] 维度相同，为(4, 1)。</p><p>所以想得到维度为 (4, 1) 的dz^[1] ，da^[1] 与 g’( z^[1] ) 直接的关系为<strong>内积</strong></p><p>这里给我们的启发是：<strong>向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行内积操作</strong>。这里就解释了说明（1）留下的问题。</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204516.png" alt></p><blockquote><p>检查维度：（4，3）=（4，1）*（1，3），正确</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204639.png" alt></p><h2 id="多个样本的梯度运算"><a href="#多个样本的梯度运算" class="headerlink" title="多个样本的梯度运算"></a>多个样本的梯度运算</h2><p>用m个样本作为输入，并进行<strong>向量化</strong>后正向传播的公式：</p><blockquote><p>Z^[1] = W^[1]X + b^[1]</p><p>A^[1] = g^[1] ( Z^[1] )</p><p>Z^[2] = W^[2]A^[1] + b^[2]</p><p>A^[2] = g^[2] ( Z^[2] )</p></blockquote><p>由于引入了m个样本，我们也要有个成本函数来衡量整体的表现情况，我们的目的是让J达到最小值</p><p>下图中左边列举了我们上面所推导的各种式子，其中图中所用的激活函数 g^[2] () 为sigmoid函数，因而dz^[2] = a^[2] - y。这些左边的式子都是针对单个样本而言的，而右边则是将m个样本作为输入并向量化后的公式的表达形式。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923210356.png" alt></p><blockquote><p>上图的说明：</p><p>1、由于成本函数 J 是所有的 损失函数 L 的值求和后的<strong>平均值</strong>， <strong>那么 J 对 W^[1], b^[1], W^[2], b^[2] 的导数 等价于 各个 L 对 W^[1], b^[1], W^[2], b^[2] 的导数求和后的平均值</strong>。所以dW^[1], db^[1], dW^[2], db^[2]的式子中需要乘上 1 / m。</p><p>2、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。</p><p>3、keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。</p></blockquote><h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><p>最后，不要忘了要更新参数（式子中的&alpha;是学习率）</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923210037.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;符号约定&quot;&gt;&lt;a href=&quot;#符号约定&quot; class=&quot;headerlink&quot; title=&quot;符号约定&quot;&gt;&lt;/a&gt;符号约定&lt;/h2&gt;&lt;p&gt;首先看下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bric
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅层神经网络的正向传播</title>
    <link href="http://yoursite.com/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/09/23/浅层神经网络的正向传播/</id>
    <published>2019-09-23T07:02:03.000Z</published>
    <updated>2019-09-25T10:27:28.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络概览"><a href="#神经网络概览" class="headerlink" title="神经网络概览"></a>神经网络概览</h2><p>在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150442.png" alt></p><p>如果把上面的图片抽象化，可以得到以下模型：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150528.png" alt></p><p>再进一步简化得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150605.png" alt></p><p>上图里，小圆圈是sigmoid单元，它要完成的工作就是先计算 z 然后计算 a 最终作为 y帽 输出。</p><p>进而，我们看看下图，这是较为简单的神经网络的样子，它无非就是将多个 sigmoid 单元组合在一起，形成了更复杂的结构。图中每个单元都需要接收数据的输入，并完成数据的输出，其每个单元的计算过程与 logistic回归 的正向传播类似。可以看到，图片里给神经网络分了层次，最左边的是<strong>输入层</strong>，也就是第0层；中间的是<strong>隐藏层</strong>，也就是第一层；最右边的是<strong>输出层</strong>，是第二层。通常，我们不将输入层看做神经网络的一层，因而下图是一个2层的神经网络。 另外要清楚的是，本图中隐藏层只有一个，但实际上，隐藏层可以有多个。由于对用户而言，隐藏层计算得到的数据用户不可预见，也没有太大必要知道，所以称之为隐藏层。也正是因为如此，神经网络的解释器很差。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151156.png" alt></p><p>再进一步认识下这张图片里的标记，隐藏层的每个单元都需要接收输入层的数据，并且各个单元都需要计算 z , 并经过 sigmoid 函数得到各自的 a ，为了便于区分不同层的不同单元的 a，我们做如下约定：</p><p>a 的右上角有个角标[i]，表示这是第i层的单元；a 的右下角有个角标 j 用于区分这是该层自上向下的第 j 个单元。例如我们用 a^[1]_3 表示这是第一层的第三个单元。输入层的 x1, x2, … xn 可以看做是 a^[0]_1, a^[0]_2 … a^[0]_n.</p><p>前一层a[ i ] 的输出，便是后一层 a[ i+1 ] 的<strong>所有单元</strong>输入。除了输入层外的其它层，也就是隐藏层和输出层的单元都有各自的参数 w 和 b 用于计算 z ，同样是用w^[i]_j, b^[i]_j, z^[i]_j 来区分他们；得到 z^[i]_j 后用sigmoid函数计算 a^[i]_j ，再将本层n个单元计算得到的n个 a 作为下一层的输入，最终在输出层得到预测值 y帽。其计算过程大致如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151444.png" alt></p><h2 id="计算一个样本的神经网络输出"><a href="#计算一个样本的神经网络输出" class="headerlink" title="计算一个样本的神经网络输出"></a>计算一个样本的神经网络输出</h2><p>下图是输入单个样本(该样本含有特征x1, x2, x3)的神经网络图，隐藏层有4个单元：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151606.png" alt></p><p>根据上面的说明，要计算该神经网络的输出，我们首先要计算隐藏层4个单元的输出</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151804.png" alt></p><p>第一步就是计算第一层各个单元的 z^[1]_j ，第二步是计算出各个单元的 a^[i]_j，不难想到可以用<strong>向量化计算</strong>来化简上述操作，我们将所有 w^[1]_j 的转置纵向叠加得到下图的内容，我们将这个大的矩阵记为W^[1]，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151938.png" alt></p><p>由于输入的 x 是三维的列向量，所以每个分量x1, x2, x3 都需要一个 w1, w2, w3 对应，因此 w^[1]_j 的转置是一个(1, 3) 的矩阵，又因为隐藏层有4个单元，即 j 的取值为1, 2, 3, 4，故 <strong>W^[1] 是 (4, 3) 的矩阵</strong>。</p><p>同理，第二层，也就是输出层的 W^[2] 由于有4个输入的特征，1个单元，所以 <strong>W^[2] 是 (1, 4)的矩阵</strong>。</p><p>对于只有一个样本的情况，我们不难得到如下式子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152132.png" alt></p><p>z^[1] 是个 (4, 1) 的矩阵。可以再进一步通过 sigmoid 函数得到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152206.png" alt></p><p>至此，第一层神经网络得任务已完成</p><p>第二层也就是隐藏层的工作，无非就是把第一层的 a^[1] 作为输入，继续用 W^[2] 与 a^[1] 相乘 再加上 b^[2] 得到 z^[2]，再通过 sigmoid 函数得到 a^[2] 也就是最终的预测结果 y帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152532.png" alt></p><p>至此，我们就完成了一个样本的输出。接下来看看如何用矩阵实现多样本的正向输出。</p><h2 id="计算多样本的输出"><a href="#计算多样本的输出" class="headerlink" title="计算多样本的输出"></a>计算多样本的输出</h2><p>假设我们有m个样本，每个样本有3个特征，并且隐藏层也是4个单元。</p><p>那么，通常我们需要使用一个 for 循环将 从 1 遍历至 m 完成以下操作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152921.png" alt></p><p>角标 ^(i) 表示第 i 个样本。我们可以构造这样一个矩阵 x：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153223.png" alt></p><p>它将所有样本的特征，按列叠加在一起，构成 (3, m) 的矩阵。</p><p>如果我们替换上面计算 z^[1] 的过程中使用的 单个样本 x^(1) 为(3, m) 的矩阵 x (也就是下图的绿色框)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153359.png" alt></p><p>就可以得到下面的式子（为了方便表达，下面的公式中假设 b 等于0）：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153641.png" alt></p><p>到这里，我们求出了 Z^[1] ，并且由于 Z^[1] 是(4, 3)的矩阵W^[1]乘以(3, m)的矩阵x 再加上b，所以它是 (4, m) 的矩阵。再经过 sigmoid 函数即可得到同样是 (4, m) 的矩阵 A^[1]，到此隐藏层的工作完成了。</p><h2 id="输出数据"><a href="#输出数据" class="headerlink" title="输出数据"></a>输出数据</h2><p>矩阵 A^[1]，作为下一层（也就是输出层）的输入参数，经过类似的计算也可以得到 Z^[2] = W^[2] × A^[1] + b^[2]，上面我们分析到 W^[2] 是 (1, 4)的矩阵，所以得到的Z^[2]是 (1, m) 的矩阵，同样经过sigmoid函数处理得到的 A^[2] 也是 (1, m) 的矩阵，A^[2]的每个元素，代表一个样本输出的预测值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络概览&quot;&gt;&lt;a href=&quot;#神经网络概览&quot; class=&quot;headerlink&quot; title=&quot;神经网络概览&quot;&gt;&lt;/a&gt;神经网络概览&lt;/h2&gt;&lt;p&gt;在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Yolov1</title>
    <link href="http://yoursite.com/2019/09/20/Yolov1/"/>
    <id>http://yoursite.com/2019/09/20/Yolov1/</id>
    <published>2019-09-20T09:35:22.000Z</published>
    <updated>2019-09-20T09:53:09.792Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="YOLO" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/YOLO/"/>
    
    
      <category term="YOLO" scheme="http://yoursite.com/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>关系数据库</title>
    <link href="http://yoursite.com/2019/09/19/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://yoursite.com/2019/09/19/关系数据库/</id>
    <published>2019-09-19T12:54:22.000Z</published>
    <updated>2019-09-20T09:22:57.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="域"><a href="#域" class="headerlink" title="域"></a>域</h2><p>域是一组具有相同数据类型的值的集合。基数是域中数据的个数</p><h2 id="笛卡儿积"><a href="#笛卡儿积" class="headerlink" title="笛卡儿积"></a>笛卡儿积</h2><p>笛卡尔积直观意义是诸集合各元素间一切可能的组合，可表示为一个二维表。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E6%95%B0%E6%8D%AE%E5%BA%935.png" alt></p><h2 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h2><p>关系是笛卡尔积的有限集合</p><p>关系可以有三种类型：</p><p>​    基本表：实际存储数据的逻辑表示</p><p>​    查询表：查询结果对应的标</p><p>​    视图表：是虚表，由基本表或其他试图导出，不对应实际存储的数据</p><p>关系的基本性质：</p><p>​    列是同质的，每一列中的分量是同一类型的数据，来自同一个域。</p><p>​    不同的列可出自同一个域，其中的每一列称为一个属性，不同的属性要给予不同的属性名。</p><p>​    列的顺序无所谓，列的次序可以任意交换。</p><p>​    行的顺序无所谓，行的次序可以任意交换</p><p>​    任意两个元祖不能完全相同</p><p>​    分量必须取原子值，每一个分量都必须是不可分的数据项。这是规范条件中最基本的一条</p><h2 id="几个术语："><a href="#几个术语：" class="headerlink" title="几个术语："></a>几个术语：</h2><p>若关系中的某一属性组的值能唯一识别一个元祖，则称该属性为候选码。</p><p>若一个关系有多个候选码，则选定其中一个作为主码。</p><p>候选码的诸属性称为非码属性</p><p>不包含在任何候选码中的属性称为非码属性</p><p>若关系模式的所有属性组是这个关系模式的候选码，则称为全码</p><h2 id="关系模式"><a href="#关系模式" class="headerlink" title="关系模式"></a>关系模式</h2><p>关系模式是型，关系是值。关系模式是对关系的描述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;域&quot;&gt;&lt;a href=&quot;#域&quot; class=&quot;headerlink&quot; title=&quot;域&quot;&gt;&lt;/a&gt;域&lt;/h2&gt;&lt;p&gt;域是一组具有相同数据类型的值的集合。基数是域中数据的个数&lt;/p&gt;
&lt;h2 id=&quot;笛卡儿积&quot;&gt;&lt;a href=&quot;#笛卡儿积&quot; class=&quot;head
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib画图总结</title>
    <link href="http://yoursite.com/2019/09/18/matplotlib%E7%94%BB%E5%9B%BE%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/09/18/matplotlib画图总结/</id>
    <published>2019-09-18T07:00:59.000Z</published>
    <updated>2019-09-19T11:13:24.152Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧</p><p>没有这个库的，pip安装一下。</p><p>首先将matplotlib中的pyplot导入，如下：</p><p>import matplotlib.pyplot as plt，所以以下的plt都是pyplot。</p><h4 id="plt-scatter"><a href="#plt-scatter" class="headerlink" title="plt.scatter"></a>plt.scatter</h4><p>画散点图</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = list(range(1,1001))</span><br><span class="line">y = [x*<span class="number">*2</span> <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line"><span class="comment">#s代表点的面积</span></span><br><span class="line"><span class="comment">#c代表颜色</span></span><br><span class="line">plt.scatter(x,y,<span class="attribute">c</span>=<span class="string">"green"</span>,s=20)</span><br><span class="line"><span class="comment">#设置标题并加上轴标签</span></span><br><span class="line">plt.title(<span class="string">"Squares Numbers"</span>,<span class="attribute">fontsize</span>=24)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line">plt.xlabel(<span class="string">"Square of Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line"><span class="comment">#设置刻度标记的大小</span></span><br><span class="line">plt.tick_params(<span class="attribute">axis</span>=<span class="string">'both'</span>,which='major',labelsize=14)</span><br><span class="line"><span class="comment">#设置每个坐标的取值范围</span></span><br><span class="line">plt.axis([0,1100,0,1100000])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>上面的代码运行后，是一条线。那是因为点太多了。绘制很多点的时候，轮廓会连在一起。要删除数据点的轮廓可用scatter，传递参数edgecolor=”none”</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/matplotlib1.png" alt></p><p>模块pyplot内置了一组颜色映射，要使用这些颜色映射，你需要告诉pyplot该如何设置数据集中每个点的颜色。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = list(range(1,1001))</span><br><span class="line">y = [x*<span class="number">*2</span> <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line"><span class="comment">#s代表点的面积</span></span><br><span class="line"><span class="comment">#cmap设置颜色映射</span></span><br><span class="line">plt.scatter(x,y,<span class="attribute">s</span>=20,c=y,cmap=plt.cm.Greens)</span><br><span class="line"><span class="comment">#设置标题并加上轴标签</span></span><br><span class="line">plt.title(<span class="string">"Squares Numbers"</span>,<span class="attribute">fontsize</span>=24)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line">plt.xlabel(<span class="string">"Square of Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line"><span class="comment">#设置刻度标记的大小</span></span><br><span class="line">plt.tick_params(<span class="attribute">axis</span>=<span class="string">'both'</span>,which='major',labelsize=14)</span><br><span class="line"><span class="comment">#设置每个坐标的取值范围</span></span><br><span class="line">plt.axis([0,1100,0,1100000])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>保存散点图：savefig</p><p>保存图片之前，一定要把plt.show()注释掉，否则会保存一张空白的图片</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#plt</span><span class="selector-class">.show</span>()</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.savefig</span>(<span class="string">"save.jpg"</span>,bbox_inches=<span class="string">"tight"</span>)</span><br></pre></td></tr></table></figure><h4 id="plt-bar"><a href="#plt-bar" class="headerlink" title="plt.bar"></a>plt.bar</h4><p>画柱状图，默认是竖直条形图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/matplotlib2.png" alt></p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"><span class="attr">y</span> = [<span class="number">20</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">25</span>, <span class="number">15</span>]</span><br><span class="line"><span class="attr">x</span> = np.arange(<span class="number">5</span>)</span><br><span class="line"><span class="attr">p1</span> = plt.bar(x, <span class="attr">height=y,</span> <span class="attr">width=0.5,</span> )</span><br><span class="line"><span class="comment"># 展示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>水平条形图：需要把orientation改为horizontal，然后x与y的数据交换</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">x = [20, 10, 30, 25, 15]</span><br><span class="line">y = np.arange(5)</span><br><span class="line"><span class="comment"># x= 起始位置，bottom= 水平条的底部(左侧), y轴， height：水平条的宽度</span></span><br><span class="line"><span class="comment">#width：水平条的长度</span></span><br><span class="line">p1 = plt.bar(<span class="attribute">x</span>=0, <span class="attribute">bottom</span>=y, <span class="attribute">height</span>=0.5, <span class="attribute">width</span>=x, <span class="attribute">orientation</span>=<span class="string">"horizontal"</span>)</span><br><span class="line"><span class="comment"># 展示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="plt-plot-x-y-color-’red’-linewidth-2-5-linestyle-’-‘"><a href="#plt-plot-x-y-color-’red’-linewidth-2-5-linestyle-’-‘" class="headerlink" title="plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘)"></a>plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘)</h4><p> 画线，也可以用来画折线图。x是横坐标的值。y是纵坐标的值。color参数设置曲线颜色，linewidth设置曲线宽度，linestyle设置曲线风格。</p><p>linestyle的可选参数：</p><p>‘-‘       solid line style<br> ‘–’      dashed line style<br> ‘-.’      dash-dot line style<br> ‘:’       dotted line style</p><h4 id="plt-figure"><a href="#plt-figure" class="headerlink" title="plt.figure()"></a>plt.figure()</h4><p>自定义画布大小，画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小。</p><p>figure语法说明</p><p>figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True)</p><p>num:图像编号或名称，数字为编号 ，字符串为名称</p><p>figsize:指定figure的宽和高，单位为英寸；</p><p>dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80</p><p>facecolor:背景颜色</p><p>edgecolor:边框颜色</p><p>frameon:是否显示边框</p><h4 id="plt-xticks"><a href="#plt-xticks" class="headerlink" title="plt.xticks()"></a>plt.xticks()</h4><p>设置x轴刻度的表现方式</p><h4 id="plt-yticks"><a href="#plt-yticks" class="headerlink" title="plt.yticks()"></a>plt.yticks()</h4><p>设置y轴刻度的表现方式</p><h4 id="plt-xlim"><a href="#plt-xlim" class="headerlink" title="plt.xlim()"></a>plt.xlim()</h4><p>设置x轴刻度的取值范围</p><h4 id="plt-ylim"><a href="#plt-ylim" class="headerlink" title="plt.ylim()"></a>plt.ylim()</h4><p>设置y轴刻度的取值范围</p><h4 id="plt-text-1-2-“I’m-a-text”"><a href="#plt-text-1-2-“I’m-a-text”" class="headerlink" title="plt.text(1, 2, “I’m a text”)"></a>plt.text(1, 2, “I’m a text”)</h4><p>前两个参数表示文本坐标, 第三个参数为要添加的文本</p><h4 id="plt-legend"><a href="#plt-legend" class="headerlink" title="plt.legend()"></a>plt.legend()</h4><p>函数实现了图例功能, 他有两个参数, 第一个为样式对象, 第二个为描述字符,都可以为空</p><h4 id="plt-xlabel"><a href="#plt-xlabel" class="headerlink" title="plt.xlabel()"></a>plt.xlabel()</h4><p>添加x轴名字</p><h4 id="plt-ylabel"><a href="#plt-ylabel" class="headerlink" title="plt.ylabel()"></a>plt.ylabel()</h4><p>添加y轴名字</p><h4 id="plt-tight-layout"><a href="#plt-tight-layout" class="headerlink" title="plt.tight_layout()"></a>plt.tight_layout()</h4><p>tight_layout自动调整subplot(s)参数，以便subplot(s)适应图形区域</p><h4 id="plt-subplot"><a href="#plt-subplot" class="headerlink" title="plt.subplot()"></a>plt.subplot()</h4><p>设置画布划分以及图像在画布上输出的位置，将figure设置的画布大小分成几个部分，参数‘221’表示2(row)x2(colu),即将画布分成2x2，两行两列的4块区域，1表示选择图形输出的区域在第一块，图形输出区域参数必须在“行x列”范围</p><h4 id="plt-subplots"><a href="#plt-subplots" class="headerlink" title="plt.subplots()"></a>plt.subplots()</h4><p>subplots(nrows,ncols,sharex,sharey,squeeze,subplot_kw,gridspec_kw,**fig_kw) :创建画布和子图</p><p>1、nrows,ncols表示将画布分割成几行几列。shares，sharey表示坐标轴的属性是否相同，可选的参数：True、False、row、col。默认值为False。</p><p>2、 squeeze  bool</p><p>a.默认参数为True：额外的维度从返回的Axes(轴)对象中挤出，对于N<em>1或1</em>N个子图，返回一个1维数组，对于N*M，N&gt;1和M&gt;1返回一个2维数组。</p><p>b.为False，不进行挤压操作：返回一个元素为Axes实例的2维数组，即使它最终是1x1。</p><p>3、subplot_kw:字典类型，可选参数。把字典的关键字传递给add_subplot()来创建每个子图。</p><p>4、gridspec_kw:字典类型，可选参数。把字典的关键字传递给GridSpec构造函数创建子图放在网格里(grid)。</p><p>5、**fig_kw：把所有详细的关键字参数传给figure()函数。</p><h4 id="plt-grid"><a href="#plt-grid" class="headerlink" title="plt.grid()"></a>plt.grid()</h4><p>是否开启方格，True为开，False为不显示。默认为False</p><h4 id="plt-gca"><a href="#plt-gca" class="headerlink" title="plt.gca()"></a>plt.gca()</h4><p>获取当前的子图</p><h4 id="plt-xcale"><a href="#plt-xcale" class="headerlink" title="plt.xcale()"></a>plt.xcale()</h4><p>改变x轴的比例。pyplot不仅支持线性轴刻度，还支持对数和logit刻度。如果数据跨越许多数量级，则通常使用此方法</p><p>plt.xcale(“logit”)，还有log、symmlog等选择。还可以添加自己的比例。</p><h4 id="plt-yscale"><a href="#plt-yscale" class="headerlink" title="plt.yscale()"></a>plt.yscale()</h4><p>改变y轴的比例。用法同plt.xcale一样</p><p>随便写了个例子，其他具体的用法还是要自己去练习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter([<span class="number">1</span>,<span class="number">3</span>,<span class="number">9</span>],[<span class="number">5</span>,<span class="number">8</span>,<span class="number">2</span>], label=<span class="string">"Example one"</span>,color=<span class="string">"red"</span>,s=<span class="number">25</span>,marker=<span class="string">"o"</span>)</span><br><span class="line">plt.plot([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],[<span class="number">8</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>], label=<span class="string">"Example two"</span>, color=<span class="string">'g'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks([<span class="number">1</span>,<span class="number">10</span>])</span><br><span class="line">plt.yticks([<span class="number">1</span>,<span class="number">15</span>])</span><br><span class="line">plt.xlabel(<span class="string">'bar number'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'bar height'</span>)</span><br><span class="line">plt.title(<span class="string">'test'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧&lt;/p&gt;
&lt;p&gt;没有这个库的，pip安装一下。&lt;/p&gt;
&lt;p&gt;首先将matplotlib中的pyplot导入，如下：&lt;/p&gt;
&lt;p&gt;import matplotlib.pyplot a
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(4)</title>
    <link href="http://yoursite.com/2019/09/17/SVM%E8%A7%A3%E8%AF%BB(4)/"/>
    <id>http://yoursite.com/2019/09/17/SVM解读(4)/</id>
    <published>2019-09-17T07:13:10.000Z</published>
    <updated>2019-09-19T10:21:17.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用SVC时的其他考虑"><a href="#使用SVC时的其他考虑" class="headerlink" title="使用SVC时的其他考虑"></a>使用SVC时的其他考虑</h2><h3 id="SVC处理多分类问题"><a href="#SVC处理多分类问题" class="headerlink" title="SVC处理多分类问题"></a>SVC处理多分类问题</h3><p>之前的所有的SVM的(1)-(3)内容，全部是基于二分类的情况来说明的。因为支持向量机是天生二分类的模型。但是，它也可以做多分类。但是SVC在多分类情况的推广是很难的。因为要研究透彻多分类状况下的SVC，就必须研究透彻多分类时所需要的决策边界个数，每个决策边界所需要的支持向量个数，以及这些支持向量如何组合起来计算拉格朗日乘数。本小节只说一小部分。支持向量机是天在生二分类的模型，所以支持向量机在处理多分类问题的时候，是把多分类问题转换成了二分类问题来解决。这种转换有两种模式，一种叫做“一对一”模式（one vs one），一种叫做“一对多”模式(one vs rest)。 </p><p>在ovo模式下(一对一模式)上，标签中的所有类别都会被两两组合，每两个类别建立一个SVC模型，每个模型生成一个决策边界，分别进行二分类，这种模式下，对于n_class个标签类别的数据来说，SVC会生成总共$C^2_{n_class}$个模型，即会生成总共$C^2_{n_class}$个超平面，其中：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM74.png" alt></p><p>ovo模式下，二维空间的三分类状况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM75.png" alt></p><p>首先让提出紫色点和红色点作为一组，然后求解出两个类之间的SVC和绿色决策边界。然后让绿色点和红色点作为一组，求解出两个类之间的SVC和灰色边界。最后让绿色和紫色组成一组，组成两个类之间的SVC和黄色边界。然后基于三个边界，分别对三个类别进行分类。</p><p>ovr模式下，标签中所有的类别会分别于其他类别进行组合，建立n_class个模型，每个模型生成一个决策边界。分别进行二分类。ovr模式下，则会生成以下的决策边界：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM76.png" alt></p><p>紫色类 vs 剩下的类，生成绿色的决策边界。红色类 vs 剩下的类，生成黄色的决策边界。绿色类 vs 剩下的类，生成灰色的决策边界，当类别更多的时候，如此类推下去，我们永远需要n_class个模型。</p><p>当类别更多的时候，无论是ovr还是ovo模式需要的决策边界都会越来越多，模型也会越来越复杂，不过ovo模式下的模型计算会更加复杂，因为ovo模式中的决策边界数量增加更快，但相对的，ovo模型也会更加精确。ovr模型计算更快，但是效果往往不是很好。在硬件可以支持的情况下，还是建议选择ovo模式。 </p><p>模型和超平面的数量变化了，SVC的很多计算、接口、属性都会发生变化，而参数decision_function_shape决定我们究竟使用哪一种分类模式。</p><h4 id="decision-function-shape"><a href="#decision-function-shape" class="headerlink" title="decision_function_shape"></a>decision_function_shape</h4><p>可输入“ovo”，”ovr”，默认”ovr”，对所有分类器，选择使用ovo或者ovr模式。<br>选择ovr模式，则返回的decision_function结构为(n_samples，n_class)。但是当二分类时，尽管选用ovr模式，却会返回<br>(n_samples，)的结构。 </p><p>选择ovo模式，则使用libsvm中原始的，结构为(n_samples,n_class<em>(n_class-1)/2)的decision_function接口。在ovo模式并且核函数为线性核的情况下，属性coef_和intercepe_会分别返回(n_class\</em>(n_class-1)/2,n_features) 和(n_class*(n_class-1)/2,)的结构，每行对应一个生成的二元分类器。ovo模式只在多分类的状况下使用。</p><p>SVC的其它参数、属性和接口的列表在<a href="[https://brickexperts.github.io/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/#more](https://brickexperts.github.io/2019/09/15/SVM解读(2)/#more">SVM解读(2)</a></p><h2 id="线性支持向量机类LinearSVC"><a href="#线性支持向量机类LinearSVC" class="headerlink" title="线性支持向量机类LinearSVC"></a>线性支持向量机类LinearSVC</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM77.png" alt></p><p>线性支持向量机其实与SVC类中选择”linear”作为核函数的功能类似，但是其背后的实现库是liblinear而不是libsvm，这使得在线性数据上，linearSVC的运行速度比SVC中的“linear”核函数要快，不过两者的运行结果相似。在现实中，许多数据都是线性的，因此我们可以依赖计算得更快得LinearSVC类。除此之外，线性支持向量可以很容易地推广到大样本上，还可以支持稀疏矩阵，多分类中也支持ovr方案。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM79.png" alt></p><p>和SVC一样，LinearSVC也有C这个惩罚参数，但LinearSVC在C变大时对C不太敏感，并且在某个阈值之后就不能再改善结果了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用SVC时的其他考虑&quot;&gt;&lt;a href=&quot;#使用SVC时的其他考虑&quot; class=&quot;headerlink&quot; title=&quot;使用SVC时的其他考虑&quot;&gt;&lt;/a&gt;使用SVC时的其他考虑&lt;/h2&gt;&lt;h3 id=&quot;SVC处理多分类问题&quot;&gt;&lt;a href=&quot;#SVC处理多分
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(3)</title>
    <link href="http://yoursite.com/2019/09/16/SVM%E8%A7%A3%E8%AF%BB(3)/"/>
    <id>http://yoursite.com/2019/09/16/SVM解读(3)/</id>
    <published>2019-09-16T11:00:53.000Z</published>
    <updated>2019-09-17T07:18:46.550Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参数C的理解进阶"><a href="#参数C的理解进阶" class="headerlink" title="参数C的理解进阶"></a>参数C的理解进阶</h2><p>在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，即无法让训练误差为0，这样的数据被我们称为“存在软间隔的数据”。此时此刻，我们需要让我们决策边界能够忍受一小部分训练误差，我们就不能单纯地寻求最大边际了。 因为对于软间隔地数据来说，边际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡。因此，我们引入松弛系数ζ和松弛系数的系数C作为一个惩罚项，来惩罚我们对最大边际的追求。 </p><p>那我们的参数C如何影响我们的决策边界呢？在硬间隔的时候，我们的决策边界完全由两个支持向量和最小化损失函数（最大化边际）来决定，而我们的支持向量是两个标签类别不一致的点，即分别是正样本和负样本。然而在软间隔情况下我们的边际依然由支持向量决定，但此时此刻的支持向量可能就不再是来自两种标签类别的点了，而是分布在决策边界两边的，同类别的点。回忆一下我们的图像： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt></p><p>此时我们的虚线超平面&omega;*x+b=1-ζ<sub>i</sub>是由混杂在红色点中间的紫色点来决定的，所以此时此刻，这个蓝色点就是我们的支持向量了。所以软间隔让决定两条虚线超平面的支持向量可能是来自于同一个类别的样本点，而硬间隔的时候两条虚线超平面必须是由来自两个不同类别的支持向量决定的。而C值会决定我们究竟是依赖红色点作为支持向量（只追求最大边界），还是我们要依赖软间隔中，混杂在红色点中的紫色点来作为支持向量（追求最大边界和判断正确的平衡）。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，尽量将掉落在决策边界另一方的样本点预测正确，决策功能会更简单，但代价是训练的准确度，因为此时会有更多红色的点被分类错误。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">10</span>,<span class="number">16</span>))</span><br><span class="line">    <span class="comment">#第一层循环：在不同的数据集中循环</span></span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">                   , zorder=<span class="number">10</span></span><br><span class="line">                   , cmap=plt.cm.Paired, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">100</span>,</span><br><span class="line">                   facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'white'</span>)</span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">                   levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">                , size=<span class="number">15</span></span><br><span class="line">                , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">                <span class="comment"># 为分数添加一个白色的格子作为底色</span></span><br><span class="line">                , transform=ax.transAxes  <span class="comment"># 确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">                , horizontalalignment=<span class="string">'right'</span>  <span class="comment"># 位于坐标轴的什么方向</span></span><br><span class="line">                )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>白色圈圈出的就是我们的支持向量，大家可以看到，所有在两条虚线超平面之间的点，和虚线超平面外，但属于另一个类别的点，都被我们认为是支持向量。并不是因为这些点都在我们的超平面上，而是因为我们的超平面由所有的这些点来决定，我们可以通过调节C来移动我们的超平面，让超平面过任何一个白色圈圈出的点。参数C就是这样影响了我们的决策，可以说是彻底改变了SVM的决策过程。</p><h2 id="样本不均衡问题"><a href="#样本不均衡问题" class="headerlink" title="样本不均衡问题"></a>样本不均衡问题</h2><p>分类问题永远逃不过的痛点是样本不均衡问题。样本不均衡是指在一组数据集中，标签的一类天生占有很大的比例，但我们有着捕捉出某种特定的分类的需求的状况。</p><p>分类问题天生会倾向于多数的类，让多数类更容易被判断准确，少数类被牺牲掉。因此对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。如果我们希望捕获少数类，模型就会失败。其次，模型评估指标会失去意义。 </p><h3 id="class-weight"><a href="#class-weight" class="headerlink" title="class_weight"></a>class_weight</h3><p>可输入字典或者”balanced”，可不填，默认None 对SVC，将类i的参数C设置为class_weight [i] * C。如果没有给出具体的class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如{“标签的值1”：权重1，”标签的值2”：权重2}的字典，则参数C将会自动被设为：标签的值1的C：权重1 * C，标签的值2的C：权重2*C或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为n_samples/(n_classes * np.bincount(y)) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#其中红色点是少数类，紫色点是多数类</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">1.0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"><span class="comment">#设定class_weight</span></span><br><span class="line">wclf = svm.SVC(kernel=<span class="string">'linear'</span>, class_weight=&#123;<span class="number">1</span>: <span class="number">10</span>&#125;)</span><br><span class="line">wclf.fit(X, y)</span><br><span class="line"><span class="comment">#给两个模型分别打分看看，这个分数是accuracy准确度</span></span><br><span class="line">print(<span class="string">"没设定class_weight："</span>,clf.score(X,y))</span><br><span class="line">print(<span class="string">"设定class_weight："</span>,wclf.score(X,y))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">ax = plt.gca() <span class="comment">#获取当前的子图，如果不存在，则创建新的子图</span></span><br><span class="line">xlim = ax.get_xlim()</span><br><span class="line">ylim = ax.get_ylim()</span><br><span class="line">xx = np.linspace(xlim[<span class="number">0</span>], xlim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">yy = np.linspace(ylim[<span class="number">0</span>], ylim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">YY, XX = np.meshgrid(yy, xx)</span><br><span class="line">xy = np.vstack([XX.ravel(), YY.ravel()]).T</span><br><span class="line"><span class="comment">#第二步：找出我们的样本点到决策边界的距离</span></span><br><span class="line">Z_clf = clf.decision_function(xy).reshape(XX.shape)</span><br><span class="line">a = ax.contour(XX, YY, Z_clf, colors=<span class="string">'black'</span>, levels=[<span class="number">0</span>], alpha=<span class="number">0.5</span>, linestyles=[<span class="string">'-'</span>])</span><br><span class="line">Z_wclf = wclf.decision_function(xy).reshape(XX.shape)</span><br><span class="line">b = ax.contour(XX, YY, Z_wclf, colors=<span class="string">'red'</span>, levels=[<span class="number">0</span>], alpha=<span class="number">0.5</span>, linestyles=[<span class="string">'-'</span>])</span><br><span class="line"><span class="comment">#第三步：画图例 a.collections调用这个等高线对象中画的所有线，返回一个惰性对象</span></span><br><span class="line">plt.legend([a.collections[<span class="number">0</span>], b.collections[<span class="number">0</span>]], [<span class="string">"non weighted"</span>, <span class="string">"weighted"</span>],</span><br><span class="line">loc=<span class="string">"upper right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从图像上可以看出，灰色是我们做样本平衡之前的决策边界。灰色线上方的点被分为一类，下方的点被分为另一类。可以看到，大约有一半少数类（红色）被分错，多数类（紫色点）几乎都被分类正确了。红色是我们做样本平衡之后的决策边界，同样是红色线上方一类，红色线下方一类。可以看到，做了样本平衡后，少数类几乎全部都被分类正确了，但是多数类有许多被分错了。</p><p>从准确率的角度来看，不做样本平衡的时候准确率反而更高，做了样本平衡准确率反而变低了，这是因为做了样本平衡后，为了要更有效地捕捉出少数类，模型误伤了许多多数类样本，而多数类被分错的样本数量 &gt; 少数类被分类正确的样本数量，使得模型整体的精确性下降。现在，如果我们的目的是模型整体的准确率，那我们就要拒绝样本平衡，使用class_weight被设置之前的模型。而在现实情况中，我们往往都在捕捉少数类。因为有些情况是宁愿误伤，也要尽量的捕捉少数类。</p><h2 id="SVC的模型评估指标"><a href="#SVC的模型评估指标" class="headerlink" title="SVC的模型评估指标"></a>SVC的模型评估指标</h2><p>上面说了，我们往往都在捕捉少数类的。但是单纯地追求捕捉出少数类，就会成本太高，而不顾及少数类，又会无法达成模型的效果。所以在现实中，我们往往在寻找捕获少数类的能力和将多数类判错后需要付出的成本的平衡。如果一个模型在能够尽量捕获少数类的情况下，还能够尽量对多数类判断正确，则这个模型就非常优秀了。为了评估这样的能力，我们将引入新的模型评估指标：混淆矩阵和ROC曲线来帮助我们 </p><h3 id="混淆矩阵-Confusion-Matrix"><a href="#混淆矩阵-Confusion-Matrix" class="headerlink" title="混淆矩阵(Confusion Matrix)"></a>混淆矩阵(Confusion Matrix)</h3><h4 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h4><p>精确度Precision，又叫查准率，表示所有被我们预测为是少数类的样本中，真正的少数类所占的比例 </p><p>召回率Recall，又被称为敏感度(sensitivity)，真正率，查全率，表示所有真实为1的样本中，被我们预测正确的样<br>本所占的比例。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt></p><p>如果我们希望不计一切代价，找出少数类，那我们就会追求高召回率，相反如果我们的目标不是尽量捕获少数类，那我们就不需要在意召回率。 注意召回率和精确度的分子是相同的，只是分母不同。而召回率和精确度是此消彼长的，两者之间的平衡代表了捕捉少数类的需求和尽量不要误伤多数类的需求的平衡。究竟要偏向于哪一方，取决于我们的业务需求：<br>究竟是误伤多数类的成本更高，还是无法捕捉少数类的代价更高。</p><p>为了同时兼顾精确度和召回率，我们创造了两者的调和平均数作为考量两者平衡的综合性指标，称之为F1measure。两个数之间的调和平均倾向于靠近两个数中比较小的那一个数，因此我们追求尽量高的F1 measure，能够保证我们的精确度和召回率都比较高。F1 measure在[0,1]之间分布，越接近1越好：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM68.png" alt></p><h4 id="假负率、特异度-真负率-、假正率"><a href="#假负率、特异度-真负率-、假正率" class="headerlink" title="假负率、特异度(真负率)、假正率"></a>假负率、特异度(真负率)、假正率</h4><p>从Recall延申出来的另一个评估指标叫做假负率（False Negative Rate），它等于 1 - Recall，用于衡量所有真实为1的样本中，被我们错误判断为0的，通常用得不多。 </p><p>特异度(Specificity)，也叫做真负率。表示所有真实为0的样本中，被正确预测为0的样本所占的比例。在支持向量机中，可以形象地表示为，决策边界下方的点占所有蓝色点的比例。特异度衡量了一个模型将多数类判断正确的能力，而1 - specificity就是一个模型将多数类判断错误的能力，叫做假正率。</p><p>sklearn当中提供了大量的类来帮助我们了解和使用混淆矩阵：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM69.png" alt></p><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线，全称The Receiver Operating Characteristic Curve，译为受试者操作特性曲线。这是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。让我们先从概率和阈值开始看起。 </p><h4 id="阈值-threshold"><a href="#阈值-threshold" class="headerlink" title="阈值(threshold)"></a>阈值(threshold)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">0.5</span>, <span class="number">1</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LogiR</span><br><span class="line">clf_lo=LogiR().fit(X,y)</span><br><span class="line">prob=clf_lo.predict_proba(X)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">prob=pd.DataFrame(prob)</span><br><span class="line">prob.columns=[<span class="string">"0"</span>,<span class="string">"1"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(prob.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="comment">#将0.5设立为阈值</span></span><br><span class="line">    <span class="keyword">if</span> prob.loc[i,<span class="string">"1"</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">    <span class="comment">#创建了一个新的列名为pred的列</span></span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#创建了名为y_true的列，且将y赋值为该列</span></span><br><span class="line">prob[<span class="string">"y_true"</span>] = y</span><br><span class="line"><span class="comment">#通过“1”列拍序，ascending表示正序or逆序</span></span><br><span class="line">prob = prob.sort_values(by=<span class="string">"1"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM, precision_score <span class="keyword">as</span> P, recall_score <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score <span class="keyword">as</span> A</span><br><span class="line">print(<span class="string">"混淆矩阵："</span>,CM(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"精确度："</span>,P(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"召回率："</span>,R(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"准确率："</span>,A(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>]))</span><br></pre></td></tr></table></figure><p>我们可以设置不同阈值来实验模型的结果，在不同阈值下，我们的模型评估指标会发生变化。我们正利用这一点来观察Recall和FPR之间如何互相影响。但是注意，并不是升高阈值，就一定能够增加或者减少Recall，一切要根据数据的实际分布来进行判断。</p><h4 id="概率-probability"><a href="#概率-probability" class="headerlink" title="概率(probability)"></a>概率(probability)</h4><p>我们在画等高线，也就是决策边界的时候曾经使用SVC的接口decision_function，它返回我们输入的特征矩阵中每个样本到划分数据集的超平面的距离。我们在SVM中利用超平面来判断我们的样本，本质上来说，当两个点的距离是相同的符号的时候，越远离超平面的样本点归属于某个标签类的概率就很大。比如说，一个距离超平面0.1的点，和一个距离超平面100的点，明显是距离为0.1的点更有可能是负类别的点混入了边界。同理，一个距离超平面距离为-0.1的点，和一个离超平面距离为-100的点，明显是-100的点的标签更有可能是负类接口decision_function返回的值也因此被我们认为是SVM中的置信度(confidence)。</p><p>不过，置信度始终不是概率，它没有边界，可以无限大。大部分时候不是也小数的形式呈现，而SVC的判断过程又不像决策树一样求解出一个比例。为了解决这个矛盾，SVC有重要参数probability。</p><p><strong>probability</strong>：是否启用概率估计，布尔值，可不填，默认时False。必须在调用fit之前使用它，启用此功能会减慢SVM的计算速度。设置为True会启动，SVC的接口predict_proba和predict_log_proba将生效。在二分类情况下，SVC将使用Platt缩放生成概率，即在decision_function生成的距离上进行Sigmoid压缩，并附加训练数据的交叉验证拟合，来生成类逻辑回归的SVM分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#其中红色点是少数类，紫色点是多数类</span></span><br><span class="line">clf_proba = svm.SVC(kernel=<span class="string">"linear"</span>,C=<span class="number">1.0</span>,probability=<span class="literal">True</span>).fit(X,y)</span><br><span class="line">print(<span class="string">"返回预测某标签的概率："</span>,clf_proba.predict_proba(X))</span><br><span class="line">print(<span class="string">"行数和列数："</span>,clf_proba.predict_proba(X).shape)</span><br><span class="line">print(<span class="string">"每个样本到划分数据集的超平面的距离："</span>,clf_proba.decision_function(X))</span><br></pre></td></tr></table></figure><h4 id="绘制SVM的ROC曲线"><a href="#绘制SVM的ROC曲线" class="headerlink" title="绘制SVM的ROC曲线"></a>绘制SVM的ROC曲线</h4><p>现在，我们理解了什么是阈值（threshold），了解了不同阈值会让混淆矩阵产生变化，也了解了如何从我们的分类算法中获取概率。现在，我们就可以开始画我们的ROC曲线了。<strong>ROC是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。</strong>简单地来说，只要我们有数据和模型，我们就可以在python中绘制出我们的ROC曲线。思考一下，我们要绘制ROC曲线，就必须在我们的数据中去不断调节阈值，不断求解混淆矩阵，然后不断获得我们的横坐标和纵坐标，最后才能够将曲线绘制出来。接下来，我们就来执行这个过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LogiR</span><br><span class="line">clf_lo=LogiR().fit(X,y)</span><br><span class="line">prob=clf_lo.predict_proba(X)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">prob=pd.DataFrame(prob)</span><br><span class="line">prob.columns=[<span class="string">"0"</span>,<span class="string">"1"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(prob.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="comment">#将0.5设立为阈值</span></span><br><span class="line">    <span class="keyword">if</span> prob.loc[i,<span class="string">"1"</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">    <span class="comment">#创建了一个新的列名为pred的列</span></span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#创建了名为y_true的列，且将y赋值为该列</span></span><br><span class="line">prob[<span class="string">"y_true"</span>] = y</span><br><span class="line">prob = prob.sort_values(by=<span class="string">"1"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">cm = CM(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment">#开始绘图</span></span><br><span class="line">recall = []</span><br><span class="line">FPR = []</span><br><span class="line">clf_proba = svm.SVC(kernel=<span class="string">"linear"</span>,C=<span class="number">1.0</span>,probability=<span class="literal">True</span>).fit(X,y)</span><br><span class="line">probrange = np.linspace(clf_proba.predict_proba(X)</span><br><span class="line">[:,<span class="number">1</span>].min(),clf_proba.predict_proba(X)[:,<span class="number">1</span>].max(),num=<span class="number">50</span>,endpoint=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM, recall_score <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plot</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> probrange:</span><br><span class="line">    y_predict = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> clf_proba.predict_proba(X)[j,<span class="number">1</span>] &gt; i:</span><br><span class="line">            y_predict.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_predict.append(<span class="number">0</span>)</span><br><span class="line">    cm = CM(y,y_predict,labels=[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    recall.append(cm[<span class="number">0</span>,<span class="number">0</span>]/cm[<span class="number">0</span>,:].sum())</span><br><span class="line">    FPR.append(cm[<span class="number">1</span>,<span class="number">0</span>]/cm[<span class="number">1</span>,:].sum())</span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">recall.sort()</span><br><span class="line">FPR.sort()</span><br><span class="line">plt.plot(FPR,recall,c=<span class="string">"red"</span>)</span><br><span class="line">plt.plot(probrange+<span class="number">0.05</span>,probrange+<span class="number">0.05</span>,c=<span class="string">"black"</span>,linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行后，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM70.png" alt></p><p>我们建立ROC曲线的根本目的是找寻Recall和FPR之间的平衡，让我们能够衡量模型在尽量捕捉少数类的时候，误伤多数类的情况会如何变化。横坐标是FPR，代表着模型将多数类判断错误的能力，纵坐标Recall，代表着模型捕捉少数类的能力，所以ROC曲线代表着，随着Recall的不断增加，FPR如何增加。我们希望随着Recall的不断提升，FPR增加得越慢越好，这说明我们可以尽量高效地捕捉出少数类，而不会将很多地多数类判断错误。所以，我们希望看到的图像是，纵坐标急速上升，横坐标缓慢增长，也就是在整个图像左上方的一条弧线。这代表模型的效果很不错，拥有较好的捕获少数类的能力。 </p><p>中间的虚线代表着，当recall增加1%，我们的FPR也增加1%，也就是说，我们每捕捉出一个少数类，就会有一个多数类被判错，这种情况下，模型的效果就不好，这种模型捕获少数类的结果，会让许多多数类被误伤，从而增加我们的成本。ROC曲线通常都是凸型的。对于一条凸型ROC曲线来说，曲线越靠近左上角越好，越往下越糟糕，曲线如果在虚线的下方，则证明模型完全无法使用。但是它也有可能是一条凹形的ROC曲线。对于一条凹型ROC曲线来说，应该越靠近右下角越好，凹形曲线代表模型的预测结果与真实情况完全相反，那也不算非常糟糕，只要我们手动将模型的结果逆转，就可以得到一条左上方的弧线了。最糟糕的就是，无论曲线是凹形还是凸型，曲线位于图像中间，和虚线非常靠近，那我们拿它无能为力。</p><p>现在，我们虽然拥有了这条曲线，但是还是没有具体的数字帮助我们理解ROC曲线和模型效果。接下来将AUC面加，它代表了ROC曲线下方的面积，这个面积越大，代表ROC曲线越接近左上角，模型就越好。</p><h3 id="AUC面积"><a href="#AUC面积" class="headerlink" title="AUC面积"></a>AUC面积</h3><p>sklearn中，我们有帮助我们计算ROC曲线的横坐标假正率FPR，纵坐标Recall和对应的阈值的类<br>sklearn.metrics.roc_curve。同时，我们还有帮助我们计算AUC面积的类sklearn.metrics.roc_auc_score。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM71.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM72.png" alt></p><p>AUC面积的分数使用以上类来进行计算，输入的参数也比较简单，就是真实标签，和与roc_curve中一致的置信度分数或者概率值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">FPR, recall, thresholds = roc_curve(y,clf_proba.decision_function(X), pos_label=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#阈值可以也为负</span></span><br><span class="line">print(FPR,recall,thresholds)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score <span class="keyword">as</span> AUC</span><br><span class="line">area = AUC(y,clf_proba.decision_function(X))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(FPR, recall, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">'ROC curve (area = %0.2f)'</span> % area)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'Receiver operating characteristic example'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="利用ROC曲线找出最佳阈值"><a href="#利用ROC曲线找出最佳阈值" class="headerlink" title="利用ROC曲线找出最佳阈值"></a>利用ROC曲线找出最佳阈值</h2><p>对ROC曲线的理解来：ROC曲线反应的是recall增加的时候FPR如何变化，也就是当模型捕获少数类的能力变强的时候，会误伤多数类的情况是否严重。我们的希望是，模型在捕获少数类的能力变强的时候，尽量不误伤多数类，也就是说，随着recall的变大，FPR的大小越小越好。所以我们希望找到的最有点，其实是Recall和FPR差距最大的点。这个点，又叫做约登指数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">maxindex=(recall-FPR).tolist().index(max(recall-FPR))</span><br><span class="line"><span class="comment">#得出最佳阈值</span></span><br><span class="line">print(<span class="string">"最佳阈值："</span>,thresholds[maxindex])</span><br><span class="line">plt.scatter(FPR[maxindex],recall[maxindex],c=<span class="string">"black"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(FPR, recall, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">'ROC curve (area = %0.2f)'</span> % area)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.scatter(FPR[maxindex],recall[maxindex],c=<span class="string">"black"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'Receiver operating characteristic example'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这样，最佳阈值和最好的点都找了出来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参数C的理解进阶&quot;&gt;&lt;a href=&quot;#参数C的理解进阶&quot; class=&quot;headerlink&quot; title=&quot;参数C的理解进阶&quot;&gt;&lt;/a&gt;参数C的理解进阶&lt;/h2&gt;&lt;p&gt;在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(2)</title>
    <link href="http://yoursite.com/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/"/>
    <id>http://yoursite.com/2019/09/15/SVM解读(2)/</id>
    <published>2019-09-15T05:47:15.000Z</published>
    <updated>2019-09-16T10:54:44.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="非线性SVM与核函数"><a href="#非线性SVM与核函数" class="headerlink" title="非线性SVM与核函数"></a>非线性SVM与核函数</h2><h3 id="SVC在非线性数据上的推广"><a href="#SVC在非线性数据上的推广" class="headerlink" title="SVC在非线性数据上的推广"></a>SVC在非线性数据上的推广</h3><p>为了能够找出非线性数据的线性决策边界，我们需要将数据从原始的空间x投射到新空间Φ(x)中，Φ是一个映射函数，它代表了某种非线性的变换，如同我们之前所做过的使用r来升维一样，这种非线性变换看起来是一种非常有效的方式。使用这种变换，线性SVM的原理可以被很容易推广到非线性情况下，其推到过程核逻辑都与线性SVM一摸一样，只不过在第一决策边界之前，我们必须先对数据进行升维，即将原始的x转换成Φ。</p><h3 id="重要参数kernel"><a href="#重要参数kernel" class="headerlink" title="重要参数kernel"></a>重要参数kernel</h3><p>这种变换非常巧妙，但也带有一些实现问题。首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。其次，已知适当的映射函数，我们想要计算类似于Φ(x)<sub>i</sub>* Φ(x<sub>test</sub>)这样的点积，计算量无法巨大。要找出超平面所付出的代价是非常昂贵的。</p><p><strong>关键概念</strong>：核函数</p><p>而解决上面这些问题的数学方式，叫做”核技巧”。是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。具体体现为，K(u,v)= Φ(u)*Φ(v)。而这个原始空间中的点积函数K(u,v)，就被叫做“核函数”。</p><p>核函数能够帮助我们解决三个问题：</p><p>第一，有了核函数之后，我们无需去担心$\Phi$究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数(positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量的点积一定可以被低维空间中的这两个向量的某种计算来表示。</p><p>第二，使用核函数计算低维度中的向量关系比计算原来的Φ(x<sub>i</sub>) * Φ(x<sub>test</sub>)要简单太多了。</p><p>第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。 </p><p>选用不同的核函数，就可以解决不同数据分布下的寻找超平面问题。在SVC中，这个功能由参数“kernel”和一系列与核函数相关的参数来进行控制。之前的代码中我们一直使用这个参数并输入”linear”，但却没有给大家详细讲解，也是因为如果不先理解核函数本身，很难说明这个参数到底在做什么。参数“kernel”在sklearn中可选以下几种选项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt></p><p>可以看出，除了选项”linear”之外，其他核函数都可以处理非线性问题。多项式核函数有次数d，当d为1的时候它就是再处理线性问题，当d为更高次项的时候它就是在处理非线性问题。我们之前画图时使用的是选项“linear”，自然不能处理环形数据这样非线性的状况。而刚才我们使用的计算r的方法，其实是高斯径向基核函数所对应的功能，在参数”kernel“中输入”rbf“就可以使用这种核函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line">X,y = make_circles(<span class="number">100</span>, factor=<span class="number">0.1</span>, noise=<span class="number">.1</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(model,ax=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ax = plt.gca()</span><br><span class="line">    xlim = ax.get_xlim()</span><br><span class="line">    ylim = ax.get_ylim()</span><br><span class="line">    x = np.linspace(xlim[<span class="number">0</span>],xlim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    y = np.linspace(ylim[<span class="number">0</span>],ylim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    Y,X = np.meshgrid(y,x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    ax.contour(X, Y, P,colors=<span class="string">"k"</span>,levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>],alpha=<span class="number">0.5</span>,linestyles=[<span class="string">"--"</span>,<span class="string">"-"</span>,<span class="string">"--"</span>])</span><br><span class="line">    ax.set_xlim(xlim)</span><br><span class="line">    ax.set_ylim(ylim)</span><br><span class="line">    plt.show()</span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">r = np.exp(-(X**<span class="number">2</span>).sum(<span class="number">1</span>))</span><br><span class="line">rlim = np.linspace(min(r),max(r),<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3D</span><span class="params">(elev=<span class="number">30</span>,azim=<span class="number">30</span>,X=X,y=y)</span>:</span></span><br><span class="line">    ax = plt.subplot(projection=<span class="string">"3d"</span>)</span><br><span class="line">    ax.scatter3D(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],r,c=y,s=<span class="number">50</span>,cmap=<span class="string">'rainbow'</span>)</span><br><span class="line">    ax.view_init(elev=elev,azim=azim)</span><br><span class="line">    ax.set_xlabel(<span class="string">"x"</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">"y"</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">"r"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_3D()</span><br><span class="line">clf = SVC(kernel = <span class="string">"rbf"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plot_svc_decision_function(clf)</span><br></pre></td></tr></table></figure><p>运行后，从效果图可以看到，决策边界被完美的找了出来。</p><h3 id="探索核函数在不同数据集上的表现"><a href="#探索核函数在不同数据集上的表现" class="headerlink" title="探索核函数在不同数据集上的表现"></a>探索核函数在不同数据集上的表现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>,<span class="string">"poly"</span>,<span class="string">"rbf"</span>,<span class="string">"sigmoid"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">20</span>,<span class="number">16</span>))</span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line"><span class="comment">#在图像中的第一列，放置原数据的分布</span></span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="comment">#第二层循环：在不同的核函数中循环</span></span><br><span class="line">    <span class="comment">#从图像的第二列开始，一个个填充分类结果</span></span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">    <span class="comment">#定义子图位置</span></span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        <span class="comment">#建模</span></span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        <span class="comment">#绘制图像本身分布的散点图</span></span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">        ,zorder=<span class="number">10</span></span><br><span class="line">        ,cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制支持向量</span></span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">50</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制决策边界</span></span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        <span class="comment">#np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法</span></span><br><span class="line">        <span class="comment">#一次性使用最大值和最小值来生成网格</span></span><br><span class="line">        <span class="comment">#表示为[起始值：结束值：步长]</span></span><br><span class="line">        <span class="comment">#如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        <span class="comment">#np.c_，类似于np.vstack的功能</span></span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        <span class="comment">#填充等高线不同区域的颜色</span></span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        <span class="comment">#绘制等高线</span></span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">        levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#设定坐标轴为不显示</span></span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="comment">#将标题放在第一行的顶上</span></span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        <span class="comment">#为每张图添加分类的分数</span></span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">        , size=<span class="number">15</span></span><br><span class="line">        , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">        <span class="comment">#为分数添加一个白色的格子作为底色</span></span><br><span class="line">        , transform=ax.transAxes <span class="comment">#确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">        , horizontalalignment=<span class="string">'right'</span> <span class="comment">#位于坐标轴的什么方向</span></span><br><span class="line">        )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。 </p><h3 id="选取与核函数相关的参数"><a href="#选取与核函数相关的参数" class="headerlink" title="选取与核函数相关的参数"></a>选取与核函数相关的参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt></p><p>在知道如何选取核函数后，我们还要观察一下除了kernel之外的核函数相关的参数。对于线性核函数，”kernel”是唯一能够影响它的参数，但是对于其他三种非线性核函数，他们还受到参数gamma，degree以及coef0的影响。参数gamma就是表达式中的&gamma;，degree就是多项式核函数的次数d，参数coef0就是常数项&gamma;。其中，高斯径向基核函数受到gamma的影响，而多项式核函数受到全部三个参数的影响。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM54.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用交叉验证得出最好的参数和准确率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">gamma_range = np.logspace(<span class="number">-10</span>,<span class="number">1</span>,<span class="number">20</span>)</span><br><span class="line">coef0_range = np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">param_grid = dict(gamma = gamma_range</span><br><span class="line">,coef0 = coef0_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid = GridSearchCV(svm.SVC(kernel = <span class="string">"poly"</span>,degree=<span class="number">1</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid, cv=cv)</span><br><span class="line">grid.fit(X, y)</span><br><span class="line">print(<span class="string">"The best parameters are %s with a score of %0.5f"</span> % (grid.best_params_,</span><br><span class="line">grid.best_score_))</span><br></pre></td></tr></table></figure><h3 id="重要参数C"><a href="#重要参数C" class="headerlink" title="重要参数C"></a>重要参数C</h3><p><strong>关键概念</strong>：硬件隔与软件隔</p><p>当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在“硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt></p><p>看上图，原来的决策边界&omega;<em>x+b=0，原本的平行于决策边界的两个虚线超平面&omega;\</em>x+b=1和&omega;*x+b=-1都依然有效。我们的原始判别函数是:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM56.png" alt></p><p>不过，这些超平面现在无法让数据上的训练误差等于0了，因为此时存在了一个混杂在红色点中的紫色点。于是，我们需要放松我们原始判别函数中的不等条件，来让决策边界能够适用于我们的异常点。于是我们引入松弛系数Φ来帮助我们优化原始的判别函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM57.png" alt></p><p>其中ζ<sub>i</sub>&gt;0。可以看的出，这其实是将原来的虚线超平面向图像的上方和下方平移。松弛系数其实蛮好理解的，看上图，在红色点附近的蓝色点在原本的判别函数中必定会被分为红色，所有一定会判断错。我们现在做一条与决策边界平行，且过蓝色点的直线&omega;<em>x<sub>i</sub>+b=1- ζ<sub>i</sub>(图中的蓝色虚线)。这条直线是由&omega;*x<sub>i</sub>+b=1平移得到，所以两条直线在纵坐标上的差异就是ζ<sub>i</sub>。而点&omega;x<sub>i</sub>+b=1的距离就可以表示为 ζ<sub>i</sub>\</em>&omega;/||&omega;||，即ζ<sub>i</sub>在&omega;方向上的投影。由于单位向量是固定的，所以ζ<sub>i</sub>可以作为蓝色点在原始的决策边界上的分类错误的程度的表示。隔得越远，分的越错。<strong>注意：ζ<sub>i</sub>并不是点到决策超平面的距离本身。</strong></p><p>不难注意到，我们让&omega;*x<sub>i</sub>+b&gt;=1-ζ<sub>i</sub>作为我们的新决策超平面，是有一定的问题的。虽然我们把异常的蓝色点分类正确了，但我们同时也分错了一系列红色的点。所以我们必须在我们求解最大边际的损失函数中加上一个惩罚项，用来惩罚我们具有巨大松弛系数的决策超平面。我们的拉格朗日函数，拉格朗日对偶函数，也因此都被松弛系数改变。现在，我们的损失函数为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM58.png" alt></p><p>C是用来控制惩罚力度的系数</p><p>我们的拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM59.png" alt></p><p>需要满足的KKT条件为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM60.png" alt></p><p>拉格朗日对偶函数为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM61.png" alt></p><p>sklearn类SVC背后使用的最终公式。公式中现在唯一的新变量，松弛系数的惩罚力度C，由我们的参数C来进行控制。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM62.png" alt></p><p>在实际使用中，C和核函数的相关参数（gamma，degree等等）们搭配，往往是SVM调参的重点。与gamma不同，C没有在对偶函数中出现，并且是明确了调参目标的，所以我们可以明确我们究竟是否需要训练集上的高精确度来调整C的方向。默认情况下C为1，通常来说这都是一个合理的参数。 如果我们的数据很嘈杂，那我们往往减小C。当然，我们也可以使用网格搜索或者学习曲线来调整C的值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">C1_range = np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C2_range=np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C3_range=np.linspace(<span class="number">5</span>,<span class="number">7</span>,<span class="number">50</span>)</span><br><span class="line">param_grid1 = dict(C=C1_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid1 = GridSearchCV(svm.SVC(kernel = <span class="string">"linear"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid1, cv=cv)</span><br><span class="line">grid1.fit(X, y)</span><br><span class="line">print(<span class="string">"linear  The best parameters are %s with a score of %0.5f"</span> % (grid1.best_params_,</span><br><span class="line">grid1.best_score_))</span><br><span class="line">param_grid2=dict(C=C2_range)</span><br><span class="line">grid2 = GridSearchCV(svm.SVC(kernel = <span class="string">"rbf"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid2, cv=cv)</span><br><span class="line">grid2.fit(X, y)</span><br><span class="line">print(<span class="string">"rbf(0.01,30,50)   The best parameters are %s with a score of %0.5f"</span> % (grid2.best_params_,</span><br><span class="line">grid2.best_score_))</span><br><span class="line">param_grid3=dict(C=C3_range)</span><br><span class="line">grid3=GridSearchCV(svm.SVC(kernel=<span class="string">"rbf"</span>,gamma=<span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),param_grid=param_grid3)</span><br><span class="line">grid3.fit(X,y)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_params_)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_score_)</span><br></pre></td></tr></table></figure><h2 id="SVC的参数、属性和接口"><a href="#SVC的参数、属性和接口" class="headerlink" title="SVC的参数、属性和接口"></a>SVC的参数、属性和接口</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM63.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM64.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM65.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM66.png" alt></p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM67.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;非线性SVM与核函数&quot;&gt;&lt;a href=&quot;#非线性SVM与核函数&quot; class=&quot;headerlink&quot; title=&quot;非线性SVM与核函数&quot;&gt;&lt;/a&gt;非线性SVM与核函数&lt;/h2&gt;&lt;h3 id=&quot;SVC在非线性数据上的推广&quot;&gt;&lt;a href=&quot;#SVC在非线性数
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost</title>
    <link href="http://yoursite.com/2019/09/12/XGBoost/"/>
    <id>http://yoursite.com/2019/09/12/XGBoost/</id>
    <published>2019-09-12T14:19:52.000Z</published>
    <updated>2019-09-16T15:04:21.643Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h2><p>在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中和MAC使用pip来安装xgboost的代码： </p><p>windows：</p><p>pip install xgboost #安装xgboost库<br>pip install –upgrade xgboost #更新xgboost库</p><p>我在这步遇到超时报错，查了以下，改成如下安装：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost7.png" alt></p><p>后面看到一些帖子，发现下面这个方法才是真的好用，在C:\Users\湛蓝星空 这个路径下创建一个pip文件夹，在文件夹创建一个txt，将下面内容加入文件里面，再将文件后缀名改为 .ini。</p><p>[global]<br> index-url = <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost50.png" alt></p><p>这其实就是将源改为清华的源，防止被墙。非常管用</p><p>MAC：</p><p>brew install gcc@7<br>pip3 install xgboost </p><p>安装好XGBoost库后，我们有两种方式来使用我们的XGBoost库。第一种方式。是直接使用XGBoost库自己的建模流程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost1.png" alt></p><p>其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。params可能的取值以及xgboost.train的列表：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost2.png" alt></p><p>我们也可以选择第二种方法，使用xgboost库中的sklearn的API：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost3.png" alt></p><p>有人发现，这两种方法的参数是不同的。其实只是写法不同，功能是相同的。使用xgboost中设定的建模流程来建模，和使用sklearnAPI中的类来建模，模型效果是比较相似的，但是xgboost库本身的运算速度（尤其是交叉验证）以及调参手段比sklearn要简单。</p><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的弱评估器，以及应用中的其他过程。</p><h3 id="梯度提升树"><a href="#梯度提升树" class="headerlink" title="梯度提升树"></a>梯度提升树</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost3.png" alt></p><h4 id="Boosting过程"><a href="#Boosting过程" class="headerlink" title="Boosting过程"></a>Boosting过程</h4><p>XGBoost的基础是梯度提升算法，因此我们必须先从了解梯度提升算法开始。梯度提升（Gradient boosting）是构建预测模型的最强大技术之一，它是集成算法中提升法（Boosting）的代表算法。集成算法通过在数据上构建多个弱评估器，汇总所有弱评估器的建模结果，以获取比单个模型更好的回归或分类表现。弱评估器被定义为是表现至少比随机猜测更好的模型，即预测准确率不低于50%的任意模型。 集成不同弱评估器的方法有很多种。有像我们曾经在随机森林的课中介绍的，一次性建立多个平行独立的弱评估器的装袋法。也有像我们今天要介绍的提升法这样，逐一构建弱评估器，经过多次迭代逐渐累积多个弱评估器的方法。提升法的中最著名的算法包括Adaboost和梯度提升树，XGBoost就是由梯度提升树发展而来的。梯度提升树中可以有回归树也可以有分类树，两者都以CART树算法作为主流，XGBoost背后也是CART树，<strong>这意味着XGBoost中所有的树都是二叉的</strong>。</p><p>接下来，我们来了解一些Boosting算法是上面工作：首先，梯度提升回归树是专注于回归的树模型的提升集成模型，其建模过程大致如下：最开始先建立一棵树，然后逐渐迭代，每次迭代过程中都增加一棵树，逐渐形成众多树模型集成的强评估器。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoosting6.png" alt></p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost6.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor <span class="keyword">as</span> RFR</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LinearR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score <span class="keyword">as</span> CVS, train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">reg = XGBR(n_estimators=<span class="number">100</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"预测："</span>,reg.predict(Xtest)) <span class="comment">#传统接口predict</span></span><br><span class="line">print(<span class="string">"准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line">print(<span class="string">"均方误差："</span>,MSE(Ytest,reg.predict(Xtest)))</span><br><span class="line">print(<span class="string">"模型的重要性分数："</span>,reg.feature_importances_)</span><br></pre></td></tr></table></figure><p>使用参数学习曲线观察n_estimators对模型的影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">axisx = range(<span class="number">10</span>,<span class="number">1010</span>,<span class="number">50</span>)</span><br><span class="line">cv = KFold(n_splits=<span class="number">5</span>, shuffle = <span class="literal">True</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=i,random_state=<span class="number">420</span>)</span><br><span class="line">    rs.append(CVS(reg,Xtrain,Ytrain,cv=cv).mean())</span><br><span class="line">print(axisx[rs.index(min(rs))],max(rs))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"red"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="方差与泛化误差"><a href="#方差与泛化误差" class="headerlink" title="方差与泛化误差"></a>方差与泛化误差</h4><p>机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定。其中偏差就是训练集上的拟合程度决定，方差是模型的稳定性决定，噪音是不可控的。而泛化误差越小，模型就越理想。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost8.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">cv=KFold(n_splits=<span class="number">5</span>,shuffle=<span class="literal">True</span>,random_state=<span class="number">42</span>)</span><br><span class="line">axisx = range(<span class="number">100</span>,<span class="number">300</span>,<span class="number">10</span>)</span><br><span class="line">rs = []</span><br><span class="line">var = []</span><br><span class="line">ge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=i,random_state=<span class="number">420</span>)</span><br><span class="line">    cvresult = CVS(reg,Xtrain,Ytrain,cv=cv)</span><br><span class="line">    rs.append(cvresult.mean())</span><br><span class="line">    var.append(cvresult.var())</span><br><span class="line">    ge.append((<span class="number">1</span> - cvresult.mean())**<span class="number">2</span>+cvresult.var())</span><br><span class="line"><span class="comment">#得出最好的n_estimators</span></span><br><span class="line">print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])</span><br><span class="line">print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))</span><br><span class="line">print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))</span><br><span class="line">rs = np.array(rs)</span><br><span class="line">var = np.array(var)*<span class="number">0.01</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"black"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line"><span class="comment">#添加方差线</span></span><br><span class="line">plt.plot(axisx,rs+var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.plot(axisx,rs-var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#泛化误差的可控部分</span></span><br><span class="line">plt.plot(axisx,ge,c=<span class="string">"gray"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从这个过程中观察n_estimators参数对模型的影响，我们可以得出以下结论： </p><p>首先，XGB中的树的数量决定了模型的学习能力，树的数量越多，模型的学习能力越强。只要XGB中树的数量足够 了，即便只有很少的数据， 模型也能够学到训练数据100%的信息，所以XGB也是天生过拟合的模型。但在这种情况 下，模型会变得非常不稳定。</p><p>第二，XGB中树的数量很少的时候，对模型的影响较大，当树的数量已经很多的时候，对模型的影响比较小，只能有 微弱的变化。当数据本身就处于过拟合的时候，再使用过多的树能达到的效果甚微，反而浪费计算资源。当唯一指标 或者准确率给出的n_estimators看起来不太可靠的时候，我们可以改造学习曲线来帮助我们。</p><p> 第三，树的数量提升对模型的影响有极限，开始，模型的表现会随着XGB的树的数量一起提升，但到达某个点之 后，树的数量越多，模型的效果会逐步下降，这也说明了暴力增加n_estimators不一定有效果。<br>这些都和随机森林中的参数n_estimators表现出一致的状态。在随机森林中我们总是先调整n_estimators，当 n_estimators的极限已达到，我们才考虑其他参数，但XGB中的状况明显更加复杂，当数据集不太寻常的时候会更加 复杂。这是我们要给出的第一个超参数，因此还是建议优先调整n_estimators，一般都不会建议一个太大的数目， 300以下为佳。</p><h4 id="subsample"><a href="#subsample" class="headerlink" title="subsample"></a>subsample</h4><p>我们训练模型之前，必然会有一个巨大的数据集。我们都知道树模型是天生过拟合的模型，并且如果数据量太过巨 大，树模型的计算会非常缓慢，因此，我们要对我们的原始数据集进行有放回抽样（bootstrap）。有放回的抽样每 次只能抽取一个样本，若我们需要总共N个样本，就需要抽取N次。每次抽取一个样本的过程是独立的，这一次被抽 到的样本会被放回数据集中，下一次还可能被抽到，因此抽出的数据集中，可能有一些重复的数据。在无论是装袋还是提升的集成算法中，有放回抽样都是我们防止过拟合。</p><p>在sklearn中，我们使用参数subsample来控制我们的随机抽样。在xgb和sklearn中，这个参数都默认为1且不能取到0，所以取值范围是(0,1]。这说明我们无法控制模型是否进行随机有放回抽样，只能控制抽样抽出来的样本量大概是多少。</p><h4 id="eta-or-learning-rate"><a href="#eta-or-learning-rate" class="headerlink" title="eta   or  learning_rate"></a>eta   or  learning_rate</h4><p>在逻辑回归中，我们自定义步长&alpha;来干涉我们的迭代速率，在XGB中看起来却没有这样的设置，但其实不然。在XGB<br>中，我们完整的迭代决策树的公式应该写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost11.png" alt></p><p>其中&eta;读作”eta”，是迭代决策树时的步长（shrinkage），又叫做学习率（learning rate）。和逻辑回归中的&alpha;类似，&eta;<br>越大，迭代的速度越快，算法的极限很快被达到，有可能无法收敛到真正的最佳。&eta;越小，越有可能找到更精确的最<br>佳值，更多的空间被留给了后面建立的树，但迭代速度会比较缓慢。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost12.png" alt></p><p>在sklearn中，我们使用参数learning_rate来干涉我们的学习速率：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost13.png" alt></p><p>梯度提升树是XGB的基础，本节中已经介绍了XGB中与梯度提升树的过程相关的四个参数：n_estimators，learning_rate ，silent，subsample。这四个参数的主要目的，其实并不是提升模型表现，更多是了解梯度提升树的原理。现在来看，我们的梯度提升树可是说是由三个重要的部分组成：</p><ol><li>一个能够衡量集成算法效果的，能够被最优化的损失函数</li><li>一个能够实现预测的弱评估器</li><li>一种能够让弱评估器集成的手段，包括我们讲解的迭代方法，抽样手段，样本加权等等过程 </li></ol><h4 id="booster"><a href="#booster" class="headerlink" title="booster"></a>booster</h4><p>梯度提升算法中不只有梯度提升树，XGB作为梯度提升算法的进化，自然也不只有树模型一种弱评估器。在XGB中，除了树模型，我们还可以选用线性模型，比如线性回归，来进行集成。虽然主流的XGB依然是树模型，但我们也可以使用其他的模型。基于XGB的这种性质，我们有参数“booster”来控制我们究竟使用怎样的弱评估器。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost14.png" alt></p><p>两个参数都默认为”gbtree”，如果不想使用树模型，则可以自行调整。当XGB使用线性模型的时候，它的许多数学过<br>程就与使用普通的Boosting集成非常相似。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> booster <span class="keyword">in</span> [<span class="string">"gbtree"</span>,<span class="string">"gblinear"</span>,<span class="string">"dart"</span>]:</span><br><span class="line">reg = XGBR(<span class="attribute">n_estimators</span>=180,learning_rate=0.1,random_state=420,booster=booster).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="builtin-name">print</span>(booster)</span><br><span class="line"><span class="builtin-name">print</span>(reg.score(Xtest,Ytest))</span><br></pre></td></tr></table></figure><h4 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h4><p>在众多机器学习算法中，损失函数的核心是衡量模型的泛化能力，即模型在未知数据上的预测的准确与否，我们训练模型的核心目标也是希望模型能够预测准确。在XGB中，预测准确自然是非常重要的因素，但我们之前提到过，XGB的是实现了模型表现和运算速度的平衡的算法。普通的损失函数，比如错误率，均方误差等，都只能够衡量模型的表现，无法衡量模型的运算速度。回忆一下，我们曾在许多模型中使用空间复杂度和时间复杂度来衡量模型的运算效率。XGB因此引入了模型复杂度来衡量算法的运算效率。因此XGB的目标函数被写作：传统损失函数 + 模型复杂度。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost15png" alt></p><p>其中i代表数据集中的第i个样本，m表示导入第K棵树的数据总量，K代表建立的所有树(n_estimators)。 第二项代表模型的复杂度，使用树模型的某种变换$\Omega$表示，这个变化代表了一个从树的结构来衡量树模型的复杂度的式子,可以有多种定义。我们在迭代每一颗的过程中，都最小化Obj来求最优的yi。</p><p>在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。一个集成模型(f)<br>在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定，而泛化误差越小，模型就越理想。从下面的图可以看出来，方差和偏差是此消彼长的，并且模型的复杂度越高，方差越大，偏差越小。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest6.png" alt></p><p>方差可以被简单地解释为模型在不同数据集上表现出来地稳定性，而偏差是模型预测的准确度。那方差-偏差困境就<br>可以对应到我们的Obj中了： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost17.png" alt></p><p>第一项是衡量我们的偏差，模型越不准确，第一项就会越大。第二项是衡量我们的方差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。所以我们求解Obj的最小值，其实是在求解方差与偏差的平衡点，以求模型的泛化误差最小，运行速度最快。 </p><p>在应用中，我们使用参数“objective”来确定我们目标函数的第一部分，也就是衡量损失的部分。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost18.png" alt></p><p>xgb自身的调用方式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost1.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#默认reg:linear</span></span><br><span class="line">reg = XGBR(n_estimators=<span class="number">180</span>,random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"sklearn中的XGboost准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line">print(<span class="string">"sklearn中的xgb均方误差："</span>,MSE(Ytest,reg.predict(Xtest)))</span><br><span class="line"><span class="comment">#xgb实现法</span></span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="comment">#使用类Dmatrix读取数据</span></span><br><span class="line">dtrain = xgb.DMatrix(Xtrain,Ytrain)</span><br><span class="line">dtest = xgb.DMatrix(Xtest,Ytest)</span><br><span class="line"><span class="comment">#写明参数，silent默认为False，通常需要手动将它关闭</span></span><br><span class="line">param = &#123;<span class="string">'silent'</span>:<span class="literal">False</span>,<span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="string">"eta"</span>:<span class="number">0.1</span>&#125;</span><br><span class="line">num_round = <span class="number">180</span></span><br><span class="line"><span class="comment">#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment">#接口predict</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">print(<span class="string">"xgb中的准确率："</span>,r2_score(Ytest,bst.predict(dtest)))</span><br><span class="line">print(<span class="string">"xgb中的均方误差："</span>,MSE(Ytest,bst.predict(dtest)))</span><br></pre></td></tr></table></figure><p>看得出来，无论是从R<sup>2</sup>还是从MSE的角度来看，都是xgb库本身表现更优秀。</p><h4 id="alpha-or-reg-alpha-amp-lambda-or-reg-lambda"><a href="#alpha-or-reg-alpha-amp-lambda-or-reg-lambda" class="headerlink" title="(alpha or reg_alpha)  &amp; (lambda or  reg_lambda)"></a>(alpha or reg_alpha)  &amp; (lambda or  reg_lambda)</h4><p>对于XGB来说，每个叶子节点上会有一个预测分数（prediction score），也被称为叶子权重。这个叶子权重就是所有在这个叶子节点上的样本在这一棵树上的回归取值,用f<sub>k</sub>(x<sub>i</sub>)或者&omega;来表示。</p><p>当有多棵树的时候，集成模型的回归结果就是所有树的预测分数之和，假设这个集成模型中总共有K棵决策树，则整<br>个模型在这个样本i上给出的预测结果为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost21.png" alt></p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost22.png" alt></p><p>设一棵树上总共包含了T个叶子节点，其中每个叶子节点的索引为j，则这个叶子节点上的样本权重是w<sub>j</sub>。依据这个，我们定义模型的复杂度$\Omega$(f)为（注意这不是唯一可能的定义，我们当然还可以使用其他的定义，只要满足叶子越多/深度越大，复杂度越大的理论):</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost23.png" alt></p><p>使用L2正则项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost24.png" alt></p><p>使用L1正则项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost25.png" alt></p><p>还可以两个一起用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost26.png" alt></p><p>这个结构中有两部分内容，一部分是控制树结构的&gamma;，另一部分则是我们的正则项。叶子数量<em>T</em>可以代表整个树结构，这是因为在XGBoost中所有的树都是CART树（二叉树），所以我们可以根据叶子的数量<em>T</em>判断出树的深度，而&gamma;是我们自定的控制叶子数量的参数。 至于第二部分正则项，类比一下我们岭回归和Lasso的结构，参数&alpha;和&lambda;的作用其实非常容易理解，他们都是控制正则化强度的参数，我们可以二选一使用，也可以一起使用加大正则化的力度。当 和 都为0的时候，目标函数就是普通的梯度提升树的目标函数。</p><p>来看正则化系数分别对应的参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost20.png" alt></p><h4 id="gamma"><a href="#gamma" class="headerlink" title="gamma"></a>gamma</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost28.png" alt></p><p>回忆一下决策树中我们是如何进行计算：我们使用基尼系数或信息熵来衡量分枝之后叶子节点的不纯度，分枝前的信息熵与分治后的信息熵之差叫做信息增益，信息增益最大的特征上的分枝就被我们选中，当信息增益低于某个阈值时，就让树停止生长。在XGB中，我们使用的方式是类似的：我们首先使用目标函数来衡量树的结构的优劣，然后让树从深度0开始生长，每进行一次分枝，我们就计算目标函数减少了多少，当目标函数的降低低于我们设定的某个阈值时，就让树停止生长。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost27.png" alt></p><p>原理还不是很明白，先贴最后的Gain函数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost29.png" alt></p><p>从上面的Gain函数，从上面的目标函数和结构分数之差Gain的式子来看，&gamma;使我们每增加一片叶子就会被剪去的惩罚项。增加的叶子越多，结构分数之差Gain会被惩罚越重，所以&gamma;也被称为”复杂性控制“。所以&gamma;是我们用来防止过拟合的重要参数。&gamma;是对梯度提升树影响最大的参数之一，其效果不逊色与n_estimators和放过拟合神器max_depth。同时&gamma;还是我们让树停止生长的重要参数。</p><p>在XGB中，规定只要结构分数之差Gain大于0，即只要目标函数还能减小，我们就允许继续进行分枝。也就是说，我们对于目标函数减小量的要求是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost30.png" alt></p><p>因此，我们可以直接通过设定&gamma;的大小让XGB的树停止生长。&gamma;因此被定义为，在树的叶节点上进行进一步分枝所需的最小目标函数减少量，在决策树和随机森林中也有类似的参数(min_split_loss，min_samples_split)。 设定越大，算法就越保守，树的叶子数量就越少，模型的复杂度就越低。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> CVS</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">axisx = np.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">0.05</span>)</span><br><span class="line">rs = []</span><br><span class="line">var = []</span><br><span class="line">ge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=<span class="number">180</span>,random_state=<span class="number">420</span>,gamma=i)</span><br><span class="line">    result = CVS(reg,Xtrain,Ytrain,cv=<span class="number">20</span>)</span><br><span class="line">    rs.append(result.mean())</span><br><span class="line">    var.append(result.var())</span><br><span class="line">    ge.append((<span class="number">1</span> - result.mean())**<span class="number">2</span>+result.var())</span><br><span class="line">print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])</span><br><span class="line">print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))</span><br><span class="line">print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))</span><br><span class="line">rs = np.array(rs)</span><br><span class="line">var = np.array(var)*<span class="number">0.1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"black"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line">plt.plot(axisx,rs+var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.plot(axisx,rs-var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>为了调整&gamma;，我们需要引入新的工具，xgboost库中的类xgboost.cv</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost31.png" alt></p><p>为了使用xgboost.cv，我们必须要熟悉xgboost自带的模型评估指标。xgboost在建库的时候本着大而全的目标，和sklearn类似，包括了大约20个模型评估指标，然而用于回归和分类的其实只有几个，大部分是用于一些更加高级的功能比如ranking。来看用于回归和分类的评估指标都有哪些：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost32.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">data2 = load_breast_cancer()</span><br><span class="line">x2 = data2.data</span><br><span class="line">y2 = data2.target</span><br><span class="line">dfull2 = xgb.DMatrix(x2,y2)</span><br><span class="line">param1 = &#123;<span class="string">'silent'</span>:<span class="literal">True</span>,<span class="string">'obj'</span>:<span class="string">'binary:logistic'</span>,<span class="string">"gamma"</span>:<span class="number">0</span>,<span class="string">"nfold"</span>:<span class="number">5</span>&#125;</span><br><span class="line">param2 = &#123;<span class="string">'silent'</span>:<span class="literal">True</span>,<span class="string">'obj'</span>:<span class="string">'binary:logistic'</span>,<span class="string">"gamma"</span>:<span class="number">2</span>,<span class="string">"nfold"</span>:<span class="number">5</span>&#125;</span><br><span class="line">num_round = <span class="number">100</span></span><br><span class="line">cvresult1 = xgb.cv(param1, dfull2, num_round,metrics=(<span class="string">"error"</span>))</span><br><span class="line">cvresult2 = xgb.cv(param2, dfull2, num_round,metrics=(<span class="string">"error"</span>))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult1.iloc[:,<span class="number">0</span>],c=<span class="string">"red"</span>,label=<span class="string">"train,gamma=0"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult1.iloc[:,<span class="number">2</span>],c=<span class="string">"orange"</span>,label=<span class="string">"test,gamma=0"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult2.iloc[:,<span class="number">0</span>],c=<span class="string">"green"</span>,label=<span class="string">"train,gamma=2"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult2.iloc[:,<span class="number">2</span>],c=<span class="string">"blue"</span>,label=<span class="string">"test,gamma=2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="scale-pos-weight"><a href="#scale-pos-weight" class="headerlink" title="scale_pos_weight"></a>scale_pos_weight</h4><p>XGB中存在着调节样本不平衡的参数scale_pos_weight,这个参数非常类似于之前随机森林和支持向量机中我们都使用到过的class_weight参数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost34.png" alt></p><h4 id="其它参数"><a href="#其它参数" class="headerlink" title="其它参数"></a>其它参数</h4><p>XGBoost应用的核心之一就是减轻过拟合带来的影响。作为树模型，减轻过拟合的方式主要是靠对决策树剪枝来降低模型的复杂度，以求降低方差。在之前的讲解中，我们已经学习了好几个可以用来防止过拟合的参数，包括上一节提到的复杂度控制&lambda;，正则化的两个参数&lambda;和&alpha;，控制迭代速度的参数 以及管理每次迭代前进行的随机有放回抽样的参数subsample。所有的这些参数都可以用来减轻过拟合。但除此之外，我们还有几个影响重大的，专用于剪枝的参数： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost33.png" alt></p><h2 id="使用Pickle保存和调用模型"><a href="#使用Pickle保存和调用模型" class="headerlink" title="使用Pickle保存和调用模型"></a>使用Pickle保存和调用模型</h2><p>pickle是python编程中比较标准的一个保存和调用模型的库，我们可以使用pickle和open函数的连用，来讲我们的模型保存到本地。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型的coding</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">data2 = load_breast_cancer()</span><br><span class="line">x2 = data2.data</span><br><span class="line">y2 = data2.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(x2,y2,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">dtrain = xgb.DMatrix(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#设定参数，对模型进行训练</span></span><br><span class="line">param = &#123;<span class="string">'silent'</span>:<span class="literal">True</span></span><br><span class="line">,<span class="string">'obj'</span>:<span class="string">'reg:linear'</span></span><br><span class="line">,<span class="string">"subsample"</span>:<span class="number">1</span></span><br><span class="line">,<span class="string">"eta"</span>:<span class="number">0.05</span></span><br><span class="line">,<span class="string">"gamma"</span>:<span class="number">20</span></span><br><span class="line">,<span class="string">"lambda"</span>:<span class="number">3.5</span></span><br><span class="line">,<span class="string">"alpha"</span>:<span class="number">0.2</span></span><br><span class="line">,<span class="string">"max_depth"</span>:<span class="number">4</span></span><br><span class="line">,<span class="string">"colsample_bytree"</span>:<span class="number">0.4</span></span><br><span class="line">,<span class="string">"colsample_bylevel"</span>:<span class="number">0.6</span></span><br><span class="line">,<span class="string">"colsample_bynode"</span>:<span class="number">1</span>&#125;</span><br><span class="line">num_round = <span class="number">180</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">pickle.dump(bst, open(<span class="string">"xgboostonboston.dat"</span>,<span class="string">"wb"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#调用模型的coding</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#注意，如果我们保存的模型是xgboost库中建立的模型，则导入的数据类型也必须是xgboost库中的数据类型</span></span><br><span class="line">dtest = xgb.DMatrix(Xtest,Ytest)</span><br><span class="line"><span class="comment">#导入模型</span></span><br><span class="line">loaded_model = pickle.load(open(<span class="string">"xgboostonboston.dat"</span>, <span class="string">"rb"</span>))</span><br><span class="line">print(<span class="string">"Loaded model from: xgboostonboston.dat"</span>)</span><br><span class="line"><span class="comment">#做预测</span></span><br><span class="line">ypreds = loaded_model.predict(dtest)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE, r2_score</span><br><span class="line">print(<span class="string">"均方误差："</span>,MSE(Ytest,ypreds))</span><br><span class="line">print(r2_score(Ytest,ypreds))</span><br></pre></td></tr></table></figure><h2 id="使用Joblib保存和调用模型"><a href="#使用Joblib保存和调用模型" class="headerlink" title="使用Joblib保存和调用模型"></a>使用Joblib保存和调用模型</h2><p>Joblib是SciPy生态系统中的一部分，它为Python提供保存和调用管道和对象的功能，处理NumPy结构的数据尤其高<br>效，对于很大的数据集和巨大的模型非常有用。Joblib与pickle API非常相似 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line">bst = XGBR(n_estimators=<span class="number">200</span>,eta=<span class="number">0.05</span>,gamma=<span class="number">20</span>,reg_lambda=<span class="number">3.5</span>,reg_alpha=<span class="number">0.2</span>,max_depth=<span class="number">4</span>,colsample_bytree=<span class="number">0.4</span>,colsample_bylevel=<span class="number">0.6</span>).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">joblib.dump(bst,<span class="string">"xgboost-boston.dat"</span>)</span><br><span class="line"><span class="comment">#调用模型</span></span><br><span class="line">loaded_model = joblib.load(<span class="string">"xgboost-boston.dat"</span>)</span><br><span class="line"><span class="comment">#这里可以直接导入Xtest</span></span><br><span class="line">ypreds = loaded_model.predict(Xtest)</span><br><span class="line"><span class="keyword">print</span>（MSE(Ytest, ypreds)）</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前沿&quot;&gt;&lt;a href=&quot;#前沿&quot; class=&quot;headerlink&quot; title=&quot;前沿&quot;&gt;&lt;/a&gt;前沿&lt;/h2&gt;&lt;p&gt;在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中的神经网络</title>
    <link href="http://yoursite.com/2019/09/12/sklearn%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/12/sklearn中的神经网络/</id>
    <published>2019-09-12T14:19:10.000Z</published>
    <updated>2019-09-23T07:07:37.466Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h2><p>人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学习的基础。神经网络算法试图模拟生物神经系统的学习过程，以此实现强大的预测性能。不过由于是模仿人类大脑，所以神经网络的模型复杂度很高也是众所周知。在现实应用中，神经网络可以说是解释性最差的模型之一 。</p><p>神经网络原理最开始是基于感知机提出——感知机是最古老的机器学习分类算法之一 。和今天的大部分模型比较起来，感知机的泛化能力比较弱。但支持向量机和神经网络都基于它的原理来建立。感知机的原理在支持向量机中介绍过，使用一条线性决策边界z=&omega;*x+b来划分数据集，决策边界的上方是一类数据(z&gt;=0)，决策边界的下方是另一类数据(z&lt;0)的决策过程。使用神经元表示，如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN1.png" alt></p><p>不同的特征数据被输入后，我们通过神经键将它输入我们的神经元。每条神经键上对应不同的参数&omega;，b。因此特征数据会经由神经键被匹配到一个参数向量&omega;，b。基于参数向量&omega;算法可以求解出决策边界z=&omega;*x+b，然后用决策函数sign(z)进行判断，最终预测出标签y并且将结果输出，其中函数sign(z)被称为”激活函数“。这是模拟人类的大脑激活神经元的过程所命名的，其实本质就是决定了预测标签的输出会是什么内容的预测函数。</p><p>注意：在这个过程中，有三个非常重要的点：</p><p>​    1、每个输入的特征都会匹配到一个参数&omega;，我们都知道参数向量&omega;中含有的参数数量与我们的特征数目是一致的，在感知机中也是如此。也就是说，任何基于感知机的算法，必须至少要有参数向量&omega;可求。</p><p>​    2、一个线性关系z，z是由参数和输入的数据共同决定的。这个线性关系，往往就是我们的决策边界，或者它也可以是多元线性回归，逻辑回归等算法的线性表达式</p><p>​    3、激活函数的结果，是基于激活函数的本身，参数向量&omega;和输入的数据一同计算出来的。也就是说，任何基于感知机的算法。必须要存在一个激活函数。</p><p>神经网络就相当于众多感知机的集成，因此，确定激活函数，并找出参数向量&omega;也是神经网络的计算核心。我们来看看神经网络的基本结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN2.png" alt></p><p>首先，神经网络有三层。第一层叫做输入层（Input layer），输入特征矩阵用，因此每个神经元上都是一个特征向量。极端情况下，如果一个神经网络只训练一个样本，则每个输入层的神经元上都是这个样本的一个特征取值。</p><p>最后一层叫做输出层（output layer），输出预测标签用。如果是回归类，一般输出层只有一个神经元，回归的是所有输入的样本的标签向量。如果是分类，可能会有多个神经元。二分类有两个神经元，多分类有多个神经元，分别输出所有输入的样本对应的每个标签分类下的概率。但无论如何，输出层只有一层，是用于输出预测结果用。</p><p>输入层和输出层中间的所有层，叫做隐藏层（Hidden layers），最少一层。<strong>也就是说整个神经网络是最少三层</strong>。隐藏层是我们用来让算法学习的网络层级，从更靠近输入层的地方开始叫做”上层”，相对而言，更靠近输出层的一层，叫做”下层”。在隐藏层上，每个神经元中都存在一个激活函数，我们的数据被逐层传递，每个下层的神经元都必须处理上层的神经元中的激活函数处理完毕的数据，本质是一个感知器嵌套的过程。隐藏层中上层的每个神经元，都与下层中的每个神经元相连，因此隐藏层的结构随着神经元的变多可以变得非常非常复杂。神经网络的两个要点：参数&omega;和激活函数，也都在这一层发挥作用，因此理解隐藏层是神经网络原理中最难的部分。</p><p><strong>神经网络的每一层的结果之间的关系是嵌套，不是迭代</strong>。由于我们执行的是嵌套，不是迭代。所以<strong>我们的每一个系数之间是相互独立的，每一层的系数之间也是相互独立的</strong>，我们不是在执行使用上一层或者上一个神经元的参数来求解下一层或者下一个神经元的参数的过程。我们不断求解，是激活函数的结果a，不是参数&omega;<strong>。在一次神经网络计算中，我们没有迭代参数&omega;</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN3.png" alt></p><p>上面这张图还不算真实数据中特别复杂的情况，但已经含有总共8*9*9*9*4=23328个&omega;。神经网络可能是我们遇到的最难调参的算法。接下来看看sklearn中的神经网络。</p><h2 id="sklearn中的神经网络"><a href="#sklearn中的神经网络" class="headerlink" title="sklearn中的神经网络"></a>sklearn中的神经网络</h2><p>sklearn是专注于机器学习的库，它在神经网络的模块中特地标注：sklearn不是用于深度学习的平台，因此这个神经网络不具备做深度学习的功能，也不具备处理大型数据的能力。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN4.png" alt></p><h3 id="neural-network-MLPClasifier"><a href="#neural-network-MLPClasifier" class="headerlink" title="neural_network.MLPClasifier"></a>neural_network.MLPClasifier</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN6.png" alt></p><h4 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h4><h5 id="hidden-layer-sizes"><a href="#hidden-layer-sizes" class="headerlink" title="hidden_layer_sizes"></a>hidden_layer_sizes</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN7.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier <span class="keyword">as</span> DNN</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier <span class="keyword">as</span> DTC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">times = time()</span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">100</span>,),random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"dnn的交叉验证："</span>,cv(dnn,X,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">print(<span class="string">"dnn所用的时间："</span>,time() - times)</span><br><span class="line"><span class="comment">#使用决策树进行一个对比</span></span><br><span class="line">times = time()</span><br><span class="line">clf = DTC(random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"决策树的交叉验证："</span>,cv(clf,X,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">print(<span class="string">"决策树所用的时间："</span>,time() - times)</span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">100</span>,),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dnn.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#使用重要参数n_layers_，得出层数</span></span><br><span class="line">print(<span class="string">"n_layers_："</span>,dnn.n_layers_)</span><br><span class="line"><span class="comment">#可见，默认层数是三层，由于必须要有输入和输出层，所以默认其实就只有一层隐藏层</span></span><br><span class="line"><span class="comment">#增加一个隐藏层上的神经元个数</span></span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">200</span>,),random_state=<span class="number">420</span>)</span><br><span class="line">dnn = dnn.fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"增加一个隐藏层的准确率是："</span>,dnn.score(Xtest,Ytest))</span><br><span class="line">s = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">2000</span>,<span class="number">100</span>):</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(int(i),),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">200</span>,<span class="number">2100</span>,<span class="number">100</span>),s)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#增加隐藏层，控制神经元个数</span></span><br><span class="line">s = []</span><br><span class="line">layers = [(<span class="number">100</span>,),(<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),</span><br><span class="line">(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> layers:</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(i),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">3</span>,<span class="number">9</span>),s)</span><br><span class="line">plt.xticks([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Total number of layers"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#增加隐藏层，控制神经元个数</span></span><br><span class="line">s = []</span><br><span class="line">layers = [(<span class="number">100</span>,),(<span class="number">150</span>,<span class="number">150</span>),(<span class="number">200</span>,<span class="number">200</span>,<span class="number">200</span>),(<span class="number">300</span>,<span class="number">300</span>,<span class="number">300</span>,<span class="number">300</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> layers:</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(i),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">3</span>,<span class="number">7</span>),s)</span><br><span class="line">plt.xticks([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Total number of layers"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络概述&quot;&gt;&lt;a href=&quot;#神经网络概述&quot; class=&quot;headerlink&quot; title=&quot;神经网络概述&quot;&gt;&lt;/a&gt;神经网络概述&lt;/h2&gt;&lt;p&gt;人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>降维算法</title>
    <link href="http://yoursite.com/2019/09/12/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/09/12/降维算法/</id>
    <published>2019-09-12T14:18:55.000Z</published>
    <updated>2019-09-15T06:14:16.309Z</updated>
    
    <content type="html"><![CDATA[<p>降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更快，效果更好。对图像来说，维度就是图像中特征向量的数量。</p><h2 id="sklearn中的降维算法"><a href="#sklearn中的降维算法" class="headerlink" title="sklearn中的降维算法"></a>sklearn中的降维算法</h2><p>sklearn中降维算法都被包括在模块decomposition中 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML131.png" alt></p><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>在降维过程中，我们会减少特征的数量，这意味着删除数据，数据量变少则表示模型可以获取的信息会变少，模型的表现可能会因此受影响。同时，在高维数据中，必然有一些特征是不带有有效的信息的（比如噪音），或者有一些特征带有的信息和其他一些特征是重复的（比如一些特征可能会线性相关）。我们希望能够找出一种办法来帮助我们衡量特征上所带的信息量，让我们在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵。 </p><p>在特征过程中，我们有说过一种特别的特征选择方法：方差过滤。如果一个特征的方差很小，则意味着这个特征上很可能有大量取值都相同（比如90%都是1，只有10%是0，甚至100%是1），那这一个特征的取值对样本而言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说明这个特征上带有大量的信息。因此，在降维中，PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML132.png" alt></p><p>Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML133.png" alt></p><p><strong>PCA和特征选择技术都是特征工程的一部分，他们有什么不同？</strong></p><p>特征工程中有三种方式：特征提取，特征创造和特征选择。仔细观察上面的降维例子和上周我们讲解过的特征<br>选择，你发现有什么不同了吗?<br>特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。而PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。以PCA为代表的降维算法因此是特征创造（feature creation，或feature construction）的一种。 </p><h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><p>n_compontents：n_components是我们降维后需要的维度，即降维后需要保留的特征数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">iris = load_iris()</span><br><span class="line">y = iris.target</span><br><span class="line">X = iris.data</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>) <span class="comment">#实例化</span></span><br><span class="line"><span class="comment">#也可以一步到位，x_dr=pca.fit_transform(X)</span></span><br><span class="line">pca = pca.fit(X) <span class="comment">#拟合模型</span></span><br><span class="line">X_dr = pca.transform(X) <span class="comment">#获取新矩阵</span></span><br><span class="line"><span class="string">"""可以写三行代码，也可以写成for循环</span></span><br><span class="line"><span class="string">plt.figure()</span></span><br><span class="line"><span class="string">plt.scatter(X_dr[y==0, 0], X_dr[y==0, 1], c="red", label=iris.target_names[0])</span></span><br><span class="line"><span class="string">plt.scatter(X_dr[y==1, 0], X_dr[y==1, 1], c="black", label=iris.target_names[1])</span></span><br><span class="line"><span class="string">plt.scatter(X_dr[y==2, 0], X_dr[y==2, 1], c="orange", label=iris.target_names[2])</span></span><br><span class="line"><span class="string">plt.legend()</span></span><br><span class="line"><span class="string">plt.title('PCA of IRIS dataset')</span></span><br><span class="line"><span class="string">plt.show()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">colors = [<span class="string">'red'</span>, <span class="string">'black'</span>, <span class="string">'orange'</span>]</span><br><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]:</span><br><span class="line">    plt.scatter(X_dr[y == i, <span class="number">0</span>],X_dr[y == i, <span class="number">1</span>],alpha=<span class="number">.7</span>,c=colors[i],label=iris.target_names[i])</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'PCA of IRIS dataset'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="选择最好的n-components"><a href="#选择最好的n-components" class="headerlink" title="选择最好的n_components"></a>选择最好的n_components</h4><h5 id="累积可解释方差贡献率曲线"><a href="#累积可解释方差贡献率曲线" class="headerlink" title="累积可解释方差贡献率曲线"></a>累积可解释方差贡献率曲线</h5><p>当参数n_components中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于转换了新特征空间，但没有减少特征的个数。一般来说，不会使用这种输入方式。但我们却可以使用这种输入方式来画出累计可解释方差贡献率曲线，以此选择最好的n_components的整数取值。累积可解释方差贡献率曲线是一条以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标的曲线，能够帮助我们决定n_components最好的取值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pca_line = PCA().fit(X)</span><br><span class="line"><span class="comment">#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比</span></span><br><span class="line"><span class="comment">#又叫做可解释方差贡献率</span></span><br><span class="line">plt.plot([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],np.cumsum(pca_line.explained_variance_ratio_))</span><br><span class="line">plt.xticks([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#这是为了限制坐标轴显示为整数</span></span><br><span class="line">plt.xlabel(<span class="string">"number of components after dimension reduction"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cumulative explained variance ratio"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h5 id="最大似然估计自选超参数"><a href="#最大似然估计自选超参数" class="headerlink" title="最大似然估计自选超参数"></a>最大似然估计自选超参数</h5><p>除了输入整数，n_components还有哪些选择呢？让PCA用最大似然估计(maximum likelihood estimation)自选超参数的方法，输入“mle”作为n_components的参数输入，就可以调用这种方法。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pca_mle = PCA(n_components=<span class="string">"mle"</span>)</span><br><span class="line">pca_mle = pca_mle.fit(X)</span><br><span class="line">X_mle = pca_mle.transform(X)</span><br><span class="line"><span class="comment">#从这里看出，mle自动为我们选了几个特征</span></span><br><span class="line">print(X_mle.shape)</span><br></pre></td></tr></table></figure><h5 id="按信息占比选超参数"><a href="#按信息占比选超参数" class="headerlink" title="按信息占比选超参数"></a>按信息占比选超参数</h5><p>输入[0,1]之间的浮点数，并且让参数svd_solver ==’full’，表示希望降维后的总解释性方差占比大于n_components指定的百分比，即是说，希望保留百分之多少的信息量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pca_f = PCA(n_components=<span class="number">0.97</span>,svd_solver=<span class="string">"full"</span>)</span><br><span class="line">pca_f = pca_f.fit(X)</span><br><span class="line">X_f = pca_f.transform(X)</span><br><span class="line"><span class="comment">#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比</span></span><br><span class="line">print(pca_f.explained_variance_ratio_)</span><br></pre></td></tr></table></figure><h3 id="PCA中的SVD"><a href="#PCA中的SVD" class="headerlink" title="PCA中的SVD"></a>PCA中的SVD</h3><p>其实上面的svd_solver是奇异值分解器。其实SVD可以跳过数学，不计算协方差矩阵，直接找出一个新特征向量组成的n维向量。也就是说，奇异值分解可以不计算协方差矩阵等等计算冗长的矩阵，就直接求出新特征空间和降维后的特征矩阵。简而言之，SVD在矩阵分解中的过程比PCA简单快速。</p><h4 id="重要参数-1"><a href="#重要参数-1" class="headerlink" title="重要参数"></a>重要参数</h4><h5 id="svd-solver"><a href="#svd-solver" class="headerlink" title="svd_solver"></a>svd_solver</h5><p>参数svd_solver是在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选：”auto”, “full”, “arpack”,”randomized”，默认”auto”。</p><p>“auto”：基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80％，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生</p><p>“full”：从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时间充足的情况。</p><p>“arpack”：从scipy.sparse.linalg.svds调用ARPACK分解器来运行截断奇异值分解(SVD truncated)，分解时就将特征数量降到n_components中输入的数值k，可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况，此过程包含一定的随机性。</p><p>“randomized”，通过Halko等人的随机方法进行随机SVD。在”full”方法中，分解器会根据原始数据和输入的n_components值去计算和寻找符合需求的新特征向量，但是在”randomized”方法中，分解器会先生成多个随机向量，然后一一去检测这些随机向量中是否有任何一个符合我们的分解需求，如果符合，就保留这个随机向量，并基于这个随机向量来构建后续的向量空间。这个方法已经被Halko等人证明，比”full”模式下计算快很多，并且还能够保证模型运行效果。适合特征矩阵巨大，计算量庞大的情况。</p><h5 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h5><p>参数random_state在参数svd_solver的值为”arpack” or “randomized”的时候生效，可以控制这两种SVD模式中的随机模式。通常我们就选用”auto“，不必对这个参数纠结太多。</p><h3 id="PCA参数、属性和接口"><a href="#PCA参数、属性和接口" class="headerlink" title="PCA参数、属性和接口"></a>PCA参数、属性和接口</h3><h5 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA1.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA2.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA3.png" alt></p><h5 id="属性列表"><a href="#属性列表" class="headerlink" title="属性列表"></a>属性列表</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA4.png" alt></p><h5 id="接口列表"><a href="#接口列表" class="headerlink" title="接口列表"></a>接口列表</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA5.png" alt></p><h2 id="PCA对手写数字数据集的降维"><a href="#PCA对手写数字数据集的降维" class="headerlink" title="PCA对手写数字数据集的降维"></a>PCA对手写数字数据集的降维</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">r"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line">pca_line = PCA().fit(X)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(np.cumsum(pca_line.explained_variance_ratio_))</span><br><span class="line">plt.xlabel(<span class="string">"number of components after dimension reduction"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cumulative explained variance ratio"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>接着降维后维度的学习曲线，继续缩小最佳维度的范围</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">101</span>,<span class="number">10</span>):</span><br><span class="line">    X_dr = PCA(i).fit_transform(X)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">    ,X_dr,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>,<span class="number">10</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>细化学习曲线，找出降维后的最佳维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>,<span class="number">25</span>):</span><br><span class="line">    X_dr = PCA(i).fit_transform(X)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_dr,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">10</span>,<span class="number">25</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>导入找出的最佳维度进行降维，查看模型效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_dr=PCA(<span class="number">23</span>).fit_transform(X)</span><br><span class="line">print(cross_val_score(RFC(n_estimators=<span class="number">100</span>,random_state=<span class="number">0</span>),x_dr,y,cv=<span class="number">5</span>).mean())</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更快，效果更好。对图像来说，维度就是图像中特征向量的数量。&lt;/p&gt;
&lt;h2 id=&quot;sklearn中的降维算法&quot;&gt;&lt;a href=&quot;#sklearn中的降维算法&quot; class=&quot;headerli
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>随机森林</title>
    <link href="http://yoursite.com/2019/09/08/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://yoursite.com/2019/09/08/随机森林/</id>
    <published>2019-09-08T08:10:06.000Z</published>
    <updated>2019-09-15T06:31:00.857Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成算法概述"><a href="#集成算法概述" class="headerlink" title="集成算法概述"></a>集成算法概述</h2><p>集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型建模结果。基本上所有的机器学习领域都可以看到集成学习的身影。</p><p>集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。</p><p>多个模型集成称为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器(base estimator)。通常来说，有三类集成算法：装袋法(Bagging)、提升法(Boosting)、stacking</p><p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结<br>果。装袋法的代表模型就是随机森林。<br>提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本<br>进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树 </p><h2 id="sklearn中的集成算法"><a href="#sklearn中的集成算法" class="headerlink" title="sklearn中的集成算法"></a>sklearn中的集成算法</h2><p>sklearn集成算法模块ensemble</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest1.png" alt></p><h2 id="分类树参数、属性和接口"><a href="#分类树参数、属性和接口" class="headerlink" title="分类树参数、属性和接口"></a>分类树参数、属性和接口</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest2.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h3 id="n-estimators"><a href="#n-estimators" class="headerlink" title="n_estimators"></a>n_estimators</h3><p>这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree5.png" alt></p><h3 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h3><p>随机森林的本质是一种装袋集成算法(bagging)，装袋集成算法是对基评估器的预测结果进行平均或用多数表决元则来决定集成评估器的结果。决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个功能由参数random_state控制。随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树。</p><h4 id="bootstrap-amp-oob-score"><a href="#bootstrap-amp-oob-score" class="headerlink" title="bootstrap&amp;oob_score"></a>bootstrap&amp;oob_score</h4><p>要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。<br>在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。<br><strong>bootstrap参数默认True，代表采用这种有放回的随机抽样技术</strong>。通常，这个参数不会被我们设置为False</p><p>如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果。</p><p>参数的详细解释和其它控制基评估器的参数请参考<a href="[https://brickexperts.github.io/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91/#more](https://brickexperts.github.io/2019/09/07/决策树/#more)">决策树</a></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>随机森林中有三个非常重要的属性：.estimators_，.oob_score_以及.feature_importances_。<br>.estimators_是用来查看随机森林中所有树的列表的。<br>.oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度。<br>而.feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性 </p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。</p><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest8.png" alt></p><p>所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致。 </p><h3 id="criterion："><a href="#criterion：" class="headerlink" title="criterion："></a>criterion：</h3><p>回归树衡量分枝质量的指标，支持的标准有三种：<br>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree1.png" alt></p><p>其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree2.png" alt></p><p>其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 </p><h2 id="随机森林coding："><a href="#随机森林coding：" class="headerlink" title="随机森林coding："></a>随机森林coding：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line">wine = load_wine()</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=<span class="number">0.3</span>)</span><br><span class="line">clf = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">rfc = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">clf = clf.fit(Xtrain,Ytrain)</span><br><span class="line">rfc = rfc.fit(Xtrain,Ytrain)</span><br><span class="line">score_c = clf.score(Xtest,Ytest)</span><br><span class="line">score_r = rfc.score(Xtest,Ytest)</span><br><span class="line">print(<span class="string">"Single Tree:&#123;&#125;"</span>.format(score_c),<span class="string">"Random Forest:&#123;&#125;"</span>.format(score_r))</span><br><span class="line"><span class="comment">#画出随机森林和决策树一组交叉验证的对比</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">25</span>)</span><br><span class="line">rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=<span class="number">10</span>)</span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf_s = cross_val_score(clf,wine.data,wine.target,cv=<span class="number">10</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),rfc_s,label = <span class="string">"RandomForest"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),clf_s,label = <span class="string">"Decision Tree"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#画出随机森林和决策树十组交叉验证的对比</span></span><br><span class="line">rfc_l = []</span><br><span class="line">clf_l = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=<span class="number">25</span>)</span><br><span class="line">    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    rfc_l.append(rfc_s)</span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line">    clf_s = cross_val_score(clf,wine.data,wine.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    clf_l.append(clf_s)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),rfc_l,label = <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),clf_l,label = <span class="string">"Decision Tree"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML38.png" alt></p><h2 id="机器学习中调参的基本思想"><a href="#机器学习中调参的基本思想" class="headerlink" title="机器学习中调参的基本思想"></a>机器学习中调参的基本思想</h2><h3 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h3><p>在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error） </p><p>当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果<br>不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，<br>当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力<br>就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest3.png" alt></p><p>1）模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点<br>2）模型太复杂就会过拟合，模型太简单就会欠拟合<br>3）对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂<br>4）树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest12.png" alt></p><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。<br>偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。<br>方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest4.png" alt></p><p>方差和偏差对模型的影响：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt></p><p>然而，方差和偏差是此消彼长的，不可能同时达到最小值 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest6.png" alt></p><p>从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。<br>相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。 </p><p>我们调参的目标是，达到方差和偏差的完美平衡 ！</p><h2 id="随机森林的调参"><a href="#随机森林的调参" class="headerlink" title="随机森林的调参"></a>随机森林的调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV,cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">rfc=RandomForestClassifier(n_estimators=<span class="number">100</span>,random_state=<span class="number">90</span>)</span><br><span class="line">score_pre=cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">scorel = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">200</span>,<span class="number">10</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i+<span class="number">1</span>,</span><br><span class="line">n_jobs=<span class="number">-1</span>,</span><br><span class="line">random_state=<span class="number">90</span>)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="调n-estimators"><a href="#调n-estimators" class="headerlink" title="调n_estimators"></a>调n_estimators</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从曲线看，n_estimators较平稳且准确率高的范围在35-45之间</span></span><br><span class="line">scorel = []</span><br><span class="line"><span class="comment">#在划分好的范围内，继续细化学习曲线</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">35</span>,<span class="number">45</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i,</span><br><span class="line">    n_jobs=<span class="number">-1</span>,</span><br><span class="line">    random_state=<span class="number">90</span>)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line"><span class="comment">#得出最高准确率的n_estimators，为39</span></span><br><span class="line">print(max(scorel),([*range(<span class="number">35</span>,<span class="number">45</span>)][scorel.index(max(scorel))]))</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">35</span>,<span class="number">45</span>),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="调整max-depth"><a href="#调整max-depth" class="headerlink" title="调整max_depth"></a>调整max_depth</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>)</span><br><span class="line">param_grid=&#123;<span class="string">"max_depth"</span>:np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)&#125;</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line"><span class="comment">#得出最好的深度</span></span><br><span class="line">print(<span class="string">"参数为："</span>,GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><h3 id="调整max-feature"><a href="#调整max-feature" class="headerlink" title="调整max_feature"></a>调整max_feature</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc=RandomForestClassifier(n_estimators=<span class="number">39</span>,max_depth=<span class="number">11</span>,random_state=<span class="number">90</span>)</span><br><span class="line">param_grid=&#123;<span class="string">"max_features"</span>:np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)&#125;</span><br><span class="line">GS=GridSearchCV(rfc,param_grid=param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line"><span class="comment">#得出最好的max_feature</span></span><br><span class="line">print(<span class="string">"参数为："</span>,GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：在这步的max_features升高之后，模型的准确率却没有变化。说明模型本身已经处于泛化误差最低点，已经达到了模型的预测上限，没有参数可以左右的部分了。剩下的那些误差，是噪声决定的，已经没有方差和偏差的舞台了。如果是现实案例，我们到这一步其实就可以停下了，因为复杂度和泛化误差的关系已经告诉我们，模型不能再进步了。调参和训练模型都需要很长的时间，明知道模型不能进步了还继续调整，不是一个有效率的做法。如果我们希望模型更进一步，我们会选择更换算法，或者更换做数据预处理的方式 。但我让我们的探究继续。ps：我不要你觉得，我要我觉得</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_grid=&#123;<span class="string">"min_samples_leaf"</span>:np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,max_depth=<span class="number">11</span>,random_state=<span class="number">90</span>)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>这步后的准确率还是没有变化，不要改变参数，让它默认就好</p><h3 id="调整min-samples-split"><a href="#调整min-samples-split" class="headerlink" title="调整min_samples_split"></a>调整min_samples_split</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">param_grid=&#123;<span class="string">'min_samples_split'</span>:np.arange(<span class="number">2</span>, <span class="number">2</span>+<span class="number">20</span>, <span class="number">1</span>)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>,max_depth=<span class="number">11</span></span><br><span class="line">)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>还是没有变化</p><h3 id="调整criterion"><a href="#调整criterion" class="headerlink" title="调整criterion"></a>调整criterion</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123;<span class="string">'criterion'</span>:[<span class="string">'gini'</span>, <span class="string">'entropy'</span>]&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>,max_depth=<span class="number">11</span>)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>在整个调参过程之中，我们首先调整了n_estimators（无论如何都请先走这一步），然后调整max_depth，通max_depth产生的结果，来判断模型位于复杂度-泛化误差图像的哪一边，从而选择我们应该调整的参数和调参的方向。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;集成算法概述&quot;&gt;&lt;a href=&quot;#集成算法概述&quot; class=&quot;headerlink&quot; title=&quot;集成算法概述&quot;&gt;&lt;/a&gt;集成算法概述&lt;/h2&gt;&lt;p&gt;集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据库的数据模型</title>
    <link href="http://yoursite.com/2019/09/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2019/09/07/数据库的数据模型/</id>
    <published>2019-09-07T11:17:44.000Z</published>
    <updated>2019-09-19T12:51:57.745Z</updated>
    
    <content type="html"><![CDATA[<p>建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。</p><p>由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求：</p><p>1、比较真实地描述现实世界</p><p>2、易为用户所理解</p><p>3、易于在计算机上实现</p><p><strong>为什么需要数据模型？</strong></p><p>由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。</p><p><strong>数据模型含有哪些内容？</strong></p><p>数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。</p><p>​    1、数据结构</p><p>​         用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面</p><p>​    2、数据操作</p><p>​        用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查</p><p>​    3、数据的约束条件</p><p>​        是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。</p><p><strong>实体联系数据模型的地位与作用：</strong></p><p>实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。</p><p>数据模型是用来描述数据的一组概念和定义，是描述数据的手段。</p><p>​    概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。</p><p>​    逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模</p><p>​    物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。</p><p>逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。</p><p>数据模式是对数据结构、联系和约束的描述。<strong>数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。</strong></p><p>信息世界中的基本概念：</p><p>​    (1)  实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。</p><p>​    (2)  属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画</p><p>​    (3)  键(Key)或称为码：唯一标识实体的属性集称为码</p><p>​    (4)  域(Domain)：属性的取值范围称为该属性的域</p><p>​    (5)  实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型</p><p>​    (6)  实体集(Entity Set)：同一类型实体的集合称为实体集</p><p>​    (7)  联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。</p><p>概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。</p><p>​          实体型：用矩形表示，矩形框内写明实体名</p><p>​           属性：用椭圆表示，并用无向边将其与相应的实体连接起来。</p><p>​           联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n）</p><p>​           键：用下划线表示。</p><p>最常用的数据模型</p><p>​    非关系模型</p><p>​        层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。</p><p>​    满足以下两个条件称为层次模型：</p><p>​        (1)  有且仅有一个结点无双亲。这个结点称为“根节点”</p><p>​        (2)  其它节点有且仅有一个双亲，但可以有多个后继</p><p>​        网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。</p><p>​            网状结构特点：</p><p>​                 (1) 允许一个以上的结点无双亲；</p><p>​                 (2)  一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。</p><p>​    关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。</p><p>​        关系数据模型的数据结构：</p><p>​            关系(Relation)：一个关系对应通常说的一张表</p><p>​            元祖(Tuple)：表中的一行即为一个元祖</p><p>​            属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。</p><p>​            主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。</p><p>​            域(Domain)：属性的取值范围</p><p>​            分量：元祖中的一个属性值</p><p>​            关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n)</p><p>关系数据模型的操作主要包括：查询、插入、删除、更新</p><p>关系的完整性约束条件：实体完整性、参照完整性、用户定义完整性</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。&lt;/p&gt;
&lt;p&gt;由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>模型的评判和调优</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2019/09/07/模型的评判和调优/</id>
    <published>2019-09-07T08:11:22.000Z</published>
    <updated>2019-09-16T14:39:05.715Z</updated>
    
    <content type="html"><![CDATA[<h2 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt></p><p>分类模型评估API：F1-score,反应了模型的稳健性</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML53.png" alt></p><h2 id="模型选择和调优"><a href="#模型选择和调优" class="headerlink" title="模型选择和调优"></a>模型选择和调优</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证为了让被评估的模型更加准确可信</p><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。</p><p>sklearn.model_Selection.GridSearchCV</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML54.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML55.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;精确率和召回率&quot;&gt;&lt;a href=&quot;#精确率和召回率&quot; class=&quot;headerlink&quot; title=&quot;精确率和召回率&quot;&gt;&lt;/a&gt;精确率和召回率&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bricke
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K-means</title>
    <link href="http://yoursite.com/2019/09/07/K-means/"/>
    <id>http://yoursite.com/2019/09/07/K-means/</id>
    <published>2019-09-07T08:09:47.000Z</published>
    <updated>2019-09-15T06:40:32.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习与聚类算法"><a href="#无监督学习与聚类算法" class="headerlink" title="无监督学习与聚类算法"></a>无监督学习与聚类算法</h2><p>有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。而聚类算法，就是无监督学习的代表算法。 聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。</p><p>聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要实例化，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML110.png" alt></p><h2 id="KMeans是如何工作的"><a href="#KMeans是如何工作的" class="headerlink" title="KMeans是如何工作的"></a>KMeans是如何工作的</h2><p><strong>关键概念</strong>：簇与质心</p><p>​    簇：KMeans算法将一组N个样本的特征矩阵x划分为k个无交集的簇，直观上来看是簇是一组组聚集在一起的数据，在一个簇中的数据被认为是同一类。簇就是聚类的结果表现。</p><p>​    质心：簇中所有数据的均值通常被称为这个簇的质心。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点纵坐标的均值。同理可推广到高维空间。</p><p>在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下 ：</p><p>​        1、随机抽取k个样本作为最初的质心</p><p>​        2、开始循环</p><p>​                2.1、将每个样本点分配到离他们最近的质心，生成k个簇</p><p>​                2.2、对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心</p><p>​        3、当质心的位置不在发生变化，迭代停止，聚类完成</p><p>那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。</p><p>对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距<br>离的衡量方法有多种，令x表示簇中的一个样本点，ui表示该簇中的质心，n表示每个样本点中的特征数目，i表示组<br>成点x的每个特征，则该样本点到质心的距离可以由以下距离来度量：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML111.png" alt></p><p>如我们采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML112.png" alt></p><p>其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum ofSquare），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。因此,KMeans追求的是，求解能够让Inertia最小化的质心。实际上，在质心不断变化不断迭代的过程中，总体平方和是越来越小的。我们可以使用数学来证明，当整体平方和最小的时候，质心就不再发生变化了。如此，K-Means的求解过程，就变成了一个最优化问题 </p><p>损失函数本质是用来衡量模型的拟合效果的，只有有着求解参数需求的算法，才会有损失函数。Kmeans不求解什么参数，它的模型本质也没有在拟合数据，而是在对数据进行一种探索。所以，<strong>K-Means不存在什么损失函数</strong>。Inertia更像是Kmeans的模型评估指标，而非损失函数。</p><h2 id="重要参数和属性"><a href="#重要参数和属性" class="headerlink" title="重要参数和属性"></a>重要参数和属性</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML113.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果。通常，在开始聚类之前，我们并不知道n_clusters究竟是多少，因此我们要对它进行探索。</p><p>在之前描述K-Means的基本流程时我们提到过，当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前， 我们也可以使用max_iter，大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下 来。有时候，当我们的n_clusters选择不符合数据的自然分布，或者我们为了业务需求，必须要填入与数据的自然 分布不合的n_clusters，提前让迭代停下来反而能够提升模型的表现。</p><p>max_iter：整数，默认300，单次运行的k-means算法的大迭代次数<br>tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML118.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML119.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>labels_：查看聚好的类别，每个样本所对应的类</p><p>cluster_centers_：查看质心</p><p>inertia_：查看总距离平方和</p><p>n_iter_：实际的迭代次数</p><p>coding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#这是自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数</span></span><br><span class="line"><span class="comment">#random_state代表随机性</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line">ax1.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]<span class="comment">#画点图</span></span><br><span class="line">,marker=<span class="string">'o'</span><span class="comment">#代表点的形状</span></span><br><span class="line">,s=<span class="number">8</span>)<span class="comment">#代表点的大小</span></span><br><span class="line"><span class="comment">#最开始数据集的形状</span></span><br><span class="line">plt.show()</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    ax1.scatter(X[y==i, <span class="number">0</span>], X[y==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="comment">#簇为3</span></span><br><span class="line">n_clusters = <span class="number">3</span></span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="comment">#查看聚好的列表</span></span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line">pre = cluster.fit_predict(X)</span><br><span class="line"><span class="comment">#查看质心</span></span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"><span class="comment">#查看总距离平方和</span></span><br><span class="line">inertia = cluster.inertia_</span><br><span class="line">print(inertia)</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ax1.scatter(X[y_pred==i, <span class="number">0</span>], X[y_pred==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line"><span class="comment">#画出质心</span></span><br><span class="line">ax1.scatter(centroid[:,<span class="number">0</span>],centroid[:,<span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">"x"</span></span><br><span class="line">,s=<span class="number">15</span></span><br><span class="line">,c=<span class="string">"black"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#簇为4</span></span><br><span class="line">n_clusters=<span class="number">4</span></span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="comment">#查看聚好的列表</span></span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line">pre = cluster.fit_predict(X)</span><br><span class="line"><span class="comment">#查看质心</span></span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"><span class="comment">#查看总距离平方和</span></span><br><span class="line">inertia = cluster.inertia_</span><br><span class="line">print(inertia)</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ax1.scatter(X[y_pred==i, <span class="number">0</span>], X[y_pred==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line"><span class="comment">#画出质心</span></span><br><span class="line">ax1.scatter(centroid[:,<span class="number">0</span>],centroid[:,<span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">"x"</span></span><br><span class="line">,s=<span class="number">15</span></span><br><span class="line">,c=<span class="string">"black"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="聚类算法的模型评估"><a href="#聚类算法的模型评估" class="headerlink" title="聚类算法的模型评估"></a>聚类算法的模型评估</h2><p>上面的算法的簇随着数字的增大，总的距离和在不断的变小。难道我们我们就这样实验得出簇的个数吗？难道簇越小越好吗？看看算法的评估。聚类模型的结果不是某种标签输出，并且聚类的结果是不确定的，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？ KMeans的目标是确保“簇内差异小，簇外差异大”，我们就可以通过衡量簇内差异来衡量聚类的效果。我们刚才说过，Inertia是用距离来衡量簇内差异的指标，因此，我们是否可以使用Inertia来作为聚类的衡量指标呢？Inertia越小模型越好嘛。可以，但是这个指标的缺点和极限太大。</p><h3 id="当真实标签未知的时候使用轮廓系数"><a href="#当真实标签未知的时候使用轮廓系数" class="headerlink" title="当真实标签未知的时候使用轮廓系数"></a>当真实标签未知的时候使用轮廓系数</h3><p>这样的聚类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中<strong>轮廓系数</strong>是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：<br>1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离<br>2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。单个样本的轮廓系数计算为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML114.png" alt></p><p>这个公式可以看作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML115.png" alt></p><p>很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。</p><p>在sklearn中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML51.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line">print(<span class="string">"4个簇的时候的轮廓系数："</span>,silhouette_score(X,y_pred))</span><br><span class="line">print(<span class="string">"四个簇的每个样本的轮廓系数："</span>,silhouette_samples(X,y_pred))</span><br></pre></td></tr></table></figure><h3 id="当真实标签未知的时候用CHI"><a href="#当真实标签未知的时候用CHI" class="headerlink" title="当真实标签未知的时候用CHI"></a>当真实标签未知的时候用CHI</h3><p>除了轮廓系数是常用的，我们还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML116.png" alt></p><p>在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言，Calinski-Harabaz指数s(k)写作如下公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML117.png" alt></p><p>其中N为数据集中的样本量，k为簇的个数(即类别的个数)，Bk是组间离散矩阵，即不同簇之间的协方差矩阵， Wk是簇内离散矩阵，即一个簇内数据的协方差矩阵，而tr表示矩阵的迹。在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A) 。<strong>数据之间的离散程度越高，协方差矩阵的迹就会越大</strong>。组内离散程度低，协方差的迹就会越小，Tr(Wk)也就越小，同时，组间离散程度大，协方差的的迹也会越大， Tr(Bk)就越大，这正是我们希望的，因此Calinski-harabaz指数越高越好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> calinski_harabaz_score</span><br><span class="line">print(calinski_harabaz_score(X, y_pred))</span><br></pre></td></tr></table></figure><h2 id="基于轮廓系数选择簇的个数"><a href="#基于轮廓系数选择簇的个数" class="headerlink" title="基于轮廓系数选择簇的个数"></a>基于轮廓系数选择簇的个数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数</span></span><br><span class="line"><span class="comment">#random_state代表随机性</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">n_clusters = <span class="number">4</span></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</span><br><span class="line">ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</span><br><span class="line">ax1.set_ylim([<span class="number">0</span>, X.shape[<span class="number">0</span>] + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</span><br><span class="line">clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>).fit(X)</span><br><span class="line">cluster_labels = clusterer.labels_</span><br><span class="line">silhouette_avg = silhouette_score(X, cluster_labels)</span><br><span class="line">print(<span class="string">"For n_clusters ="</span>, n_clusters,</span><br><span class="line"><span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</span><br><span class="line">sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">y_lower = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]</span><br><span class="line">    ith_cluster_silhouette_values.sort()</span><br><span class="line">    size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</span><br><span class="line">    y_upper = y_lower + size_cluster_i</span><br><span class="line">    color = cm.nipy_spectral(float(i)/n_clusters)</span><br><span class="line">    ax1.fill_betweenx(np.arange(y_lower, y_upper)</span><br><span class="line">    ,ith_cluster_silhouette_values</span><br><span class="line">    ,facecolor=color</span><br><span class="line">    ,alpha=<span class="number">0.7</span>)</span><br><span class="line">    ax1.text(<span class="number">-0.05</span></span><br><span class="line">    , y_lower + <span class="number">0.5</span> * size_cluster_i</span><br><span class="line">    , str(i))</span><br><span class="line">    y_lower = y_upper + <span class="number">10</span></span><br><span class="line">ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cluster label"</span>)</span><br><span class="line">ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">ax1.set_yticks([])</span><br><span class="line">ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)</span><br><span class="line">ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">'o'</span> <span class="comment">#点的形状</span></span><br><span class="line">,s=<span class="number">8</span> <span class="comment">#点的大小</span></span><br><span class="line">,c=colors)</span><br><span class="line">centers = clusterer.cluster_centers_</span><br><span class="line">ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], marker=<span class="string">'x'</span>,</span><br><span class="line">c=<span class="string">"red"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</span><br><span class="line">ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</span><br><span class="line">plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></span><br><span class="line"><span class="string">"with n_clusters = %d"</span> % n_clusters),</span><br><span class="line">fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]:</span><br><span class="line">    n_clusters = n_clusters</span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</span><br><span class="line">    ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</span><br><span class="line">    ax1.set_ylim([<span class="number">0</span>, X.shape[<span class="number">0</span>] + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</span><br><span class="line">    clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>).fit(X)</span><br><span class="line">    cluster_labels = clusterer.labels_</span><br><span class="line">    silhouette_avg = silhouette_score(X, cluster_labels)</span><br><span class="line">    print(<span class="string">"For n_clusters ="</span>, n_clusters,</span><br><span class="line">    <span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</span><br><span class="line">    sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">    y_lower = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]</span><br><span class="line">        ith_cluster_silhouette_values.sort()</span><br><span class="line">        size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</span><br><span class="line">        y_upper = y_lower + size_cluster_i</span><br><span class="line">        color = cm.nipy_spectral(float(i)/n_clusters)</span><br><span class="line">        ax1.fill_betweenx(np.arange(y_lower, y_upper)</span><br><span class="line">        ,ith_cluster_silhouette_values</span><br><span class="line">        ,facecolor=color</span><br><span class="line">        ,alpha=<span class="number">0.7</span></span><br><span class="line">        )</span><br><span class="line">        ax1.text(<span class="number">-0.05</span></span><br><span class="line">        , y_lower + <span class="number">0.5</span> * size_cluster_i</span><br><span class="line">        , str(i))</span><br><span class="line">        y_lower = y_upper + <span class="number">10</span></span><br><span class="line">ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cluster label"</span>)</span><br><span class="line">ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">ax1.set_yticks([])</span><br><span class="line">ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)</span><br><span class="line">ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">'o'</span> <span class="comment">#点的形状</span></span><br><span class="line">,s=<span class="number">8</span> <span class="comment">#点的大小</span></span><br><span class="line">,c=colors</span><br><span class="line">)</span><br><span class="line">centers = clusterer.cluster_centers_</span><br><span class="line">ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], marker=<span class="string">'x'</span>,</span><br><span class="line">        c=<span class="string">"red"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</span><br><span class="line">ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</span><br><span class="line">plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></span><br><span class="line"><span class="string">"with n_clusters = %d"</span> % n_clusters),</span><br><span class="line">fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无监督学习与聚类算法&quot;&gt;&lt;a href=&quot;#无监督学习与聚类算法&quot; class=&quot;headerlink&quot; title=&quot;无监督学习与聚类算法&quot;&gt;&lt;/a&gt;无监督学习与聚类算法&lt;/h2&gt;&lt;p&gt;有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/2019/09/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/逻辑回归/</id>
    <published>2019-09-07T08:09:32.000Z</published>
    <updated>2019-09-23T09:15:56.864Z</updated>
    
    <content type="html"><![CDATA[<h2 id="逻辑回归推导过程"><a href="#逻辑回归推导过程" class="headerlink" title="逻辑回归推导过程"></a>逻辑回归推导过程</h2><p>逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以下线性回归。线性回归写作一个人人熟悉的方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic1.png" alt></p><p>&theta;被统称为模型的参数，其中&theta;被称为截距。&theta;<sub>1</sub>—&theta;<sub>n</sub>被称为系数。我们可以用矩阵表示这个方程，其中x和&theta;都可以被看作是一个列矩阵：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic2.png" alt></p><p>线性回归的任务，就是构造一个预测函数z来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心<br>就是找出模型的参数：&theta;的转置矩阵和&theta;0，著名的最小二乘法就是用来求解线性回归中参数的数学方法。</p><p>通过函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务<br>（比如预测产品销量，预测股价等等）。那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型<br>变量，我们要怎么办呢？我们可以通过引入<strong>联系函数(link function)</strong>，将线性回归方程z变换为g(z)，并且令g(z)的值<br>分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分<br>类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic3.png" alt></p><p><strong>Sigmoid函数的公式和性质</strong>：Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1。</p><p>将z带入Sigmoid，得到二元逻辑回归的一般形式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic4.png" alt></p><p>而g(z)就是我们逻辑回归返回的标签值。此时,y(x)的取值都在[0,1]之间，因此 y(x)和1-y(x)相加必然为1。令y(x)除以1-y(x)可以得到形似几率得y(x)/(1-y(x))，在此基础上取对数，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic5.png" alt></p><p>不难发现，g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。</p><p>逻辑回归有着基于训练数据求解参数&theta;的需求，并且希望训练出来的模型能够尽可能地拟合训练数据，即模型在训练集上的预测准确率越靠近100%越好。因此，我们使用”损失函数“这个评估指标，来衡量参数&theta;的优劣，即这一组参数能否使模型在训练集上表现优异。</p><p>​    <strong>关键概念</strong>：损失函数，衡量参数&theta;的优劣的评估指标，用来求解最优参数的工具损失函数小，模型在训练集上表现优异，拟合充分，参数优秀损失函数大，模型在训练集上表现差劲，拟合不足，参数糟糕我们追求，能够让损失函数最小化的参数组合。<strong>注意：没有”求解参数“需求的模型没有损失函数，比如KNN，决策树。</strong> 损失函数只适用于单个训练样本，所以引入了成本函数来。损失函数是衡量单一训练样例的效果。成本函数用于衡量参数w和b的效果。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Logistic34.png" alt></p><p>我们要找到一组w和b是成本函数J最小。</p><p>推导过程：</p><p>既然是最大似然，我们的目标当然是要最大化似然概率：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Logisic32.png" alt></p><p>对于二分类问题有：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Logistic31.png" alt></p><p>用一个式子表示上面这个分段的函数(写成相乘的形式)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Logisic33.png" alt></p><p>如果用hθ(xi)表示p0，1 - hθ(xi)表示p1，将max函数换成min，则得到最终形式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic33.png" alt></p><p>由于我们追求成本函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题：如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。虽然逻辑回归和线性回归是天生欠拟合的模型，但我们还是需要控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。</p><h2 id="逻辑回归的类"><a href="#逻辑回归的类" class="headerlink" title="逻辑回归的类"></a>逻辑回归的类</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic6.png" alt></p><h2 id="linear-model-LogisticRegression"><a href="#linear-model-LogisticRegression" class="headerlink" title="linear_model.LogisticRegression"></a>linear_model.LogisticRegression</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic7.png" alt></p><h3 id="正则化重要参数"><a href="#正则化重要参数" class="headerlink" title="正则化重要参数"></a>正则化重要参数</h3><p>正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic9.png" alt></p><p>其中J(&theta;)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数。j代表每个参数。在这里，J要大于等于1，因为我们的参数向量&theta;中，第一个参数是&theta;0，使我们的截距。它通常不参与正则化。上面的式子还有另一种写法，本质是一样的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic15.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic10.png" alt></p><p>L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），参数 &theta;的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。</p><h3 id="multi-class"><a href="#multi-class" class="headerlink" title="multi_class"></a>multi_class</h3><p>输入”ovr”, “multinomial”, “auto”来告知模型，我们要处理的分类问题的类型。默认是”ovr”。</p><p>‘ovr’(one-vs-rest)：表示分类问题是二分类，或让模型使用”一对多”的形式来处理多分类问题。</p><p>‘multinomial’：表示处理多分类问题，这种输入在参数solver是’liblinear’时不可用。</p><p>“auto”：表示会根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型。比如说，如果数据是二分类，或者solver的取值为”liblinear”，”auto”会默认选择”ovr”。反之，则会选择”nultinomial”。<br><strong>注意：默认值将在0.22版本中从”ovr”更改为”auto”。</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="keyword">for</span> multi_class <span class="keyword">in</span> [<span class="string">'multinomial'</span>, <span class="string">'ovr'</span>]:</span><br><span class="line">    clf = LogisticRegression(solver=<span class="string">'sag'</span>, max_iter=<span class="number">100</span>, random_state=<span class="number">42</span>,</span><br><span class="line">    multi_class=multi_class).fit(iris.data, iris.target)</span><br><span class="line"><span class="comment">#打印两种multi_class模式下的训练分数</span></span><br><span class="line">    print(<span class="string">"training score : %.3f (%s)"</span> % (clf.score(iris.data, iris.target),multi_class))</span><br></pre></td></tr></table></figure><h3 id="solver"><a href="#solver" class="headerlink" title="solver"></a>solver</h3><p>对于小数据集，‘liblinear’是一个不错的选择，而’sag’和’saga’对于大数据集来说更快。对于多类问题，只有’newton-cg’，‘sag’，’saga’和’lbfgs’处理多项损失。‘newton-cg’，’lbfgs’和’sag’只处理L2 penalty，而’liblinear’和’saga’处理L1 penalty。</p><h3 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic19.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic20.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic21.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic22.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic23.png" alt></p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic24.png" alt></p><h2 id="逻辑回归的特征工程"><a href="#逻辑回归的特征工程" class="headerlink" title="逻辑回归的特征工程"></a>逻辑回归的特征工程</h2><h3 id="高效的embedded嵌入法"><a href="#高效的embedded嵌入法" class="headerlink" title="高效的embedded嵌入法"></a>高效的embedded嵌入法</h3><p>我们已经说明了，由于L1正则化会使得部分特征对应的参数为0，因此L1正则化可以用来做特征选择，结合嵌入法的模块SelectFromModel，我们可以很容易就筛选出让模型十分高效的特征。注意，此时我们的目的是，尽量保留原数据上的信息，让模型在降维后的数据上的拟合效果保持优秀，因此我们不考虑训练集测试集的问题，把所有的数据都放入模型进行降维。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">print(data.data.shape)</span><br><span class="line">LR_ = LR(solver=<span class="string">"liblinear"</span>,C=<span class="number">0.9</span>,random_state=<span class="number">420</span>)</span><br><span class="line">print(cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">print(X_embedded.shape)</span><br><span class="line">print(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br></pre></td></tr></table></figure><p>看看结果，特征数量被减小到个位数，并且模型的效果却没有下降太多，如果我们要求不高，在这里其实就可以停下了。但是，能否让模型的拟合效果更好呢？在这里，我们有两种调整方式：<br>1）调节SelectFromModel这个类中的参数threshold，这是嵌入法的阈值，表示删除所有参数的绝对值低于这个阈值的特征。现在threshold默认为None，所以SelectFromModel只根据L1正则化的结果来选择了特征，即选择了所有L1正则化后参数不为0的特征。我们此时，只要调整threshold的值（画出threshold的学习曲线），就可以观察不同的threshold下模型的效果如何变化。一旦调整threshold，就不是在使用L1正则化选择特征，而是使用模型的属性.coef_中生成的各个特征的系数来选择。coef_虽然返回的是特征的系数，但是系数的大小和决策树中的feature_ importances_以及降维算法中的可解释性方差explained_vairance_概念相似，其实都是衡量特征的重要程度和贡献度的，因此SelectFromModel中的参数threshold可以设置为coef_的阈值，即可以剔除系数小于threshold中输入的数字的所有特征。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">fullx = []</span><br><span class="line">fsx = []</span><br><span class="line">threshold = np.linspace(<span class="number">0</span>,abs((LR_.fit(data.data,data.target).coef_)).max(),<span class="number">20</span>)</span><br><span class="line">k=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> threshold:</span><br><span class="line">    X_embedded = SelectFromModel(LR_,threshold=i).fit_transform(data.data,data.target)</span><br><span class="line">    fullx.append(cross_val_score(LR_,data.data,data.target,cv=<span class="number">5</span>).mean())</span><br><span class="line">    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">5</span>).mean())</span><br><span class="line">    print((threshold[k],X_embedded.shape[<span class="number">1</span>]))</span><br><span class="line">    k+=<span class="number">1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(threshold,fullx,label=<span class="string">"full"</span>)</span><br><span class="line">plt.plot(threshold,fsx,label=<span class="string">"feature selection"</span>)</span><br><span class="line">plt.xticks(threshold)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这种方法其实是比较无效的，大家可以用学习曲线来跑一跑：当threshold越来越大，被删除的特征越来越多，模型的效果也越来越差。<br>2）第二种调整方法，是调逻辑回归的类LR_，通过画C的学习曲线来实现： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">fullx = []</span><br><span class="line">fsx = []</span><br><span class="line">C=np.arange(<span class="number">0.01</span>,<span class="number">10.01</span>,<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> C:</span><br><span class="line">    LR_ = LR(solver=<span class="string">"liblinear"</span>,C=i,random_state=<span class="number">420</span>)</span><br><span class="line">    fullx.append(cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">    X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">print(<span class="string">"最好的准确率和对应的index："</span>,max(fsx),C[fsx.index(max(fsx))])</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(C,fullx,label=<span class="string">"full"</span>)</span><br><span class="line">plt.plot(C,fsx,label=<span class="string">"feature selection"</span>)</span><br><span class="line">plt.xticks(C)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#继续细化了学习曲线</span></span><br><span class="line">fullx = []</span><br><span class="line">fsx = []</span><br><span class="line">C=np.arange(<span class="number">6.05</span>,<span class="number">7.05</span>,<span class="number">0.005</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> C:</span><br><span class="line">    LR_ = LR(solver=<span class="string">"liblinear"</span>,C=i,random_state=<span class="number">420</span>)</span><br><span class="line">    fullx.append(cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">    X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">print(max(fsx),C[fsx.index(max(fsx))])</span><br><span class="line">plt.figure(figsize=(<span class="number">40</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(C,fullx,label=<span class="string">"full"</span>)</span><br><span class="line">plt.plot(C,fsx,label=<span class="string">"feature selection"</span>)</span><br><span class="line">plt.xticks(C)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#验证模型效果：降维之前</span></span><br><span class="line">LR_ = LR(solver=<span class="string">"liblinear"</span>,C=<span class="number">6.069999999999999</span>,random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"降维之前的模型效果："</span>,cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line"><span class="comment">#验证模型效果：降维之后</span></span><br><span class="line">LR_ = LR(solver=<span class="string">"liblinear"</span>,C=<span class="number">6.069999999999999</span>,random_state=<span class="number">420</span>)</span><br><span class="line">X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">print(<span class="string">"降维之后的模型效果："</span>,cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">print(X_embedded.shape)</span><br></pre></td></tr></table></figure><h3 id="系数累加法"><a href="#系数累加法" class="headerlink" title="系数累加法"></a>系数累加法</h3><p>系数累加法的原理非常简单。在PCA中，我们通过绘制累积可解释方差贡献率曲线来选择超参数，在逻辑回归中我们可以使用系数coef_来这样做，并且我们选择特征个数的逻辑也是类似的：找出曲线由锐利变平滑的转折点，转折点之前被累加的特征都是我们需要的，转折点之后的我们都不需要。不过这种方法相对比较麻烦，因为我们要先对特征系数进行从大到小的排序，还要确保我们知道排序后的每个系数对应的原始特征的位置，才能够正确找出那些重要的特征。如果要使用这样的方法，不如直接使用嵌入法来得方便。</p><h3 id="包装法"><a href="#包装法" class="headerlink" title="包装法"></a>包装法</h3><p>相对的，包装法可以直接设定我们需要的特征个数。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>之前提到过，逻辑回归的数学目的是求解能够让模型最优化的参数&theta;的值，即求解能够让损失函数最小化的的值，而这个求解过程，对于二元逻辑回归来说，有多种方法可以选择，最常见的有梯度下降法(Gradient Descent)，坐标轴下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。 成本函数就是1/m的损失函数之和。损失函数可以衡量算法的结果，成本函数可以看出参数w和b在训练集上的效果。</p><h3 id="单个样本的梯度下降"><a href="#单个样本的梯度下降" class="headerlink" title="单个样本的梯度下降"></a>单个样本的梯度下降</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923132104.png" alt></p><p>其中，&sigma;就是sigmoid函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923132253.png" alt></p><p>假设某个样本的特征只有 x1 和 x2 ，即(2,1)的列向量： x = [x1, x2]^T, w 是一个(2,1)的列向量， 那么 根据 z = w^T*X + b 可以得到下面的图，其中w1, w2, b 是未知的参数，之后我们会通过训练获得一组最佳的取值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923132543.png" alt></p><p>首先，让我们计算损失函数的导数 dL(a,y)/da 和 da/dz</p><p>不难得到：</p><blockquote><p>da / dz = a * (1 - a)； ①   </p><p>​                da/dz=e^(-z)/(1+e^(-z))^2=(1+e^(-z)-1)/((1+e^(-z))^2)=1/(1+e^(-z))-1/(1+e^(-z))^2=a-a^2=a(1-a)</p><p>dL / da = - y / a + (1 - y) / (1 - a)； ②</p></blockquote><p>根据链式求导法则，从图的右边向左推，得</p><blockquote><p>dz = dL / dz = ② * ① = a - y；</p></blockquote><p>再进一步：</p><blockquote><p>dw1 = dL / dw1 = ( dL / dz ) * ( dz / dw1) = dz * x1 = ( a - y ) * x1</p><p>dw2 = dL / dw2 = ( dL / dz ) * ( dz / dw2) = dz * x2 = ( a - y ) * x2</p><p>db = dL / db = ( dL / dz ) * ( dz / db) = dz * 1 = ( a - y )</p></blockquote><p>然后更新w1, w2, b 的值即可：</p><blockquote><p>w1 = w1 - dw1 * α （α 是学习率），</p><p>w2 = w2 - dw2 * α，</p><p>b = b - α * db；</p></blockquote><p>这样我们就完成了单个样本的参数更新</p><h3 id="多个样本的梯度下降"><a href="#多个样本的梯度下降" class="headerlink" title="多个样本的梯度下降"></a>多个样本的梯度下降</h3><p>先回顾一下成本函数 J(w, b) 的含义, 它是m个样本损失函数求和后的平均值. 注意到公式里有上角标 i ,表示这是第 i 个样本的数据.</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923135611.png" alt></p><p>既然 成本函数 J 是 所有样本的 L 累加后的平均值, <strong>那么 J 对 w1 的导数 等价于 各个 L 对 w1 的导数求和后的平均值</strong>, 同理, 那么 J 对 w2, b 的导数 也是 各个L 对 w2, b 的导数求和后的平均值.</p><p>于是得出各个参数更新过程的<strong>伪代码</strong>：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 初始化变量</span><br><span class="line">J=0,dw_1=0,dw_2=0,<span class="keyword">db</span>=0</span><br><span class="line"># 遍历<span class="keyword">m</span>个数据集</span><br><span class="line"><span class="keyword">for</span> i = 1 to <span class="keyword">m</span> </span><br><span class="line">      z(i) = w^T*x(i)+b</span><br><span class="line">      a(i) = sigmoid(z(i))</span><br><span class="line">      J += -[<span class="built_in">y</span>(i)<span class="built_in">log</span>(a(i))+(1-<span class="built_in">y</span>(i)）<span class="built_in">log</span>(1-a(i))</span><br><span class="line">      dz(i) = a(i)-<span class="built_in">y</span>(i)</span><br><span class="line"># dw1 dw2 <span class="keyword">db</span> 用作累加器, 循环结束后除以<span class="keyword">m</span>即可得到平均值</span><br><span class="line">      dw1 += x1(i)dz(i)</span><br><span class="line">      dw2 += x2(i)dz(i)</span><br><span class="line">      # 假设每个样本只有两个维度, 因此只要累加 dw1, dw2 即可</span><br><span class="line">      # 如果x有<span class="keyword">n</span>个维度 ,就需要使用   <span class="keyword">for</span>循环   遍历x所有的特征,累加 dw3, dw4 .... dwn</span><br><span class="line">      # dw3 += x3(i)dz(i)</span><br><span class="line">      # .......</span><br><span class="line">      # dwn += xn(i)dz(i)</span><br><span class="line">      <span class="keyword">db</span> += dz(i)</span><br><span class="line"> J /= <span class="built_in">m</span></span><br><span class="line"> # 取平均值</span><br><span class="line"> dw1 /= <span class="built_in">m</span></span><br><span class="line"> dw2 /= <span class="built_in">m</span></span><br><span class="line"> <span class="keyword">db</span> /= <span class="built_in">m</span></span><br><span class="line"> # 更新dw1 dw2 <span class="keyword">db</span>， <span class="keyword">alpha</span> 是学习率</span><br><span class="line"> w1 = w1 - <span class="keyword">alpha</span>*dw1</span><br><span class="line"> w2 = w2 - <span class="keyword">alpha</span>*dw2</span><br><span class="line"> b  = b  - <span class="keyword">alpha</span>*<span class="keyword">db</span></span><br></pre></td></tr></table></figure><p>这样, m 个样本的各个参数也就能得到调整了. 但是这部分代码, 仅仅更新了这些参数一次, 需要多次执行, 才能让 J 沿着梯度下降到最低点。这部分代码使用的 for 循环会造成执行效率不高, 如果我们可以把部分数据<strong>向量化</strong>, 利用矩阵乘法代替 for 循环可以大幅度提高程序执行效率。</p><p>在梯度下降过程中，应该有两个for循环。第一个大的for循环代表特征数量，第二个for嵌在第一个for循环里，代表样本数量。两个显式for循环会大大减慢运行速度。为了加快速度，可以通过<strong>向量化</strong>。初始化的时候用np.zeros初始化成零向量，利用np.dot实现&omega;*x。</p><p>向量化我们对代码的改动：</p><p>① 使用 numpy 生成 (n_x, 1) 的列矩阵存放w1, w2, w3..wn , 并初始化为0</p><p>② 利用矩阵的加法，将第 i 个样本 x^(i) 的每个特征全部乘以 dz^(i) 并与 列向量 dw 相加</p><p>③ 利用 numpy 的特性， dw /= m 可以使得 dw 中每个元素都除以 m</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923140632.png" alt></p><p>去掉内层for循环后的<strong>伪代码</strong>：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Z</span> = np.dot(w^T, X) + b</span><br><span class="line"><span class="attr">A</span> = σ(Z)</span><br><span class="line"><span class="attr">dZ</span> = A - Y</span><br><span class="line"><span class="attr">dw</span> = <span class="number">1</span> / m * np.dot(X, dZ^T)</span><br><span class="line"><span class="attr">db</span> = <span class="number">1</span> / m * np.sum( dZ )</span><br><span class="line"><span class="attr">w</span> = w - dw * alpha</span><br><span class="line"><span class="attr">b</span> = b - db * alpha</span><br></pre></td></tr></table></figure><p>这样就完成了m个样本的梯度下降的一次迭代。然而，我们需要多次迭代才能使得成本函数 J 到达最低点，因此 我们仍然需要使用 for 循环，外层for循环暂时还没有办法将其去掉。</p><p><strong>步长的概念与解惑</strong>：</p><p>许多博客和教材在描述步长的时候，声称它是”梯度下降中每一步沿梯度的反方向前进的长度“，”沿着最陡峭最易下山的位置走的那一步的长度“或者”梯度下降中每一步损失函数减小的量“，甚至有说，步长是二维平面著名的求导三角形中的”斜边“或者“对边”的。<strong>这些说法都是错误的</strong></p><p>请看下图，A(&theta;a,J(&theta;a))就是小球最初的位置，B(&theta;b,J(&theta;b))就是一次滚动后小球移动到的位置。从A到B的方向就是梯度向量的反方向，指向损失函数在A点下降最快的方向。而梯度向量的大小是点A在图像上对&theta;求导后的结果，也是点A切线方向的斜率，橙色角的tan结果，记作d。 梯度下降每走一步，损失函数减小的量，是损失函数在&theta;变化之后的取值的变化，写作J(&theta;b)-J(&theta;a)。梯度下降每走一步，参数变量的变化，写作&theta;a-&theta;b，根据我们参数向量的迭代公式，就是我们的步长<em>梯度向量的大小。记作&alpha;\</em>d，这是三角形的邻边。梯度下降中每走一步，也就是三角形中的斜边。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic16.png" alt></p><p><strong>所以，步长不是任何物理距离，它甚至不是梯度下降过程中任何距离的直接变化，它是梯度向量的大小d上的一个比例，影响着参数向量&theta;每次迭代后改变的部分。</strong> </p><p>不难发现，既然参数迭代是靠梯度向量的大小d * 步长&alpha;来实现的，而J(&theta;)的降低又是靠调节&theta;来实现的，所以步长可以调节损失函数下降的速率。在损失函数降低的方向上，步长越长,&theta;的变动就越大。相对的，步长如果很短，&theta;的每次变动就很小。具体地说，如果步长太大，损失函数下降得就非常快，需要的迭代次数就很少，但梯度下降过程可能跳过损失函数的最低点，无法获取最优值。而步长太小，虽然函数会逐渐逼近我们需要的最低点，但迭代的速度却很缓慢，迭代次数就需要很多。 </p><p>在彩色图中，小球在进入深蓝色区域后，并没有直接找到某个点，而是在深蓝色区域中来回震荡了数次才停下，这种”震荡“其实就是因为我们设置的步长太大的缘故。但是在我们开始梯度下降之前，我们并不知道什么样的步长才合适，但梯度下降一定要在某个时候停止才可以，否则模型可能会无限地迭代下去。因此，在sklearn当中，我们设置参数max_iter最大迭代次数来代替步长，帮助我们控制模型的迭代速度并适时地让模型停下。max_iter越大，代表步长越小，模型迭代时间越长，反之，则代表步长设置很大，模型迭代时间很短。 迭代结束，获取到J(&theta;)的最小值后，我们就可以找出这个最小值&theta;对应的参数向量 ，逻辑回归的预测函数也就可以根据这个参数向量&theta;来建立了。 </p><p>接下来的是max_iter的学习曲线coding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">l2 = []</span><br><span class="line">l2test = []</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>):</span><br><span class="line">    lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.9</span>,max_iter=i)</span><br><span class="line">    lrl2 = lrl2.fit(Xtrain,Ytrain)</span><br><span class="line">    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))</span><br><span class="line">    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))</span><br><span class="line">graph = [l2,l2test]</span><br><span class="line">color = [<span class="string">"black"</span>,<span class="string">"gray"</span>]</span><br><span class="line">label = [<span class="string">"L2"</span>,<span class="string">"L2test"</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(graph)):</span><br><span class="line">    plt.plot(np.arange(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>),graph[i],color[i],label=label[i])</span><br><span class="line">plt.legend(loc=<span class="number">4</span>)</span><br><span class="line">plt.xticks(np.arange(<span class="number">1</span>, <span class="number">201</span>, <span class="number">10</span>))</span><br><span class="line">plt.show()</span><br><span class="line">lr = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.9</span>,max_iter=<span class="number">300</span>).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#.n_iter_来调用本次调用本次求解中真正实现的迭代次数</span></span><br><span class="line">print(lr.n_iter_)</span><br></pre></td></tr></table></figure><p>运行上面的代码。会弹出红色警告。因为max_iter中限制的步数已经走完了，逻辑回归却还没找到损失函数的最小值。参数&theta;还没有收敛，sklearn就会弹出警告：</p><p>当参数solver=”liblinear”：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic17.png" alt></p><p>当参数solver=“sag”：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic18.png" alt></p><p>虽然写法看起来略有不同，但其实都是一个含义，这是在提醒我们：参数没有收敛，请增大max_iter中输入的数字。但我们不一定要听sklearn的。max_iter很大，意味着步长小，模型运行得会更加缓慢。虽然我们在梯度下降中追求的是损失函数的最小值，但这也可能意味着我们的模型会过拟合（在训练集上表现得太好，在测试集上却不一定），因此，如果在max_iter报红条的情况下，模型的训练和预测效果都已经不错了，那我们就不需要再增大max_iter中的数目了，毕竟一切都以模型的预测效果为基准——只要最终的预测效果好，运行又快，那就一切都好，无所谓是否报红色警告了 </p><h2 id="coding"><a href="#coding" class="headerlink" title="coding"></a>coding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立两个逻辑回归，L1正则化和L2正则化。效果一目了然</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">data.data.shape</span><br><span class="line">lrl1 = LR(penalty=<span class="string">"l1"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.5</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line">lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.5</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line"><span class="comment">#逻辑回归的重要属性coef_，查看每个特征所对应的参数</span></span><br><span class="line">lrl1 = lrl1.fit(X,y)</span><br><span class="line">lrl1.coef_</span><br><span class="line">(lrl1.coef_ != <span class="number">0</span>).sum(axis=<span class="number">1</span>)</span><br><span class="line">lrl2 = lrl2.fit(X,y)</span><br><span class="line">lrl2.coef_</span><br><span class="line">l1 = []</span><br><span class="line">l2 = []</span><br><span class="line">l1test = []</span><br><span class="line">l2test = []</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0.05</span>,<span class="number">1</span>,<span class="number">19</span>):</span><br><span class="line">    lrl1 = LR(penalty=<span class="string">"l1"</span>,solver=<span class="string">"liblinear"</span>,C=i,max_iter=<span class="number">1000</span>)</span><br><span class="line">    lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=i,max_iter=<span class="number">1000</span>)</span><br><span class="line">    lrl1 = lrl1.fit(Xtrain,Ytrain)</span><br><span class="line">    l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain))</span><br><span class="line">    l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest))</span><br><span class="line">    lrl2 = lrl2.fit(Xtrain,Ytrain)</span><br><span class="line">    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))</span><br><span class="line">    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))</span><br><span class="line">graph = [l1,l2,l1test,l2test]</span><br><span class="line">color = [<span class="string">"green"</span>,<span class="string">"black"</span>,<span class="string">"lightgreen"</span>,<span class="string">"gray"</span>]</span><br><span class="line">label = [<span class="string">"L1"</span>,<span class="string">"L2"</span>,<span class="string">"L1test"</span>,<span class="string">"L2test"</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(graph)):</span><br><span class="line">    plt.plot(np.linspace(<span class="number">0.05</span>,<span class="number">1</span>,<span class="number">19</span>),graph[i],color[i],label=label[i])</span><br><span class="line">plt.legend(loc=<span class="number">4</span>) <span class="comment">#图例的位置在哪里?        4表示，右下角</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML49.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;逻辑回归推导过程&quot;&gt;&lt;a href=&quot;#逻辑回归推导过程&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归推导过程&quot;&gt;&lt;/a&gt;逻辑回归推导过程&lt;/h2&gt;&lt;p&gt;逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归</title>
    <link href="http://yoursite.com/2019/09/07/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/线性回归/</id>
    <published>2019-09-07T08:09:19.000Z</published>
    <updated>2019-09-20T08:34:18.770Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h2><p>回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向<br>量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目<br>标根本不是求解出标签，注意加以区别。</p><p>线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这<br>些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决<br>于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型<br>变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归<br>可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。</p><p>线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个<br>有n个特征的样本i而言，它的回归结果可以写作一个几乎人人熟悉的方程 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png" alt></p><p>&omega;被统称为模型的参数。&omega;0被称为截距&omega;<sub>1</sub>~&omega;<sub>n</sub>被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 X<sub>i1</sub>-X<sub>in</sub>是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML82.png" alt></p><p>我们可以使用矩阵来表示这个方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML83.png" alt></p><p>y=X&omega;，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中&omega;可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量&omega;。</p><p>在多元线性回归中，我们在损失函数如下定义：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line40.png" alt></p><p>在这个平方结果下，我们的真实标签和预测值分别如上图表示，也就是说，这个损失函数是在计算我们的真实标签和预测值之间的距离。因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML85.png" alt></p><p><strong>第一次看不明白上面红字，后面看了书，才明白。这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数</strong>。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。</p><p>现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对&omega;求导。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML88.png" alt></p><p>我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML89.png" alt></p><h2 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h2><p>sklearn中的线性模型模块是linear_model。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML90.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML75.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML76.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML77.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML78.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML79.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML80.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML81.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML74.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML91.png" alt></p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML42.png" alt></p><h2 id="模型评估指标"><a href="#模型评估指标" class="headerlink" title="模型评估指标"></a>模型评估指标</h2><p>回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等<br>评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算<br>法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不<br>同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。<br>第一，我们是否预测到了正确的数值。<br>第二，我们是否拟合到了足够的信息。 </p><p>sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML43.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML45.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML44.png" alt></p><p>​            <strong>注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue=fch()</span><br><span class="line">xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">std_x=StandardScaler()</span><br><span class="line">xtrain=std_x.fit_transform(xtrain)</span><br><span class="line">xtest=std_x.transform(xtest)</span><br><span class="line">std_y=StandardScaler()</span><br><span class="line">ytrain=std_y.fit_transform(ytrain.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">ytest=std_y.transform(ytest.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">reg=LR().fit(xtrain,ytrain)</span><br><span class="line">ypredict=reg.predict(xtest)</span><br><span class="line">print(std_y.inverse_transform(ypredict))</span><br><span class="line">print(<span class="string">"均方误差："</span>,mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))</span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>这里如果我们运行上面的代码，会在第十九行报错：</p><p>我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评<br>判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，<br>会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，<br>所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是<br>neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助:</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML93.png" alt></p><p>在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。</p><p>R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种<br>是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两<br>种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"R2"</span>,r2_score(ypredict,ytest))</span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML94.png" alt></p><p>????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是<strong>预测值在分子，真实值在分母</strong>。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用r2_score</span></span><br><span class="line">print(<span class="string">"R2"</span>,r2_score(y_true=ytest,y_pred=ypredict))</span><br><span class="line"><span class="comment">#利用接口score</span></span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#EVS的两种调用方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> explained_variance_score <span class="keyword">as</span> evs</span><br><span class="line"><span class="comment">#第一种</span></span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"explained_variance"</span>))</span><br><span class="line"><span class="comment">#第二种</span></span><br><span class="line">print(<span class="string">"evs"</span>,evs(ytest,ypredict))</span><br></pre></td></tr></table></figure><h2 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line1.png" alt></p><p>矩阵A中第一行和第三行的关系，被称为“精确相关关系”，即完全相关，一行可使另一行为0。在这种精确相关关系下，矩阵A的行列式为0，则矩阵A的逆不可能存在。在我们的最小二乘法中，如果矩阵中存在这种精确相关关系，则逆不存在，最小二乘法完全无法使用，线性回归会无法求出结果。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line2.png" alt></p><p>矩阵B中第一行和第三行的关系不太一样，他们之间非常接近于”精确相关关系“，但又不是完全相关，一行不能使另一行为0，这种关系被称为”高度相关关系“。在这种高度相关关系下，矩阵的行列式不为0，但是一个非常接近0数，矩阵A的逆存在，不过接近于无限大。在这种情况下，最小二乘法可以使用，不过得到的逆会很大，直接影响我们对参数向量w的求解：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line3.png" alt></p><p>这样求解出来的参数向量w会很大，因此会影响建模的结果，造成模型有偏差或者不可用。精确相关关系和高度相关关系并称为”多重共线性”。在多重共线性下，模型无法建立，或者模型不可用。</p><p>相对的，矩阵C的行之间结果相互独立，梯形矩阵看起来非常正常，它的对角线上没有任何元素特别接近于0，因此其行列式也就不会接近0或者为0，因此矩阵C得出的参数向量w就不会有太大偏差，对于我们拟合而言是比较理想的。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line4.png" alt></p><p>从上面的所有过程我们可以看得出来，<strong>一个矩阵如果要满秩，则要求矩阵中每个向量之间不能存在多重共线性</strong>。这也构成了线性回归算法对于特征矩阵的要求。</p><h3 id="多重共线性与相关性"><a href="#多重共线性与相关性" class="headerlink" title="多重共线性与相关性"></a>多重共线性与相关性</h3><p>多重共线性如果存在，则线性回归就无法使用最小二乘法来进行求解，或者求解就会出现偏差。幸运的是，不能存在<br>多重共线性，不代表不能存在相关性——机器学习不要求特征之间必须独立，必须不相关，只要不是高度相关或者精<br>确相关就好。</p><p><strong>关键概念：</strong>多重共线性与相关性</p><p>多重共线性是一种统计现象，是指线性模型中的特征（解释变量）之间由于存在精确相关关系或高度相关关系，多重共线性的存在会使模型无法建立，或者估计失真。多重共线性使用指标方差膨胀因子（variance inflation factor，VIF）来进行衡量（from statsmodels.stats.outliers_influence import variance_inflation_factor），通常当我们提到“共线性”，都特指多重共线性。</p><p>相关性是衡量两个或多个变量一起波动的程度的指标，它可以是正的，负的或者0。当我们说变量之间具有相关性，通常是指线性相关性，线性相关一般由皮尔逊相关系数进行衡量，非线性相关可以使用斯皮尔曼相关系数或者互信息法进行衡量。 </p><h3 id="处理多重共线性的方法"><a href="#处理多重共线性的方法" class="headerlink" title="处理多重共线性的方法"></a>处理多重共线性的方法</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line5.png" alt></p><p>这三种手段中，第一种相对耗时耗力，需要较多的人工操作，并且会需要混合各种统计学中的知识和检验来进行使<br>用。第二种手段在现实中应用较多，不过由于理论复杂，效果也不是非常高效。我们的核心是使用第三种方法：改进线性回归来处理多重共线性。为此，岭回归、Lasso、弹性网就被研究出来了。</p><h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>在线性模型之中，除了线性回归之外，最知名的就是岭回归与Lasso了。这两个算法非常神秘，他们的原理和应用都不像其他算法那样高调，学习资料也很少。这可能是因为<strong>这两个算法不是为了提升模型表现，而是为了修复漏洞而设计的。</strong></p><p>岭回归在多元线性回归的损失函数上加上了正则项，表达为系数w的L2范式（即系数w的平方项）乘以正则化系数&alpha;。岭回归的损失函数的完整表达式写作：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line6.png" alt></p><h3 id="linear-model-Ridge"><a href="#linear-model-Ridge" class="headerlink" title="linear_model.Ridge"></a>linear_model.Ridge</h3><p>在sklearn中，岭回归由线性模型库中的Ridge类来调用 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line7.png" alt></p><p>和线性回归相比，岭回归的参数稍微多了那么一点点，但是真正核心的参数就是我们的<strong>正则项的系数&alpha;</strong> ，其他的参数是当我们希望使用最小二乘法之外的求解方法求解岭回归的时候才需要的，通常我们完全不会去触碰这些参数。所以,大家只需要了解&alpha;的用法就可以了。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, LinearRegression, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line"><span class="comment">#print(housevalue.data)</span></span><br><span class="line">print(X.head())</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#数据集索引恢复</span></span><br><span class="line">print(Xtest.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [Xtrain,Xtest]:</span><br><span class="line">    i.index = range(i.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#使用岭回归来进行建模</span></span><br><span class="line">reg = Ridge(alpha=<span class="number">1</span>).fit(Xtrain,Ytrain)</span><br><span class="line">reg.score(Xtest,Ytest)</span><br><span class="line">alpharange = np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>)</span><br><span class="line">ridge, lr = [], []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpharange:</span><br><span class="line">    reg = Ridge(alpha=alpha)</span><br><span class="line">    linear = LinearRegression()</span><br><span class="line">    regs = cross_val_score(reg,X,y,cv=<span class="number">5</span>,scoring = <span class="string">"r2"</span>).mean()</span><br><span class="line">    linears = cross_val_score(linear,X,y,cv=<span class="number">5</span>,scoring = <span class="string">"r2"</span>).mean()</span><br><span class="line">    ridge.append(regs)</span><br><span class="line">    lr.append(linears)</span><br><span class="line">plt.plot(alpharange,ridge,color=<span class="string">"red"</span>,label=<span class="string">"Ridge"</span>)</span><br><span class="line">plt.plot(alpharange,lr,color=<span class="string">"orange"</span>,label=<span class="string">"LR"</span>)</span><br><span class="line">plt.title(<span class="string">"Mean"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行上面代码，可以看出，加利佛尼亚数据集上，岭回归的结果轻微上升，随后骤降。可以说，加利佛尼亚房屋价值数据集带有很轻微的一部分共线性，这种共线性被正则化参数 消除后，模型的效果提升了一点点，但是对于整个模型而言是杯水车薪。在过了控制多重共线性的点后，模型的效果飞速下降，显然是正则化的程度太重，挤占了参数 本来的估计空<br>间。从这个结果可以看出，加利佛尼亚数据集的核心问题不在于多重共线性，岭回归不能够提升模型表现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#观察模型方差</span></span><br><span class="line">alpharange = np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>)</span><br><span class="line">ridge, lr = [], []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpharange:</span><br><span class="line">    reg = Ridge(alpha=alpha)</span><br><span class="line">    linear = LinearRegression()</span><br><span class="line">    varR = cross_val_score(reg,X,y,cv=<span class="number">5</span>,scoring=<span class="string">"r2"</span>).var()</span><br><span class="line">    varLR = cross_val_score(linear,X,y,cv=<span class="number">5</span>,scoring=<span class="string">"r2"</span>).var()</span><br><span class="line">    ridge.append(varR)</span><br><span class="line">    lr.append(varLR)</span><br><span class="line">plt.plot(alpharange,ridge,color=<span class="string">"red"</span>,label=<span class="string">"Ridge"</span>)</span><br><span class="line">plt.plot(alpharange,lr,color=<span class="string">"orange"</span>,label=<span class="string">"LR"</span>)</span><br><span class="line">plt.title(<span class="string">"Variance"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以发现，模型的方差上升快速。虽然岭回归和Lasso不是设计来提升模型表现，而是专注于解决多重共线性问题的，但当&alpha;在一定范围内变动的时候，消除多重共线性也许能够一定程度上提高模型的泛化能力。 <strong>不是很明白这句话，后面补</strong></p><h3 id="选取最佳的正则化参数"><a href="#选取最佳的正则化参数" class="headerlink" title="选取最佳的正则化参数"></a>选取最佳的正则化参数</h3><p>既然要选择&alpha;的范围，我们就不可避免的进行最优参数的选择。在各种机器学习教材中，总是教导使用岭迹图来判断正则项参数的最佳取值。传统的岭迹图长成下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line8.png" alt></p><p>这一个以正则化参数为横坐标，线性模型求解的系数&alpha;为纵坐标的图像，其中每一条彩色的线都是一个系数&alpha;。其目标是建立正则化参数与系数&alpha;之间的直接关系，以此来观察正则化参数的变化如何影响了系数w的拟合。岭迹图认为，线条交叉越多，则说明特征之间的多重共线性越高。我们应该选择系数较为平稳的喇叭口所对应的&alpha;取值作为最佳的正则化参数的取值。绘制岭迹图的方法非常简单，代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="comment">#创造10*10的希尔伯特矩阵</span></span><br><span class="line">X = <span class="number">1.</span> / (np.arange(<span class="number">1</span>, <span class="number">11</span>) + np.arange(<span class="number">0</span>, <span class="number">10</span>)[:, np.newaxis])</span><br><span class="line">y = np.ones(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#计算横坐标</span></span><br><span class="line">n_alphas = <span class="number">200</span></span><br><span class="line">alphas = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, n_alphas)</span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alphas:</span><br><span class="line">    ridge = linear_model.Ridge(alpha=a, fit_intercept=<span class="literal">False</span>)</span><br><span class="line">    ridge.fit(X, y)</span><br><span class="line">    coefs.append(ridge.coef_)</span><br><span class="line"><span class="comment">#绘图展示结果</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.plot(alphas, coefs)</span><br><span class="line">ax.set_xscale(<span class="string">'log'</span>)</span><br><span class="line">ax.set_xlim(ax.get_xlim()[::<span class="number">-1</span>]) <span class="comment">#将横坐标逆转</span></span><br><span class="line">plt.xlabel(<span class="string">'正则化参数alpha'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'系数w'</span>)</span><br><span class="line">plt.title(<span class="string">'岭回归下的岭迹图'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>我非常不建议大家使用岭迹图来作为寻找最佳参数的标准。</strong><br>有这样的两个理由：</p><p>1、岭迹图的很多细节，很难以解释。比如为什么多重共线性存在会使得线与线之间有很多交点？当&alpha;很大了之后看上去所有的系数都很接近于0，难道不是那时候线之间的交点最多吗？</p><p>2、岭迹图的评判标准，非常模糊。哪里才是最佳的喇叭口？哪里才是所谓的系数开始变得”平稳“的时候？一千个读者一千个哈姆雷特的画像？未免也太不严谨了</p><p>我们应该使用交叉验证来选择最佳的正则化系数。在sklearn中，我们有带交叉验证的岭回归可以使用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line9.png" alt></p><p>RidgeCV的重要参数、属性和接口：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line10.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV, LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Ridge_ = RidgeCV(alphas=np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>),store_cv_values=<span class="literal">True</span>).fit(X, y)</span><br><span class="line"><span class="comment">#无关交叉验证的岭回归结果</span></span><br><span class="line">print(<span class="string">"没有交叉验证的岭回归："</span>,Ridge_.score(X,y))</span><br><span class="line"><span class="comment">#调用所有交叉验证的结果</span></span><br><span class="line">print(<span class="string">"调用所有交叉验证："</span>,Ridge_.cv_values_.shape)</span><br><span class="line"><span class="comment">#进行平均后可以查看每个正则化系数取值下的交叉验证结果</span></span><br><span class="line">print(<span class="string">"平均后的每个正则化系数交叉验证结果："</span>,max(Ridge_.cv_values_.mean(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="comment">#查看被选择出来的最佳正则化系数</span></span><br><span class="line">print(<span class="string">"最佳正则化系数："</span>,Ridge_.alpha_)</span><br></pre></td></tr></table></figure><h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><p>除了岭回归之外，最常被人们提到还有模型Lasso。Lasso全称最小绝对收缩和选择算子（least absolute shrinkage and selection operator），由于这个名字过于复杂所以简称为Lasso。和岭回归一样，Lasso是被创造来作用于多重共线性问题的算法，不过Lasso使用的是系数w的L1范式（L1范式则是系数w的绝对值）乘以正则化系数&alpha; ，所以,Lasso的损失函数表达式为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line11.png" alt></p><p><strong>岭回归VSLasso</strong>：岭回归可以解决特征间的精确相关关系导致的最小二乘法无法使用的问题，而Lasso不行。</p><p><strong>Lasso不是从根本上解决多重共线性问题，而是限制多重共线性带来的影响。</strong></p><h3 id="Lasso的核心作用：特征选择"><a href="#Lasso的核心作用：特征选择" class="headerlink" title="Lasso的核心作用：特征选择"></a>Lasso的核心作用：特征选择</h3><p>sklearn中我们使用类Lasso来调用lasso回归，众多参数中我们需要比较在意的就是参数&alpha; ，正则化系数。另外需要注意的就是参数positive。当这个参数为”True”的时候，是我们要求Lasso回归出的系数必须为正数，以此来保证我们的&alpha;一定以增大来控制正则化的程度。 需要注意的是，在sklearn中我们的Lasso使用的损失函数是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line14.png" alt></p><p>红色框框住的只是作为系数存在。用来消除我们对损失函数求导后多出来的那个2的（求解w时所带的1/2），然后对整体的RSS求了一个平均而已，无论时从损失函数的意义来看还是从Lasso的性质和功能来看，这个变化没有造成任何影响，只不过计算上会更加简便一些。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, LinearRegression, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#恢复索引</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [Xtrain,Xtest]:</span><br><span class="line">    i.index = range(i.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#线性回归进行拟合</span></span><br><span class="line">reg = LinearRegression().fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"线性回归："</span>,(reg.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#岭回归进行拟合</span></span><br><span class="line">Ridge_ = Ridge(alpha=<span class="number">0</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"岭回归："</span>,(Ridge_.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#Lasso进行拟合</span></span><br><span class="line">lasso_ = Lasso(alpha=<span class="number">0</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"Lasso"</span>,(lasso_.coef_*<span class="number">100</span>).tolist())</span><br></pre></td></tr></table></figure><p>可以看到，岭回归没有报出错误，但Lasso就不一样了，虽然依然对系数进行了计算，但是报出了整整三个警告：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line15.png" alt></p><p>这三条分别是这样的内容：</p><p>​    1、正则化系数为0，这样算法不可收敛！如果你想让正则化系数为0，请使用线性回归吧</p><p>​    2、没有正则项的坐标下降法可能会导致意外的结果，不鼓励这样做！</p><p>​    3、目标函数没有收敛，你也许想要增加迭代次数，使用一个非常小的alpha来拟合模型可能会造成精确度问题！ </p><p>sklearn不推荐我们使用0这样的正则化系数。如果我们的确希望取到0，那我们可以使用一个比较很小的数，比如0.01这样的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#岭回归拟合</span></span><br><span class="line">Ridge_ = Ridge(alpha=<span class="number">0.01</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print((Ridge_.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#Lasso进行拟合</span></span><br><span class="line">lasso_ = Lasso(alpha=<span class="number">0.01</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print((lasso_.coef_*<span class="number">100</span>).tolist())</span><br></pre></td></tr></table></figure><p>这样就不会报任何警告了。</p><h3 id="选取最佳的正则化"><a href="#选取最佳的正则化" class="headerlink" title="选取最佳的正则化"></a>选取最佳的正则化</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line16.png" alt></p><p><img src="C:%5CUsers%5C%E6%B9%9B%E8%93%9D%E6%98%9F%E7%A9%BA%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1568183504767.png" alt="1568183504767"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">housevalue=fch()</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#自己建立Lasso进行alpha选择的范围</span></span><br><span class="line">alpharange = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, <span class="number">200</span>,base=<span class="number">10</span>)</span><br><span class="line">lasso_ = LassoCV(alphas=alpharange <span class="comment">#自行输入的alpha的取值范围</span></span><br><span class="line">,cv=<span class="number">5</span> <span class="comment">#交叉验证的折数</span></span><br><span class="line">).fit(Xtrain, Ytrain)</span><br><span class="line"><span class="comment">#查看被选择出来的最佳正则化系数</span></span><br><span class="line">print(<span class="string">"最佳的正则化系数："</span>,lasso_.alpha_)</span><br><span class="line"><span class="comment">#调用所有交叉验证的结果</span></span><br><span class="line">print(<span class="string">"所有交叉验证的结果："</span>,lasso_.mse_path_)</span><br><span class="line">print(lasso_.mse_path_.shape) <span class="comment">#返回每个alpha下的五折交叉验证结果</span></span><br><span class="line">print(lasso_.mse_path_.mean(axis=<span class="number">1</span>)) <span class="comment">#有注意到在岭回归中我们的轴向是axis=0吗？</span></span><br><span class="line"><span class="comment">#在岭回归当中我们的交叉验证结果返回的是，每一个样本在每个alpha下的交叉验证结果</span></span><br><span class="line"><span class="comment">#因此我们要求每个alpha下的交叉验证均值，就是axis=0，跨行求均值</span></span><br><span class="line"><span class="comment">#而在这里，我们返回的是，每一个alpha取值下，每一折交叉验证的结果</span></span><br><span class="line"><span class="comment">#因此我们要求每个alpha下的交叉验证均值，就是axis=1，跨列求均值</span></span><br><span class="line"><span class="comment">#最佳正则化系数下获得的模型的系数结果</span></span><br><span class="line">print(<span class="string">"最佳正则化系数获得的模型结果："</span>,lasso_.coef_)</span><br><span class="line">print(<span class="string">"最佳正则化系数的准确率："</span>,lasso_.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#与线性回归相比如何？</span></span><br><span class="line">reg = LinearRegression().fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"线性回归的准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#使用lassoCV自带的正则化路径长度和路径中的alpha个数来自动建立alpha选择的范围</span></span><br><span class="line">ls_ = LassoCV(eps=<span class="number">0.00001</span></span><br><span class="line">,n_alphas=<span class="number">300</span></span><br><span class="line">,cv=<span class="number">5</span></span><br><span class="line">).fit(Xtrain, Ytrain)</span><br><span class="line">print(ls_.alpha_)</span><br><span class="line">print(ls_.alphas_) </span><br><span class="line">print(ls_.alphas_.shape)</span><br><span class="line">print(ls_.score(Xtest,Ytest))</span><br><span class="line">print(ls_.coef_)</span><br></pre></td></tr></table></figure><p>模型效果上表现和普通的Lasso没有太大的区别，不过他们都在各个方面对原有的Lasso做了一些相应的改进（比如说提升了本来就已经很快的计算速度，增加了模型选择的维度，因为均方误差作为损失函数只考虑了偏差，不考虑方差的在。除了解决多重共线性这个核心问题之外，<strong>线性模型还有更重要的事情要做：提升模型表现</strong>。这才是机器学习最核心的需求，而Lasso和岭回归不是为此而设计的。<strong>为了提升模型表现而做出的改进：多项式回归</strong>。 </p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>首先，“线性”这个词用于描述不同事物时有着不同的含义，我们最常使用的线性是指“变量之间的线性关系(linear relationship)”。它表示两个变量之间的关系可以展示为一条直线，即可以使用方程y=ax+b来进行拟合。要探索两个变量之间的关系是否线性，最简单的方式就是绘制散点图，如果散点图能够相对均匀地分布在一条直线的两端，则说明这两个变量之间的关系是线性的。从线性关系这个概念出发，我们有了一种说法叫做“线性数据”。通常来说，一组数据由多个特征和标签组成。当这些特征分别与标签存在线性关系的时候，我们就说这一组数据是线性数据。</p><p>但当我们在进行分类的时候，我们的数据分布往往是这样的：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line19.png" alt></p><p>这些数据都不能由一条直线来进行拟合，他们也没有均匀分布在某一条线的周围，那我们怎么判断，这些数据是线性数据还是非线性数据呢？在这里就要注意了，当我们在回归中绘制图像时，绘制的是特征与标签的关系图，横坐标是特征，纵坐标是标签，我们的标签是连续型的，所以我们可以通过是否能够使用一条直线来拟合图像判断数据究竟属于线性还是非线性。然而在分类中，我们绘制的是数据分布图，横坐标是其中一个特征，纵坐标是另一个特征，标签则是数据点的颜色。因此在分类数据中，我们使用“是否线性可分”（linearly separable）这个概念来划分分类数据集。<strong>当分类数据的分布上可以使用一条直线来将两类数据分开时，我们就说数据是线性可分的。反之，数据不是线性可分的。</strong> ps：上面那张图我也不知道是不是线性可分的，大家知道以下这个概念就好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">rnd = np.random.RandomState(<span class="number">42</span>) <span class="comment">#设置随机数种子</span></span><br><span class="line">X = rnd.uniform(<span class="number">-3</span>, <span class="number">3</span>, size=<span class="number">100</span>) <span class="comment">#random.uniform，从输入的任意两个整数中取出size个随机数</span></span><br><span class="line"><span class="comment">#生成y的思路：先使用NumPy中的函数生成一个sin函数图像，然后再人为添加噪音</span></span><br><span class="line">y = np.sin(X) + rnd.normal(size=len(X)) / <span class="number">3</span> <span class="comment">#random.normal，生成size个服从正态分布的随机数</span></span><br><span class="line"><span class="comment">#使用散点图观察建立的数据集是什么样子</span></span><br><span class="line">plt.scatter(X, y,marker=<span class="string">'o'</span>,c=<span class="string">'k'</span>,s=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#为后续建模做准备：sklearn只接受二维以上数组作为特征矩阵的输入</span></span><br><span class="line">X = X.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#使用原始数据进行建模</span></span><br><span class="line">LinearR = LinearRegression().fit(X, y)</span><br><span class="line">TreeR = DecisionTreeRegressor(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line"><span class="comment">#放置画布</span></span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#创建测试数据：一系列分布在横坐标上的点</span></span><br><span class="line">line = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="literal">False</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#将测试数据带入predict接口，获得模型的拟合效果并进行绘制</span></span><br><span class="line">ax1.plot(line, LinearR.predict(line), linewidth=<span class="number">2</span>, color=<span class="string">'green'</span>,</span><br><span class="line">label=<span class="string">"linear regression"</span>)</span><br><span class="line">ax1.plot(line, TreeR.predict(line), linewidth=<span class="number">2</span>, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">"decision tree"</span>)</span><br><span class="line"><span class="comment">#将原数据上的拟合绘制在图像上</span></span><br><span class="line">ax1.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line"><span class="comment">#其他图形选项</span></span><br><span class="line">ax1.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Regression output"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"Input feature"</span>)</span><br><span class="line">ax1.set_title(<span class="string">"Result before discretization"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从图像上可以看出，线性回归无法拟合出这条带噪音的正弦曲线的真实面貌，只能够模拟出大概的趋势，而决策树却<br>通过建立复杂的模型将几乎每个点都拟合出来了。可见，使用线性回归模型来拟合非线性数据的效果并不好，而决策<br>树这样的模型却拟合得太细致，相比之下，还是决策树的拟合效果更好一些。<strong>线性模型可以用来拟合非线性数据，而非线性模型也可以用来拟合线性数据，更神奇的是，有的算法没有模型也可以处理各类数据，而有的模型可以既可以是线性，也可以是非线性模型。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line20.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归原理&quot;&gt;&lt;a href=&quot;#线性回归原理&quot; class=&quot;headerlink&quot; title=&quot;线性回归原理&quot;&gt;&lt;/a&gt;线性回归原理&lt;/h2&gt;&lt;p&gt;回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向&lt;br&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>模型的保存和加载</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/"/>
    <id>http://yoursite.com/2019/09/07/模型的保存和加载/</id>
    <published>2019-09-07T08:09:05.000Z</published>
    <updated>2019-09-15T06:29:51.490Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML40.png" alt></p><p>joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。</p><p>joblib.load()：读取模型。参数是模型的目录</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML56.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;模型的保存和加载&quot;&gt;&lt;a href=&quot;#模型的保存和加载&quot; class=&quot;headerlink&quot; title=&quot;模型的保存和加载&quot;&gt;&lt;/a&gt;模型的保存和加载&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Br
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2019/09/07/决策树/</id>
    <published>2019-09-07T08:05:29.000Z</published>
    <updated>2019-09-15T06:40:08.859Z</updated>
    
    <content type="html"><![CDATA[<h2 id="决策树推导"><a href="#决策树推导" class="headerlink" title="决策树推导"></a>决策树推导</h2><p>首先看看下面这组数据集：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML96.png" alt></p><p>得出下面这颗决策树：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML97.png" alt></p><p><strong>关键概念</strong>：</p><p>​    信息熵公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML34.png" alt></p><p>​    信息增益公式：就是熵和特征条件熵的差</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML35.png" alt></p><p>​    随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好</p><p>决策树算法的需要解决的核心问题：</p><p>​    1、如何从数据表中找出最佳节点和最佳分支？</p><p>​    2、如何让决策树停止生长，防止过拟合？</p><p>决策树的基本过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML101.png" alt></p><p>直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。</p><h2 id="决策树五大模块"><a href="#决策树五大模块" class="headerlink" title="决策树五大模块"></a>决策树五大模块</h2><p>sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML98.png" alt></p><h2 id="分类树参数、属性和接口"><a href="#分类树参数、属性和接口" class="headerlink" title="分类树参数、属性和接口"></a>分类树参数、属性和接口</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML102.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h4><p>为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标<br>叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心<br>大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 </p><p>criterion这个参数正是用来决定不纯度的计算方法的：</p><p>​    1、输入”entropy“，使用信息熵</p><p>​    2、输入”gini“，使用基尼系数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML100.png" alt></p><p>其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵<br>时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。</p><h4 id="random-state-amp-splitter"><a href="#random-state-amp-splitter" class="headerlink" title="random_state&amp;splitter"></a>random_state&amp;splitter</h4><p>random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据<br>（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。<br>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会<br>优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在<br>分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这<br>也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能<br>性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 </p><h4 id="max-depth"><a href="#max-depth" class="headerlink" title="max_depth"></a>max_depth</h4><p>限制树的最大深度，超过设定深度的树枝全部剪掉<br>这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所<br>以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效<br>果再决定是否增加设定深度。</p><h4 id="min-samples-leaf-amp-min-samples-split"><a href="#min-samples-leaf-amp-min-samples-split" class="headerlink" title="min_samples_leaf&amp;min_samples_split"></a>min_samples_leaf&amp;min_samples_split</h4><p>min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。<br>min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则<br>分枝就不会发生。 </p><h4 id="max-features-amp-min-impurity-decrease"><a href="#max-features-amp-min-impurity-decrease" class="headerlink" title="max_features&amp;min_impurity_decrease"></a>max_features&amp;min_impurity_decrease</h4><p>max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。<br>min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的<br>功能，在0.19版本之前时使用min_impurity_split。 </p><h4 id="class-weight-amp-min-weight-fraction-leaf"><a href="#class-weight-amp-min-weight-fraction-leaf" class="headerlink" title="class_weight&amp;min_weight_fraction_leaf"></a>class_weight&amp;min_weight_fraction_leaf</h4><p>完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 </p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>分类树的七个参数，一个属性，四个接口，以及绘图所用的代码。<br>七个参数：Criterion，两个随机性相关的参数（random_state，splitter），四个剪枝参数（max_depth， min_sample_leaf，max_feature，min_impurity_decrease）<br>一个属性：feature_importances_ ，属性是在模型训练之后，能够调用查看的模型的各种性质。</p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>四个接口：ﬁt，score，apply，predicd</p><p>apply：返回每个测试样本所在叶子节点的索引</p><p>predict：返回每个测试样本的分类/回归结果</p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML37.png" alt></p><h2 id="回归树参数解读"><a href="#回归树参数解读" class="headerlink" title="回归树参数解读"></a>回归树参数解读</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/judge1.png" alt></p><p>几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。</p><h4 id="criterion："><a href="#criterion：" class="headerlink" title="criterion："></a>criterion：</h4><p>回归树衡量分枝质量的指标，支持的标准有三种：<br>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree1.png" alt></p><p>其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree2.png" alt></p><p>其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 </p><h2 id="决策树的本地保存：Graphviz"><a href="#决策树的本地保存：Graphviz" class="headerlink" title="决策树的本地保存：Graphviz"></a>决策树的本地保存：Graphviz</h2><p>windows版本下载地址：<a href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html" target="_blank" rel="noopener">https://graphviz.gitlab.io/_pages/Download/Download_windows.html</a></p><p>双击msi文件，一直next就完事了。</p><p>找到bin文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML103.png" alt></p><p>在下面这张图片的位置加入环境变量</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML106.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML105.png" alt></p><p>用dot -version检查是否安装成功</p><p>将dot文件转为png文件的命令：dot -Tpng *.dot -o *.png</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML107.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类树</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">titan=pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line">x=titan[[<span class="string">"pclass"</span>,<span class="string">"age"</span>,<span class="string">"sex"</span>]]</span><br><span class="line">y=titan[<span class="string">"survived"</span>]</span><br><span class="line">x[<span class="string">"age"</span>].fillna(x[<span class="string">"age"</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.25</span>)</span><br><span class="line">dict=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">x_train=dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">x_test=dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">dec=DecisionTreeClassifier()</span><br><span class="line">dec.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line"><span class="comment">#图形化</span></span><br><span class="line">export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">"age"</span>,<span class="string">"pclass=1st"</span>,<span class="string">"pclass=2nd"</span>,<span class="string">"pclass=3rd"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#回归树</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">X = np.sort(<span class="number">5</span> * rng.rand(<span class="number">80</span>,<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::<span class="number">5</span>] += <span class="number">3</span> * (<span class="number">0.5</span> - rng.rand(<span class="number">16</span>))</span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=<span class="number">5</span>)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line">X_test = np.arange(<span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">0.01</span>)[:, np.newaxis]</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=<span class="number">20</span>, edgecolor=<span class="string">"black"</span>,c=<span class="string">"darkorange"</span>, label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(X_test, y_1, color=<span class="string">"cornflowerblue"</span>,label=<span class="string">"max_depth=2"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_2, color=<span class="string">"yellowgreen"</span>, label=<span class="string">"max_depth=5"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">"data"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"target"</span>)</span><br><span class="line">plt.title(<span class="string">"Decision Tree Regression"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;决策树推导&quot;&gt;&lt;a href=&quot;#决策树推导&quot; class=&quot;headerlink&quot; title=&quot;决策树推导&quot;&gt;&lt;/a&gt;决策树推导&lt;/h2&gt;&lt;p&gt;首先看看下面这组数据集：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubuserconten
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
