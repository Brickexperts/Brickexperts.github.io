<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-10T13:00:00.989Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习的实用层面</title>
    <link href="http://yoursite.com/2019/10/08/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/10/08/改善深层神经网络/</id>
    <published>2019-10-08T04:00:46.000Z</published>
    <updated>2019-10-10T13:00:00.989Z</updated>
    
    <content type="html"><![CDATA[<p>在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。</p><h2 id="训练、验证和测试集"><a href="#训练、验证和测试集" class="headerlink" title="训练、验证和测试集"></a>训练、验证和测试集</h2><p>我们通常会将这些数据划分成三部分，一部分作为训练集（<strong>train set</strong>），一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念。最后一部分则作为测试集（<strong>test set</strong>）。</p><p>接下来，我们开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，我们选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况。</p><p>在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。</p><p>在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被人们称为训练集，而验证集则被称为测试集。</p><h2 id="偏差（Bias）、方差（Variance）"><a href="#偏差（Bias）、方差（Variance）" class="headerlink" title="偏差（Bias）、方差（Variance）"></a>偏差（Bias）、方差（Variance）</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123133.png" alt></p><p>假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（<strong>high bias</strong>）的情况，我们称为“欠拟合”（<strong>underfitting</strong>）。</p><p>相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（<strong>high variance</strong>），数据过度拟合（<strong>overfitting</strong>）。</p><p>在两者之间，可能还有复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（<strong>just right</strong>）是介于过度拟合和欠拟合中间的一类。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123354.png" alt></p><p>下面举例子：</p><p>假定训练集误差是1%，为了方便论证，假定验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，<strong>验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。</strong></p><p>通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有高方差。<strong>也就是说衡量训练集和验证集误差就可以得出不同结论。</strong></p><p>假设训练集误差是15%，我们把训练集误差写在首行，验证集误差是16%，假设该案例中人的错误率几乎为0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，所以这种算法偏差高，因为它甚至不能拟合训练集，这与上一张图最左边的图片相似。</p><p>上面的分析都是基于假设预测，假设人眼辨别的错误率接近0%。最优误差也被称为贝叶斯误差。如果最优误差或贝叶斯误差非常高，比如15%。我们再看看上面这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt></p><h2 id="正则化（Rugularization）"><a href="#正则化（Rugularization）" class="headerlink" title="正则化（Rugularization）"></a>正则化（Rugularization）</h2><p>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差，下面我们就来讲讲正则化的作用原理。</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>我们用逻辑回归讲解原理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008191606.png" alt></p><p>&lambda;/2m乘以&omega;范数的平方。&omega;是欧几里得范数，&omega;的平方等于&omega;<sub>j</sub> (j值从1到n<sub>x</sub>)，也可以表示为&omega;<sup>T</sup>&omega;。此方法称为L2正则化。这里的&lambda;就是正则化参数。因为这里使用了欧几里得法线，被称为向量参数&omega;的L2范式。</p><p>为什么只正则化参数&omega;呢？我们可以加上参数b吗？我们可以这么做，但是一般习惯省略不写。因为&omega;通常是一个高维参数矢量，已经可以表达高偏差问题，&omega;可能包含很多参数，我们不可能拟合所有参数，而b只是单个数字，所以&omega;几乎涵盖所有参数，而不是b。如果加了参数b，没什么影响。因为b也是众多参数的一个，想加就加，没有问题。</p><p>L2正则化是最常见的正则化类型，还有L1正则化。L1正则化加的不是L2范式，而是正则项 &lambda;/m乘以$\sum_{j=1}^n $ |&omega;|，这也被称为参数&omega;向量的L1范数。(这里的n就是项数，和L2范式的nx一样)</p><p>如果用的L1是正则化，最终会是稀疏的，也就是说&omega;向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是L1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用L2正则化。</p><h3 id="为什么正则化有利于预防过拟合？"><a href="#为什么正则化有利于预防过拟合？" class="headerlink" title="为什么正则化有利于预防过拟合？"></a>为什么正则化有利于预防过拟合？</h3><p>如果正则化&lambda;设置得足够大，权重矩阵W被设置为接近于0的值（我也没看懂）。实际上不会发生这种情况的。直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近高偏差状态。</p><p>假设我们用tanh作为我们的激活函数，用g(z)表示tanh(z)。那么我们发现，只要z非常小，z只涉及少量参数。这里我们利用双曲正切函数的线性状态，只要z可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。</p><p>总结一下，如果正则化参数变得很大，参数W很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上，z的取值范围很小，这个激活函数，也就是曲线函数tanh会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p><p>###dropout正则化</p><p>除了L2正则化，还有一个非常使用的正则化方法——Dropout(随机失活)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008212828.png" alt></p><p>假设你在训练上图这样的神经网络，它存在过拟合，这就是<strong>dropout</strong>所要处理的，我们复制这个神经网络，<strong>dropout</strong>会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008213026.png" alt></p><p>这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。不过可想而知，我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p><h3 id="如何实施dropout？"><a href="#如何实施dropout？" class="headerlink" title="如何实施dropout？"></a>如何实施dropout？</h3><p>吴恩达老师讲了最常用的方法。就是inverted dropout(随机失活)。</p><p>首先定义变量d，d3表示一个三层的dropout向量:</p><blockquote><p>d3 = np.random.rand(a3.shape[0],a3.shape[1])</p></blockquote><p>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，keep-prob是一个具体数字，上个示例中它是0.5，而假设在本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2。keep-prob的作用是生成随机矩阵，如果对a3进行因子分解，效果也是一样的。d3是一个矩阵，其中d3中的值为1的概率都是0.8，对应为0的概率是0.2。</p><p>接下来要做的就是从第三层中获取激活函数，这里我们叫它a3，a3含有要计算的激活函数，等于上面的a3乘以d3</p><blockquote><p>a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为a3*=d3。</p></blockquote><p>它的作用就是让过滤d3中所有等于0的元素，而各个元素等于0的概率只有20%，乘法运算最终把d3中相应元素归零，即让d3中0元素与a3中相对元素归零。</p><p>最后，我们向外扩展a3。用它除以0.8，或者除以keep-prob参数。</p><blockquote><p>a3/= (keep-prob)</p></blockquote><p>解释一下最后一步，我们假设第三隐藏层上有50个神经元，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%）个。现在我们看下z<sup>[4]</sup>，z<sup>[4]</sup>=&omega;<sup>[4]</sup>*a<sup>[3]</sup>+b<sup>[4]</sup>，我们的预期是，a<sup>[3]</sup>减少20%，也就是说a<sup>[3]</sup>中有20%的元素被归零。为了不影响z<sup>[4]</sup>的期望值，我们需要用&omega;<sup>[4]</sup>a<sup>[3]</sup>/0.8，它将会修正或弥补我们所需的那20%，a<sup>[3]</sup>的期望值不会变。</p><p>它的功能是，不论keep-prop的值是多少0.8，0.9甚至是1，如果keep-prop设置为1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以keep-prob，确保a<sup>[3]</sup>的期望值不变。</p><h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><p><strong>Dropout</strong>可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？</p><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，所以不愿意给任何一个输入加上太多权重。因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果。和之前讲的L2正则化类似，实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009170758.png" alt></p><p>实施dropout的另一个细节是，这是一个拥有三个输入特征的网络，其中一个要选择的参数是keep-prob，它代表每一层上保留单元的概率。所以不同层的keep-prob也可以变化。第一层，矩阵W<sup>[1]</sup>是7×3，第二个权重矩阵W<sup>[2]</sup>是7×7，第三个权重矩阵W<sup>[3]</sup>是3×7，以此类推，W<sup>[2]</sup>是最大的权重矩阵，因为W<sup>[2]</sup>拥有最大参数集，即7×7，为了预防矩阵的过拟合，对于这一层，它的keep-prob值应该相对较低，假设是0.5。对于其它层，过拟合的程度可能没那么严重，它们的keep-prob值可能高一些，可能是0.7甚至更高，假设这里是0.7。如果在某一层，我们不必担心其过拟合的问题，那么keep-prob可以为1。</p><p><strong>总结：</strong>如果我们担心某些层比其他层更容易发生过拟合，可以把某些层的keep-prob值设置得比其他层更低，缺点是为了使用交叉验证，我们要搜索更多得超级参数。另一种方案是在一些层上应用dropout，而有些层不用dropout应用。dropout的层只含有keep-prob这一个超参数。</p><p>dropout一大缺点就是代价函数J不再被明确定义，因为每次迭代都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。所以吴恩达老师推荐通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。</p><h3 id="其它正则化方法"><a href="#其它正则化方法" class="headerlink" title="其它正则化方法"></a>其它正则化方法</h3><h4 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h4><p>假设我们正在拟合猫咪图片分类器，如果我们想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。</p><p>除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。</p><p>通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009193402.png" alt></p><p>对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字4看起来是波形的，其实不用对数字4做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个4看起来有点扭曲。所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。</p><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009203859.png" alt></p><p>因为在训练过程中，我们希望训练误差，代价函数J都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升。early stopping 代表提早停止训练神经网络。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009200928.png" alt></p><p>当我们还未在神经网络上运行太多迭代过程的时候，参数&omega;接近0，因为随机初始化&omega;值时，它的值可能都是较小的随机值，所以在我们长期训练神经网络之前&omega;依然很小，在迭代过程和训练过程中&omega;的值会变得越来越大，比如在这儿，神经网络中参数&omega;的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个&omega;值中等大小的弗罗贝尼乌斯范数，与L2正则化相似，选择参数&omega;范数较小的神经网络，但愿那时的神经网络过度拟合不严重。</p><p>early stopping 有一个缺点，接下来了解一下。</p><p>在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。在重点优化代价函数时，你只需要留意&omega;和b，J(&omega;,b)的值越小越好，你只需要想办法减小这个值，其它的不用关注。然后，预防过拟合还有其他任务，就是减少方差”。</p><p>缺点就是我们不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p><h2 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h2><p>假设我们有一个数据集，它有两个输入特征，所以数据是二维的。下图即为数据集散点图:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212210.png" alt></p><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：</p><ol><li>零均值</li><li>归一化方差；</li></ol><p><strong>我们希望无论是训练集还是测试集都是通过相同的μ和&sigma;<sup>2</sup>定义的数据转换。</strong></p><p>第一步是零均值。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$x<sup>(i)</sup></p></blockquote><p>μ是一个向量，x等于每个训练数据x减去μ，意思是移动数据集，直到完成零均值化。</p><p>输入数据经过零均值后，得出下面的散点图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212709.png" alt></p><p>第二步是归一化方差，注意特征x<sub>1</sub>的方差比特征x<sub>2</sub>的方差要大得多。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$(x<sup>(i)</sup>)<sup>2</sup></p></blockquote><p> &sigma;<sup>2</sup>是一个向量，它的每个特征都有方差。经过归一化方差后，得出下面结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009214916.png" alt></p><p>如果你用它来调整训练数据，那么用相同的μ和σ<sup>2</sup>来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论的μ值是什么，也不论的σ<sup>2</sup>值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估μ和σ<sup>2</sup>。因为我们希望不论是训练数据还是测试数据，都是通过相同μ和σ<sup>2</sup>定义的相同数据转换，其中μ和σ<sup>2</sup>是由训练集数据计算得来的。</p><p>为什么我们需要归一化输入特征？回想一下代价函数</p><blockquote><p>J(&omega;,b)=1/m x $\sum_{i=1}^m$L(y<sup>(i)</sup> hat , y<sup>(i)</sup>)</p></blockquote><p>如果输入未归一化的输入特征，代价函数如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009221916.png" alt></p><p>然而如果你归一化特征，代价函数平均起来看更对称，如果你在上图这样的代价函数上运行梯度下降法，你必须使用一个非常小的学习率。因为如果是在这个位置，梯度下降法可能需要多次迭代过程，直到最后找到最小值。但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，你可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行。</p><p>下图即为输入归一化输入特征的代价函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009222132.png" alt></p><p>如果输入特征处于不同范围内，可能有些特征值从0到1，有些从1到1000，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。</p><h2 id="梯度消失-梯度爆炸"><a href="#梯度消失-梯度爆炸" class="headerlink" title="梯度消失/梯度爆炸"></a>梯度消失/梯度爆炸</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p><p>吴恩达老师给我们的理解是：权重W只比1略大一点（或者说比单位矩阵略大一点），深度神经网络的激活函数将爆炸式增长，如果W比单位矩阵略小一点，激活函数将以指数级递减。</p><h2 id="神经网络的权重初始化"><a href="#神经网络的权重初始化" class="headerlink" title="神经网络的权重初始化"></a>神经网络的权重初始化</h2><p>补。。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。&lt;/p&gt;
&lt;h2 id=&quot;训练、验证和测试集&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深度神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>人脸验证和神经风格转换</title>
    <link href="http://yoursite.com/2019/10/05/%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2/"/>
    <id>http://yoursite.com/2019/10/05/人脸验证和神经风格转换/</id>
    <published>2019-10-05T01:39:06.000Z</published>
    <updated>2019-10-05T12:34:33.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="One—Shot学习"><a href="#One—Shot学习" class="headerlink" title="One—Shot学习"></a>One—Shot学习</h2><p>人脸识别所面临的一个挑战就是你需要解决一次学习问题，这意味着在大多数人脸识别应用中，你需要通过单单一张图片或者单单一个人脸样例就能去识别这个人。而在我们的学习过程中，发现深度学习只有一个训练样例时，它的表现并不好。接下来解决一下这个问题。</p><p>假设我们的数据库有以下四张照片：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005095438.png" alt></p><p>有一种办法是，将人的照片放进卷积神经网络中，使用softmax单元来输出4种，或者说5种标签，分别对应这4个人，或者4个都不是，所以softmax里我们会有5种输出。但实际上这样效果并不好，因为如此小的训练集不足以去训练一个稳健的神经网络。而且，假如有新人加入你的团队，你现在将会有5个组员需要识别，所以输出就变成了6种，这时你要重新训练你的神经网络吗？这听起来实在不像一个好办法。</p><p>所以要让人脸识别能够做到一次学习，为了能有更好的效果，你现在要做的应该是学习<strong>Similarity</strong>函数。详细地说，你想要神经网络学习这样一个用d表示的函数：d(img1,img2)=degree of difference between images。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005100413.png" alt></p><p>它以两张图片作为输入，然后输出这两张图片的差异值。如果你放进同一个人的两张照片，你希望它能输出一个很小的值，如果放进两个长相差别很大的人的照片，它就输出一个很大的值。所以在识别过程中，如果这两张图片的差异值小于某个阈值τ，它是一个超参数，那么这时就能预测这两张图片是同一个人，如果差异值大于τ，就能预测这是不同的两个人，这就是解决人脸验证问题的一个可行办法。</p><h2 id="Siamese网络"><a href="#Siamese网络" class="headerlink" title="Siamese网络"></a>Siamese网络</h2><p>通过上一小节我们知道我们该怎么去做人脸识别，通过输入两张图片。它将让你解决一次学习问题。接下来我们学习如何训练我们的神经网络学会这个函数d。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005102040.png" alt></p><p>看上图，我们经常看到这样的卷积网络，输入图片，然后通过一些列卷积，池化和全连接层，最终得到这样的特征向量（编号1）。有时这个会被送进<strong>softmax</strong>单元来做分类，但在这个视频里我们不会这么做。我们关注的重点是这个向量（编号1），假如它有128个数，它是由网络深层的全连接层计算出来的，我要给这128个数命个名字，把它叫做f(x<sup>(1)</sup>)。你可以把f(x(<sup>(1)</sup>)看作是输入图像x<sup>(1)</sup>的编码，取这个输入图像（编号2），然后表示成128维的向量。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005102947.png" alt></p><p>建立一个人脸识别系统的方法就是，如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我要把第二张图片的编码叫做f(x<sup>(2)</sup>)。这里我用x<sup>(1)</sup>和x<sup>(2)</sup>仅仅代表两个输入图片，他们没必要非是第一个和第二个训练样本，可以是任意两个图片。最后如果你相信这些编码很好地代表了这两个图片，你要做的就是定义d，将x<sup>(1)</sup>和x<sup>(2)</sup>的距离定义为这两幅图片的编码之差的范数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005104620.png" alt></p><p>那么怎么训练这个Siamese神经网络呢？不要忘了这两个网络有相同的参数，所以你实际要做的就是训练一个网络，它计算得到的编码可以用于函数d，它可以告诉你两张图片是否是同一个人。更准确地说，神经网络的参数定义了一个编码函数，如果给定输入图像，这个网络会输出的128维的编码。你要做的就是学习参数，使得如果两个图片和是同一个人，那么你得到的两个编码的距离就小。相反，如果和是不同的人，那么你会想让它们之间的编码距离大一点。</p><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>训练神经网络两种方法：Triplet 损失 和 人脸识别二分类</p><h3 id="Triplet损失"><a href="#Triplet损失" class="headerlink" title="Triplet损失"></a>Triplet损失</h3><p>要想学习神经网络参数来得到优质的人脸图片编码，方法之一就是定义三元组损失函数然后应用梯度下降。看到这个的时候，我也一脸懵逼。这是什么啊？接下来我们一起看下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005110336.png" alt></p><p>我们看下这是什么意思，为了应用三元组损失函数，你需要比较成对的图像，比如这个图片，为了学习网络的参数，你需要同时看几幅图片，比如这对图片（编号1和编号2），你想要它们的编码相似，因为这是同一个人。然而假如是这对图片（编号3和编号4），你会想要它们的编码差异大一些，因为这是不同的人。</p><p>用三元组损失的术语来说，你要做的通常是看一个 Anchor 图片，你想让Anchor图片和Positive图片（Positive意味着是同一个人）的距离很接近。然而，当Anchor图片与Negative图片（Negative意味着是非同一个人）对比时，你会想让他们的距离离得更远一点。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005111313.png" alt></p><p>这就是为什么叫做三元组损失，它代表你通常会同时看三张图片，你需要看Anchor图片、Postive图片，还有Negative图片，我要把Anchor图片、Positive图片和Negative图片简写成A、P、N。</p><p>把以上内容写成公式的话，d= ||f(A)-f(P)||<sup>2</sup>。我们希望||f(A)-f(P)||<sup>2</sup> &lt;= ||f(A)-f(N)||<sup>2</sup>。||f(A)-f(P)||<sup>2</sup> 就是d(A,P)，||f(A)-f(N)||<sup>2</sup>时d(A,N)，我们可以把d看为距离函数。</p><p>对表达式修改一下，因为有一种情况满足这个表达式，但是没有用处，就是把所有的东西都学成0，如果总是输出0，即0-0≤0，这就是0减去0还等于0，如果所有图像的都是一个零向量，那么总能满足这个方程f。所以为了确保网络对于所有的编码不会总是输出0，也为了确保它不会把所有的编码都设成互相相等的。另一种方法能让网络得到这种没用的输出，就是如果每个图片的编码和其他图片一样，这种情况，你还是得到0-0。</p><p>为了阻止网络出现上述状况，我们需要修改表达式。也就是||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>不能刚好小于等于0。应该是比0还要小，所以这个应该小于一个-a，也就是||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup> &lt;= -a。这里的a是一个超参数，是为了阻止网络输出无用的结果。</p><p>举个例子，假如间隔设置成0.2，如果在这个例子中，d(A,P)=0.5，如果 Anchor和 Negative图片的，即只大一点，比如说0.51，条件就不能满足。虽然0.51也是大于0.5的，但还是不够好，我们想要比大很多。你会想让这个值d(A,N)至少是0.7或者更高，这样间距至少达到0.2，你可以把这项调大或者这个调小。超参数a至少是0.2，在d(A,P)和d(A,N)之间至少相差0.2，这就是间隔参数的作用。</p><p>接下来定义损失函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005114357.png" alt></p><p>这个max函数的作用就是，只要这个||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a &lt;= 0，那么损失函数就是0。另一方面如果||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a&gt;=0，然后取最大值。最后得到||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a，这样就会得到一个正的损失值。通过最小化这个损失函数达到的效果就是使这部分||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a小于等于0，只要这个损失函数小于等于0，网络不会关心它负值有多大。</p><p>这是一个三元组定义的损失，整个网络的代价函数应该是训练集中这些单个三元组损失的总和。假如你有一个10000个图片的训练集，里面是1000个不同的人的照片，你要做的就是取这10000个图片，然后生成这样的三元组，然后训练你的学习算法，对这种代价函数用梯度下降，这个代价函数就是定义在你数据集里的这样的三元组图片上。</p><p>注意，为了定义三元组的数据集你需要成对的A和P，即同一个人的成对的图片，为了训练你的系统你确实需要一个数据集，<strong>里面有同一个人的多个照片</strong>。这也是为什么在这个例子中，我说假设你有1000个不同的人的10000张照片，也许是这1000个人平均每个人10张照片，组成了你整个数据集。如果你只有每个人一张照片，那么根本没法训练这个系统。当然，训练完这个系统之后，你可以应用到你的一次学习问题上，对于你的人脸识别系统，可能你只有想要识别的某个人的一张照片。但对于训练集，你需要确保有同一个人的多个图片，至少是你训练集里的一部分人，这样就有成对的Anchor和Positive图片了。</p><p>那么我们该怎么选择这些三元组来形成训练集呢？一个问题是如果你从训练集中，随机地选择A、P和N，遵守A和P是同一个人，而 A 和 N 是不同的人这一原则。有个问题就是，如果随机的选择它们，那么这个约束条件 d(A,P)+a&lt;=d(A,N) 很容易达到，因为随机选择的图片，A和N 比 A和P差别很大的概率很大。所以有很大的可能性|| f(A)-f(N) ||会比||(f(A) -f(P) )||大，而且差距远大于a，这样网络并不能从中学到什么。</p><p>所以为了构建一个数据集，你要做的就是尽可能选择难训练的三元组A、P和N。具体而言，你想要所有的三元组都满足这个条件 d(A,P)+a&lt;=d(A,N)。难训练的三元组就是，你的A、P和N的选择使得很接近，即d(A,P)约等于d(A,N)。这样你的学习算法会竭尽全力使d(A,N)式子变大，或者使d(A,P)变小，这样左右两边至少有一个的间隔。并且选择这样的三元组还可以增加你的学习算法的计算效率，如果随机的选择这些三元组，其中有太多会很简单，梯度算法不会有什么效果，因为网络总是很容易就得到了正确的结果，只有选择难的三元组梯度下降法才能发挥作用，使得这两边离得尽可能远。</p><h3 id="人脸验证与二分类"><a href="#人脸验证与二分类" class="headerlink" title="人脸验证与二分类"></a>人脸验证与二分类</h3><p>上述的Triplet loss是一个学习人脸识别卷积网络参数的好方法。另一个训练神经网络的方法是选取一对神经网络Siamese网络，使其同时计算这些嵌入，比如说128维的嵌入（编号1），或者更高维，然后将其输入到逻辑回归单元，然后进行预测，如果是相同的人，那么输出是1，若是不同的人，输出是0。这就把人脸识别问题转换为一个二分类问题，训练这种系统时可以替换Triplet loss的方法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005162142.png" alt></p><p>最后的逻辑回归单元输出 y帽：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005162852.png" alt></p><p>f(x<sup>(i)</sup>)<sub>k</sub> 代表图片x<sup>(i)</sup> 的编码，下标 k代表选择这个向量中的第k个元素。| f(x<sup>(i)</sup>)<sub>k</sub> -f(x<sup>(i)</sup>)<sub>k</sub> |是对着两个编码取元素差的绝对值。把这128个元素当作特征，然后把他们放入逻辑回归中，最后的逻辑回归可以增加参数&omega;<sub>i</sub> 和 b，就像普通的逻辑回归一样。你将在这128个单元上训练合适的权重，用来预测两张图片是否是一个人，这是一个很合理的方法来学习预测0或者1，即是否是同一个人。</p><p>   <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005165455.png" alt></p><p>但是在这个学习公式中，输入是一对图片，这是你的训练输入x（编号1、2），输出y是0或者1，取决于你的输入是相似图片还是非相似图片。与之前类似，你正在训练一个Siamese网络，意味着上面这个神经网络拥有的参数和下面神经网络的相同（编号3和4所示的网络），两组参数是绑定的，这样的系统效果很好。</p><p>如果这是一张新图片（编号1），当员工走进门时，希望门可以自动为他们打开，这个（编号2）是在数据库中的图片，不需要每次都计算这些特征（编号6），不需要每次都计算这个嵌入，你可以提前计算好，那么当一个新员工走近时，你可以使用上方的卷积网络来计算这些编码（编号5），然后使用它，和预先计算好的编码进行比较，然后输出预测值。因此不需要存储原始图像，如果你有一个很大的员工数据库，你不需要为每个员工每次都计算这些编码。这个预先计算的思想，可以节省大量的计算。</p><h2 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h2><p>这是卷积神经网络最有趣的应用。什么是神经风格迁移？</p><p>看例子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005170737.png" alt></p><p>为了描述如何实现神经网络迁移，我将使用来C表示内容图像，S表示风格图像，G表示生成的图像。这只是一个提出，在深入了解如何实现神经风格迁移之前，我们先看神经网络不同层之间的具体运算。</p><h2 id="CNN特征可视化"><a href="#CNN特征可视化" class="headerlink" title="CNN特征可视化"></a>CNN特征可视化</h2><p>其实我一直觉得神经网络的解释性真的蛮差的。让我们接下来看一下，深度卷积网络到底在学什么？</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005193429.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;One—Shot学习&quot;&gt;&lt;a href=&quot;#One—Shot学习&quot; class=&quot;headerlink&quot; title=&quot;One—Shot学习&quot;&gt;&lt;/a&gt;One—Shot学习&lt;/h2&gt;&lt;p&gt;人脸识别所面临的一个挑战就是你需要解决一次学习问题，这意味着在大多数人脸识别
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="人脸验证" scheme="http://yoursite.com/tags/%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>目标检测组件</title>
    <link href="http://yoursite.com/2019/10/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%84%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/10/03/目标检测组件/</id>
    <published>2019-10-03T13:16:58.000Z</published>
    <updated>2019-10-05T01:29:03.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个问题，即定位分类问题。这意味着，我们不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来，这就是定位分类问题。其中“定位”的意思是判断汽车在图片中的具体位置。之后，我们再讲讲当图片中有多个对象时，应该如何检测它们，并确定出位置。比如，你正在做一个自动驾驶程序，程序不但要检测其它车辆，还要检测其它对象，如行人、摩托车等等，稍后我们再详细讲。</p><p>本节我们要研究的分类定位问题，通常只有一个较大的对象位于图片中间位置，我们要对它进行识别和定位。而在对象检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。因此，图片分类的思路可以帮助学习分类定位，而对象定位的思路又有助于学习对象检测，我们先从分类和定位开始讲起。</p><p>图片分类问题你已经并不陌生了，例如，输入一张图片到多层卷积神经网络。这就是卷积神经网络，它会输出一个特征向量，并反馈给<strong>softmax</strong>单元来预测图片类型。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004134535.png" alt></p><p>如果你正在构建汽车自动驾驶系统，那么对象可能包括以下几类：行人、汽车、摩托车和背景，这意味着图片中不含有前三种对象，也就是说图片中没有行人、汽车和摩托车，输出结果会是背景对象，这四个分类就是softmax函数可能输出的结果。</p><p>下面是如何为监督学习任务定义目标标签y，请注意，这有四个分类，神经网络输出的是这四个数字和一个分类标签，或分类标签出现的概率。目标标签y的定义如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004140243.png" alt></p><p>它是一个向量，第一个组件p<sub>c</sub>表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则p<sub>c</sub>=1，如果是背景，则图片中没有要检测的对象，则p<sub>c</sub>=0 。我们可以这样理解p<sub>c</sub>，它表示被检测对象属于某一分类的概率，背景分类除外。</p><p>如果检测到对象，就输出被检测对象的边界框参数b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>和b<sub>w</sub>。最后，如果存在某个对象，那么p<sub>c</sub>，同时输出c<sub>1</sub>、c<sub>2</sub> 和c<sub>3</sub>，表示该对象属于1-3类中的哪一类，是行人，汽车还是摩托车。鉴于我们所要处理的问题，我们假设图片中只含有一个对象，所以针对这个分类定位问题，图片最多只会出现其中一个对象。</p><p>我们再看几个例子，假如下图是一张训练集图片。在y当中，第一个元素p<sub>c</sub>，因为图中有一辆车，b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>和b<sub>w</sub>会指明边界框的位置，所以标签训练集需要标签的边界框。图片中是一辆车，所以结果属于分类2，因为定位目标不是行人或摩托车，而是汽车，所以c<sub>1</sub>=0，c<sub>2</sub>=1，c<sub>3</sub>=0，c<sub>1</sub>、c<sub>2</sub>和c<sub>3</sub>中最多只有一个等于1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143054.png" alt></p><p>上图是只有一个检测对象的情况，如果图片中没有检测对象呢？看下图</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143246.png" alt></p><p>这种情况下，p<sub>c</sub>=0，y的其它参数将变得毫无意义。这里我全部写成问号，表示”毫无意义“的参数，因为图片中不存在检测对象，所以不用考虑网络输出中边界框的大小。也不用考虑图片中的对象是属于c<sub>1</sub>、c<sub>2</sub>、c<sub>3</sub>中的哪一类。</p><p>最后，我们介绍一下神经网络的损失函数，其参数为类别y和网络输出y帽，如果采用平方误差策略，则</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143820.png" alt></p><p>损失值等于每个元素相应差值的平方和。</p><p>如果图片中存在定位对象，那么y<sub>1</sub> = 1，所以y<sub>1 </sub>= p<sub>c</sub>，同样地，如果图片中存在定位对象，p<sub>c</sub> = 1，损失值就是不同元素的平方和。</p><p>另一种情况是，y<sub>1</sub>=0，也就是p<sub>c</sub>=0，损失值是(y<sub>1</sub>帽 - y<sub>1</sub>)^2，因为对于这种情况，我们不用考虑其它元素，只需要关注神经网络输出p<sub>c</sub>的准确度。</p><h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>假设我们正在构建一个人脸识别应用，如下图。出于某种原因，我们希望算法可以给出眼角的具体位置。眼角坐标为(x,y)，你可以让神经网络的最后一层多输出两个数字 l<sub>x</sub> 和 l<sub>y</sub>，作为眼角的坐标值。如果你想知道两只眼睛的四个眼角的具体位置，那么从左到右，依次用四个特征点来表示这四个眼角。对神经网络稍做些修改，输出第一个特征点（l<sub>1x</sub>，l<sub>1y</sub>），第二个特征点（l<sub>2x</sub>，l<sub>2y</sub>），依此类推，这四个脸部特征点的位置就可以通过神经网络输出了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004145802.png" alt></p><p>也许除了这四个特征点，你还想得到更多的特征点输出值，这些（图中眼眶上的红色特征点）都是眼睛的特征点，你还可以根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。如下图，为了便于说明，你可以设定特征点的个数，假设脸部有64个特征点，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004145926.png" alt></p><p>具体做法是，准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1或0，1表示有人脸，0表示没有人脸，然后输出（l<sub>1x</sub>，l<sub>1y</sub>）……直到（l<sub>64x</sub>，l<sub>64y</sub>）。这里我用 l 代表一个特征，这里有129个输出单元，其中1表示图片中有人脸，因为有64个特征，64×2=128，所以最终输出128+1=129个单元。</p><p>最后一个例子，看下图。如果你对人体姿态检测感兴趣，你还可以定义一些关键特征点，假设有32个，如胸部的中点，左肩，左肘，腰等等。然后通过神经网络标注人物姿态的关键特征点，再输出这些标注过的特征点，就相当于输出了人物的姿态动作。当然，要实现这个功能，你需要设定这些关键特征点，从胸部中心(l<sub>1x</sub>，l<sub>1y</sub>)一直向下，直到（l<sub>32x</sub>，l<sub>32y</sub>）。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004150615.png" alt></p><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。接下来，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151006.png" alt></p><p>假如你想构建一个汽车检测算法，步骤是，首先创建一个标签训练集，也就是x和y表示适当剪切的汽车图片样本，这张图片（编号1）x是一个正样本，因为它是一辆汽车图片，这几张图片（编号2、3）也有汽车，但这两张（编号4、5）没有汽车。出于我们对这个训练集的期望，你一开始可以使用适当剪切的图片，就是整张图片x几乎都被汽车占据，你可以照张照片，然后剪切，剪掉汽车以外的部分，使汽车居于中间位置，并基本占据整张图片。有了这个标签训练集，你就可以开始训练卷积网络了，输入这些适当剪切过的图片（编号6），卷积网络输出y，0或1表示图片中有汽车或没有汽车。训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004172940.png" alt></p><p>假设上图这是一张测试图片，首先选定一个特定大小的窗口，比如图片下方这个窗口，将这个红色小方块输入卷积神经网络，卷积网络开始进行预测，即判断红色方框内有没有汽车。</p><p>滑动窗口目标检测算法接下来会继续处理第二个图像，即红色方框稍向右滑动之后的区域，并输入给卷积网络，因此输入给卷积网络的只有红色方框内的区域，再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151258.png" alt></p><p>接着我们重复上述过程，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或1。如下图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151527.png" alt></p><p>再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151813.png" alt></p><p>这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。</p><p>滑动窗口目标检测算法也有很明显的缺点，就是计算成本，因为你在图片中剪切出太多小方块，卷积网络要一个个地处理。如果你选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本。</p><h2 id="滑动窗口的卷积实现"><a href="#滑动窗口的卷积实现" class="headerlink" title="滑动窗口的卷积实现"></a>滑动窗口的卷积实现</h2><p>为了构建滑动窗口的卷积应用，首先要知道如何把<strong>神经网络的全连接层转化为卷积层</strong>。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004152353.png" alt></p><p>假设对象检测算法输入一个14×14×3的图像，图像很小，不过演示起来方便。在这里过滤器大小为5×5，数量是16，14×14×3的图像在过滤器处理之后映射为10×10×16。然后通过参数为2×2的最大池化操作，图像减小到5×5×16。然后添加一个连接400个单元的全连接层，接着再添加一个全连接层，最后通过<strong>softmax</strong>单元输出。为了跟下图区分开，我先做一点改动，用4个数字来表示，它们分别对应softmax单元所输出的4个分类出现的概率。这4个分类可以是行人、汽车、摩托车和背景或其它对象。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004152537.png" alt></p><p>画一个这样的卷积网络，它的前几层和之前的一样，而对于下一层，也就是这个全连接层，我们可以用5×5的过滤器来实现，数量是400个（编号1所示），输入图像大小为5×5×16，用5×5的过滤器对它进行卷积操作，过滤器实际上是5×5×16，因为在卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致，输出结果为1×1。假设应用400个这样的5×5×16过滤器，输出维度就是1×1×400，我们不再把它看作一个含有400个节点的集合，而是一个1×1×400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5×5×16维度的过滤器，所以每个值都是上一层这些5×5×16激活值经过某个任意线性函数的输出结果。接着我们再添加另外一个卷积层（编号2所示），这里用的是1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，它其实就是上个网络中的这一全连接层。最后经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，我们最终得到这个1×1×4的输出层，而不是这4个数字（编号3所示）。以上就是用卷积层代替全连接层的过程，结果这几个单元集变成了1×1×400和1×1×4的维度。</p><p>掌握了卷积知识，我们再看看如何通过卷积实现滑动窗口对象检测算法。</p><p>假设向滑动窗口卷积网络输入14×14×3的图片，为了简化演示和计算过程，这里我们依然用14×14的小图片。和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4，我画得比较简单，严格来说，14×14×3应该是一个长方体，第二个10×10×16也是一个长方体。为了方便，立体部分随便画了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154252.png" alt></p><p>上图是输入给卷积网络的图片大小是14×14×3，测试集图片是16×16×3，如下图。现在给这个输入图片加上黄色条块，在最初的滑动窗口算法中，你会把这片蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。我们在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154529.png" alt></p><p>把这两个图放一起：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154910.png" alt></p><p>结果发现，这4次卷积操作中很多计算都是重复的。所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在这一步操作中（编号1），卷积网络运行同样的参数，使得相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化（编号2），输出结果6×6×16。照旧应用400个5×5的过滤器（编号3），得到一个2×2×400的输出层，现在输出层为2×2×400，而不是1×1×400。应用1×1过滤器（编号4）得到另一个2×2×400的输出层。再做一次全连接的操作（编号5），最终得到2×2×4的输出层，而不是1×1×4。最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），也就是这个14×14区域经过卷积网络处理后的结果，同样，右下角这个方块是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果。</p><p>具体的计算步骤是以绿色方块为例，假设你剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6）</p><p>所以该卷积操作的原理是我们不需要把输入图像分割成四个子集，分别执行前向传播，而是把<strong>它们作为一张图片输入给卷积网络进行计算</strong>，其中的公共区域可以共享很多计算。</p><p>以上就是在卷积层上应用滑动窗口算法的内容，它提高了整个算法的效率。不过这种算法仍然存在一个缺点，就是边界框的位置可能不够准确。</p><h2 id="Bounding-Box预测"><a href="#Bounding-Box预测" class="headerlink" title="Bounding Box预测"></a>Bounding Box预测</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004161102.png" alt></p><p>看上面的预测结果，这些边界框没有一个能完美匹配汽车的位置。</p><p>其中一个能得到更精准边框的算法是YOLO算法。它是这么做的，比如你的输入图像是100×100的，然后在图像上放一个网格。为了介绍起来简单一些，我用3×3网格，实际实现时会用更精细的网格，可能是19×19，可能更精细。基本思路是：采用图像分类和定位算法，逐一应用到图像的九个格子中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004162024.png" alt></p><p>所以对于这里9个格子中任何一个，你都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。因为这里有3×3格子，然后对于每个格子，你都有一个8维向量，所以目标输出尺寸是3×3×8。</p><p>注意：把对象分配到一个格子的过程是，<strong>你观察对象的中点，然后将这个对象分配到其中点所在的格子</strong>。所以即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，就是3×3网络的其中一个格子，或者19×19网络的其中一个格子。如果我们在现实实践时，采用19x19的网格会更精细。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。</p><h2 id="交并比（IOU）"><a href="#交并比（IOU）" class="headerlink" title="交并比（IOU）"></a>交并比（IOU）</h2><p>在对象检测任务中，你希望能够同时定位对象，所以如果实际边界框是这样的，你的算法给出这个紫色的边界框，那么这个结果是好还是坏？所以交并比（lOU）函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域（绿色阴影表示区域），而交集就是这个比较小的区域（橙色阴影表示区域），那么交并比就是交集的大小，这个橙色阴影面积，然后除以绿色阴影的并集面积。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004162947.png" alt></p><p>一般约定，在计算机检测任务中，如果IOU&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，lOU就是1，因为交集就等于并集。但一般来说只要IOU&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将IOU定得更高，比如说大于0.6或者更大的数字，但IOU越高，边界框越精确。</p><h2 id="非极大值抑制（NMS）"><a href="#非极大值抑制（NMS）" class="headerlink" title="非极大值抑制（NMS）"></a>非极大值抑制（NMS）</h2><p>到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004165505.png" alt></p><p>假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上右边这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004165617.png" alt></p><p>图中的绿色点和黄色点分别为两辆车的中点。</p><p>实践中当你运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。对于左边的车子也一样，所以不仅仅是这个格子，如果这是你们以前见过的图像，不仅这个格（编号4）子会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。</p><p>我们分步介绍一下非极大抑制是怎么起效的，因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的p<sub>c</sub>，我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后可能会对同一个对象做出多次检测，所以非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。</p><p>所以具体上，这个算法做的是，首先看看每次报告每个检测结果相关的概率p<sub>c</sub>，首先看概率最大的那个，这个例子（右边车辆）中是0.9，然后就说这是最可靠的检测，所以我们就用高亮标记，就说我这里找到了一辆车。这么做之后，非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制。所以这两个矩形分别是0.6和0.7，这两个矩形和淡蓝色矩形重叠程度很高，所以会被抑制，变暗，表示它们被抑制了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004170554.png" alt></p><p>接下来，逐一审视剩下的矩形，找出概率最高p<sub>c</sub>= 0.8，我们就认为这里检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他lOU值很高的矩形。所以现在每个矩形都会被高亮显示或者变暗，如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些，这就是最后得到的两个预测结果。</p><p>所以这就是非极大值抑制，非最大值意味着你只输出概率最大的分类结果，但抑制很接近，但不是最大的其他预测结果，所以这方法叫做非极大值抑制。</p><p>在这里只介绍了算法检测单个对象的情况，如果你尝试同时检测三个对象，比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。</p><h2 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h2><p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果你想让一个格子检测出多个对象，你可以这么做，就是使用<strong>anchor box</strong>这个概念。</p><p>假设有这样一张图片，对于这个例子，我们继续使用3x3网格。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004175438.png" alt></p><p>注意人的中点和汽车的中点几乎在同一个地方，两者都落入同一个格子。对于那个同一个格子，y输出</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004140243.png" alt></p><p>我们可以检测着三个类别：行人、汽车、摩托车。它将无法输出检测结果，所以我必须从两个检测结果中选一个。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004185914.png" alt></p><p>而anchor box的思路是，这样子，预先定义两个不同形状的anchor box，或者anchor box形状，你要做的是把预测结果和这两个anchor box关联起来。一般来说，你可能会用更多的anchor box，可能要5个甚至更多，但对于这个视频，我们就用两个anchor box，这样介绍起来简单一些。</p><p>我们要做的是定义类别标签。用的向量不再是上面那个y=[p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>]^T，而是重复两次，y=[p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>  p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>]^T。前面的p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub> 是和anchor box1关联的8个参数，后面的8个参数适合anchor box2相关联。</p><p>因为行人的形状更类似于anchor box 1的形状，而不是anchor box 2的形状，所以你可以用这8个数值（前8个参数），这么编码p<sub>c</sub>=1代表有个行人，用b<sub>x</sub>,b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>编码包住行人的边界框，然后用c<sub>1</sub>=1,c<sub>2</sub>=0,c<sub>3</sub>=0来说明这个对象是个行人。然后是车子，因为车子的边界框比起anchor box 1更像anchor box 2的形状。这样编码，p<sub>c</sub>=1,b<sub>x</sub>,b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>, c<sub>1</sub>=0,c<sub>2</sub>=1,c<sub>3</sub>=0。</p><p>假设车子的边界框形状是这样，更像anchor box 2，如果这里只有一辆车，行人走开了，那么anchor box 2分量还是一样的，要记住这是向量对应anchor box 2的分量和anchor box 1对应的向量分量，你要填的就是，里面没有任何对象，所以 ，然后剩下的就是？。编码：y=[0  ?  ?  ?  ?  ?  ?  ?  1 b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  0  1  0]。</p><p>最后，你应该怎么选择anchor box呢？人们一般手工指定anchor box形状，你可以选择5到10个anchor box形状，覆盖到多种不同的形状，可以涵盖你想要检测的对象的各种形状。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目标定位&quot;&gt;&lt;a href=&quot;#目标定位&quot; class=&quot;headerlink&quot; title=&quot;目标定位&quot;&gt;&lt;/a&gt;目标定位&lt;/h2&gt;&lt;p&gt;图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度卷积网络</title>
    <link href="http://yoursite.com/2019/10/02/%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/10/02/深度卷积网络/</id>
    <published>2019-10-02T04:25:39.000Z</published>
    <updated>2019-10-03T12:56:52.847Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet - 5"></a>LeNet - 5</h3><p>LeNet-5是专门为灰度图训练的。所以他的图像样本的深度都是1。</p><p>LeNet-5的网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002154958.png" alt></p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002155614.png" alt></p><p>实际上论文的原文是使用224x224x3的，但经过试验发现227x227x3效果更好。</p><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p>网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002161015.png" alt></p><p>从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。</p><p>可以看到 VGG16 是13个卷积层+3个全连接层叠加而成。</p><h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>网络越深越难训练，因为存在梯度消失和梯度爆炸的问题。本小节学习跳远连接。跳远连接可从某一个网络层激活，然后迅速反馈给另外一层甚至是神经网络的更深层。我们可以利用跳远连接构建能够训练深度网络的ResNets。</p><p>ResNets是由残差块构建的。那什么是残差块？看下图</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002163255.png" alt></p><p>这是一个两层的神经网络，它在L层进行激活，得到a[ l +1]再次进行激活，两层之后得到a[ l +2]。下图中的黑色部分即为计算过程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/img_0007.png" alt></p><p>而在残差网络中，我们直接将a^[ l ]拷贝到蓝色箭头所指位置。在线性激活之后、Relu非线性激活之前加上a[ l ]，不在沿着原来的主路径传递。这就意味者我们主路径过程的第四个式子替换为蓝色式子，也正是这个蓝色式子中加上的a[ l ]产生了一个残差块。</p><h3 id="为什么残差网络有用？"><a href="#为什么残差网络有用？" class="headerlink" title="为什么残差网络有用？"></a>为什么残差网络有用？</h3><p>一个网络深度越深，它在训练集上训练网络的效率会有所减弱，但对于ResNets就不完全是这样了。</p><p>上一张图已经说过a^[ l +2]=g(z^[ l +2]+a^[ l ])，添加项a^[ l ]是刚添加的跳远连接的输入。解开这个式子，得：</p><p>a^[ l+2]=g(w^[ l+2] * a^[ l+1]+b^[ l+2]+a^[ l ])，这里w和b为关键值。如果w和b均为0，那a^[ l+2]=g(a^[ l ])=a^[ l ](假设这里得激活函数是Relu)。结果表明，残差块学习这个恒等式函数残差块并不难，跳远连接让我们很容易的得到a^[ l+2]=a^[ l ]</p><p>这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络。因为对它来说，学习恒等函数对它来说很简单，尽管它多了两层，也只是把a^[ l ]的值赋给a^[ l+2]。</p><p><strong>所以，残差网络有用的原因是这些函数残差块学习恒等函数非常容易。</strong></p><h2 id="1x1的卷积"><a href="#1x1的卷积" class="headerlink" title="1x1的卷积"></a>1x1的卷积</h2><p>看到这个标题，你也许会迷惑，1x1的卷积能做什么呢？不就是乘以数字吗？结果并非如此</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002205234.png" alt></p><p>看上图，你会觉得这个1x1的过滤器没什么用，只是对输入矩阵乘以某个数字。但这个1x1的过滤器仅仅是对于6x6x1的信道图片效果不好 。如果是一张6x6x32的图片就不一样了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002205913.png" alt></p><p>具体来说，1x1卷积所实现的功能是遍历这36个单元格。计算输入图中32个数字和过滤器中32个数字的乘积，然后应用Relu函数。我们以一个单元格为例，用着36个数字乘以这个输入层上1x1的切片，得到一个实数画在下面图中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002211057.png" alt></p><p>这个1x1x32的过滤器中的32可以这样理解，一个神经元的输入是32个数字，乘以相同高度和宽度上某个切片上的32个数字。这三十二个数字具有不同信道，乘以32个权重，然后应用Relu非线性函数。一般来说，如果过滤器不止一个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6x6x过滤器数量。所以1x1卷积可以从根本上理解为这32个单元都应用了一个全连接神经网络。全连接层的作用是输入32个数字和过滤器数量，标记为nc^[ l+1]。在36个单元上重复此过程，输出结果是6x6x过滤器数量。这种方法通常称为1x1卷积，也成为Network in Network。</p><p>下面是一个1x1卷积的应用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002213518.png" alt></p><p>假设这是一个28x28x192的输入层，我们可以利用池化层压缩它的高度和宽度。但是如果信道数量很大，我们该如何把它压缩为28x28x32维度的层呢？我们可以用32个大小为1x1的过滤器，每个过滤器的大小都是1x1x192维，因为过滤器中信道的数量必须与输入层中信道的数量一致。因此过滤器数量为32，输出层为28x28x32。这就是压缩nc的方法。</p><h2 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h2><p>构建卷积层是，我们需要决定过滤器的大小是3x3还是5x5或者其它大小？或者要不要添加池化层？而我们接下来要讲的Inception就是代替我们来做决定的。虽然网络结构会变得非常复杂，但网络表现得非常好。我们来看一下原理。</p><p>基本思想：不需要人为的决定使用哪个过滤器，或者是否需要池化，而是由网络自行确定这些参数。人们只需给出这些参数的所有可能值，然后把这些输出连起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合。</p><p>举个例子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003102559.png" alt></p><p>如果我们直接计算上图，我们的计算成本为28x28x32x5x5x192</p><p>但是我们用1x1卷积后：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003103150.png" alt></p><p>我们的计算成本变为28x28x16x192+28x28x32x5x5x16，使用1x1卷积后计算成本是没使用前的1/10。</p><p>下面再举一个例子：</p><p>例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=”SAME”)，输出数据的大小为100x100x256。其中，卷积层的参数为5x5x128x256。假如上一层输出先经过具有32个输出的1x1卷积层，这时得图片输出为100x100x32，接着再经过具有256个输出的5x5卷积层，那么最终的输出数据的大小仍为100x100x256，但卷积参数量已经减少为1x1x128x32 + 5x5x32x256，大约减少了4倍。</p><p>在inception结构中，大量采用了1x1的矩阵，主要是两点作用：1）对数据进行降维；2）引入更多的非线性，提高泛化能力，因为卷积后要经过ReLU激活函数。</p><h3 id="搭建inception网络："><a href="#搭建inception网络：" class="headerlink" title="搭建inception网络："></a>搭建inception网络：</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003111139.png" alt></p><p>inception就是将这些模块都组合到一起。</p><h3 id="Inception-network："><a href="#Inception-network：" class="headerlink" title="Inception network："></a>Inception network：</h3><p>下面这个图就是Inception的网络：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003111653.png" alt></p><p>看起来很复杂，但我们截取其中一个环节，如上图的红色框。就会发现这不正是我们搭建的inception吗。另外，再一些inception模块前网络会使用最大池化层来修改高和宽的维度。</p><p>其实上图的Inception并不完整，看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003112235.png" alt></p><p>多出了红色框住的部分。这些分支是做什么的呢?这些分支是通过隐藏层做出预测。</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>我们在做计算机视觉的应用时，相对于从头训练权重，下载别人训练好的网络结构的权重作为我们预训练，然后转换到感兴趣的任务上。别人的训练过程可能是需要花费好几周，并且需要很多的GPU找最优的过程，这就意味着我们可以下载别人开源的权重参数并把它当做一个很好的初始化，用在我们的神经网络上。这就是迁移学习。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003130518.png" alt></p><p>假如我们在做一个识别任务，却没有很多的训练集。我们就可以把别人的网络下载，冻结所有层的参数。我们只需要训练和我们Softmax层有关的参数，然后把别人Softmax改成我们自己Softmax。通过别人的训练的权重，我们可能会得到一个好的结果，即使我们的训练集并不多。</p><p>由于前面的层都冻结了，相当于一个固定函数，不需要改变，因为我们不训练它。</p><p><strong>网络层数越多，需要冻结的层数越少，需要训练的层数就越多。</strong></p><h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><p> 数据扩充也叫数据增强。因为计算机视觉相对于机器学习，数据较少。所以数据增强为了增加数据的数量。下面我们讲一下数据增强的办法：</p><h3 id="垂直镜像对称"><a href="#垂直镜像对称" class="headerlink" title="垂直镜像对称"></a>垂直镜像对称</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003200138.png" alt></p><h3 id="随机裁剪"><a href="#随机裁剪" class="headerlink" title="随机裁剪"></a>随机裁剪</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003200328.png" alt></p><h3 id="色彩转换"><a href="#色彩转换" class="headerlink" title="色彩转换"></a>色彩转换</h3><p>给RGB三个通道加上不同的失真值</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003201235.png" alt></p><p>这些可以轻易改变图像的颜色，但是对目标的识别还是保持不变的。所以使用这种数据增强方法使得模型对照片的颜色更改更具鲁棒性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;经典网络&quot;&gt;&lt;a href=&quot;#经典网络&quot; class=&quot;headerlink&quot; title=&quot;经典网络&quot;&gt;&lt;/a&gt;经典网络&lt;/h2&gt;&lt;h3 id=&quot;LeNet-5&quot;&gt;&lt;a href=&quot;#LeNet-5&quot; class=&quot;headerlink&quot; title=&quot;LeN
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>CNN详解</title>
    <link href="http://yoursite.com/2019/10/01/CNN%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2019/10/01/CNN详解/</id>
    <published>2019-10-01T09:00:13.000Z</published>
    <updated>2019-10-07T12:03:04.374Z</updated>
    
    <content type="html"><![CDATA[<h2 id="全连接神经网络的局限性"><a href="#全连接神经网络的局限性" class="headerlink" title="全连接神经网络的局限性"></a>全连接神经网络的局限性</h2><p>过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为每张图片都有 3 个颜色通道，它的数据量是 64×64×3=12288。</p><p>现代计算机中，64×64 真的是很小的图片，如果想处理大一点的图片，比如1000×1000，那么最终输入层的数据量是300万，假设我们有一个隐藏层，含有1000个神经元构成的全连接网络，那么数据量将达到 1000*300万，也就是30亿。在这样的情况下很难获得足够的数据防止过拟合，并且需要的内存大小很难得到满足。</p><p>本篇讲解的卷积神经网络（也称作 <strong>ConvNets</strong> 或 <strong>CNN</strong>）不仅可以达到减少参数的作用，而且在图像分类任务中还有其他优于全连接神经网络的特性。</p><h2 id="卷积神经网络概览"><a href="#卷积神经网络概览" class="headerlink" title="卷积神经网络概览"></a>卷积神经网络概览</h2><p>一个图像的相互靠近的局部具有联系性，卷积神经网络的思想就是用不同的神经元去学习图片的局部特征，<strong>每个神经元用来感受局部的图像区域</strong>，如下图不同颜色的圆圈所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001172911.png" alt></p><p>然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。</p><p>卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了由<strong>卷积层</strong>和<strong>子采样层</strong>构成的特征抽取器，并且在最后有几个<strong>全连接层</strong>用于对提取的特征进行分类。一个简单的卷积神经网络模型如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173024.png" alt></p><p>接下来我们讲解以下这几个层究竟在做的是什么。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层主要是做卷积运算。本文用 “*” 表示卷积操作。</p><h4 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h4><p>假如我们拥有一个6<em>6的灰度图，矩阵如下图表示，在矩阵右边还有一个3\</em>3的矩阵。我们称之为过滤器(filter)或核。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173735.png" alt></p><p>卷积的第一步就是将过滤器覆盖在灰度图的左上角的数字就是过滤器对应位置上的数字，如下图蓝色部分：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174317.png" alt></p><p>蓝色矩阵每个小个子的左上角的数字就是过滤器对应位置上的数字，将每个格子的两个数字相乘：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174505.png" alt></p><p>然后将得到的矩阵所有元素相加，即3+1+2+0+0+0+(-1)+(-8)+(-2)= -5，然后将这个值放到新的矩阵的左上角，最后的新的矩阵是一个4*4的矩阵。<strong>规律：n*n维的矩阵经过一个f*f过滤器后，得到一个（n-f+1）*（n-f+1）维度的新矩阵。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174907.png" alt></p><p>我们以1为步长(stride)向右移动一格，再进行类似的计算：</p><p>0 × 1 + 5 × 1 + 7 × 1 + 1 × 0 + 8 × 0 + 2 × 0 + 2 × (-1) + 9 × (-1) + 5 × (-1) = -4 写在刚刚得到的-5的右边。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174952.png" alt></p><p>重复向右移动，直到第一行都计算完成：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175037.png" alt></p><p>然后将过滤器向下移动一格，从左边开始继续运算，得到-10，写在新的矩阵第二行第一个位置上：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175208.png" alt></p><p>不断的向右、向下移动，直到计算出所有的数据，这样就完成了卷积运算，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175359.png" alt></p><p>下面这张图也可以帮助我们理解卷积运算(黄色的矩阵式过滤器)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/%E5%8A%A8%E6%80%81.gif" alt></p><p>最终经过卷积得到的图像，我们称之为<strong>特征图(Feature Map)</strong></p><p>对于上述过程，也许你有很多疑问，比如过滤器是什么，为什么是3×3的，作用是什么，滑动的步长stride为什么是1，为什么得到4×4的矩阵，卷积有什么作用等，我们一一解答。</p><h4 id="过滤器-filter"><a href="#过滤器-filter" class="headerlink" title="过滤器(filter)"></a>过滤器(filter)</h4><p>我们来看看下面的例子，本例子中我们用正数表示白色，0表示灰色，负数表示黑色。</p><p>左边 6×6 的矩阵是一个灰度图，左半部分是白色，右半部分是灰色。下面的3×3的过滤器是一种垂直边缘过滤器。</p><p>对二者进行卷积得到新的矩阵，这个矩阵的中间部分数值大于0，显示为白色，而两边为 0， 显示为灰色。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203230.png" alt></p><p>上图中三个红色箭头都是我们过滤得到的边缘。在这里，你可能会疑问为什么会有这么大的边缘，那是因为我们的原始图片太小，如果我们把原始灰度图放大一点：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203434.png" alt></p><p>再进行卷积，就可以得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203509.png" alt></p><p>这样就能大致看出过滤器确实过滤出了垂直边缘。如果想得到水平的边缘，将过滤器转置一下即可。而如果想得到其他角度的边缘，则需要在过滤器中设置不同的数值。下图是水平边缘过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203557.png" alt></p><p>当然很难手动设置过滤器的值去过滤某个特殊角度（比如31°）的边缘，我们通常是<strong>将过滤器中的9个数字当成9个参数</strong>。随机初始化这些参数后，通过神经网络的学习来获得一组值，用于过滤某个角度的边缘。</p><p>目前主流网络的过滤器的大小一般是1x1、3x3、5x5等，数值一般不会太大，因为我们希望这些过滤器可以获得一些更细微的边缘特征。另外，一般将长和宽都取奇数，具体原因我们讲到Padding的时候再解释。</p><h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>Padding是填充。什么是填充呢？</p><p>在前面，我们的例子一个6*6的矩阵经过一个3*3的过滤器后，得到一个4*4的新矩阵。但是这样的话会有两个缺点，第一个缺点就是我们每次做卷积运算的时候，我们的图像都会缩小。可能我们在做完几次卷积之后，我们的图像就会变得很小。第二个缺点是角落边的像素点只会被使用一次，像下图的绿色。但是如果是中间的像素点(类似图中的红色框)，就会被使用很多次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001210310.png" alt></p><p>所以，那些边缘或者角落的像素点在输出时使用较少。这意味者，我们将丢掉图片边缘区域。如果这里有重要信息，那我们就得不偿失了。</p><p>那我们该怎么解决这个问题呢？这时候就用到Padding了。我们可以在卷积操作之前填充这个图像。我们可以沿着图像的边缘再填充一层像素(通常为0，因为用0填充不影响原来的图片特征)。这样做的话，我们的原始图片像素就有6<em>6变成了8\</em>8。接着我们对这幅8*8的图像进行卷积，得到的新矩阵就不是4*4的，而是6*6的。<strong>规律：n*n维的矩阵填充了P个像素点后再经过一个f*f过滤器后，得到一个（n+2p-f+1）*（n+2p-f+1）维度的新矩阵。</strong></p><p>由上面这个规律：我们得出p=(f-1)/2，为了使p是对称填充，所以我们的f通常都是奇数。还有一个原因，是因为当我们有一个奇数的过滤器，我们会有一个中间像素点。这就是为什么前面我们说过滤器通常都是奇数维度。</p><p>那我们Padding填充的时候到底选择多少呢？1还是2还是更大的数字？</p><p>其实Padding有两个选择：一个Valid，一个是Same。</p><p>Valid：意味不填充。也就是之前的6<em>6矩阵经过一个3\</em>3的矩阵后，得出一个4*4的矩阵。</p><p>Same：意味我们的图片输出大小和输入大小是一样的。也就是上面的6<em>6矩阵在Padding=1之后经过一个3\</em>3的矩阵后，得出一个6*6的矩阵。图片输出大小和输入大小一样。</p><h4 id="卷积步长"><a href="#卷积步长" class="headerlink" title="卷积步长"></a>卷积步长</h4><p>我们先用 3x3 的过滤器以2为步长卷积 7x7 的图像，看看会得到什么。</p><p>第一步，对左上角的区域进行计算：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214116.png" alt></p><p>第二步，向右移动，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214201.png" alt></p><p>再继续向右向下移动过滤器，就可以得到下面的结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214424.png" alt></p><p><strong>规律：我们用一个f<em>f的过滤器卷积一个n*n的图像，Padding=p，步长为s，我们得到一个(n+2p-f)/s+1\</em>(n+2p-f)/s+1</strong></p><p>最后一个问题：如果我们的商不是整数呢？这时我们采用向下求整的策略。</p><h4 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h4><p>上面的讲解你已经知道如何对灰度图进行卷积运算了，现在看看如何在有三个通道(RGB)的图像上进行卷积。</p><p>假设彩色图像尺寸 6×6×3，这里的 3 指的是三个颜色通道。<strong>我们约定图像的通道数必须和过滤器的通道数匹配</strong>，所以需要增加过滤器的个数到3个。将他们叠在一起，进行卷积操作。6*6*3的图像经过一个3<em>3\</em>3过滤器得出一个新的4*4*1图像。</p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223726.png" alt></p><p>具体过程就是，首先，把这个 3×3×3 的过滤器先放到最左上角的位置，依次取原图和过滤器对应位置上的各27 个数相乘后求和，得到的数值放到新矩阵的左上角。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223830.png" alt></p><p><strong>注意，由于我们是用三维的过滤器对三维的原图进行卷积操作，所以最终得到的矩阵只有一维。</strong></p><p>然后不断向右向下进行类似操作，直到“遍历”完整个原图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223939.png" alt></p><p>至于效果是怎么样的。结合之前的知识，我们假设了一组垂直边界过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002081928.png" alt></p><p>它可以完成RGB三个通道上垂直边缘的检测工作。这也解释了为什么过滤器使用3个通道：也就是说如果你想获得RGB三个通道上的特征，那么你使用的过滤器也得是3个通道的。</p><p>如果你除了垂直边界，还想要检测更多的边界怎么办？可以使用更多 3维 甚至更多的过滤器，如下图框住的部分，我们增加了一组过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002082118.png" alt></p><p>这样，两组过滤器就得到了两组边界，我们一般会将卷积后得到的图叠加在一起，得到上图左边的 4x4x2 的图像。如果你还想得到更多的边界特征，使用更多的3维的过滤器即可。</p><p>现在我们对维度进行总结：</p><blockquote><p>首先我们假设我们的没有padding且步长为1，这时候使用 n × n × nc 原图和 nc’ 个 f × f × fc 的过滤器进行卷积，得到 (n-f+1) × (n-f+1) × nc’ 的图像</p></blockquote><p>nc 是 原图的通道数(3)，过滤器的通道数和原图的通道数必须一样，都是 nc。</p><p>nc’ 是最终得到的图像的通道数(在上面的例子为2)，由使用的过滤器个数决定(而 nc’ 又是下一层的输入图像的通道数)</p><h4 id="卷积层全貌"><a href="#卷积层全貌" class="headerlink" title="卷积层全貌"></a>卷积层全貌</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002083956.png" alt></p><p>对于卷积层，我们同样需要进行以下操作，这也是前向传播的操作：</p><blockquote><p>z^[1] = w^[1] * a^[0]+b^[1]</p><p>a^[1]=g(z^[1])</p></blockquote><p>上面所用到的变量解释：</p><blockquote><p>a^[0] 是我们原图的数据 X ，也就是上图 6x6x3 的RGB图。</p><p>w^[1] 是这一层的权重矩阵，由 2 个 3x3x3 的过滤器组成。</p><p>b^[1] 是第一层的偏置项，不同的过滤器对应不同的偏置值</p><p>g() 是激活函数，本例子中使用 ReLu</p></blockquote><p>卷积层的工作说明：</p><p>式子中 w^[1] * a^[0] 是代表进行<strong>卷积运算</strong>得到两个特征图(也就是2个4x4的矩阵)，如下图的绿色框所示，接着再对两个特征图<strong>加上不同的偏置</strong>就得到了z^[1]，如下图红色框所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002084726.png" alt></p><p>进一步<strong>应用激活函数</strong>就得到了两个处理过的图像，将他们叠加在一起，就得到了这一层的输出 a^[1](上图右下角)。</p><p><strong>注意</strong>：上图中的加偏置(b1,b2)均使用了python的广播</p><p>简单来说，卷积层的工作就是：<strong>将输入的数据a^[l - 1]进行卷积操作，加上偏置，再经过激活函数非线性化处理得到a^[l]</strong>。到这里卷积层的任务就完成了。</p><h4 id="为什么要使用卷积？"><a href="#为什么要使用卷积？" class="headerlink" title="为什么要使用卷积？"></a>为什么要使用卷积？</h4><p>卷积层的主要优势在稀疏连接就和权值共享</p><h5 id="稀疏连接"><a href="#稀疏连接" class="headerlink" title="稀疏连接"></a>稀疏连接</h5><p>假设我们输入的是32×32×3的RGB图像。</p><p>如果是<strong>全连接网络</strong>，对于隐藏层的某个神经元，它不得不和前一层的所有输入（32×32×3）都保持连接。</p><p>但是，对<strong>卷积神经网络</strong>而言，神经元只和输入图像的局部相连接，如下图圆圈所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002115607.png" alt></p><p>这个局部连接的区域，我们称之为“感受野”，大小等同于过滤器的大小(3*3*3)。</p><p>看下面的图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002121345.png" alt></p><p>右边的绿色或者红色的输出单元仅仅与36个输入特征中的九个相连接，其它像素值对输出不会产生任何影响。</p><p>相比之下，卷积神经网络的神经元与图像的连接要稀疏的多，我们称之为<strong>稀疏连接</strong>。</p><h5 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h5><p>权值共享其实就是过滤器共享。特征检测如垂直边缘检测过滤器如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，如果我们用垂直过滤器扫描整张图片，就可以得到整张图片的所有垂直边缘，而换做水平过滤器就可以扫描出图片的水平边缘。</p><p>在CNN中，我们<strong>用不同的神经元(过滤器)去学习整张图片的不同特征</strong>，而不是利用不同的神经元 学习图片不同的局部特征。因为图像的不同部分也可能有相同的特征。每个特征检测过滤器都可以在输入图片的不同区域中使用同样的参数(即方格内的9个数字)，以便提取垂直边缘或其它特征。右边两张图每个位置是被<strong>同样</strong>的过滤器扫描而得的，所以<strong>权重</strong>是一样的，也就是<strong>共享</strong>。假设有100个神经元，全连接神经网络就需要32×32×3×100=307200个参数。</p><p>因为共享了权值，提取一个特征只需要 f×f 个参数，上图右边两张图的每个像素只与使用 3x3 的过滤器相关，顶多再加上一个偏置项。在本例子中，得到一个feature map 只用到了 3*3+1=10 个参数。如果想得到100个特征，也不过是用去1000个参数。这就解释了为什么利用卷积神经网络可以减少参数个数了。</p><p>当你对提取到的 水平垂直或者其他角度 的特征，再进行卷积操作，就可以得到更复杂的特征，比如一个曲线。如果对得到的曲线再进行卷积操作，又能得到更高维度的特征，比如一个车轮，如此往复，最终可以提取到整个图像全局的特征。</p><p>到这里卷积层的内容就结束了。</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>对于一个32x32像素的图像，假设我们使用400个3x3的过滤器提取特征，每一个过滤器和图像卷积都会得到含有 (32-3 + 1) × (32 - 3 + 1) = 900 个特征的feature map，由于有 400 个过滤器，所以每个样例 (example) 都会得到 900 × 400 = 360000 个特征。而如果是96x96的图像，使用400个8x8的过滤器提取特征，最终得到一个样本的特征个数为 3168400。对于如此大的输入，训练难度大，也容易发生过拟合。</p><p>为了减少下一个卷积层的输入，引入了采样层(也叫池化层)，采样操作分为最大值采样(Max pooling)，平均值采样(Mean pooling)。</p><p><strong>最大值采样</strong></p><p>举个例子，我们用3x3的过滤器以1为步长去扫描图像，每次将重叠的部分的最大值提取出来，作为这部分的特征，过程如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093125.png" alt></p><p>我们也可以使用2x2的过滤器。以2为步长进行特征值的提取。最终得到的图像长度和宽度都缩小一半：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093418.png" alt></p><p><strong>平均值采样</strong></p><p>与最大值采样不同的是 将覆盖部分特征的平均值作为特征，其他过程都一样。经过采样处理，可以提取更为抽象的特征，减少数据处理量的同时保留有效信息。</p><p>下图是过滤器为2x2，步长为2进行特征值的提取：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093447.png" alt></p><p><strong>注意</strong>：池化层没有需要学习的参数。且padding大多数时候为0</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>在经过几次卷积和池化的操作之后，卷积神经网络的最后一步是全连接层，全连接层的个数可能不止一个：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173024.png" alt></p><p><strong>最后一个采样层到全连接层的过渡是通过卷积来实现的</strong>，比如前层输出为 3x3x5 的feature map，那我们就用 3x3x5 的过滤器对前层的这些feature map进行卷积，于是得到了一个神经元的值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002101140.png" alt></p><p>全连接层有1024个神经元，那我们就用1024个3x3x5 的过滤器进行卷积即可。</p><p><strong>第一个全连接层到第二个全连接层的过渡也是通过卷积实现的</strong>，若前层有1024个神经元，这层有512个，那可以看做前层有1024x1x1个feature map，然后用1x1的卷积核对这些feature map进行卷积，则得到100x1x1个feature map。</p><p>卷积神经网络要做的事情，大致分为两步：</p><ol><li>卷积层、采样层负责提取特征</li><li>全连接层用于对提取到的特性进行分类</li></ol><p>最后一步就和普通的神经网络没有什么区别，充当一个分类器的作用，输入是不同特征的特征值，输出是分类。在全连接层后，就可以根据实际情况用softmax对图片进行分类了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;全连接神经网络的局限性&quot;&gt;&lt;a href=&quot;#全连接神经网络的局限性&quot; class=&quot;headerlink&quot; title=&quot;全连接神经网络的局限性&quot;&gt;&lt;/a&gt;全连接神经网络的局限性&lt;/h2&gt;&lt;p&gt;过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深层神经网络</title>
    <link href="http://yoursite.com/2019/09/29/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/29/深层神经网络/</id>
    <published>2019-09-29T13:11:55.000Z</published>
    <updated>2019-10-03T13:10:50.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h2><p>深层神经网络样子也许是下面这样的，也有可能是更多层的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190930110158.png" alt></p><p>上图是一个四层的神经网络(输入层一般记为零层或不记住)，我们用L表示神经网络的层数，n^[l] 表示第 l 层的神经元数量</p><p>由于神经网络具有许多层，在下图中用方框代表一层，每个层都要完成各自的任务，流程图大致如下所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward.png" alt></p><p>蓝框中的部分完成正向传播：</p><blockquote><p>该过程的输入是 X 也就是 A^[0]，一层一层向后计算，最后得到A^[L]。</p><p>并且，在各层 l 计算出 A^[l] 的同时，<strong>缓存</strong>各层的输出值 A^[l] 到变量 cache 中，因为反向传播将会用上。<strong>图中这里写错了，应该是A^[1]、A^[2]、A^[3]的。</strong></p></blockquote><p>绿框中的部分完成反向传播：</p><blockquote><p>该过程的输入是dA^[L]，并根据dA^[L]一步步向前计算，得到各层对应的dZ^[l]，dW^[l]，db^[l]</p><p>此外，还要计算出 dA^[l - 1] 作为前一层的输入</p></blockquote><p>具体过程请看<a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的正向传播/#more)">浅层神经网络的正向传播</a> 和 <a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的反向传播/#more)">浅层神经网络的反向传播</a></p><h2 id="核对矩阵的维数"><a href="#核对矩阵的维数" class="headerlink" title="核对矩阵的维数"></a>核对矩阵的维数</h2><p>为了在写代码的过程中减少出现bug，尤其是在进行反向传播的时候，我们要注意核对矩阵的维数，这个知识点我们前面将 <a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的反向传播/#more)">浅层神经网络的反向传播</a>的文章中也提到了，大致规律如下：</p><blockquote><p>W^[l] ： (n^[l], n^[l - 1])</p><p>Z^[l] ： (n^[l], m),</p><p>A^[l] ： (n^[l], m)</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward4.png" alt></p><p>另外，还有一个规律：</p><blockquote><p>对任意层而言：</p><p>dW 的维度和 W 的维度相同</p><p>dZ 的维度和 Z 的维度相同</p><p>dA 的维度和 A 的维度相同</p></blockquote><h2 id="为什么要使用深度神经网络？"><a href="#为什么要使用深度神经网络？" class="headerlink" title="为什么要使用深度神经网络？"></a>为什么要使用深度神经网络？</h2><p>我们都知道深度神经网络可以解决很多问题，网络并不一定要很大，但一定要有深度，需要比较多的隐藏层。这到底为什么呢？</p><p>如果我们在建立一个人脸识别或者检测系统，当我们输入一张脸部的图片时，我们可以把深度神经网络第一层当成一个特征探测器或者边缘检测器。在这个例子中，我们建立一个大概有20个隐藏单元的深度神经网络。看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190930164522.png" alt></p><p>隐藏单元就是这个图里这些小方块。每个小方块都是一个隐藏单元，它会去寻找一张图的边缘方向。它可能在水平方向找、也可能在竖直方向找。这个东西在卷积神经网络我们会细讲(别问，问就是当时不会)。这里就告诉你们我们可以先把神经网络的第一层当作看图，然后去找这张图片的各个边缘。接着我们把组成图片边缘的像素放在一起看。它可以把探测到的边缘组合成面部的不同部分。比如说，可能有一个神经元回去找眼睛的部分，另外还有找鼻子或其它的部分。然后把这许多边缘结合在一起，就可以开始检测人脸的不同部分，最后再把这些部分放在一起，就可以识别或检测不同的人脸。我们把前几层的神经网络当作简单的检测函数，例如：边缘检测等，之后把它们跟后几层结合在一起。注意：边缘探测器其实相对来说都是针对图片中非常小块的面积。</p><p>具体原因移步网易云课堂的吴恩达的深度学习。<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&_trace_c_p_k2_=613c87c1215241179940b8e203431a3d#/learn/content?type=detail&id=2001701022&cid=2001694285" target="_blank" rel="noopener">链接</a></p><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>在学习算法中还有其它参数，需要输入到学习算法中，比如学习率&alpha;、隐藏层的数量、使用的激活函数种类等都是超参数。因为它们影响着最终W和b的值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;深层神经网络&quot;&gt;&lt;a href=&quot;#深层神经网络&quot; class=&quot;headerlink&quot; title=&quot;深层神经网络&quot;&gt;&lt;/a&gt;深层神经网络&lt;/h2&gt;&lt;p&gt;深层神经网络样子也许是下面这样的，也有可能是更多层的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://r
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>上传项目到GitHub</title>
    <link href="http://yoursite.com/2019/09/28/%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E5%88%B0GitHub/"/>
    <id>http://yoursite.com/2019/09/28/上传项目到GitHub/</id>
    <published>2019-09-28T10:05:42.000Z</published>
    <updated>2019-09-28T11:53:40.265Z</updated>
    
    <content type="html"><![CDATA[<p>首先登录GitHub，没有账号的申请一个。很简单，跳过。</p><p>新建一个仓库：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193118.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193719.png" alt></p><p>记住这个网址，之后用到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928194701.png" alt></p><p>进入项目的目录，点击空白处，选择git Bash。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192144.png" alt></p><p>输入git init，会发现当前目录下多了一个.git文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928191933.png" alt></p><p>接着输入git add .   这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把.换成这个特定的文件名即可。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192510.png" alt></p><p>输入git commit -m “first commit”，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192735.png" alt></p><p>输入git remote add origin https://自己的仓库url地址（上面有说到） 将本地的仓库关联到github上，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928194818.png" alt></p><p>输入git push -u origin master，这是把代码上传到github仓库的意思</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928195326.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先登录GitHub，没有账号的申请一个。很简单，跳过。&lt;/p&gt;
&lt;p&gt;新建一个仓库：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Github的使用</title>
    <link href="http://yoursite.com/2019/09/28/Github%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/09/28/Github的使用/</id>
    <published>2019-09-28T08:10:43.000Z</published>
    <updated>2019-09-28T10:01:01.820Z</updated>
    
    <content type="html"><![CDATA[<p>github的账户创建和仓库创建比较简单，就不赘述了</p><h2 id="github添加ssh账户"><a href="#github添加ssh账户" class="headerlink" title="github添加ssh账户"></a>github添加ssh账户</h2><p>首先，点击账户，选择setting</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928164421.png" alt></p><p>直接添加就完事了。那我们怎么生成ssh公钥呢？</p><p>先回到用户的主目录下，编辑文件.gitconfig，修改某台机器的git配置。修改为注册github的邮箱，填写用户名</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928165020.png" alt></p><p>接着执行命令，ssh-keygen -t rsa -C “邮箱地址”，一路yes。最后生成三个文件：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928165739.png" alt></p><p>id_rsa是机器私钥，自己保留。id_rsa.pub就是我们的公钥。把公钥内容复制到第一张图New SSH Key后的位置。名字可以随便取。</p><h2 id="克隆项目"><a href="#克隆项目" class="headerlink" title="克隆项目"></a>克隆项目</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928170507.png" alt></p><p>接着git clone SSH</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928172714.png" alt></p><p>这个再cmd下执行同样有效。</p><p>如果出错了，执行以下代码：先执行eval “$(ssh-agent -s)”，再执行ssh-add</p><p>不只是可以使用SSH，也可以使用HTTPS。</p><p>git  clone  网址         </p><p>在cmd上同样是有效的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928172950.png" alt></p><h2 id="推送代码"><a href="#推送代码" class="headerlink" title="推送代码"></a>推送代码</h2><p>推送前：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928174030.png" alt></p><p>图中的红框是分支。</p><p>推送分支就是把给分支上的所有本地提交库推送到远程库，推送时要指定本地分支，这样，git就会把该分支推送到远程库对应的远程分支上。</p><p>git push origin 分支名称</p><h2 id="本地分支跟踪远程分支"><a href="#本地分支跟踪远程分支" class="headerlink" title="本地分支跟踪远程分支"></a>本地分支跟踪远程分支</h2><p>git branch –set-upstream-to=origin/远程分支名词  本地分支名称</p><p>跟踪后，如果本地分支和远程分支的进度不一样，使用命令 git status 会提醒。</p><h2 id="拉取代码"><a href="#拉取代码" class="headerlink" title="拉取代码"></a>拉取代码</h2><p>git pull orgin 分支名称</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;github的账户创建和仓库创建比较简单，就不赘述了&lt;/p&gt;
&lt;h2 id=&quot;github添加ssh账户&quot;&gt;&lt;a href=&quot;#github添加ssh账户&quot; class=&quot;headerlink&quot; title=&quot;github添加ssh账户&quot;&gt;&lt;/a&gt;github添加ssh账户
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>git的分支管理</title>
    <link href="http://yoursite.com/2019/09/27/git%E7%9A%84%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/27/git的分支管理/</id>
    <published>2019-09-27T10:01:21.000Z</published>
    <updated>2019-09-28T08:11:44.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分支原理"><a href="#分支原理" class="headerlink" title="分支原理"></a>分支原理</h2><p>git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。HEAD严格来说不是指向提交，而是指向master。master才是指向提交的版本，所以，HEAD指向的就是当前分支。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927191539.png" alt></p><p>每次提交，master分支都会向前移动一步。这样，随着不断提交，master分支的线也越来越长。</p><p>当我们创建新的分支dev，git创建了一个指针叫dev。指向master相同的提交。再把HEAD指向dev，就表示当前分支在dev上。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927191917.png" alt></p><p>git创建一个分支很快，因为除了增加一个dev指针，改变HEAD的指向，工作区的文件没有任何变化。</p><p>假如我们在dev上的工作完成了，就可以把dev合并到master上。怎么合并的呢？git直接把master指向dev的当前提交，就完成了合并。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928102253.png" alt></p><p>合并分支也很快，就改改指针。工作区的内容不变。</p><p>合并完分支后，甚至可以删除dev分支，删除dev分支就是把dev指针给删掉。删掉后，我们只剩下一条master指针。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928102344.png" alt></p><h2 id="分支作用"><a href="#分支作用" class="headerlink" title="分支作用"></a>分支作用</h2><p>分支在实际中有很大用处。假设你准备开发一个新功能，但是需要两周才能完成，第一周只写了一半的代码。如果立刻提交，由于代码还没写完，不完整的代码库会导致别人无法干活，如果等代码全部写完，又存在丢失每天进度的巨大风险。有了分支，就不用怕这些事情。你创建自己的一个分支，别人看不到，也可以在原来的分支上工作。而你还在自己的分支上干活，想提交就提交。直到全部开发完，一次性合并到<strong>主分支</strong>。这样既安全，又不影响别人工作。</p><h2 id="分支基本操作"><a href="#分支基本操作" class="headerlink" title="分支基本操作"></a>分支基本操作</h2><h3 id="查看当前几个分支且能看到在哪个分支工作"><a href="#查看当前几个分支且能看到在哪个分支工作" class="headerlink" title="查看当前几个分支且能看到在哪个分支工作"></a>查看当前几个分支且能看到在哪个分支工作</h3><p>git branch</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928103803.png" alt></p><h3 id="创建分支"><a href="#创建分支" class="headerlink" title="创建分支"></a>创建分支</h3><p>git branch 分支名</p><h3 id="创建分支并切换到其上工作"><a href="#创建分支并切换到其上工作" class="headerlink" title="创建分支并切换到其上工作"></a>创建分支并切换到其上工作</h3><p>git checkout -b dev</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928104927.png" alt></p><h3 id="切换回master分支"><a href="#切换回master分支" class="headerlink" title="切换回master分支"></a>切换回master分支</h3><p>git checkoout master</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928110538.png" alt></p><h3 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h3><p>git merge 分支名字</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928114835.png" alt></p><p>注意上面的Fast-forward，也就是红色框。git告诉我们，这次合并是快速合并，也就是直接把master指向dev的当前提交，所以合并速度非常快</p><h4 id="解决冲突"><a href="#解决冲突" class="headerlink" title="解决冲突"></a>解决冲突</h4><p>合并冲突并不是一番风顺的。在两个分支上修改同一个文件并提交，就会起冲突。解决办法：手动解决冲突，再提交一次</p><p>看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928130118.png" alt></p><p>git告诉我们，git_test2.txt文件存在冲突，必须手动解决冲突再提交。</p><p>打开刚才修改的文件，发现文件修改了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928131316.png" alt></p><p>将文件中新增加的&lt;、= 和 &gt;手动删掉，再提交一次</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928131626.png" alt></p><h3 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h3><p>git branch -d 分支名字</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928115530.png" alt></p><p>这个操作也是非常快的，直接把dev这个指针删了就好了。</p><h2 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h2><p>通常，合并分支时，如果可能，git会用fast forward模式。但是有些快速合并并不能合并但不会起冲突，合并之后并做一次新的提交。在这种模式下，删除分支后，会丢掉部分信息。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928141314.png" alt></p><p>如上图，merge并不会起冲突(因为不是同一个文件)。但是，会出来一个框：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928142350.png" alt></p><p>为什么会出现这个框？前面我们说了，合并分支无法合并但不会起冲突且做一次提交。在这次提交中，需要输入描述信息。就在弹窗输入描述信息</p><p><strong>PS：我太难了，我接下来一直退不出这个框。所以这个例子就这样吧。可以的跟我说一下，拜托了。</strong></p><p>禁用快速合并：</p><p>git merge –no-ff  -m  “描述”  分支名</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928152949.png" alt></p><h2 id="Bug分支"><a href="#Bug分支" class="headerlink" title="Bug分支"></a>Bug分支</h2><p>软件开发中，bug就像家常便饭，有了bug就要修复。在git中，由于分支是如此强大，所以，每个bug都可以通过一个新的临时分支来修复。修复后，合并分支，然后将临时分支删除。这个例子不难，就不贴图了。</p><p>假如你正在写代码，突然老大让你修改一个bug。你需要先保存一下工作现场，修复完bug后，再恢复工作现场。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928154851.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分支原理&quot;&gt;&lt;a href=&quot;#分支原理&quot; class=&quot;headerlink&quot; title=&quot;分支原理&quot;&gt;&lt;/a&gt;分支原理&lt;/h2&gt;&lt;p&gt;git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。H
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>git介绍和基本操作</title>
    <link href="http://yoursite.com/2019/09/26/git%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2019/09/26/git介绍和基本操作/</id>
    <published>2019-09-26T08:02:15.000Z</published>
    <updated>2019-09-28T08:11:56.423Z</updated>
    
    <content type="html"><![CDATA[<p>最近经常用到git和GitHub，但是命令很多都忘记了。故写一篇博客回顾并总结一下</p><h2 id="git介绍"><a href="#git介绍" class="headerlink" title="git介绍"></a>git介绍</h2><p>git是目前世界上最先进的分布式版本系统。</p><p>git的两大特点</p><p>版本控制：可以解决多人同时开发的代码问题，也可以解决找回历史代码的问题</p><p>分布式：git是分布式版本控制系统，同一个git仓库，可以分布到不同的机器上，首先找一台电脑充当服务器的角色。每天24小时开机，其它每个人都从这个“服务器”仓库克隆一份自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交，可以自己搭建服务器，也可以使用github网站。</p><p>git的安装就不赘述了，直接下好安转包。一路next就完事了(不用改安装路径)。</p><h2 id="git基本操作"><a href="#git基本操作" class="headerlink" title="git基本操作"></a>git基本操作</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>通过git我们可以管理一个目录下的代码，首先我们通过<strong>git init</strong>创建一个版本库</p><p>随机切到一个目录，点击空白区域。点击Git Bash Here，如图中所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926172454.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926172852.png" alt></p><p>这样我们就有一个格式为 .git 文件</p><h3 id="版本创建"><a href="#版本创建" class="headerlink" title="版本创建"></a>版本创建</h3><p>首先，git add 文件名.文件格式：添加某个特定的文件</p><p>或</p><p> git add . ：将目录上所有的文件添加到仓库 </p><p>接着，git commit -m “对这个版本的说明信息”</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926175439.png" alt></p><h3 id="查看版本记录"><a href="#查看版本记录" class="headerlink" title="查看版本记录"></a>查看版本记录</h3><p>使用 git log</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926175826.png" alt></p><p>图中的commit 12c6a……代表版本序列号</p><p>随便放了一张图片进项目，第二次提交项目</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926181404.png" alt></p><p>如果版本记录过程，以简短形式显示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927174449.png" alt></p><h3 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h3><p>在git中，有个HEAD指针一直指向当前的版本。用HEAD^代表HEAD前一个版本，HEAD^^代表前两个版本。其它版本类推。那前一百个要写100个^吗？答案当然是不用的，用HEAD~100。HEAD~1 等价于  HEAD^。</p><p>git reset –hard HEAD</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926195030.png" alt></p><p>git reset –hard 版本号：这里的版本号就是commit后的版本序列号</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926195243.png" alt></p><p>版本号不需要写完，写完前几个号码就好了。</p><h3 id="查看操作记录"><a href="#查看操作记录" class="headerlink" title="查看操作记录"></a>查看操作记录</h3><p>git reflog</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926195734.png" alt></p><h3 id="工作区和暂存区"><a href="#工作区和暂存区" class="headerlink" title="工作区和暂存区"></a>工作区和暂存区</h3><p>工作区就相当于我们的目录</p><p>在工作区中，有一个隐藏目录 .git，这个不是工作区。而是git的版本库。其中存了很多东西，其中最重要的是称为stage(或者叫index)的暂存区，还有git为我们自动创建的第一个分支master，以及指向master的一个指针HEAD。</p><p>前面讲了我们把文件往git版本库里添加进去，是分两步执行的：</p><p>第一步是git add把文件添加进去，实际上就是把文件添加到暂存区</p><p>第二步是git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支</p><p>用git status查看当前的状态。如果有新的文件或有文件在修改之后、在add之前，会出现以下情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926205054.png" alt></p><p>把新的文件add之后，得出以下情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926205425.png" alt></p><p>经历add和commit后使用status：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926205807.png" alt></p><p>git管理文件的修改，<strong>它只会提交暂存区的修改来创建版本</strong>。也就是说，在修改或创建一个新文件后，不add到暂存区而直接commit，git也会说文件被修改。</p><h3 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h3><p>git checkout –文件名.文件格式</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926223148.png" alt></p><p>如果是add后，先利用reset取消暂存的变更重新回到工作区：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926223758.png" alt></p><p>接着用git checkout –文件名.文件格式 丢弃工作区的改动</p><p>如果是commit了，就版本回退</p><h3 id="对比文件的不同"><a href="#对比文件的不同" class="headerlink" title="对比文件的不同"></a>对比文件的不同</h3><p><strong>对比工作区和某个版本中文件的不同</strong></p><p>git diff 某个版本库  – 比较的文件</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927164855.png" alt></p><p><strong>对比两个版本间的不同</strong></p><p>git diff 某个版本库  某个版本库 – 比较的文件</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927170025.png" alt></p><p>两个版本交换位置，得出的效果是不同的</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927170142.png" alt></p><h3 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927171813.png" alt></p><p>如果是add到暂存区，也是和之前一样，先reset，再checkout。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927174247.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近经常用到git和GitHub，但是命令很多都忘记了。故写一篇博客回顾并总结一下&lt;/p&gt;
&lt;h2 id=&quot;git介绍&quot;&gt;&lt;a href=&quot;#git介绍&quot; class=&quot;headerlink&quot; title=&quot;git介绍&quot;&gt;&lt;/a&gt;git介绍&lt;/h2&gt;&lt;p&gt;git是目前世界上
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Python实现浅层神经网络</title>
    <link href="http://yoursite.com/2019/09/24/Python%E5%AE%9E%E7%8E%B0%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/24/Python实现浅层神经网络/</id>
    <published>2019-09-24T03:06:10.000Z</published>
    <updated>2019-09-25T14:16:03.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开始前的准备"><a href="#开始前的准备" class="headerlink" title="开始前的准备"></a>开始前的准备</h2><p>本文用到的库：numpy、sklearn、matplotlib</p><p>另外，我们还要借助一下吴恩达老师的工具函数testCases.py 和 planar_utils.py。<a href="[https://github.com/Brickexperts/superficial-net/tree/master/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C](https://github.com/Brickexperts/superficial-net/tree/master/浅层神经网络)">地址</a></p><p>​    testCases：提供测试样本来评估我们的模型。</p><p>​    planar_utils：提供各种有用的函数。</p><p>这两个文件的函数就不具体阐述了，有兴趣的自己研究去吧。</p><h2 id="导入必要的库"><a href="#导入必要的库" class="headerlink" title="导入必要的库"></a>导入必要的库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br></pre></td></tr></table></figure><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#<span class="keyword">X</span>是训练集，<span class="keyword">Y</span>是测试集</span><br><span class="line"><span class="keyword">X</span>,<span class="keyword">Y</span>=load_planar_dataset()</span><br></pre></td></tr></table></figure><p>数据集的说明：</p><ul><li>X是维度为(2, 400)的训练集，两个维度分别代表平面的两个坐标。</li><li>Y是为(1, 400)的测试集，每个元素的值是0（代表红色），或者1（代表蓝色）</li></ul><p>利用 matplotlib 进行数据的可视化操作，这是一个由红点和蓝色的点构成的类似花状的图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">    <span class="comment"># 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23</span></span><br><span class="line">    <span class="comment"># 目的是确保 cost 是一个浮点数</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924125032.png" alt></p><h2 id="构建神经网络模型"><a href="#构建神经网络模型" class="headerlink" title="构建神经网络模型"></a>构建神经网络模型</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130208.png" alt></p><p>对于某个样本x^[i]：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130601.png" alt></p><p>成本函数J：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130811.png" alt></p><p>接下来我们要用代码实现神经网络的结构，步骤大致如下： 1. 定义神经网络结构 2. 随机初始化参数 3. 不断迭代: - 正向传播 - 计算成本函数值 - 利用反向传播得到梯度值 - 更新参数（梯度下降）</p><h3 id="定义神经网络的结构"><a href="#定义神经网络的结构" class="headerlink" title="定义神经网络的结构"></a>定义神经网络的结构</h3><p>主要任务是描述网络各个层的节点个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]  <span class="comment"># 输入层单元数量</span></span><br><span class="line">    n_h = <span class="number">4</span>  <span class="comment"># 隐藏层单元数量，在这个模型中，我们设置成4即可</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]  <span class="comment"># 输出层单元数量</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="随机初始化参数"><a href="#随机初始化参数" class="headerlink" title="随机初始化参数"></a>随机初始化参数</h3><p>参数W的随机初始化是很重要的，如果W和b都初始化成0，那么这一层的所有单元的输出值都一样，导致反向传播时，所有的单元的参数都做出完全相同的调整，这样多个单元的效果和只有一个单元的效果是相同的。那么多层神经网络就没有任何意义。为了避免这样的情况发生，我们会把参数W初始化成非零。</p><p>另外，随机初始化W之后，我们还会乘上一个较小的常数，比如 0.01 ，这样是为了保证输出值 Z 数值都不会太大。为什么这么做？设想我们用的激活函数是 sigmoid 或者 tanh ，那么，太大的 Z 会导致最终 A 会落在函数图像中比较平坦的区域内，这样的地方梯度都接近于0，会降低梯度下降的速度，因此我们会将权重初始化为较小的随机值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">  <span class="comment">#随机初始化</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="循环迭代"><a href="#循环迭代" class="headerlink" title="循环迭代"></a>循环迭代</h3><h4 id="实现正向传播"><a href="#实现正向传播" class="headerlink" title="实现正向传播"></a>实现正向传播</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># 从parameters中取出参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment"># 执行正向传播操作</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">#如果不相等，直接报错</span></span><br><span class="line">    <span class="keyword">assert</span> (A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h4 id="计算成本函数J"><a href="#计算成本函数J" class="headerlink" title="计算成本函数J"></a>计算成本函数J</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130811.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    <span class="comment"># 下一行的结果是 (1, m)的矩阵</span></span><br><span class="line">    logprobs = np.multiply(Y, np.log(A2)) + np.multiply(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A2))</span><br><span class="line">    <span class="comment"># 将 (1, m)的矩阵求和后取平均值</span></span><br><span class="line">    cost = - <span class="number">1</span> / m * np.sum(logprobs)</span><br><span class="line">    <span class="comment"># np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">    <span class="comment"># 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23</span></span><br><span class="line">    <span class="comment"># 目的是确保 cost 是一个浮点数</span></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>利用正向传播函数返回的cache，我们可以实现反向传播了。</p><h4 id="实现反向传播"><a href="#实现反向传播" class="headerlink" title="实现反向传播"></a>实现反向传播</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924133512.png" alt></p><blockquote><p>说明：</p><p>我们使用的 g^[1] () 是 tanh ，并且 A1 = g^[1] ( Z^[1] ) ,那么求导变形后可以得到 g‘ ^[1] ( Z^[1] ) = 1-( A^[1] )^2 用python表示为： 1 - np.power(A1, 2)</p><p>我们使用的 g^[2] () 是 sigmoid ，并且 A2 = g^[2] ( Z^[2] ) ,那么求导变形后可以得到 g‘ ^[2] ( Z^[2] ) =A^2 - Y</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 获取样本的数量</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 从 parameters 和 cache 中取得参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment"># 计算梯度 dW1, db1, dW2, db2.</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span> / m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), <span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><blockquote><p>说明</p><p>1、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。</p><p>2、 keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。</p></blockquote><h4 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h4><p>利用上面反向传播的代码段获得的梯度去更新 (W1, b1, W2, b2)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924134345.png" alt></p><p>式子中 α 是学习率，较小的学习率的值会降低梯度下降的速度，但是可以保证成本函数 J 可以在最小值附近收敛，而较大的学习率让梯度下降速度较快，但是可能会因为下降的步子太大而越过最低点，最终无法在最低点附近出收敛。</p><p>本篇博客选择1.2作为学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    W1 -= learning_rate * dW1</span><br><span class="line">    b1 -= learning_rate * db1</span><br><span class="line">    W2 -= learning_rate * dW2</span><br><span class="line">    b2 -= learning_rate * db2</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="构建完整的神经网络模型"><a href="#构建完整的神经网络模型" class="headerlink" title="构建完整的神经网络模型"></a>构建完整的神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="comment"># 输入层单元数</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>] </span><br><span class="line">    <span class="comment"># 输出层单元数</span></span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 10000次梯度下降的迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播，得到Z1、A1、Z2、A2</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本函数的值</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 反向传播，得到各个参数的梯度值</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate = <span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每迭代1000下就输出一次cost值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回最终训练好的参数</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="对样本进行预测"><a href="#对样本进行预测" class="headerlink" title="对样本进行预测"></a>对样本进行预测</h4><p>我们约定 预测值大于0.5的之后取1，否则取0：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924135028.png" alt></p><p>对一个矩阵M而言，如果你希望它的每个元素 如果大于某个阈值 threshold 就用1表示，小于这个阈值就用 0 表示，那么，在python中可以这么实现：M_new = (M &gt; threshold)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h2 id="利用模型预测"><a href="#利用模型预测" class="headerlink" title="利用模型预测"></a>利用模型预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用含有4个神经元的单隐藏层的神经网络构建分类模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h=<span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 可视化分类结果</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line">print(<span class="string">'准确率: %d'</span> % float((np.dot(Y, predictions.T) + np.dot(<span class="number">1</span> - Y, <span class="number">1</span> - predictions.T)) / float(Y.size) * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h3 id="尝试换一下隐藏层的单元个数"><a href="#尝试换一下隐藏层的单元个数" class="headerlink" title="尝试换一下隐藏层的单元个数"></a>尝试换一下隐藏层的单元个数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><h3 id="换数据集"><a href="#换数据集" class="headerlink" title="换数据集"></a>换数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"><span class="comment"># 在这里选择你要选用的数据集</span></span><br><span class="line">dataset = <span class="string">"noisy_circles"</span></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 测试代码如下：</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 画出边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;开始前的准备&quot;&gt;&lt;a href=&quot;#开始前的准备&quot; class=&quot;headerlink&quot; title=&quot;开始前的准备&quot;&gt;&lt;/a&gt;开始前的准备&lt;/h2&gt;&lt;p&gt;本文用到的库：numpy、sklearn、matplotlib&lt;/p&gt;
&lt;p&gt;另外，我们还要借助一下吴恩达老
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅层神经网络的反向传播</title>
    <link href="http://yoursite.com/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/09/23/浅层神经网络的反向传播/</id>
    <published>2019-09-23T08:11:03.000Z</published>
    <updated>2019-09-30T14:29:04.720Z</updated>
    
    <content type="html"><![CDATA[<h2 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h2><p>首先看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923161512.png" alt></p><p>我们假设n[l] 表示第 l 层的神经元的数量，那么这个单隐层的神经网络的输入层、隐含层和输出层的维度分别是：n[0]= n_x =3、n[1]=4、n[2]=1。</p><p>那么根据<a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的正向传播/#more)">浅层神经网络的正向传播</a>的分析，如果一次性输入带有 m 个样本的矩阵 X 我们可以得到：</p><blockquote><p>W^[1] = (4, 3), Z^[1] = (4, m), A^[1] = (4, m)</p><p>W^[2] = (1, 4), Z^[2] = (1, m), A^[2] = (1, m)</p></blockquote><p>可以总结出如下规律，对第 l 层的单元而言：</p><blockquote><p>W^[l] = (n^[l], n^[l-1]),</p><p>Z^[l] = (n^[l], m),</p><p>A^[l] = (n^[l], m).</p></blockquote><p>另外，还有一个规律：</p><blockquote><p>对任意层而言：</p><p>dW 的维度和 W 的维度相同</p><p>dZ 的维度和 Z 的维度相同</p><p>dA 的维度和 A 的维度相同</p></blockquote><p>注意：以上出现的符号都是将m个样本向量化后的表达。</p><h2 id="单个样本的梯度计算"><a href="#单个样本的梯度计算" class="headerlink" title="单个样本的梯度计算"></a>单个样本的梯度计算</h2><p>讲反向传播前，我们先回顾一下单个样本的正向传播的过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923162211.png" alt></p><p>其中计算z^[1]时用到的 x 可以看做是 a^[0]，σ( ) 是激活函数中一种，一般情况下，我们更习惯用用 g( ) 来表示激活函数。于是将上述式子一般化可以得到第 l 层的公式为：</p><blockquote><p>z^[l] = W^[l]a^[l-1] + b^[l] ——— ①</p><p>a^[l] = g^[l] ( z^[l] ) ——————- ②</p></blockquote><p>由于我们输入的样本只有一个，所以各个向量的维度如下：</p><blockquote><p>W^[l] = (n^[l], n^[l-1]),</p><p>z^[l] = (n^[l], 1),</p><p>a^[1] = (n^[l], 1).</p></blockquote><p>我们现在根据式子 ① 和 ② 来讨论反向传播的过程。</p><p>因为正向传播的时候我们依次计算了z^[1], a^[1], z^[2], a^[2]最终得到了损失函数L。所以反向传播的时候，我们要从L向前计算梯度。</p><p>第一步计算dz^[2]和da^[2]进而算出 dW^[2] 和 db^[2]：</p><blockquote><p>da^[2]=dL/da^[2]=  -y/a^[2]+(1-y)/(1-a^[2])</p><p>dz^[2]= dL/dz^[2] = dL/da^[2]*da^[2]/dz^[2]=-y(1-a^[2])+(1-y)a^[2] = a^[2] - y</p></blockquote><blockquote><p>说明1：</p><p>dz^[2] 的维度和z^[2]相同，da^[2] 的维度和a^[2]相同，为(1, 1)</p><p>g’( z^[2] ) 的维度与 z^[2]维度相同，为(1, 1)</p><p>在第二层中，a^[2] 与 z^[2]的维度也相同，为(1, 1)</p><p>实际上，dz^[2] 应该等于da^[2] 与 g’( z^[2] ) 的内积的结果，理由我们我们先向下看</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203151.png" alt></p><blockquote><p>说明2 ：</p><p>上图中我们可以得到 dW^[2] = dz^[2]乘上 a^[1] 的转置，这里是因为 :</p><p>dW^[2] 和 W^[2] 的形状是一样的(n^[2], n^[1])也就是(1, 4)，</p><p>dz^[2] 和 z^[2] 的形状是一样的(n^[2], 1)也就是(1, 1)，</p><p>a^[1] 的形状是 (n^[1], 1) 也就是(4, 1)，可以得到 a^[1] 的转置 a^( [1]T ) 的形状是 (1, 4)，</p><p>这样 dz^[2] 和 a^[1] 的转置 的乘积的形状才能是 dW^[2] 的形状 (1, 4)；</p><p>因此 dW^[2] = dz^[2]乘上 a^[1] 的转置。</p><p>这里给我们的启发是：<strong>向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行转置操作</strong>。</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203936.png" alt></p><p>第二步计算 da^[1] 和 dz^[1]进而算出 dW^[1] 和 db^[1]</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203959.png" alt></p><blockquote><p>检查矩阵形状是否匹配：（4，1）* （1，1），匹配</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204207.png" alt></p><blockquote><p>说明3：</p><p>dz^[1] 的维度和z^[1]相同，da^[1] 的维度和a^[1]相同，均为(4, 1)</p><p>W^[2]T 的维度是 (4, 1)，dz^[2] 的维度是 (1, 1) ，二者的乘积与da^[1] 维度相同。</p><p>g’( z^[1] ) 的维度与 z^[1]维度相同，也与da^[1] 维度相同，为(4, 1)。</p><p>所以想得到维度为 (4, 1) 的dz^[1] ，da^[1] 与 g’( z^[1] ) 直接的关系为<strong>内积</strong></p><p>这里给我们的启发是：<strong>向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行内积操作</strong>。这里就解释了说明（1）留下的问题。</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204516.png" alt></p><blockquote><p>检查维度：（4，3）=（4，1）*（1，3），正确</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204639.png" alt></p><p>公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward4.png" alt></p><h2 id="多个样本的梯度运算"><a href="#多个样本的梯度运算" class="headerlink" title="多个样本的梯度运算"></a>多个样本的梯度运算</h2><p>用m个样本作为输入，并进行<strong>向量化</strong>后正向传播的公式：</p><blockquote><p>Z^[1] = W^[1]X + b^[1]</p><p>A^[1] = g^[1] ( Z^[1] )</p><p>Z^[2] = W^[2]A^[1] + b^[2]</p><p>A^[2] = g^[2] ( Z^[2] )</p></blockquote><p>由于引入了m个样本，我们也要有个成本函数来衡量整体的表现情况，我们的目的是让J达到最小值</p><p>下图中左边列举了我们上面所推导的各种式子，其中图中所用的激活函数 g^[2] () 为sigmoid函数，因而dz^[2] = a^[2] - y。<strong>这些左边的式子都是针对单个样本而言的，而右边则是将m个样本作为输入并向量化后的公式的表达形式</strong>。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923210356.png" alt></p><blockquote><p>上图的说明：</p><p>1、由于成本函数 J 是所有的 损失函数 L 的值求和后的<strong>平均值</strong>， <strong>那么 J 对 W^[1], b^[1], W^[2], b^[2] 的导数 等价于 各个 L 对 W^[1], b^[1], W^[2], b^[2] 的导数求和后的平均值</strong>。所以dW^[1], db^[1], dW^[2], db^[2]的式子中需要乘上 1 / m。</p><p>2、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。</p><p>3、keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。</p></blockquote><h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><p>最后，不要忘了要更新参数（式子中的&alpha;是学习率）</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923210037.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;符号约定&quot;&gt;&lt;a href=&quot;#符号约定&quot; class=&quot;headerlink&quot; title=&quot;符号约定&quot;&gt;&lt;/a&gt;符号约定&lt;/h2&gt;&lt;p&gt;首先看下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bric
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅层神经网络的正向传播</title>
    <link href="http://yoursite.com/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/09/23/浅层神经网络的正向传播/</id>
    <published>2019-09-23T07:02:03.000Z</published>
    <updated>2019-09-29T11:32:04.164Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络概览"><a href="#神经网络概览" class="headerlink" title="神经网络概览"></a>神经网络概览</h2><p>在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150442.png" alt></p><p>如果把上面的图片抽象化，可以得到以下模型：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150528.png" alt></p><p>再进一步简化得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150605.png" alt></p><p>上图里，小圆圈是sigmoid单元，它要完成的工作就是先计算 z 然后计算 a 最终作为 y帽 输出。</p><p>进而，我们看看下图，这是较为简单的神经网络的样子，它无非就是将多个 sigmoid 单元组合在一起，形成了更复杂的结构。图中每个单元都需要接收数据的输入，并完成数据的输出，其每个单元的计算过程与 logistic回归 的正向传播类似。可以看到，图片里给神经网络分了层次，最左边的是<strong>输入层</strong>，也就是第0层；中间的是<strong>隐藏层</strong>，也就是第一层；最右边的是<strong>输出层</strong>，是第二层。通常，我们不将输入层看做神经网络的一层，因而下图是一个2层的神经网络。 另外要清楚的是，本图中隐藏层只有一个，但实际上，隐藏层可以有多个。由于对用户而言，隐藏层计算得到的数据用户不可预见，也没有太大必要知道，所以称之为隐藏层。也正是因为如此，神经网络的解释器很差。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151156.png" alt></p><p>再进一步认识下这张图片里的标记，隐藏层的每个单元都需要接收输入层的数据，并且各个单元都需要计算 z , 并经过 sigmoid 函数得到各自的 a ，为了便于区分不同层的不同单元的 a，我们做如下约定：</p><p>a 的右上角有个角标[i]，表示这是第i层的单元；a 的右下角有个角标 j 用于区分这是该层自上向下的第 j 个单元。例如我们用 a^[1]_3 表示这是第一层的第三个单元。输入层的 x1, x2, … xn 可以看做是 a^[0]_1, a^[0]_2 … a^[0]_n.</p><p>前一层a[ i ] 的输出，便是后一层 a[ i+1 ] 的<strong>所有单元</strong>输入。除了输入层外的其它层，也就是隐藏层和输出层的单元都有各自的参数 w 和 b 用于计算 z ，同样是用w^[i]_j, b^[i]_j, z^[i]_j 来区分他们；得到 z^[i]_j 后用sigmoid函数计算 a^[i]_j ，再将本层n个单元计算得到的n个 a 作为下一层的输入，最终在输出层得到预测值 y帽。其计算过程大致如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151444.png" alt></p><h2 id="计算一个样本的神经网络输出"><a href="#计算一个样本的神经网络输出" class="headerlink" title="计算一个样本的神经网络输出"></a>计算一个样本的神经网络输出</h2><p>下图是输入单个样本(该样本含有特征x1, x2, x3)的神经网络图，隐藏层有4个单元：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151606.png" alt></p><p>根据上面的说明，要计算该神经网络的输出，我们首先要计算隐藏层4个单元的输出</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151804.png" alt></p><p>第一步就是计算第一层各个单元的 z^[1]_j ，第二步是计算出各个单元的 a^[i]_j，不难想到可以用<strong>向量化计算</strong>来化简上述操作，我们将所有 w^[1]_j 的转置纵向叠加得到下图的内容，我们将这个大的矩阵记为W^[1]，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151938.png" alt></p><p>由于输入的 x 是三维的列向量，所以每个分量x1, x2, x3 都需要一个 w1, w2, w3 对应，因此 w^[1]_j 的转置是一个(1, 3) 的矩阵，又因为隐藏层有4个单元，即 j 的取值为1, 2, 3, 4，故 <strong>W^[1] 是 (4, 3) 的矩阵</strong>。</p><p>同理，第二层，也就是输出层的 W^[2] 由于有4个输入的特征，1个单元，所以 <strong>W^[2] 是 (1, 4)的矩阵</strong>。</p><p>对于只有一个样本的情况，我们不难得到如下式子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152132.png" alt></p><p>z^[1] 是个 (4, 1) 的矩阵。可以再进一步通过 sigmoid 函数得到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152206.png" alt></p><p>至此，第一层神经网络得任务已完成</p><p>第二层也就是输出层的工作，无非就是把第一层的 a^[1] 作为输入，继续用 W^[2] 与 a^[1] 相乘 再加上 b^[2] 得到 z^[2]，再通过 sigmoid 函数得到 a^[2] 也就是最终的预测结果 y帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152532.png" alt></p><p>至此，我们就完成了一个样本的输出。接下来看看如何用矩阵实现多样本的正向输出。</p><h2 id="计算多样本的输出"><a href="#计算多样本的输出" class="headerlink" title="计算多样本的输出"></a>计算多样本的输出</h2><p>假设我们有m个样本，每个样本有3个特征，并且隐藏层也是4个单元。</p><p>那么，通常我们需要使用一个 for 循环将 从 1 遍历至 m 完成以下操作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152921.png" alt></p><p>角标 ^(i) 表示第 i 个样本。我们可以构造这样一个矩阵 x：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153223.png" alt></p><p>它将所有样本的特征，按列叠加在一起，构成 (3, m) 的矩阵。</p><p>如果我们替换上面计算 z^[1] 的过程中使用的 单个样本 x^(1) 为(3, m) 的矩阵 x (也就是下图的绿色框)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153359.png" alt></p><p>就可以得到下面的式子（为了方便表达，下面的公式中假设 b 等于0）：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153641.png" alt></p><p>到这里，我们求出了 Z^[1] ，并且由于 Z^[1] 是(4, 3)的矩阵W^[1]乘以(3, m)的矩阵x 再加上b，所以它是 (4, m) 的矩阵。再经过 sigmoid 函数即可得到同样是 (4, m) 的矩阵 A^[1]，到此隐藏层的工作完成了。</p><h2 id="输出数据"><a href="#输出数据" class="headerlink" title="输出数据"></a>输出数据</h2><p>矩阵 A^[1]，作为下一层（也就是输出层）的输入参数，经过类似的计算也可以得到 Z^[2] = W^[2] × A^[1] + b^[2]，上面我们分析到 W^[2] 是 (1, 4)的矩阵，所以得到的Z^[2]是 (1, m) 的矩阵，同样经过sigmoid函数处理得到的 A^[2] 也是 (1, m) 的矩阵，A^[2]的每个元素，代表一个样本输出的预测值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络概览&quot;&gt;&lt;a href=&quot;#神经网络概览&quot; class=&quot;headerlink&quot; title=&quot;神经网络概览&quot;&gt;&lt;/a&gt;神经网络概览&lt;/h2&gt;&lt;p&gt;在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关系数据库</title>
    <link href="http://yoursite.com/2019/09/19/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://yoursite.com/2019/09/19/关系数据库/</id>
    <published>2019-09-19T12:54:22.000Z</published>
    <updated>2019-09-20T09:22:57.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="域"><a href="#域" class="headerlink" title="域"></a>域</h2><p>域是一组具有相同数据类型的值的集合。基数是域中数据的个数</p><h2 id="笛卡儿积"><a href="#笛卡儿积" class="headerlink" title="笛卡儿积"></a>笛卡儿积</h2><p>笛卡尔积直观意义是诸集合各元素间一切可能的组合，可表示为一个二维表。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E6%95%B0%E6%8D%AE%E5%BA%935.png" alt></p><h2 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h2><p>关系是笛卡尔积的有限集合</p><p>关系可以有三种类型：</p><p>​    基本表：实际存储数据的逻辑表示</p><p>​    查询表：查询结果对应的标</p><p>​    视图表：是虚表，由基本表或其他试图导出，不对应实际存储的数据</p><p>关系的基本性质：</p><p>​    列是同质的，每一列中的分量是同一类型的数据，来自同一个域。</p><p>​    不同的列可出自同一个域，其中的每一列称为一个属性，不同的属性要给予不同的属性名。</p><p>​    列的顺序无所谓，列的次序可以任意交换。</p><p>​    行的顺序无所谓，行的次序可以任意交换</p><p>​    任意两个元祖不能完全相同</p><p>​    分量必须取原子值，每一个分量都必须是不可分的数据项。这是规范条件中最基本的一条</p><h2 id="几个术语："><a href="#几个术语：" class="headerlink" title="几个术语："></a>几个术语：</h2><p>若关系中的某一属性组的值能唯一识别一个元祖，则称该属性为候选码。</p><p>若一个关系有多个候选码，则选定其中一个作为主码。</p><p>候选码的诸属性称为非码属性</p><p>不包含在任何候选码中的属性称为非码属性</p><p>若关系模式的所有属性组是这个关系模式的候选码，则称为全码</p><h2 id="关系模式"><a href="#关系模式" class="headerlink" title="关系模式"></a>关系模式</h2><p>关系模式是型，关系是值。关系模式是对关系的描述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;域&quot;&gt;&lt;a href=&quot;#域&quot; class=&quot;headerlink&quot; title=&quot;域&quot;&gt;&lt;/a&gt;域&lt;/h2&gt;&lt;p&gt;域是一组具有相同数据类型的值的集合。基数是域中数据的个数&lt;/p&gt;
&lt;h2 id=&quot;笛卡儿积&quot;&gt;&lt;a href=&quot;#笛卡儿积&quot; class=&quot;head
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib画图总结</title>
    <link href="http://yoursite.com/2019/09/18/matplotlib%E7%94%BB%E5%9B%BE%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/09/18/matplotlib画图总结/</id>
    <published>2019-09-18T07:00:59.000Z</published>
    <updated>2019-09-19T11:13:24.152Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧</p><p>没有这个库的，pip安装一下。</p><p>首先将matplotlib中的pyplot导入，如下：</p><p>import matplotlib.pyplot as plt，所以以下的plt都是pyplot。</p><h4 id="plt-scatter"><a href="#plt-scatter" class="headerlink" title="plt.scatter"></a>plt.scatter</h4><p>画散点图</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = list(range(1,1001))</span><br><span class="line">y = [x*<span class="number">*2</span> <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line"><span class="comment">#s代表点的面积</span></span><br><span class="line"><span class="comment">#c代表颜色</span></span><br><span class="line">plt.scatter(x,y,<span class="attribute">c</span>=<span class="string">"green"</span>,s=20)</span><br><span class="line"><span class="comment">#设置标题并加上轴标签</span></span><br><span class="line">plt.title(<span class="string">"Squares Numbers"</span>,<span class="attribute">fontsize</span>=24)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line">plt.xlabel(<span class="string">"Square of Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line"><span class="comment">#设置刻度标记的大小</span></span><br><span class="line">plt.tick_params(<span class="attribute">axis</span>=<span class="string">'both'</span>,which='major',labelsize=14)</span><br><span class="line"><span class="comment">#设置每个坐标的取值范围</span></span><br><span class="line">plt.axis([0,1100,0,1100000])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>上面的代码运行后，是一条线。那是因为点太多了。绘制很多点的时候，轮廓会连在一起。要删除数据点的轮廓可用scatter，传递参数edgecolor=”none”</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/matplotlib1.png" alt></p><p>模块pyplot内置了一组颜色映射，要使用这些颜色映射，你需要告诉pyplot该如何设置数据集中每个点的颜色。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = list(range(1,1001))</span><br><span class="line">y = [x*<span class="number">*2</span> <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line"><span class="comment">#s代表点的面积</span></span><br><span class="line"><span class="comment">#cmap设置颜色映射</span></span><br><span class="line">plt.scatter(x,y,<span class="attribute">s</span>=20,c=y,cmap=plt.cm.Greens)</span><br><span class="line"><span class="comment">#设置标题并加上轴标签</span></span><br><span class="line">plt.title(<span class="string">"Squares Numbers"</span>,<span class="attribute">fontsize</span>=24)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line">plt.xlabel(<span class="string">"Square of Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line"><span class="comment">#设置刻度标记的大小</span></span><br><span class="line">plt.tick_params(<span class="attribute">axis</span>=<span class="string">'both'</span>,which='major',labelsize=14)</span><br><span class="line"><span class="comment">#设置每个坐标的取值范围</span></span><br><span class="line">plt.axis([0,1100,0,1100000])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>保存散点图：savefig</p><p>保存图片之前，一定要把plt.show()注释掉，否则会保存一张空白的图片</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#plt</span><span class="selector-class">.show</span>()</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.savefig</span>(<span class="string">"save.jpg"</span>,bbox_inches=<span class="string">"tight"</span>)</span><br></pre></td></tr></table></figure><h4 id="plt-bar"><a href="#plt-bar" class="headerlink" title="plt.bar"></a>plt.bar</h4><p>画柱状图，默认是竖直条形图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/matplotlib2.png" alt></p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"><span class="attr">y</span> = [<span class="number">20</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">25</span>, <span class="number">15</span>]</span><br><span class="line"><span class="attr">x</span> = np.arange(<span class="number">5</span>)</span><br><span class="line"><span class="attr">p1</span> = plt.bar(x, <span class="attr">height=y,</span> <span class="attr">width=0.5,</span> )</span><br><span class="line"><span class="comment"># 展示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>水平条形图：需要把orientation改为horizontal，然后x与y的数据交换</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">x = [20, 10, 30, 25, 15]</span><br><span class="line">y = np.arange(5)</span><br><span class="line"><span class="comment"># x= 起始位置，bottom= 水平条的底部(左侧), y轴， height：水平条的宽度</span></span><br><span class="line"><span class="comment">#width：水平条的长度</span></span><br><span class="line">p1 = plt.bar(<span class="attribute">x</span>=0, <span class="attribute">bottom</span>=y, <span class="attribute">height</span>=0.5, <span class="attribute">width</span>=x, <span class="attribute">orientation</span>=<span class="string">"horizontal"</span>)</span><br><span class="line"><span class="comment"># 展示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="plt-plot-x-y-color-’red’-linewidth-2-5-linestyle-’-‘"><a href="#plt-plot-x-y-color-’red’-linewidth-2-5-linestyle-’-‘" class="headerlink" title="plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘)"></a>plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘)</h4><p> 画线，也可以用来画折线图。x是横坐标的值。y是纵坐标的值。color参数设置曲线颜色，linewidth设置曲线宽度，linestyle设置曲线风格。</p><p>linestyle的可选参数：</p><p>‘-‘       solid line style<br> ‘–’      dashed line style<br> ‘-.’      dash-dot line style<br> ‘:’       dotted line style</p><h4 id="plt-figure"><a href="#plt-figure" class="headerlink" title="plt.figure()"></a>plt.figure()</h4><p>自定义画布大小，画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小。</p><p>figure语法说明</p><p>figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True)</p><p>num:图像编号或名称，数字为编号 ，字符串为名称</p><p>figsize:指定figure的宽和高，单位为英寸；</p><p>dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80</p><p>facecolor:背景颜色</p><p>edgecolor:边框颜色</p><p>frameon:是否显示边框</p><h4 id="plt-xticks"><a href="#plt-xticks" class="headerlink" title="plt.xticks()"></a>plt.xticks()</h4><p>设置x轴刻度的表现方式</p><h4 id="plt-yticks"><a href="#plt-yticks" class="headerlink" title="plt.yticks()"></a>plt.yticks()</h4><p>设置y轴刻度的表现方式</p><h4 id="plt-xlim"><a href="#plt-xlim" class="headerlink" title="plt.xlim()"></a>plt.xlim()</h4><p>设置x轴刻度的取值范围</p><h4 id="plt-ylim"><a href="#plt-ylim" class="headerlink" title="plt.ylim()"></a>plt.ylim()</h4><p>设置y轴刻度的取值范围</p><h4 id="plt-text-1-2-“I’m-a-text”"><a href="#plt-text-1-2-“I’m-a-text”" class="headerlink" title="plt.text(1, 2, “I’m a text”)"></a>plt.text(1, 2, “I’m a text”)</h4><p>前两个参数表示文本坐标, 第三个参数为要添加的文本</p><h4 id="plt-legend"><a href="#plt-legend" class="headerlink" title="plt.legend()"></a>plt.legend()</h4><p>函数实现了图例功能, 他有两个参数, 第一个为样式对象, 第二个为描述字符,都可以为空</p><h4 id="plt-xlabel"><a href="#plt-xlabel" class="headerlink" title="plt.xlabel()"></a>plt.xlabel()</h4><p>添加x轴名字</p><h4 id="plt-ylabel"><a href="#plt-ylabel" class="headerlink" title="plt.ylabel()"></a>plt.ylabel()</h4><p>添加y轴名字</p><h4 id="plt-tight-layout"><a href="#plt-tight-layout" class="headerlink" title="plt.tight_layout()"></a>plt.tight_layout()</h4><p>tight_layout自动调整subplot(s)参数，以便subplot(s)适应图形区域</p><h4 id="plt-subplot"><a href="#plt-subplot" class="headerlink" title="plt.subplot()"></a>plt.subplot()</h4><p>设置画布划分以及图像在画布上输出的位置，将figure设置的画布大小分成几个部分，参数‘221’表示2(row)x2(colu),即将画布分成2x2，两行两列的4块区域，1表示选择图形输出的区域在第一块，图形输出区域参数必须在“行x列”范围</p><h4 id="plt-subplots"><a href="#plt-subplots" class="headerlink" title="plt.subplots()"></a>plt.subplots()</h4><p>subplots(nrows,ncols,sharex,sharey,squeeze,subplot_kw,gridspec_kw,**fig_kw) :创建画布和子图</p><p>1、nrows,ncols表示将画布分割成几行几列。shares，sharey表示坐标轴的属性是否相同，可选的参数：True、False、row、col。默认值为False。</p><p>2、 squeeze  bool</p><p>a.默认参数为True：额外的维度从返回的Axes(轴)对象中挤出，对于N<em>1或1</em>N个子图，返回一个1维数组，对于N*M，N&gt;1和M&gt;1返回一个2维数组。</p><p>b.为False，不进行挤压操作：返回一个元素为Axes实例的2维数组，即使它最终是1x1。</p><p>3、subplot_kw:字典类型，可选参数。把字典的关键字传递给add_subplot()来创建每个子图。</p><p>4、gridspec_kw:字典类型，可选参数。把字典的关键字传递给GridSpec构造函数创建子图放在网格里(grid)。</p><p>5、**fig_kw：把所有详细的关键字参数传给figure()函数。</p><h4 id="plt-grid"><a href="#plt-grid" class="headerlink" title="plt.grid()"></a>plt.grid()</h4><p>是否开启方格，True为开，False为不显示。默认为False</p><h4 id="plt-gca"><a href="#plt-gca" class="headerlink" title="plt.gca()"></a>plt.gca()</h4><p>获取当前的子图</p><h4 id="plt-xcale"><a href="#plt-xcale" class="headerlink" title="plt.xcale()"></a>plt.xcale()</h4><p>改变x轴的比例。pyplot不仅支持线性轴刻度，还支持对数和logit刻度。如果数据跨越许多数量级，则通常使用此方法</p><p>plt.xcale(“logit”)，还有log、symmlog等选择。还可以添加自己的比例。</p><h4 id="plt-yscale"><a href="#plt-yscale" class="headerlink" title="plt.yscale()"></a>plt.yscale()</h4><p>改变y轴的比例。用法同plt.xcale一样</p><p>随便写了个例子，其他具体的用法还是要自己去练习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter([<span class="number">1</span>,<span class="number">3</span>,<span class="number">9</span>],[<span class="number">5</span>,<span class="number">8</span>,<span class="number">2</span>], label=<span class="string">"Example one"</span>,color=<span class="string">"red"</span>,s=<span class="number">25</span>,marker=<span class="string">"o"</span>)</span><br><span class="line">plt.plot([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],[<span class="number">8</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>], label=<span class="string">"Example two"</span>, color=<span class="string">'g'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks([<span class="number">1</span>,<span class="number">10</span>])</span><br><span class="line">plt.yticks([<span class="number">1</span>,<span class="number">15</span>])</span><br><span class="line">plt.xlabel(<span class="string">'bar number'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'bar height'</span>)</span><br><span class="line">plt.title(<span class="string">'test'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧&lt;/p&gt;
&lt;p&gt;没有这个库的，pip安装一下。&lt;/p&gt;
&lt;p&gt;首先将matplotlib中的pyplot导入，如下：&lt;/p&gt;
&lt;p&gt;import matplotlib.pyplot a
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(4)</title>
    <link href="http://yoursite.com/2019/09/17/SVM%E8%A7%A3%E8%AF%BB(4)/"/>
    <id>http://yoursite.com/2019/09/17/SVM解读(4)/</id>
    <published>2019-09-17T07:13:10.000Z</published>
    <updated>2019-09-19T10:21:17.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用SVC时的其他考虑"><a href="#使用SVC时的其他考虑" class="headerlink" title="使用SVC时的其他考虑"></a>使用SVC时的其他考虑</h2><h3 id="SVC处理多分类问题"><a href="#SVC处理多分类问题" class="headerlink" title="SVC处理多分类问题"></a>SVC处理多分类问题</h3><p>之前的所有的SVM的(1)-(3)内容，全部是基于二分类的情况来说明的。因为支持向量机是天生二分类的模型。但是，它也可以做多分类。但是SVC在多分类情况的推广是很难的。因为要研究透彻多分类状况下的SVC，就必须研究透彻多分类时所需要的决策边界个数，每个决策边界所需要的支持向量个数，以及这些支持向量如何组合起来计算拉格朗日乘数。本小节只说一小部分。支持向量机是天在生二分类的模型，所以支持向量机在处理多分类问题的时候，是把多分类问题转换成了二分类问题来解决。这种转换有两种模式，一种叫做“一对一”模式（one vs one），一种叫做“一对多”模式(one vs rest)。 </p><p>在ovo模式下(一对一模式)上，标签中的所有类别都会被两两组合，每两个类别建立一个SVC模型，每个模型生成一个决策边界，分别进行二分类，这种模式下，对于n_class个标签类别的数据来说，SVC会生成总共$C^2_{n_class}$个模型，即会生成总共$C^2_{n_class}$个超平面，其中：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM74.png" alt></p><p>ovo模式下，二维空间的三分类状况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM75.png" alt></p><p>首先让提出紫色点和红色点作为一组，然后求解出两个类之间的SVC和绿色决策边界。然后让绿色点和红色点作为一组，求解出两个类之间的SVC和灰色边界。最后让绿色和紫色组成一组，组成两个类之间的SVC和黄色边界。然后基于三个边界，分别对三个类别进行分类。</p><p>ovr模式下，标签中所有的类别会分别于其他类别进行组合，建立n_class个模型，每个模型生成一个决策边界。分别进行二分类。ovr模式下，则会生成以下的决策边界：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM76.png" alt></p><p>紫色类 vs 剩下的类，生成绿色的决策边界。红色类 vs 剩下的类，生成黄色的决策边界。绿色类 vs 剩下的类，生成灰色的决策边界，当类别更多的时候，如此类推下去，我们永远需要n_class个模型。</p><p>当类别更多的时候，无论是ovr还是ovo模式需要的决策边界都会越来越多，模型也会越来越复杂，不过ovo模式下的模型计算会更加复杂，因为ovo模式中的决策边界数量增加更快，但相对的，ovo模型也会更加精确。ovr模型计算更快，但是效果往往不是很好。在硬件可以支持的情况下，还是建议选择ovo模式。 </p><p>模型和超平面的数量变化了，SVC的很多计算、接口、属性都会发生变化，而参数decision_function_shape决定我们究竟使用哪一种分类模式。</p><h4 id="decision-function-shape"><a href="#decision-function-shape" class="headerlink" title="decision_function_shape"></a>decision_function_shape</h4><p>可输入“ovo”，”ovr”，默认”ovr”，对所有分类器，选择使用ovo或者ovr模式。<br>选择ovr模式，则返回的decision_function结构为(n_samples，n_class)。但是当二分类时，尽管选用ovr模式，却会返回<br>(n_samples，)的结构。 </p><p>选择ovo模式，则使用libsvm中原始的，结构为(n_samples,n_class<em>(n_class-1)/2)的decision_function接口。在ovo模式并且核函数为线性核的情况下，属性coef_和intercepe_会分别返回(n_class\</em>(n_class-1)/2,n_features) 和(n_class*(n_class-1)/2,)的结构，每行对应一个生成的二元分类器。ovo模式只在多分类的状况下使用。</p><p>SVC的其它参数、属性和接口的列表在<a href="[https://brickexperts.github.io/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/#more](https://brickexperts.github.io/2019/09/15/SVM解读(2)/#more">SVM解读(2)</a></p><h2 id="线性支持向量机类LinearSVC"><a href="#线性支持向量机类LinearSVC" class="headerlink" title="线性支持向量机类LinearSVC"></a>线性支持向量机类LinearSVC</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM77.png" alt></p><p>线性支持向量机其实与SVC类中选择”linear”作为核函数的功能类似，但是其背后的实现库是liblinear而不是libsvm，这使得在线性数据上，linearSVC的运行速度比SVC中的“linear”核函数要快，不过两者的运行结果相似。在现实中，许多数据都是线性的，因此我们可以依赖计算得更快得LinearSVC类。除此之外，线性支持向量可以很容易地推广到大样本上，还可以支持稀疏矩阵，多分类中也支持ovr方案。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM79.png" alt></p><p>和SVC一样，LinearSVC也有C这个惩罚参数，但LinearSVC在C变大时对C不太敏感，并且在某个阈值之后就不能再改善结果了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用SVC时的其他考虑&quot;&gt;&lt;a href=&quot;#使用SVC时的其他考虑&quot; class=&quot;headerlink&quot; title=&quot;使用SVC时的其他考虑&quot;&gt;&lt;/a&gt;使用SVC时的其他考虑&lt;/h2&gt;&lt;h3 id=&quot;SVC处理多分类问题&quot;&gt;&lt;a href=&quot;#SVC处理多分
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(3)</title>
    <link href="http://yoursite.com/2019/09/16/SVM%E8%A7%A3%E8%AF%BB(3)/"/>
    <id>http://yoursite.com/2019/09/16/SVM解读(3)/</id>
    <published>2019-09-16T11:00:53.000Z</published>
    <updated>2019-09-17T07:18:46.550Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参数C的理解进阶"><a href="#参数C的理解进阶" class="headerlink" title="参数C的理解进阶"></a>参数C的理解进阶</h2><p>在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，即无法让训练误差为0，这样的数据被我们称为“存在软间隔的数据”。此时此刻，我们需要让我们决策边界能够忍受一小部分训练误差，我们就不能单纯地寻求最大边际了。 因为对于软间隔地数据来说，边际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡。因此，我们引入松弛系数ζ和松弛系数的系数C作为一个惩罚项，来惩罚我们对最大边际的追求。 </p><p>那我们的参数C如何影响我们的决策边界呢？在硬间隔的时候，我们的决策边界完全由两个支持向量和最小化损失函数（最大化边际）来决定，而我们的支持向量是两个标签类别不一致的点，即分别是正样本和负样本。然而在软间隔情况下我们的边际依然由支持向量决定，但此时此刻的支持向量可能就不再是来自两种标签类别的点了，而是分布在决策边界两边的，同类别的点。回忆一下我们的图像： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt></p><p>此时我们的虚线超平面&omega;*x+b=1-ζ<sub>i</sub>是由混杂在红色点中间的紫色点来决定的，所以此时此刻，这个蓝色点就是我们的支持向量了。所以软间隔让决定两条虚线超平面的支持向量可能是来自于同一个类别的样本点，而硬间隔的时候两条虚线超平面必须是由来自两个不同类别的支持向量决定的。而C值会决定我们究竟是依赖红色点作为支持向量（只追求最大边界），还是我们要依赖软间隔中，混杂在红色点中的紫色点来作为支持向量（追求最大边界和判断正确的平衡）。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，尽量将掉落在决策边界另一方的样本点预测正确，决策功能会更简单，但代价是训练的准确度，因为此时会有更多红色的点被分类错误。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">10</span>,<span class="number">16</span>))</span><br><span class="line">    <span class="comment">#第一层循环：在不同的数据集中循环</span></span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">                   , zorder=<span class="number">10</span></span><br><span class="line">                   , cmap=plt.cm.Paired, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">100</span>,</span><br><span class="line">                   facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'white'</span>)</span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">                   levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">                , size=<span class="number">15</span></span><br><span class="line">                , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">                <span class="comment"># 为分数添加一个白色的格子作为底色</span></span><br><span class="line">                , transform=ax.transAxes  <span class="comment"># 确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">                , horizontalalignment=<span class="string">'right'</span>  <span class="comment"># 位于坐标轴的什么方向</span></span><br><span class="line">                )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>白色圈圈出的就是我们的支持向量，大家可以看到，所有在两条虚线超平面之间的点，和虚线超平面外，但属于另一个类别的点，都被我们认为是支持向量。并不是因为这些点都在我们的超平面上，而是因为我们的超平面由所有的这些点来决定，我们可以通过调节C来移动我们的超平面，让超平面过任何一个白色圈圈出的点。参数C就是这样影响了我们的决策，可以说是彻底改变了SVM的决策过程。</p><h2 id="样本不均衡问题"><a href="#样本不均衡问题" class="headerlink" title="样本不均衡问题"></a>样本不均衡问题</h2><p>分类问题永远逃不过的痛点是样本不均衡问题。样本不均衡是指在一组数据集中，标签的一类天生占有很大的比例，但我们有着捕捉出某种特定的分类的需求的状况。</p><p>分类问题天生会倾向于多数的类，让多数类更容易被判断准确，少数类被牺牲掉。因此对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。如果我们希望捕获少数类，模型就会失败。其次，模型评估指标会失去意义。 </p><h3 id="class-weight"><a href="#class-weight" class="headerlink" title="class_weight"></a>class_weight</h3><p>可输入字典或者”balanced”，可不填，默认None 对SVC，将类i的参数C设置为class_weight [i] * C。如果没有给出具体的class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如{“标签的值1”：权重1，”标签的值2”：权重2}的字典，则参数C将会自动被设为：标签的值1的C：权重1 * C，标签的值2的C：权重2*C或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为n_samples/(n_classes * np.bincount(y)) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#其中红色点是少数类，紫色点是多数类</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">1.0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"><span class="comment">#设定class_weight</span></span><br><span class="line">wclf = svm.SVC(kernel=<span class="string">'linear'</span>, class_weight=&#123;<span class="number">1</span>: <span class="number">10</span>&#125;)</span><br><span class="line">wclf.fit(X, y)</span><br><span class="line"><span class="comment">#给两个模型分别打分看看，这个分数是accuracy准确度</span></span><br><span class="line">print(<span class="string">"没设定class_weight："</span>,clf.score(X,y))</span><br><span class="line">print(<span class="string">"设定class_weight："</span>,wclf.score(X,y))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">ax = plt.gca() <span class="comment">#获取当前的子图，如果不存在，则创建新的子图</span></span><br><span class="line">xlim = ax.get_xlim()</span><br><span class="line">ylim = ax.get_ylim()</span><br><span class="line">xx = np.linspace(xlim[<span class="number">0</span>], xlim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">yy = np.linspace(ylim[<span class="number">0</span>], ylim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">YY, XX = np.meshgrid(yy, xx)</span><br><span class="line">xy = np.vstack([XX.ravel(), YY.ravel()]).T</span><br><span class="line"><span class="comment">#第二步：找出我们的样本点到决策边界的距离</span></span><br><span class="line">Z_clf = clf.decision_function(xy).reshape(XX.shape)</span><br><span class="line">a = ax.contour(XX, YY, Z_clf, colors=<span class="string">'black'</span>, levels=[<span class="number">0</span>], alpha=<span class="number">0.5</span>, linestyles=[<span class="string">'-'</span>])</span><br><span class="line">Z_wclf = wclf.decision_function(xy).reshape(XX.shape)</span><br><span class="line">b = ax.contour(XX, YY, Z_wclf, colors=<span class="string">'red'</span>, levels=[<span class="number">0</span>], alpha=<span class="number">0.5</span>, linestyles=[<span class="string">'-'</span>])</span><br><span class="line"><span class="comment">#第三步：画图例 a.collections调用这个等高线对象中画的所有线，返回一个惰性对象</span></span><br><span class="line">plt.legend([a.collections[<span class="number">0</span>], b.collections[<span class="number">0</span>]], [<span class="string">"non weighted"</span>, <span class="string">"weighted"</span>],</span><br><span class="line">loc=<span class="string">"upper right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从图像上可以看出，灰色是我们做样本平衡之前的决策边界。灰色线上方的点被分为一类，下方的点被分为另一类。可以看到，大约有一半少数类（红色）被分错，多数类（紫色点）几乎都被分类正确了。红色是我们做样本平衡之后的决策边界，同样是红色线上方一类，红色线下方一类。可以看到，做了样本平衡后，少数类几乎全部都被分类正确了，但是多数类有许多被分错了。</p><p>从准确率的角度来看，不做样本平衡的时候准确率反而更高，做了样本平衡准确率反而变低了，这是因为做了样本平衡后，为了要更有效地捕捉出少数类，模型误伤了许多多数类样本，而多数类被分错的样本数量 &gt; 少数类被分类正确的样本数量，使得模型整体的精确性下降。现在，如果我们的目的是模型整体的准确率，那我们就要拒绝样本平衡，使用class_weight被设置之前的模型。而在现实情况中，我们往往都在捕捉少数类。因为有些情况是宁愿误伤，也要尽量的捕捉少数类。</p><h2 id="SVC的模型评估指标"><a href="#SVC的模型评估指标" class="headerlink" title="SVC的模型评估指标"></a>SVC的模型评估指标</h2><p>上面说了，我们往往都在捕捉少数类的。但是单纯地追求捕捉出少数类，就会成本太高，而不顾及少数类，又会无法达成模型的效果。所以在现实中，我们往往在寻找捕获少数类的能力和将多数类判错后需要付出的成本的平衡。如果一个模型在能够尽量捕获少数类的情况下，还能够尽量对多数类判断正确，则这个模型就非常优秀了。为了评估这样的能力，我们将引入新的模型评估指标：混淆矩阵和ROC曲线来帮助我们 </p><h3 id="混淆矩阵-Confusion-Matrix"><a href="#混淆矩阵-Confusion-Matrix" class="headerlink" title="混淆矩阵(Confusion Matrix)"></a>混淆矩阵(Confusion Matrix)</h3><h4 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h4><p>精确度Precision，又叫查准率，表示所有被我们预测为是少数类的样本中，真正的少数类所占的比例 </p><p>召回率Recall，又被称为敏感度(sensitivity)，真正率，查全率，表示所有真实为1的样本中，被我们预测正确的样<br>本所占的比例。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt></p><p>如果我们希望不计一切代价，找出少数类，那我们就会追求高召回率，相反如果我们的目标不是尽量捕获少数类，那我们就不需要在意召回率。 注意召回率和精确度的分子是相同的，只是分母不同。而召回率和精确度是此消彼长的，两者之间的平衡代表了捕捉少数类的需求和尽量不要误伤多数类的需求的平衡。究竟要偏向于哪一方，取决于我们的业务需求：<br>究竟是误伤多数类的成本更高，还是无法捕捉少数类的代价更高。</p><p>为了同时兼顾精确度和召回率，我们创造了两者的调和平均数作为考量两者平衡的综合性指标，称之为F1measure。两个数之间的调和平均倾向于靠近两个数中比较小的那一个数，因此我们追求尽量高的F1 measure，能够保证我们的精确度和召回率都比较高。F1 measure在[0,1]之间分布，越接近1越好：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM68.png" alt></p><h4 id="假负率、特异度-真负率-、假正率"><a href="#假负率、特异度-真负率-、假正率" class="headerlink" title="假负率、特异度(真负率)、假正率"></a>假负率、特异度(真负率)、假正率</h4><p>从Recall延申出来的另一个评估指标叫做假负率（False Negative Rate），它等于 1 - Recall，用于衡量所有真实为1的样本中，被我们错误判断为0的，通常用得不多。 </p><p>特异度(Specificity)，也叫做真负率。表示所有真实为0的样本中，被正确预测为0的样本所占的比例。在支持向量机中，可以形象地表示为，决策边界下方的点占所有蓝色点的比例。特异度衡量了一个模型将多数类判断正确的能力，而1 - specificity就是一个模型将多数类判断错误的能力，叫做假正率。</p><p>sklearn当中提供了大量的类来帮助我们了解和使用混淆矩阵：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM69.png" alt></p><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线，全称The Receiver Operating Characteristic Curve，译为受试者操作特性曲线。这是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。让我们先从概率和阈值开始看起。 </p><h4 id="阈值-threshold"><a href="#阈值-threshold" class="headerlink" title="阈值(threshold)"></a>阈值(threshold)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">0.5</span>, <span class="number">1</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LogiR</span><br><span class="line">clf_lo=LogiR().fit(X,y)</span><br><span class="line">prob=clf_lo.predict_proba(X)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">prob=pd.DataFrame(prob)</span><br><span class="line">prob.columns=[<span class="string">"0"</span>,<span class="string">"1"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(prob.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="comment">#将0.5设立为阈值</span></span><br><span class="line">    <span class="keyword">if</span> prob.loc[i,<span class="string">"1"</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">    <span class="comment">#创建了一个新的列名为pred的列</span></span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#创建了名为y_true的列，且将y赋值为该列</span></span><br><span class="line">prob[<span class="string">"y_true"</span>] = y</span><br><span class="line"><span class="comment">#通过“1”列拍序，ascending表示正序or逆序</span></span><br><span class="line">prob = prob.sort_values(by=<span class="string">"1"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM, precision_score <span class="keyword">as</span> P, recall_score <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score <span class="keyword">as</span> A</span><br><span class="line">print(<span class="string">"混淆矩阵："</span>,CM(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"精确度："</span>,P(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"召回率："</span>,R(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"准确率："</span>,A(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>]))</span><br></pre></td></tr></table></figure><p>我们可以设置不同阈值来实验模型的结果，在不同阈值下，我们的模型评估指标会发生变化。我们正利用这一点来观察Recall和FPR之间如何互相影响。但是注意，并不是升高阈值，就一定能够增加或者减少Recall，一切要根据数据的实际分布来进行判断。</p><h4 id="概率-probability"><a href="#概率-probability" class="headerlink" title="概率(probability)"></a>概率(probability)</h4><p>我们在画等高线，也就是决策边界的时候曾经使用SVC的接口decision_function，它返回我们输入的特征矩阵中每个样本到划分数据集的超平面的距离。我们在SVM中利用超平面来判断我们的样本，本质上来说，当两个点的距离是相同的符号的时候，越远离超平面的样本点归属于某个标签类的概率就很大。比如说，一个距离超平面0.1的点，和一个距离超平面100的点，明显是距离为0.1的点更有可能是负类别的点混入了边界。同理，一个距离超平面距离为-0.1的点，和一个离超平面距离为-100的点，明显是-100的点的标签更有可能是负类接口decision_function返回的值也因此被我们认为是SVM中的置信度(confidence)。</p><p>不过，置信度始终不是概率，它没有边界，可以无限大。大部分时候不是也小数的形式呈现，而SVC的判断过程又不像决策树一样求解出一个比例。为了解决这个矛盾，SVC有重要参数probability。</p><p><strong>probability</strong>：是否启用概率估计，布尔值，可不填，默认时False。必须在调用fit之前使用它，启用此功能会减慢SVM的计算速度。设置为True会启动，SVC的接口predict_proba和predict_log_proba将生效。在二分类情况下，SVC将使用Platt缩放生成概率，即在decision_function生成的距离上进行Sigmoid压缩，并附加训练数据的交叉验证拟合，来生成类逻辑回归的SVM分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#其中红色点是少数类，紫色点是多数类</span></span><br><span class="line">clf_proba = svm.SVC(kernel=<span class="string">"linear"</span>,C=<span class="number">1.0</span>,probability=<span class="literal">True</span>).fit(X,y)</span><br><span class="line">print(<span class="string">"返回预测某标签的概率："</span>,clf_proba.predict_proba(X))</span><br><span class="line">print(<span class="string">"行数和列数："</span>,clf_proba.predict_proba(X).shape)</span><br><span class="line">print(<span class="string">"每个样本到划分数据集的超平面的距离："</span>,clf_proba.decision_function(X))</span><br></pre></td></tr></table></figure><h4 id="绘制SVM的ROC曲线"><a href="#绘制SVM的ROC曲线" class="headerlink" title="绘制SVM的ROC曲线"></a>绘制SVM的ROC曲线</h4><p>现在，我们理解了什么是阈值（threshold），了解了不同阈值会让混淆矩阵产生变化，也了解了如何从我们的分类算法中获取概率。现在，我们就可以开始画我们的ROC曲线了。<strong>ROC是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。</strong>简单地来说，只要我们有数据和模型，我们就可以在python中绘制出我们的ROC曲线。思考一下，我们要绘制ROC曲线，就必须在我们的数据中去不断调节阈值，不断求解混淆矩阵，然后不断获得我们的横坐标和纵坐标，最后才能够将曲线绘制出来。接下来，我们就来执行这个过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LogiR</span><br><span class="line">clf_lo=LogiR().fit(X,y)</span><br><span class="line">prob=clf_lo.predict_proba(X)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">prob=pd.DataFrame(prob)</span><br><span class="line">prob.columns=[<span class="string">"0"</span>,<span class="string">"1"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(prob.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="comment">#将0.5设立为阈值</span></span><br><span class="line">    <span class="keyword">if</span> prob.loc[i,<span class="string">"1"</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">    <span class="comment">#创建了一个新的列名为pred的列</span></span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#创建了名为y_true的列，且将y赋值为该列</span></span><br><span class="line">prob[<span class="string">"y_true"</span>] = y</span><br><span class="line">prob = prob.sort_values(by=<span class="string">"1"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">cm = CM(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment">#开始绘图</span></span><br><span class="line">recall = []</span><br><span class="line">FPR = []</span><br><span class="line">clf_proba = svm.SVC(kernel=<span class="string">"linear"</span>,C=<span class="number">1.0</span>,probability=<span class="literal">True</span>).fit(X,y)</span><br><span class="line">probrange = np.linspace(clf_proba.predict_proba(X)</span><br><span class="line">[:,<span class="number">1</span>].min(),clf_proba.predict_proba(X)[:,<span class="number">1</span>].max(),num=<span class="number">50</span>,endpoint=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM, recall_score <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plot</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> probrange:</span><br><span class="line">    y_predict = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> clf_proba.predict_proba(X)[j,<span class="number">1</span>] &gt; i:</span><br><span class="line">            y_predict.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_predict.append(<span class="number">0</span>)</span><br><span class="line">    cm = CM(y,y_predict,labels=[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    recall.append(cm[<span class="number">0</span>,<span class="number">0</span>]/cm[<span class="number">0</span>,:].sum())</span><br><span class="line">    FPR.append(cm[<span class="number">1</span>,<span class="number">0</span>]/cm[<span class="number">1</span>,:].sum())</span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">recall.sort()</span><br><span class="line">FPR.sort()</span><br><span class="line">plt.plot(FPR,recall,c=<span class="string">"red"</span>)</span><br><span class="line">plt.plot(probrange+<span class="number">0.05</span>,probrange+<span class="number">0.05</span>,c=<span class="string">"black"</span>,linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行后，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM70.png" alt></p><p>我们建立ROC曲线的根本目的是找寻Recall和FPR之间的平衡，让我们能够衡量模型在尽量捕捉少数类的时候，误伤多数类的情况会如何变化。横坐标是FPR，代表着模型将多数类判断错误的能力，纵坐标Recall，代表着模型捕捉少数类的能力，所以ROC曲线代表着，随着Recall的不断增加，FPR如何增加。我们希望随着Recall的不断提升，FPR增加得越慢越好，这说明我们可以尽量高效地捕捉出少数类，而不会将很多地多数类判断错误。所以，我们希望看到的图像是，纵坐标急速上升，横坐标缓慢增长，也就是在整个图像左上方的一条弧线。这代表模型的效果很不错，拥有较好的捕获少数类的能力。 </p><p>中间的虚线代表着，当recall增加1%，我们的FPR也增加1%，也就是说，我们每捕捉出一个少数类，就会有一个多数类被判错，这种情况下，模型的效果就不好，这种模型捕获少数类的结果，会让许多多数类被误伤，从而增加我们的成本。ROC曲线通常都是凸型的。对于一条凸型ROC曲线来说，曲线越靠近左上角越好，越往下越糟糕，曲线如果在虚线的下方，则证明模型完全无法使用。但是它也有可能是一条凹形的ROC曲线。对于一条凹型ROC曲线来说，应该越靠近右下角越好，凹形曲线代表模型的预测结果与真实情况完全相反，那也不算非常糟糕，只要我们手动将模型的结果逆转，就可以得到一条左上方的弧线了。最糟糕的就是，无论曲线是凹形还是凸型，曲线位于图像中间，和虚线非常靠近，那我们拿它无能为力。</p><p>现在，我们虽然拥有了这条曲线，但是还是没有具体的数字帮助我们理解ROC曲线和模型效果。接下来将AUC面加，它代表了ROC曲线下方的面积，这个面积越大，代表ROC曲线越接近左上角，模型就越好。</p><h3 id="AUC面积"><a href="#AUC面积" class="headerlink" title="AUC面积"></a>AUC面积</h3><p>sklearn中，我们有帮助我们计算ROC曲线的横坐标假正率FPR，纵坐标Recall和对应的阈值的类<br>sklearn.metrics.roc_curve。同时，我们还有帮助我们计算AUC面积的类sklearn.metrics.roc_auc_score。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM71.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM72.png" alt></p><p>AUC面积的分数使用以上类来进行计算，输入的参数也比较简单，就是真实标签，和与roc_curve中一致的置信度分数或者概率值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">FPR, recall, thresholds = roc_curve(y,clf_proba.decision_function(X), pos_label=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#阈值可以也为负</span></span><br><span class="line">print(FPR,recall,thresholds)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score <span class="keyword">as</span> AUC</span><br><span class="line">area = AUC(y,clf_proba.decision_function(X))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(FPR, recall, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">'ROC curve (area = %0.2f)'</span> % area)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'Receiver operating characteristic example'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="利用ROC曲线找出最佳阈值"><a href="#利用ROC曲线找出最佳阈值" class="headerlink" title="利用ROC曲线找出最佳阈值"></a>利用ROC曲线找出最佳阈值</h2><p>对ROC曲线的理解来：ROC曲线反应的是recall增加的时候FPR如何变化，也就是当模型捕获少数类的能力变强的时候，会误伤多数类的情况是否严重。我们的希望是，模型在捕获少数类的能力变强的时候，尽量不误伤多数类，也就是说，随着recall的变大，FPR的大小越小越好。所以我们希望找到的最有点，其实是Recall和FPR差距最大的点。这个点，又叫做约登指数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">maxindex=(recall-FPR).tolist().index(max(recall-FPR))</span><br><span class="line"><span class="comment">#得出最佳阈值</span></span><br><span class="line">print(<span class="string">"最佳阈值："</span>,thresholds[maxindex])</span><br><span class="line">plt.scatter(FPR[maxindex],recall[maxindex],c=<span class="string">"black"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(FPR, recall, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">'ROC curve (area = %0.2f)'</span> % area)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.scatter(FPR[maxindex],recall[maxindex],c=<span class="string">"black"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'Receiver operating characteristic example'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这样，最佳阈值和最好的点都找了出来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参数C的理解进阶&quot;&gt;&lt;a href=&quot;#参数C的理解进阶&quot; class=&quot;headerlink&quot; title=&quot;参数C的理解进阶&quot;&gt;&lt;/a&gt;参数C的理解进阶&lt;/h2&gt;&lt;p&gt;在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(2)</title>
    <link href="http://yoursite.com/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/"/>
    <id>http://yoursite.com/2019/09/15/SVM解读(2)/</id>
    <published>2019-09-15T05:47:15.000Z</published>
    <updated>2019-09-16T10:54:44.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="非线性SVM与核函数"><a href="#非线性SVM与核函数" class="headerlink" title="非线性SVM与核函数"></a>非线性SVM与核函数</h2><h3 id="SVC在非线性数据上的推广"><a href="#SVC在非线性数据上的推广" class="headerlink" title="SVC在非线性数据上的推广"></a>SVC在非线性数据上的推广</h3><p>为了能够找出非线性数据的线性决策边界，我们需要将数据从原始的空间x投射到新空间Φ(x)中，Φ是一个映射函数，它代表了某种非线性的变换，如同我们之前所做过的使用r来升维一样，这种非线性变换看起来是一种非常有效的方式。使用这种变换，线性SVM的原理可以被很容易推广到非线性情况下，其推到过程核逻辑都与线性SVM一摸一样，只不过在第一决策边界之前，我们必须先对数据进行升维，即将原始的x转换成Φ。</p><h3 id="重要参数kernel"><a href="#重要参数kernel" class="headerlink" title="重要参数kernel"></a>重要参数kernel</h3><p>这种变换非常巧妙，但也带有一些实现问题。首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。其次，已知适当的映射函数，我们想要计算类似于Φ(x)<sub>i</sub>* Φ(x<sub>test</sub>)这样的点积，计算量无法巨大。要找出超平面所付出的代价是非常昂贵的。</p><p><strong>关键概念</strong>：核函数</p><p>而解决上面这些问题的数学方式，叫做”核技巧”。是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。具体体现为，K(u,v)= Φ(u)*Φ(v)。而这个原始空间中的点积函数K(u,v)，就被叫做“核函数”。</p><p>核函数能够帮助我们解决三个问题：</p><p>第一，有了核函数之后，我们无需去担心$\Phi$究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数(positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量的点积一定可以被低维空间中的这两个向量的某种计算来表示。</p><p>第二，使用核函数计算低维度中的向量关系比计算原来的Φ(x<sub>i</sub>) * Φ(x<sub>test</sub>)要简单太多了。</p><p>第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。 </p><p>选用不同的核函数，就可以解决不同数据分布下的寻找超平面问题。在SVC中，这个功能由参数“kernel”和一系列与核函数相关的参数来进行控制。之前的代码中我们一直使用这个参数并输入”linear”，但却没有给大家详细讲解，也是因为如果不先理解核函数本身，很难说明这个参数到底在做什么。参数“kernel”在sklearn中可选以下几种选项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt></p><p>可以看出，除了选项”linear”之外，其他核函数都可以处理非线性问题。多项式核函数有次数d，当d为1的时候它就是再处理线性问题，当d为更高次项的时候它就是在处理非线性问题。我们之前画图时使用的是选项“linear”，自然不能处理环形数据这样非线性的状况。而刚才我们使用的计算r的方法，其实是高斯径向基核函数所对应的功能，在参数”kernel“中输入”rbf“就可以使用这种核函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line">X,y = make_circles(<span class="number">100</span>, factor=<span class="number">0.1</span>, noise=<span class="number">.1</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(model,ax=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ax = plt.gca()</span><br><span class="line">    xlim = ax.get_xlim()</span><br><span class="line">    ylim = ax.get_ylim()</span><br><span class="line">    x = np.linspace(xlim[<span class="number">0</span>],xlim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    y = np.linspace(ylim[<span class="number">0</span>],ylim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    Y,X = np.meshgrid(y,x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    ax.contour(X, Y, P,colors=<span class="string">"k"</span>,levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>],alpha=<span class="number">0.5</span>,linestyles=[<span class="string">"--"</span>,<span class="string">"-"</span>,<span class="string">"--"</span>])</span><br><span class="line">    ax.set_xlim(xlim)</span><br><span class="line">    ax.set_ylim(ylim)</span><br><span class="line">    plt.show()</span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">r = np.exp(-(X**<span class="number">2</span>).sum(<span class="number">1</span>))</span><br><span class="line">rlim = np.linspace(min(r),max(r),<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3D</span><span class="params">(elev=<span class="number">30</span>,azim=<span class="number">30</span>,X=X,y=y)</span>:</span></span><br><span class="line">    ax = plt.subplot(projection=<span class="string">"3d"</span>)</span><br><span class="line">    ax.scatter3D(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],r,c=y,s=<span class="number">50</span>,cmap=<span class="string">'rainbow'</span>)</span><br><span class="line">    ax.view_init(elev=elev,azim=azim)</span><br><span class="line">    ax.set_xlabel(<span class="string">"x"</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">"y"</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">"r"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_3D()</span><br><span class="line">clf = SVC(kernel = <span class="string">"rbf"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plot_svc_decision_function(clf)</span><br></pre></td></tr></table></figure><p>运行后，从效果图可以看到，决策边界被完美的找了出来。</p><h3 id="探索核函数在不同数据集上的表现"><a href="#探索核函数在不同数据集上的表现" class="headerlink" title="探索核函数在不同数据集上的表现"></a>探索核函数在不同数据集上的表现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>,<span class="string">"poly"</span>,<span class="string">"rbf"</span>,<span class="string">"sigmoid"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">20</span>,<span class="number">16</span>))</span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line"><span class="comment">#在图像中的第一列，放置原数据的分布</span></span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="comment">#第二层循环：在不同的核函数中循环</span></span><br><span class="line">    <span class="comment">#从图像的第二列开始，一个个填充分类结果</span></span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">    <span class="comment">#定义子图位置</span></span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        <span class="comment">#建模</span></span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        <span class="comment">#绘制图像本身分布的散点图</span></span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">        ,zorder=<span class="number">10</span></span><br><span class="line">        ,cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制支持向量</span></span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">50</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制决策边界</span></span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        <span class="comment">#np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法</span></span><br><span class="line">        <span class="comment">#一次性使用最大值和最小值来生成网格</span></span><br><span class="line">        <span class="comment">#表示为[起始值：结束值：步长]</span></span><br><span class="line">        <span class="comment">#如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        <span class="comment">#np.c_，类似于np.vstack的功能</span></span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        <span class="comment">#填充等高线不同区域的颜色</span></span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        <span class="comment">#绘制等高线</span></span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">        levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#设定坐标轴为不显示</span></span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="comment">#将标题放在第一行的顶上</span></span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        <span class="comment">#为每张图添加分类的分数</span></span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">        , size=<span class="number">15</span></span><br><span class="line">        , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">        <span class="comment">#为分数添加一个白色的格子作为底色</span></span><br><span class="line">        , transform=ax.transAxes <span class="comment">#确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">        , horizontalalignment=<span class="string">'right'</span> <span class="comment">#位于坐标轴的什么方向</span></span><br><span class="line">        )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。 </p><h3 id="选取与核函数相关的参数"><a href="#选取与核函数相关的参数" class="headerlink" title="选取与核函数相关的参数"></a>选取与核函数相关的参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt></p><p>在知道如何选取核函数后，我们还要观察一下除了kernel之外的核函数相关的参数。对于线性核函数，”kernel”是唯一能够影响它的参数，但是对于其他三种非线性核函数，他们还受到参数gamma，degree以及coef0的影响。参数gamma就是表达式中的&gamma;，degree就是多项式核函数的次数d，参数coef0就是常数项&gamma;。其中，高斯径向基核函数受到gamma的影响，而多项式核函数受到全部三个参数的影响。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM54.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用交叉验证得出最好的参数和准确率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">gamma_range = np.logspace(<span class="number">-10</span>,<span class="number">1</span>,<span class="number">20</span>)</span><br><span class="line">coef0_range = np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">param_grid = dict(gamma = gamma_range</span><br><span class="line">,coef0 = coef0_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid = GridSearchCV(svm.SVC(kernel = <span class="string">"poly"</span>,degree=<span class="number">1</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid, cv=cv)</span><br><span class="line">grid.fit(X, y)</span><br><span class="line">print(<span class="string">"The best parameters are %s with a score of %0.5f"</span> % (grid.best_params_,</span><br><span class="line">grid.best_score_))</span><br></pre></td></tr></table></figure><h3 id="重要参数C"><a href="#重要参数C" class="headerlink" title="重要参数C"></a>重要参数C</h3><p><strong>关键概念</strong>：硬件隔与软件隔</p><p>当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在“硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt></p><p>看上图，原来的决策边界&omega;<em>x+b=0，原本的平行于决策边界的两个虚线超平面&omega;\</em>x+b=1和&omega;*x+b=-1都依然有效。我们的原始判别函数是:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM56.png" alt></p><p>不过，这些超平面现在无法让数据上的训练误差等于0了，因为此时存在了一个混杂在红色点中的紫色点。于是，我们需要放松我们原始判别函数中的不等条件，来让决策边界能够适用于我们的异常点。于是我们引入松弛系数Φ来帮助我们优化原始的判别函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM57.png" alt></p><p>其中ζ<sub>i</sub>&gt;0。可以看的出，这其实是将原来的虚线超平面向图像的上方和下方平移。松弛系数其实蛮好理解的，看上图，在红色点附近的蓝色点在原本的判别函数中必定会被分为红色，所有一定会判断错。我们现在做一条与决策边界平行，且过蓝色点的直线&omega;<em>x<sub>i</sub>+b=1- ζ<sub>i</sub>(图中的蓝色虚线)。这条直线是由&omega;*x<sub>i</sub>+b=1平移得到，所以两条直线在纵坐标上的差异就是ζ<sub>i</sub>。而点&omega;x<sub>i</sub>+b=1的距离就可以表示为 ζ<sub>i</sub>\</em>&omega;/||&omega;||，即ζ<sub>i</sub>在&omega;方向上的投影。由于单位向量是固定的，所以ζ<sub>i</sub>可以作为蓝色点在原始的决策边界上的分类错误的程度的表示。隔得越远，分的越错。<strong>注意：ζ<sub>i</sub>并不是点到决策超平面的距离本身。</strong></p><p>不难注意到，我们让&omega;*x<sub>i</sub>+b&gt;=1-ζ<sub>i</sub>作为我们的新决策超平面，是有一定的问题的。虽然我们把异常的蓝色点分类正确了，但我们同时也分错了一系列红色的点。所以我们必须在我们求解最大边际的损失函数中加上一个惩罚项，用来惩罚我们具有巨大松弛系数的决策超平面。我们的拉格朗日函数，拉格朗日对偶函数，也因此都被松弛系数改变。现在，我们的损失函数为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM58.png" alt></p><p>C是用来控制惩罚力度的系数</p><p>我们的拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM59.png" alt></p><p>需要满足的KKT条件为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM60.png" alt></p><p>拉格朗日对偶函数为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM61.png" alt></p><p>sklearn类SVC背后使用的最终公式。公式中现在唯一的新变量，松弛系数的惩罚力度C，由我们的参数C来进行控制。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM62.png" alt></p><p>在实际使用中，C和核函数的相关参数（gamma，degree等等）们搭配，往往是SVM调参的重点。与gamma不同，C没有在对偶函数中出现，并且是明确了调参目标的，所以我们可以明确我们究竟是否需要训练集上的高精确度来调整C的方向。默认情况下C为1，通常来说这都是一个合理的参数。 如果我们的数据很嘈杂，那我们往往减小C。当然，我们也可以使用网格搜索或者学习曲线来调整C的值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">C1_range = np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C2_range=np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C3_range=np.linspace(<span class="number">5</span>,<span class="number">7</span>,<span class="number">50</span>)</span><br><span class="line">param_grid1 = dict(C=C1_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid1 = GridSearchCV(svm.SVC(kernel = <span class="string">"linear"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid1, cv=cv)</span><br><span class="line">grid1.fit(X, y)</span><br><span class="line">print(<span class="string">"linear  The best parameters are %s with a score of %0.5f"</span> % (grid1.best_params_,</span><br><span class="line">grid1.best_score_))</span><br><span class="line">param_grid2=dict(C=C2_range)</span><br><span class="line">grid2 = GridSearchCV(svm.SVC(kernel = <span class="string">"rbf"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid2, cv=cv)</span><br><span class="line">grid2.fit(X, y)</span><br><span class="line">print(<span class="string">"rbf(0.01,30,50)   The best parameters are %s with a score of %0.5f"</span> % (grid2.best_params_,</span><br><span class="line">grid2.best_score_))</span><br><span class="line">param_grid3=dict(C=C3_range)</span><br><span class="line">grid3=GridSearchCV(svm.SVC(kernel=<span class="string">"rbf"</span>,gamma=<span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),param_grid=param_grid3)</span><br><span class="line">grid3.fit(X,y)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_params_)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_score_)</span><br></pre></td></tr></table></figure><h2 id="SVC的参数、属性和接口"><a href="#SVC的参数、属性和接口" class="headerlink" title="SVC的参数、属性和接口"></a>SVC的参数、属性和接口</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM63.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM64.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM65.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM66.png" alt></p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM67.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;非线性SVM与核函数&quot;&gt;&lt;a href=&quot;#非线性SVM与核函数&quot; class=&quot;headerlink&quot; title=&quot;非线性SVM与核函数&quot;&gt;&lt;/a&gt;非线性SVM与核函数&lt;/h2&gt;&lt;h3 id=&quot;SVC在非线性数据上的推广&quot;&gt;&lt;a href=&quot;#SVC在非线性数
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost</title>
    <link href="http://yoursite.com/2019/09/12/XGBoost/"/>
    <id>http://yoursite.com/2019/09/12/XGBoost/</id>
    <published>2019-09-12T14:19:52.000Z</published>
    <updated>2019-09-16T15:04:21.643Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h2><p>在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中和MAC使用pip来安装xgboost的代码： </p><p>windows：</p><p>pip install xgboost #安装xgboost库<br>pip install –upgrade xgboost #更新xgboost库</p><p>我在这步遇到超时报错，查了以下，改成如下安装：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost7.png" alt></p><p>后面看到一些帖子，发现下面这个方法才是真的好用，在C:\Users\湛蓝星空 这个路径下创建一个pip文件夹，在文件夹创建一个txt，将下面内容加入文件里面，再将文件后缀名改为 .ini。</p><p>[global]<br> index-url = <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost50.png" alt></p><p>这其实就是将源改为清华的源，防止被墙。非常管用</p><p>MAC：</p><p>brew install gcc@7<br>pip3 install xgboost </p><p>安装好XGBoost库后，我们有两种方式来使用我们的XGBoost库。第一种方式。是直接使用XGBoost库自己的建模流程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost1.png" alt></p><p>其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。params可能的取值以及xgboost.train的列表：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost2.png" alt></p><p>我们也可以选择第二种方法，使用xgboost库中的sklearn的API：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost3.png" alt></p><p>有人发现，这两种方法的参数是不同的。其实只是写法不同，功能是相同的。使用xgboost中设定的建模流程来建模，和使用sklearnAPI中的类来建模，模型效果是比较相似的，但是xgboost库本身的运算速度（尤其是交叉验证）以及调参手段比sklearn要简单。</p><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的弱评估器，以及应用中的其他过程。</p><h3 id="梯度提升树"><a href="#梯度提升树" class="headerlink" title="梯度提升树"></a>梯度提升树</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost3.png" alt></p><h4 id="Boosting过程"><a href="#Boosting过程" class="headerlink" title="Boosting过程"></a>Boosting过程</h4><p>XGBoost的基础是梯度提升算法，因此我们必须先从了解梯度提升算法开始。梯度提升（Gradient boosting）是构建预测模型的最强大技术之一，它是集成算法中提升法（Boosting）的代表算法。集成算法通过在数据上构建多个弱评估器，汇总所有弱评估器的建模结果，以获取比单个模型更好的回归或分类表现。弱评估器被定义为是表现至少比随机猜测更好的模型，即预测准确率不低于50%的任意模型。 集成不同弱评估器的方法有很多种。有像我们曾经在随机森林的课中介绍的，一次性建立多个平行独立的弱评估器的装袋法。也有像我们今天要介绍的提升法这样，逐一构建弱评估器，经过多次迭代逐渐累积多个弱评估器的方法。提升法的中最著名的算法包括Adaboost和梯度提升树，XGBoost就是由梯度提升树发展而来的。梯度提升树中可以有回归树也可以有分类树，两者都以CART树算法作为主流，XGBoost背后也是CART树，<strong>这意味着XGBoost中所有的树都是二叉的</strong>。</p><p>接下来，我们来了解一些Boosting算法是上面工作：首先，梯度提升回归树是专注于回归的树模型的提升集成模型，其建模过程大致如下：最开始先建立一棵树，然后逐渐迭代，每次迭代过程中都增加一棵树，逐渐形成众多树模型集成的强评估器。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoosting6.png" alt></p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost6.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor <span class="keyword">as</span> RFR</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LinearR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score <span class="keyword">as</span> CVS, train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">reg = XGBR(n_estimators=<span class="number">100</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"预测："</span>,reg.predict(Xtest)) <span class="comment">#传统接口predict</span></span><br><span class="line">print(<span class="string">"准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line">print(<span class="string">"均方误差："</span>,MSE(Ytest,reg.predict(Xtest)))</span><br><span class="line">print(<span class="string">"模型的重要性分数："</span>,reg.feature_importances_)</span><br></pre></td></tr></table></figure><p>使用参数学习曲线观察n_estimators对模型的影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">axisx = range(<span class="number">10</span>,<span class="number">1010</span>,<span class="number">50</span>)</span><br><span class="line">cv = KFold(n_splits=<span class="number">5</span>, shuffle = <span class="literal">True</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=i,random_state=<span class="number">420</span>)</span><br><span class="line">    rs.append(CVS(reg,Xtrain,Ytrain,cv=cv).mean())</span><br><span class="line">print(axisx[rs.index(min(rs))],max(rs))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"red"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="方差与泛化误差"><a href="#方差与泛化误差" class="headerlink" title="方差与泛化误差"></a>方差与泛化误差</h4><p>机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定。其中偏差就是训练集上的拟合程度决定，方差是模型的稳定性决定，噪音是不可控的。而泛化误差越小，模型就越理想。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost8.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">cv=KFold(n_splits=<span class="number">5</span>,shuffle=<span class="literal">True</span>,random_state=<span class="number">42</span>)</span><br><span class="line">axisx = range(<span class="number">100</span>,<span class="number">300</span>,<span class="number">10</span>)</span><br><span class="line">rs = []</span><br><span class="line">var = []</span><br><span class="line">ge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=i,random_state=<span class="number">420</span>)</span><br><span class="line">    cvresult = CVS(reg,Xtrain,Ytrain,cv=cv)</span><br><span class="line">    rs.append(cvresult.mean())</span><br><span class="line">    var.append(cvresult.var())</span><br><span class="line">    ge.append((<span class="number">1</span> - cvresult.mean())**<span class="number">2</span>+cvresult.var())</span><br><span class="line"><span class="comment">#得出最好的n_estimators</span></span><br><span class="line">print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])</span><br><span class="line">print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))</span><br><span class="line">print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))</span><br><span class="line">rs = np.array(rs)</span><br><span class="line">var = np.array(var)*<span class="number">0.01</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"black"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line"><span class="comment">#添加方差线</span></span><br><span class="line">plt.plot(axisx,rs+var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.plot(axisx,rs-var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#泛化误差的可控部分</span></span><br><span class="line">plt.plot(axisx,ge,c=<span class="string">"gray"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从这个过程中观察n_estimators参数对模型的影响，我们可以得出以下结论： </p><p>首先，XGB中的树的数量决定了模型的学习能力，树的数量越多，模型的学习能力越强。只要XGB中树的数量足够 了，即便只有很少的数据， 模型也能够学到训练数据100%的信息，所以XGB也是天生过拟合的模型。但在这种情况 下，模型会变得非常不稳定。</p><p>第二，XGB中树的数量很少的时候，对模型的影响较大，当树的数量已经很多的时候，对模型的影响比较小，只能有 微弱的变化。当数据本身就处于过拟合的时候，再使用过多的树能达到的效果甚微，反而浪费计算资源。当唯一指标 或者准确率给出的n_estimators看起来不太可靠的时候，我们可以改造学习曲线来帮助我们。</p><p> 第三，树的数量提升对模型的影响有极限，开始，模型的表现会随着XGB的树的数量一起提升，但到达某个点之 后，树的数量越多，模型的效果会逐步下降，这也说明了暴力增加n_estimators不一定有效果。<br>这些都和随机森林中的参数n_estimators表现出一致的状态。在随机森林中我们总是先调整n_estimators，当 n_estimators的极限已达到，我们才考虑其他参数，但XGB中的状况明显更加复杂，当数据集不太寻常的时候会更加 复杂。这是我们要给出的第一个超参数，因此还是建议优先调整n_estimators，一般都不会建议一个太大的数目， 300以下为佳。</p><h4 id="subsample"><a href="#subsample" class="headerlink" title="subsample"></a>subsample</h4><p>我们训练模型之前，必然会有一个巨大的数据集。我们都知道树模型是天生过拟合的模型，并且如果数据量太过巨 大，树模型的计算会非常缓慢，因此，我们要对我们的原始数据集进行有放回抽样（bootstrap）。有放回的抽样每 次只能抽取一个样本，若我们需要总共N个样本，就需要抽取N次。每次抽取一个样本的过程是独立的，这一次被抽 到的样本会被放回数据集中，下一次还可能被抽到，因此抽出的数据集中，可能有一些重复的数据。在无论是装袋还是提升的集成算法中，有放回抽样都是我们防止过拟合。</p><p>在sklearn中，我们使用参数subsample来控制我们的随机抽样。在xgb和sklearn中，这个参数都默认为1且不能取到0，所以取值范围是(0,1]。这说明我们无法控制模型是否进行随机有放回抽样，只能控制抽样抽出来的样本量大概是多少。</p><h4 id="eta-or-learning-rate"><a href="#eta-or-learning-rate" class="headerlink" title="eta   or  learning_rate"></a>eta   or  learning_rate</h4><p>在逻辑回归中，我们自定义步长&alpha;来干涉我们的迭代速率，在XGB中看起来却没有这样的设置，但其实不然。在XGB<br>中，我们完整的迭代决策树的公式应该写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost11.png" alt></p><p>其中&eta;读作”eta”，是迭代决策树时的步长（shrinkage），又叫做学习率（learning rate）。和逻辑回归中的&alpha;类似，&eta;<br>越大，迭代的速度越快，算法的极限很快被达到，有可能无法收敛到真正的最佳。&eta;越小，越有可能找到更精确的最<br>佳值，更多的空间被留给了后面建立的树，但迭代速度会比较缓慢。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost12.png" alt></p><p>在sklearn中，我们使用参数learning_rate来干涉我们的学习速率：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost13.png" alt></p><p>梯度提升树是XGB的基础，本节中已经介绍了XGB中与梯度提升树的过程相关的四个参数：n_estimators，learning_rate ，silent，subsample。这四个参数的主要目的，其实并不是提升模型表现，更多是了解梯度提升树的原理。现在来看，我们的梯度提升树可是说是由三个重要的部分组成：</p><ol><li>一个能够衡量集成算法效果的，能够被最优化的损失函数</li><li>一个能够实现预测的弱评估器</li><li>一种能够让弱评估器集成的手段，包括我们讲解的迭代方法，抽样手段，样本加权等等过程 </li></ol><h4 id="booster"><a href="#booster" class="headerlink" title="booster"></a>booster</h4><p>梯度提升算法中不只有梯度提升树，XGB作为梯度提升算法的进化，自然也不只有树模型一种弱评估器。在XGB中，除了树模型，我们还可以选用线性模型，比如线性回归，来进行集成。虽然主流的XGB依然是树模型，但我们也可以使用其他的模型。基于XGB的这种性质，我们有参数“booster”来控制我们究竟使用怎样的弱评估器。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost14.png" alt></p><p>两个参数都默认为”gbtree”，如果不想使用树模型，则可以自行调整。当XGB使用线性模型的时候，它的许多数学过<br>程就与使用普通的Boosting集成非常相似。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> booster <span class="keyword">in</span> [<span class="string">"gbtree"</span>,<span class="string">"gblinear"</span>,<span class="string">"dart"</span>]:</span><br><span class="line">reg = XGBR(<span class="attribute">n_estimators</span>=180,learning_rate=0.1,random_state=420,booster=booster).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="builtin-name">print</span>(booster)</span><br><span class="line"><span class="builtin-name">print</span>(reg.score(Xtest,Ytest))</span><br></pre></td></tr></table></figure><h4 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h4><p>在众多机器学习算法中，损失函数的核心是衡量模型的泛化能力，即模型在未知数据上的预测的准确与否，我们训练模型的核心目标也是希望模型能够预测准确。在XGB中，预测准确自然是非常重要的因素，但我们之前提到过，XGB的是实现了模型表现和运算速度的平衡的算法。普通的损失函数，比如错误率，均方误差等，都只能够衡量模型的表现，无法衡量模型的运算速度。回忆一下，我们曾在许多模型中使用空间复杂度和时间复杂度来衡量模型的运算效率。XGB因此引入了模型复杂度来衡量算法的运算效率。因此XGB的目标函数被写作：传统损失函数 + 模型复杂度。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost15png" alt></p><p>其中i代表数据集中的第i个样本，m表示导入第K棵树的数据总量，K代表建立的所有树(n_estimators)。 第二项代表模型的复杂度，使用树模型的某种变换$\Omega$表示，这个变化代表了一个从树的结构来衡量树模型的复杂度的式子,可以有多种定义。我们在迭代每一颗的过程中，都最小化Obj来求最优的yi。</p><p>在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。一个集成模型(f)<br>在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定，而泛化误差越小，模型就越理想。从下面的图可以看出来，方差和偏差是此消彼长的，并且模型的复杂度越高，方差越大，偏差越小。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest6.png" alt></p><p>方差可以被简单地解释为模型在不同数据集上表现出来地稳定性，而偏差是模型预测的准确度。那方差-偏差困境就<br>可以对应到我们的Obj中了： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost17.png" alt></p><p>第一项是衡量我们的偏差，模型越不准确，第一项就会越大。第二项是衡量我们的方差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。所以我们求解Obj的最小值，其实是在求解方差与偏差的平衡点，以求模型的泛化误差最小，运行速度最快。 </p><p>在应用中，我们使用参数“objective”来确定我们目标函数的第一部分，也就是衡量损失的部分。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost18.png" alt></p><p>xgb自身的调用方式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost1.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#默认reg:linear</span></span><br><span class="line">reg = XGBR(n_estimators=<span class="number">180</span>,random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"sklearn中的XGboost准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line">print(<span class="string">"sklearn中的xgb均方误差："</span>,MSE(Ytest,reg.predict(Xtest)))</span><br><span class="line"><span class="comment">#xgb实现法</span></span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="comment">#使用类Dmatrix读取数据</span></span><br><span class="line">dtrain = xgb.DMatrix(Xtrain,Ytrain)</span><br><span class="line">dtest = xgb.DMatrix(Xtest,Ytest)</span><br><span class="line"><span class="comment">#写明参数，silent默认为False，通常需要手动将它关闭</span></span><br><span class="line">param = &#123;<span class="string">'silent'</span>:<span class="literal">False</span>,<span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="string">"eta"</span>:<span class="number">0.1</span>&#125;</span><br><span class="line">num_round = <span class="number">180</span></span><br><span class="line"><span class="comment">#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment">#接口predict</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">print(<span class="string">"xgb中的准确率："</span>,r2_score(Ytest,bst.predict(dtest)))</span><br><span class="line">print(<span class="string">"xgb中的均方误差："</span>,MSE(Ytest,bst.predict(dtest)))</span><br></pre></td></tr></table></figure><p>看得出来，无论是从R<sup>2</sup>还是从MSE的角度来看，都是xgb库本身表现更优秀。</p><h4 id="alpha-or-reg-alpha-amp-lambda-or-reg-lambda"><a href="#alpha-or-reg-alpha-amp-lambda-or-reg-lambda" class="headerlink" title="(alpha or reg_alpha)  &amp; (lambda or  reg_lambda)"></a>(alpha or reg_alpha)  &amp; (lambda or  reg_lambda)</h4><p>对于XGB来说，每个叶子节点上会有一个预测分数（prediction score），也被称为叶子权重。这个叶子权重就是所有在这个叶子节点上的样本在这一棵树上的回归取值,用f<sub>k</sub>(x<sub>i</sub>)或者&omega;来表示。</p><p>当有多棵树的时候，集成模型的回归结果就是所有树的预测分数之和，假设这个集成模型中总共有K棵决策树，则整<br>个模型在这个样本i上给出的预测结果为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost21.png" alt></p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost22.png" alt></p><p>设一棵树上总共包含了T个叶子节点，其中每个叶子节点的索引为j，则这个叶子节点上的样本权重是w<sub>j</sub>。依据这个，我们定义模型的复杂度$\Omega$(f)为（注意这不是唯一可能的定义，我们当然还可以使用其他的定义，只要满足叶子越多/深度越大，复杂度越大的理论):</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost23.png" alt></p><p>使用L2正则项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost24.png" alt></p><p>使用L1正则项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost25.png" alt></p><p>还可以两个一起用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost26.png" alt></p><p>这个结构中有两部分内容，一部分是控制树结构的&gamma;，另一部分则是我们的正则项。叶子数量<em>T</em>可以代表整个树结构，这是因为在XGBoost中所有的树都是CART树（二叉树），所以我们可以根据叶子的数量<em>T</em>判断出树的深度，而&gamma;是我们自定的控制叶子数量的参数。 至于第二部分正则项，类比一下我们岭回归和Lasso的结构，参数&alpha;和&lambda;的作用其实非常容易理解，他们都是控制正则化强度的参数，我们可以二选一使用，也可以一起使用加大正则化的力度。当 和 都为0的时候，目标函数就是普通的梯度提升树的目标函数。</p><p>来看正则化系数分别对应的参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost20.png" alt></p><h4 id="gamma"><a href="#gamma" class="headerlink" title="gamma"></a>gamma</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost28.png" alt></p><p>回忆一下决策树中我们是如何进行计算：我们使用基尼系数或信息熵来衡量分枝之后叶子节点的不纯度，分枝前的信息熵与分治后的信息熵之差叫做信息增益，信息增益最大的特征上的分枝就被我们选中，当信息增益低于某个阈值时，就让树停止生长。在XGB中，我们使用的方式是类似的：我们首先使用目标函数来衡量树的结构的优劣，然后让树从深度0开始生长，每进行一次分枝，我们就计算目标函数减少了多少，当目标函数的降低低于我们设定的某个阈值时，就让树停止生长。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost27.png" alt></p><p>原理还不是很明白，先贴最后的Gain函数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost29.png" alt></p><p>从上面的Gain函数，从上面的目标函数和结构分数之差Gain的式子来看，&gamma;使我们每增加一片叶子就会被剪去的惩罚项。增加的叶子越多，结构分数之差Gain会被惩罚越重，所以&gamma;也被称为”复杂性控制“。所以&gamma;是我们用来防止过拟合的重要参数。&gamma;是对梯度提升树影响最大的参数之一，其效果不逊色与n_estimators和放过拟合神器max_depth。同时&gamma;还是我们让树停止生长的重要参数。</p><p>在XGB中，规定只要结构分数之差Gain大于0，即只要目标函数还能减小，我们就允许继续进行分枝。也就是说，我们对于目标函数减小量的要求是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost30.png" alt></p><p>因此，我们可以直接通过设定&gamma;的大小让XGB的树停止生长。&gamma;因此被定义为，在树的叶节点上进行进一步分枝所需的最小目标函数减少量，在决策树和随机森林中也有类似的参数(min_split_loss，min_samples_split)。 设定越大，算法就越保守，树的叶子数量就越少，模型的复杂度就越低。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> CVS</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">axisx = np.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">0.05</span>)</span><br><span class="line">rs = []</span><br><span class="line">var = []</span><br><span class="line">ge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=<span class="number">180</span>,random_state=<span class="number">420</span>,gamma=i)</span><br><span class="line">    result = CVS(reg,Xtrain,Ytrain,cv=<span class="number">20</span>)</span><br><span class="line">    rs.append(result.mean())</span><br><span class="line">    var.append(result.var())</span><br><span class="line">    ge.append((<span class="number">1</span> - result.mean())**<span class="number">2</span>+result.var())</span><br><span class="line">print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])</span><br><span class="line">print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))</span><br><span class="line">print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))</span><br><span class="line">rs = np.array(rs)</span><br><span class="line">var = np.array(var)*<span class="number">0.1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"black"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line">plt.plot(axisx,rs+var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.plot(axisx,rs-var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>为了调整&gamma;，我们需要引入新的工具，xgboost库中的类xgboost.cv</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost31.png" alt></p><p>为了使用xgboost.cv，我们必须要熟悉xgboost自带的模型评估指标。xgboost在建库的时候本着大而全的目标，和sklearn类似，包括了大约20个模型评估指标，然而用于回归和分类的其实只有几个，大部分是用于一些更加高级的功能比如ranking。来看用于回归和分类的评估指标都有哪些：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost32.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">data2 = load_breast_cancer()</span><br><span class="line">x2 = data2.data</span><br><span class="line">y2 = data2.target</span><br><span class="line">dfull2 = xgb.DMatrix(x2,y2)</span><br><span class="line">param1 = &#123;<span class="string">'silent'</span>:<span class="literal">True</span>,<span class="string">'obj'</span>:<span class="string">'binary:logistic'</span>,<span class="string">"gamma"</span>:<span class="number">0</span>,<span class="string">"nfold"</span>:<span class="number">5</span>&#125;</span><br><span class="line">param2 = &#123;<span class="string">'silent'</span>:<span class="literal">True</span>,<span class="string">'obj'</span>:<span class="string">'binary:logistic'</span>,<span class="string">"gamma"</span>:<span class="number">2</span>,<span class="string">"nfold"</span>:<span class="number">5</span>&#125;</span><br><span class="line">num_round = <span class="number">100</span></span><br><span class="line">cvresult1 = xgb.cv(param1, dfull2, num_round,metrics=(<span class="string">"error"</span>))</span><br><span class="line">cvresult2 = xgb.cv(param2, dfull2, num_round,metrics=(<span class="string">"error"</span>))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult1.iloc[:,<span class="number">0</span>],c=<span class="string">"red"</span>,label=<span class="string">"train,gamma=0"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult1.iloc[:,<span class="number">2</span>],c=<span class="string">"orange"</span>,label=<span class="string">"test,gamma=0"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult2.iloc[:,<span class="number">0</span>],c=<span class="string">"green"</span>,label=<span class="string">"train,gamma=2"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult2.iloc[:,<span class="number">2</span>],c=<span class="string">"blue"</span>,label=<span class="string">"test,gamma=2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="scale-pos-weight"><a href="#scale-pos-weight" class="headerlink" title="scale_pos_weight"></a>scale_pos_weight</h4><p>XGB中存在着调节样本不平衡的参数scale_pos_weight,这个参数非常类似于之前随机森林和支持向量机中我们都使用到过的class_weight参数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost34.png" alt></p><h4 id="其它参数"><a href="#其它参数" class="headerlink" title="其它参数"></a>其它参数</h4><p>XGBoost应用的核心之一就是减轻过拟合带来的影响。作为树模型，减轻过拟合的方式主要是靠对决策树剪枝来降低模型的复杂度，以求降低方差。在之前的讲解中，我们已经学习了好几个可以用来防止过拟合的参数，包括上一节提到的复杂度控制&lambda;，正则化的两个参数&lambda;和&alpha;，控制迭代速度的参数 以及管理每次迭代前进行的随机有放回抽样的参数subsample。所有的这些参数都可以用来减轻过拟合。但除此之外，我们还有几个影响重大的，专用于剪枝的参数： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost33.png" alt></p><h2 id="使用Pickle保存和调用模型"><a href="#使用Pickle保存和调用模型" class="headerlink" title="使用Pickle保存和调用模型"></a>使用Pickle保存和调用模型</h2><p>pickle是python编程中比较标准的一个保存和调用模型的库，我们可以使用pickle和open函数的连用，来讲我们的模型保存到本地。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型的coding</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">data2 = load_breast_cancer()</span><br><span class="line">x2 = data2.data</span><br><span class="line">y2 = data2.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(x2,y2,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">dtrain = xgb.DMatrix(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#设定参数，对模型进行训练</span></span><br><span class="line">param = &#123;<span class="string">'silent'</span>:<span class="literal">True</span></span><br><span class="line">,<span class="string">'obj'</span>:<span class="string">'reg:linear'</span></span><br><span class="line">,<span class="string">"subsample"</span>:<span class="number">1</span></span><br><span class="line">,<span class="string">"eta"</span>:<span class="number">0.05</span></span><br><span class="line">,<span class="string">"gamma"</span>:<span class="number">20</span></span><br><span class="line">,<span class="string">"lambda"</span>:<span class="number">3.5</span></span><br><span class="line">,<span class="string">"alpha"</span>:<span class="number">0.2</span></span><br><span class="line">,<span class="string">"max_depth"</span>:<span class="number">4</span></span><br><span class="line">,<span class="string">"colsample_bytree"</span>:<span class="number">0.4</span></span><br><span class="line">,<span class="string">"colsample_bylevel"</span>:<span class="number">0.6</span></span><br><span class="line">,<span class="string">"colsample_bynode"</span>:<span class="number">1</span>&#125;</span><br><span class="line">num_round = <span class="number">180</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">pickle.dump(bst, open(<span class="string">"xgboostonboston.dat"</span>,<span class="string">"wb"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#调用模型的coding</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#注意，如果我们保存的模型是xgboost库中建立的模型，则导入的数据类型也必须是xgboost库中的数据类型</span></span><br><span class="line">dtest = xgb.DMatrix(Xtest,Ytest)</span><br><span class="line"><span class="comment">#导入模型</span></span><br><span class="line">loaded_model = pickle.load(open(<span class="string">"xgboostonboston.dat"</span>, <span class="string">"rb"</span>))</span><br><span class="line">print(<span class="string">"Loaded model from: xgboostonboston.dat"</span>)</span><br><span class="line"><span class="comment">#做预测</span></span><br><span class="line">ypreds = loaded_model.predict(dtest)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE, r2_score</span><br><span class="line">print(<span class="string">"均方误差："</span>,MSE(Ytest,ypreds))</span><br><span class="line">print(r2_score(Ytest,ypreds))</span><br></pre></td></tr></table></figure><h2 id="使用Joblib保存和调用模型"><a href="#使用Joblib保存和调用模型" class="headerlink" title="使用Joblib保存和调用模型"></a>使用Joblib保存和调用模型</h2><p>Joblib是SciPy生态系统中的一部分，它为Python提供保存和调用管道和对象的功能，处理NumPy结构的数据尤其高<br>效，对于很大的数据集和巨大的模型非常有用。Joblib与pickle API非常相似 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line">bst = XGBR(n_estimators=<span class="number">200</span>,eta=<span class="number">0.05</span>,gamma=<span class="number">20</span>,reg_lambda=<span class="number">3.5</span>,reg_alpha=<span class="number">0.2</span>,max_depth=<span class="number">4</span>,colsample_bytree=<span class="number">0.4</span>,colsample_bylevel=<span class="number">0.6</span>).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">joblib.dump(bst,<span class="string">"xgboost-boston.dat"</span>)</span><br><span class="line"><span class="comment">#调用模型</span></span><br><span class="line">loaded_model = joblib.load(<span class="string">"xgboost-boston.dat"</span>)</span><br><span class="line"><span class="comment">#这里可以直接导入Xtest</span></span><br><span class="line">ypreds = loaded_model.predict(Xtest)</span><br><span class="line"><span class="keyword">print</span>（MSE(Ytest, ypreds)）</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前沿&quot;&gt;&lt;a href=&quot;#前沿&quot; class=&quot;headerlink&quot; title=&quot;前沿&quot;&gt;&lt;/a&gt;前沿&lt;/h2&gt;&lt;p&gt;在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>sklearn中的神经网络</title>
    <link href="http://yoursite.com/2019/09/12/sklearn%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/12/sklearn中的神经网络/</id>
    <published>2019-09-12T14:19:10.000Z</published>
    <updated>2019-09-23T07:07:37.466Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h2><p>人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学习的基础。神经网络算法试图模拟生物神经系统的学习过程，以此实现强大的预测性能。不过由于是模仿人类大脑，所以神经网络的模型复杂度很高也是众所周知。在现实应用中，神经网络可以说是解释性最差的模型之一 。</p><p>神经网络原理最开始是基于感知机提出——感知机是最古老的机器学习分类算法之一 。和今天的大部分模型比较起来，感知机的泛化能力比较弱。但支持向量机和神经网络都基于它的原理来建立。感知机的原理在支持向量机中介绍过，使用一条线性决策边界z=&omega;*x+b来划分数据集，决策边界的上方是一类数据(z&gt;=0)，决策边界的下方是另一类数据(z&lt;0)的决策过程。使用神经元表示，如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN1.png" alt></p><p>不同的特征数据被输入后，我们通过神经键将它输入我们的神经元。每条神经键上对应不同的参数&omega;，b。因此特征数据会经由神经键被匹配到一个参数向量&omega;，b。基于参数向量&omega;算法可以求解出决策边界z=&omega;*x+b，然后用决策函数sign(z)进行判断，最终预测出标签y并且将结果输出，其中函数sign(z)被称为”激活函数“。这是模拟人类的大脑激活神经元的过程所命名的，其实本质就是决定了预测标签的输出会是什么内容的预测函数。</p><p>注意：在这个过程中，有三个非常重要的点：</p><p>​    1、每个输入的特征都会匹配到一个参数&omega;，我们都知道参数向量&omega;中含有的参数数量与我们的特征数目是一致的，在感知机中也是如此。也就是说，任何基于感知机的算法，必须至少要有参数向量&omega;可求。</p><p>​    2、一个线性关系z，z是由参数和输入的数据共同决定的。这个线性关系，往往就是我们的决策边界，或者它也可以是多元线性回归，逻辑回归等算法的线性表达式</p><p>​    3、激活函数的结果，是基于激活函数的本身，参数向量&omega;和输入的数据一同计算出来的。也就是说，任何基于感知机的算法。必须要存在一个激活函数。</p><p>神经网络就相当于众多感知机的集成，因此，确定激活函数，并找出参数向量&omega;也是神经网络的计算核心。我们来看看神经网络的基本结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN2.png" alt></p><p>首先，神经网络有三层。第一层叫做输入层（Input layer），输入特征矩阵用，因此每个神经元上都是一个特征向量。极端情况下，如果一个神经网络只训练一个样本，则每个输入层的神经元上都是这个样本的一个特征取值。</p><p>最后一层叫做输出层（output layer），输出预测标签用。如果是回归类，一般输出层只有一个神经元，回归的是所有输入的样本的标签向量。如果是分类，可能会有多个神经元。二分类有两个神经元，多分类有多个神经元，分别输出所有输入的样本对应的每个标签分类下的概率。但无论如何，输出层只有一层，是用于输出预测结果用。</p><p>输入层和输出层中间的所有层，叫做隐藏层（Hidden layers），最少一层。<strong>也就是说整个神经网络是最少三层</strong>。隐藏层是我们用来让算法学习的网络层级，从更靠近输入层的地方开始叫做”上层”，相对而言，更靠近输出层的一层，叫做”下层”。在隐藏层上，每个神经元中都存在一个激活函数，我们的数据被逐层传递，每个下层的神经元都必须处理上层的神经元中的激活函数处理完毕的数据，本质是一个感知器嵌套的过程。隐藏层中上层的每个神经元，都与下层中的每个神经元相连，因此隐藏层的结构随着神经元的变多可以变得非常非常复杂。神经网络的两个要点：参数&omega;和激活函数，也都在这一层发挥作用，因此理解隐藏层是神经网络原理中最难的部分。</p><p><strong>神经网络的每一层的结果之间的关系是嵌套，不是迭代</strong>。由于我们执行的是嵌套，不是迭代。所以<strong>我们的每一个系数之间是相互独立的，每一层的系数之间也是相互独立的</strong>，我们不是在执行使用上一层或者上一个神经元的参数来求解下一层或者下一个神经元的参数的过程。我们不断求解，是激活函数的结果a，不是参数&omega;<strong>。在一次神经网络计算中，我们没有迭代参数&omega;</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN3.png" alt></p><p>上面这张图还不算真实数据中特别复杂的情况，但已经含有总共8*9*9*9*4=23328个&omega;。神经网络可能是我们遇到的最难调参的算法。接下来看看sklearn中的神经网络。</p><h2 id="sklearn中的神经网络"><a href="#sklearn中的神经网络" class="headerlink" title="sklearn中的神经网络"></a>sklearn中的神经网络</h2><p>sklearn是专注于机器学习的库，它在神经网络的模块中特地标注：sklearn不是用于深度学习的平台，因此这个神经网络不具备做深度学习的功能，也不具备处理大型数据的能力。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN4.png" alt></p><h3 id="neural-network-MLPClasifier"><a href="#neural-network-MLPClasifier" class="headerlink" title="neural_network.MLPClasifier"></a>neural_network.MLPClasifier</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN6.png" alt></p><h4 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h4><h5 id="hidden-layer-sizes"><a href="#hidden-layer-sizes" class="headerlink" title="hidden_layer_sizes"></a>hidden_layer_sizes</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN7.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier <span class="keyword">as</span> DNN</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier <span class="keyword">as</span> DTC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">times = time()</span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">100</span>,),random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"dnn的交叉验证："</span>,cv(dnn,X,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">print(<span class="string">"dnn所用的时间："</span>,time() - times)</span><br><span class="line"><span class="comment">#使用决策树进行一个对比</span></span><br><span class="line">times = time()</span><br><span class="line">clf = DTC(random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"决策树的交叉验证："</span>,cv(clf,X,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">print(<span class="string">"决策树所用的时间："</span>,time() - times)</span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">100</span>,),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dnn.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#使用重要参数n_layers_，得出层数</span></span><br><span class="line">print(<span class="string">"n_layers_："</span>,dnn.n_layers_)</span><br><span class="line"><span class="comment">#可见，默认层数是三层，由于必须要有输入和输出层，所以默认其实就只有一层隐藏层</span></span><br><span class="line"><span class="comment">#增加一个隐藏层上的神经元个数</span></span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">200</span>,),random_state=<span class="number">420</span>)</span><br><span class="line">dnn = dnn.fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"增加一个隐藏层的准确率是："</span>,dnn.score(Xtest,Ytest))</span><br><span class="line">s = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">2000</span>,<span class="number">100</span>):</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(int(i),),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">200</span>,<span class="number">2100</span>,<span class="number">100</span>),s)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#增加隐藏层，控制神经元个数</span></span><br><span class="line">s = []</span><br><span class="line">layers = [(<span class="number">100</span>,),(<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),</span><br><span class="line">(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> layers:</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(i),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">3</span>,<span class="number">9</span>),s)</span><br><span class="line">plt.xticks([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Total number of layers"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#增加隐藏层，控制神经元个数</span></span><br><span class="line">s = []</span><br><span class="line">layers = [(<span class="number">100</span>,),(<span class="number">150</span>,<span class="number">150</span>),(<span class="number">200</span>,<span class="number">200</span>,<span class="number">200</span>),(<span class="number">300</span>,<span class="number">300</span>,<span class="number">300</span>,<span class="number">300</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> layers:</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(i),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">3</span>,<span class="number">7</span>),s)</span><br><span class="line">plt.xticks([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Total number of layers"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络概述&quot;&gt;&lt;a href=&quot;#神经网络概述&quot; class=&quot;headerlink&quot; title=&quot;神经网络概述&quot;&gt;&lt;/a&gt;神经网络概述&lt;/h2&gt;&lt;p&gt;人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
