<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-18T14:02:38.480Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>超参数调试、batch正则化</title>
    <link href="http://yoursite.com/2019/10/13/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81batch%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://yoursite.com/2019/10/13/超参数调试、batch正则化/</id>
    <published>2019-10-13T07:19:36.000Z</published>
    <updated>2019-10-18T14:02:38.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="调试处理"><a href="#调试处理" class="headerlink" title="调试处理"></a>调试处理</h2><p>关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentum或Adam优化算法的参数，&beta;<sub>1</sub>，&beta;<sub>2</sub>和ξ，也许你还得选择层数，也许还得选择不同层中隐藏单元的数量，也许还想使用学习率衰减。所以，使用的不是单一的学习率a。接着，当然可能还需要选择mini-batch的大小。</p><p>&beta;<sub>1</sub>、&beta;<sub>2</sub>、ξ推荐使用0.9、0.999、10<sup>-10</sup>。a是学习速率，学习速率是需要调试的最重要的超参数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014135821.png" alt=""></p><p>现在，如果我们尝试调整一些超参数，该如何选择调试值呢？在早一代的机器学习算法中，如果有两个超参数，这里会称之为超参1，超参2，常见的做法是在网格中取样点，像这样，然后系统的研究这些数值。这里放置的是5×5的网格，实践证明，网格可以是5×5，也可多可少，但对于这个例子，我们可以尝试这所有的25个点，然后选择哪个参数效果最好。当参数的数量相对较少时，这个方法很实用。</p><p>在深度学习领域，吴恩达老师推荐我们使用随机选择点方法，所以我们可以选择同等数量的点，对吗？25个点，接着，用这些随机取的点试验超参数的效果。之所以这么做是因为，对于要解决的问题而言，你很难提前知道哪个超参数最重要，正如之前看到的，一些超参数的确要比其它的更重要。</p><p>假如我们拥有三个超参数呢？这时我们搜索的就不只是一个方格了，而是一个立方体。超参数3代表第三维，接着，在三维立方体中取值，我们会实验更多的值。</p><p>实践中，需要搜索的可能不止三个超参数有时很难预知，哪个是最重要的超参数，对于具体应用而言，随机取值而不是网格取值表明，我们要探究了更多重要超参数的潜在值，无论结果是什么。</p><p>当我们给超参数取值时，另一个惯例是采用由粗糙到精细的策略。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014141132.png" alt=""></p><p>比如在二维的那个例子中，你进行了取值，也许你会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内），然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索，如果你怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，你会知道接下来应该聚焦到更小的方格中。在更小的方格中，你可以更密集得取点。所以这种从粗到细的搜索也经常使用。</p><h2 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h2><p>讲个例子，假如我们在搜索超参数a，假设我们怀疑其值最小是0.0001，最大是1。假如我们把这个取值范围当作数轴，沿其随机均匀取值，那90%的值都会落在0.1到1之间。只有10%的搜索资源在0.0001到0.1之间。反而，用数标尺搜索超参数的方式会更合理，因此不使用线性轴。分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用。</p><p>在python中，这可以通过numpy模块实现。</p><blockquote><p>r=-4 x np.random.rand()</p><p>a=10<sup>r</sup></p></blockquote><p>对a随机取值，由上面第一行代码得出r的取值在[-4,0]之间，a的取值[10<sup>-4</sup>,10<sup>0</sup>]之间。如果我们在10<sup>a</sup>和10<sup>b</sup>之间取值，在此例中，我们可以通过0.0001算出a的值为-4，b的值为0。我们要做的就是在[a,b]区间随机均匀的给r取值，然后设置a的值。所以总结一下，在对数坐标下取值，取最小值的对数就得到a的值，取最大值的对数就得到b值，所以现在你在对数轴上的10<sup>a</sup>到10<sup>b</sup>区间取值，在a，b间随意均匀的选取值，将超参数设置为10<sup>r</sup>，这就是在对数轴上取值的过程。</p><p>最后，还有一个例子是对&beta;取值。用于计算指数的加权平均数。假设我们认为&beta;是0.9到0.999之间的某个值，这也是我们想搜索的范围。</p><p>上面哪个例子说了如果想在0.9到0.999区间搜索，那就不能用线性轴取值。不要随机均匀在此区间取值，所以考虑这个问题最好的方法就是，我们要探究的是1-&beta;，此值在0.1到0.001区间内，所以我们会给1-&beta;取值，大概是从0.1到0.001。所以我们要做的是在[-3,-1]里随机均匀的给r取值。由1-&beta;=10<sup>r</sup>推出&beta;=1-10<sup>r</sup>，然后就变成了在特定的选择范围内超参数的随机取值。</p><h2 id="超参数调试的方法"><a href="#超参数调试的方法" class="headerlink" title="超参数调试的方法"></a>超参数调试的方法</h2><p>两种方法：</p><p>一种是你照看一个模型，通常是有庞大的数据组，但没有许多计算资源或足够的CPU和GPU的前提下，基本而言，只可以一次负担起试验一个模型或一小批模型，在这种情况下，即使当它在试验时，也可以逐渐改良。这是一个人照料一个模型的方法，观察它的表现，耐心的调试学习率。</p><p>另一种方法则是同时试验多种模型，你设置了一些超参数，尽管让它自己运行，或者是一天甚至多天，然后你会获得像这样的学习曲线，这可以是损失函数J或实验误差或损失或数据误差的损失，但都是你曲线轨迹的度量。同时你可以开始一个有着不同超参数设定的不同模型。</p><p>所以这两种方式的选择，是由你拥有的计算资源决定的，如果你拥有足够的计算机去平行试验许多模型，那绝对采用第二种方式，尝试许多不同的超参数，看效果怎么样。</p><h2 id="归一化的激活函数"><a href="#归一化的激活函数" class="headerlink" title="归一化的激活函数"></a>归一化的激活函数</h2><p>在深度学习兴起后，最重要的一个思想是它的一个算法——Batch归一化。batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易。接下来看一下原理。</p><p>之前的逻辑回归，我们讲过归一化输入特征可以加快学习过程。计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化数据集。那么更深的模型呢？我们不仅输入了特征值x，而且第一层有激活值a<sup>[1]</sup>，第二层有激活层a<sup>[2]</sup>等。那么如果我们想训练&omega;<sup>[3]</sup>，b<sup>[3]</sup>，那归一化a<sup>[2]</sup>岂不是更好？便于我们训练&omega;<sup>[3]</sup>和b<sup>[3]</sup>。</p><p>那么，我们能归一化每个隐藏层的a值吗？比如a<sup>[2]</sup>，但不仅仅是a<sup>[2]</sup>，可以是任何隐藏层的。a<sup>[2]</sup>的值是下一层的输入值，所以a<sup>[2]</sup>会影响&omega;<sup>[3]</sup>，b<sup>[3]</sup>的训练。这就是batch归一化的作用。严格来说，归一化的是z<sup>[2]</sup>，并不是a<sup>[2]</sup>。</p><p>在神经网络中，假设你有一些隐藏单元值从z<sup>[1]</sup>到z<sup>[m]</sup>，这些来源于隐藏层，所以这样写会更准确，即z<sup>[ l ]( i )</sup>为隐藏层，i从1到m，所以已知这些值。如下，你要计算平均值，强调一下，所有这些都是针对 l 层，但已省略 l 及方括号,计算方法如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014182957.png" alt=""></p><p>分母中加上ε以防止δ为0的情况。</p><p>所以现在我们已把这些z值标准化，化为含平均值 0 和标准单位方差，所以z的每一个分量都含有平均值 0 和方差 1，但我们不想让隐藏单元总是含有平均值 0 和方差 1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算，我们称之为z~(i)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183405.png" alt=""></p><p>这里γ和β是模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法，比如 Momentum 或者 Nesterov， Adam，接着更新γ和β，正如更新神经网络的权重一样。</p><p>γ和β的作用是可以随意设置z~(i) 的平均值，事实上，如果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183506.png" alt=""></p><p>那么：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183533.png" alt=""></p><p>Batch 归一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏层。应用 Batch 归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，也许不想隐藏单元值必须是平均值 0 和方差 1。γ和β参数控制使得均值和方差可以是 0 和 1，也可以是其它值。 </p><p><strong>注意</strong>：均值不是平均值，是数学期望。</p><h2 id="将Batch-Norm拟合进神经网络"><a href="#将Batch-Norm拟合进神经网络" class="headerlink" title="将Batch Norm拟合进神经网络"></a>将Batch Norm拟合进神经网络</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014212240.png" alt=""></p><p>假设有一个这样的神经网络，之前的知识说过，我们可以认为每个单元负责计算两件事。第一，它先计算z，然后应用其到激活函数中再计算a，所以可以认为，每个圆圈代表着两步的计算过程。同样的，对于下一层而言，那就是$z^{[2]}_{1}$和$z^{[2]}_{2}$等。所以如果没有应用Batch归一化，会把输入X拟合到第一隐藏层，然后首先计算z<sup>[1]</sup>，这是由&omega;<sup>[1]</sup>和b<sup>[1]</sup>两个参数控制的。接着，通常而言，会把z<sup>[1]</sup>拟合到激活函数以计算a<sup>[1]</sup>。但Batch归一化的做法是将z<sup>[1]</sup>值进行Batch归一化，简称<strong>BN</strong>，此过程将由&beta;<sup>[1]</sup>和&gamma;<sup>[1]</sup>两参数控制，这一操作会给你一个新的规范化的z<sup>[1]</sup>值（z~<sup>[1]</sup>），然后将其输入激活函数中得到a<sup>[1]</sup>，即a<sup>[1]</sup>=g<sup>[1]</sup>(z~<sup>[1]</sup>)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014215107.png" alt=""></p><p>现在，你已在第一层进行了计算，此时Batch归一化发生在z的计算和a之间，接下来，你需要应用a<sup>[1]</sup>值来计算，此过程是由&omega;<sup>[1]</sup>和b<sup>[1]</sup>控制的。与第一层所做的类似，也会将进行Batch归一化，现在我们简称<strong>BN</strong>，这是由下一层的Batch归一化参数所管制的，即&beta;<sup>[2]</sup>和&gamma;<sup>[2]</sup>，现在你得到z~<sup>[2]</sup>，再通过激活函数计算出a<sup>[2]</sup>。</p><p>所以，得出结论的是batch归一化是发生在计算z和a之间的。与其使用没有归一化的z值，不如用归一化的z~值，也就是第一层的z~<sup>[1]</sup>。第二层同理，也是与其应用z值，不如应用z~<sup>[2]</sup>值。所以，我们以前的网络参数&omega;<sup>[1]</sup>、b<sup>[1]</sup>、&omega;<sup>[2]</sup>、b<sup>[2]</sup>将加上&beta;<sup>[1]</sup>、&beta;<sup>[2]</sup>、&gamma;<sup>[1]</sup>、&gamma;<sup>[2]</sup>等参数。<strong>注意</strong>：这里的&beta;<sup>[1]</sup>、&beta;<sup>[2]</sup>和超参数&beta;没有关系</p><p>&beta;<sup>1]</sup>、&beta;<sup>[2]</sup>、&gamma;<sup>[1]</sup>、&gamma;<sup>[2]</sup>等是算法的新参数。接下来就是使用梯度下降法来执行它。举个例子，对于给定层，计算d&beta;<sup>[1]</sup>，接着更新参数为&beta;<sup>[1]</sup>=&beta;<sup>[1]</sup>-&alpha;d&beta;<sup>[1]</sup>。你也可以使用Adam或RMSprop或Momentum，以更新参数&beta;和&gamma;，并不是只应用梯度下降法。</p><p>实践中，我们常将batch归一化和mibi-bacth一起使用。我们用第一个mini-batch{X<sup>[1]</sup>}，然后应用&omega;<sup>[1]</sup>和b<sup>[1]</sup>计算z<sup>[1]</sup>，接着用batch归一化得到z~<sup>[1]</sup>，再应用激活函数得到a<sup>[1]</sup>。然后接着用&omega;<sup>[2]</sup>和b<sup>[2]</sup>计算z<sup>[2]</sup>。</p><p>类似的工作，你会在第二个mini-batch（X<sup>{2}</sup>）上计算z<sup>[1]</sup>，然后用Batch归一化来计算，所以Batch归一化的此步中，用的是第二个mini-batch（X<sup>{2}</sup>）中的数据使归一化。然后在mini-batch（X<sup>{3}</sup>）上同样这样做，继续训练。</p><p>先前说过每层的参数是&omega;<sup>[ l ]</sup>和b<sup>[ l ]</sup>，还有&beta;<sup>[ l ]</sup>和&gamma;<sup>[ l ]</sup>，请注意计算z的方式如下，z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>，但Batch归一化做的是，要看这个mini-batch，先将z<sup>[ l ]</sup>归一化，结果为均值0和标准方差，再由&beta;和&gamma;重缩放，但这意味着，无论b<sup>[ l ]</sup>的值是多少，都是要被减去的，因为在Batch归一化的过程中，要计算z<sup>[ l ]</sup>的均值，再减去平均值，在此例中的mini-batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消。</p><p>所以，我们在使用batch归一化时，我们可以消除b<sup>[ l ]</sup>这个参数。或者也可以设置为0。那么式子将从z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>变为z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>。然后归一化z<sup>[ l ]</sup>，得z~<sup>[ l ]</sup>=&gamma;<sup>[ l ]</sup> * z<sup>[ l ]</sup>+&beta;<sup>[ l ]</sup>，所以最后我们会用&beta;<sup>[ l ]</sup>，以便决定z~<sup>[ l ]</sup>的取值。</p><p>总结一下关于如何用 Batch 归一化来应用梯度下降法：</p><p>假设使用 mini-batch梯度下降法，运行t = 1到 batch 数量的 for 循环，会在 mini-batchX<sup>{t}</sup>上应用正向 prop，每个隐藏层都应用正向 prop，用 Batch 归一化代替z<sup>[l]</sup>为z~<sup>[l]</sup>。接下来，它确保在这个 mini-batch 中，z值有归一化的均值和方差，归一化均值和方差后是z~<sup>[ l ]</sup> ，然后，你用反向 prop 计算：dw<sup>[ l ]</sup>，db[l]，dβ<sup>[ l ]</sup> ，dγ<sup>[ l ]</sup>。 尽管严格来说，因为要去掉b，这部分其实已经去掉了。最后，更新这些参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015143440.png" alt=""></p><h2 id="batch-norm为什么会管用？"><a href="#batch-norm为什么会管用？" class="headerlink" title="batch norm为什么会管用？"></a>batch norm为什么会管用？</h2><p>一个原因是，你已经看到如何归一化输入特征值x，使其均值为0，方差1，它又是怎样加速学习的，有一些从0到1而不是从1到1000的特征值，通过归一化所有的输入特征值x，以获得类似范围的值，可以加速学习。所以Batch归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值，这只是Batch归一化作用的冰山一角，还有些深层的原理，它会有助于你对Batch归一化的作用有更深的理解，让我们一起来看看吧。</p><p>Batch归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层，比如，第10层的权重相比于神经网络中前层的权重更能经受得住变化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016170223.png" alt=""></p><p>看上面的素材。假设我们已经在某个网络上训练了所有黑猫的图像，如果我们将这个网络应用于有色猫。这种情况下，我们的效果可能不是很好。因为正面的例子不止左边的黑猫，还有右边其它颜色的猫。</p><p>所以我们要使数据改变分布，想法名字叫“Covariate shift”。想法是这样的，如果你已经学习了x到y的映射，如果x的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由x到y映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016171250.png" alt=""></p><p>看上图，看第三层网络。此网络已经学习了参数&omega;<sup>[3]</sup>和b<sup>[3]</sup>。它还得到了一些值，称为$a^{[2]}_{1}$、$a^{[2]}_{2}$、$a^{[2]}_{3}$、$a^{[2]}_{4}$，但这些值也可以变为x<sub>1</sub>、x<sub>2</sub>、x<sub>3</sub>、x<sub>4</sub>。第三层隐藏层要做的是，找到一种方式使这些值映射到y帽。再看网络左边前三层(包括输入层)，这个网络还有参数&omega;<sup>[2]</sup>、b<sup>[2]</sup>、&omega;<sup>[1]</sup>、b<sup>[1]</sup>，如果这些参数改变，这些a<sup>[2]</sup>的值也会变。所以从第三层隐藏层的角度来看，这些隐藏单元的值在不断地改变，所以它就有了“Covariate shift”的问题。</p><p>Batch归一化做的，是它减少了这些隐藏值分布变化的数量。batch归一化讲的是当神经网络层更新参数时，batch归一化可以确保无论$z^{[2]}_{1}$、$z^{[2]}_{2}$、$z^{[2]}_{3}$、$z^{[2]}_{4}$怎么变化它们的均值和方差都保持不变。均值和方差是由&beta;<sup>[2]</sup>和&gamma;<sup>[2]</sup>决定的值，如果神经网络选择的话，可强制均值为0，方差为1，或其它任何均值和方差。</p><p>Batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，我们可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。</p><p>batch归一化还有一个作用，它有轻微的正则化效果。在mini-batch计算中，由均值和方差缩放的，因为是在mini-batch上计算的均值和方差，而不是在整个数据集上，它只是由一小部分数据估计得出的。所以和dropout相似，它往每个隐藏层的激活值上增加了噪音，dropout有增加噪音的方式，它使一个隐藏的单元，以一定的概率乘以0，以一定的概率乘以1，所以你的dropout含几重噪音，因为它乘以0或1。对比而言，Batch归一化含几重噪音，因为标准偏差的缩放和减去均值带来的额外噪音。这里的均值和标准差的估计值也是有噪音的，所以类似于dropout，Batch归一化有轻微的正则化效果，因为给隐藏单元添加了噪音，这迫使后部单元不过分依赖任何一个隐藏单元，类似于dropout。batch归一化给隐藏层增加了噪音，因此有轻微的正则化效果。因为添加的噪音很微小，所以并不是巨大的正则化效果。如果想得到dropout更大的正则化效果，可以将Batch归一化和dropout一起使用。</p><h2 id="测试时的batch-norm"><a href="#测试时的batch-norm" class="headerlink" title="测试时的batch norm"></a>测试时的batch norm</h2><p>batch归一化将数据以mini-batch的形式逐一处理，但在测试时，需要对每个样本逐一处理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016201010.png" alt=""></p><p>在训练时，这些就是用来执行Batch归一化的等式。在一个mini-batch中，你将mini-batch的z<sup>(i)</sup>值求和，计算均值，所以这里你只把一个mini-batch中的样本都加起来，我用m来表示这个mini-batch中的样本数量，而不是整个训练集。然后计算方差，再算$z^{(i)}_{norm}$，即用均值和标准差来调整，加上ξ是为了数值稳定性。z~<sup>(i)</sup>是用&gamma;和&beta;再次调整$z^{(i)}_{norm}$得到的。</p><p>请注意用于调节计算的μ和σ<sup>2</sup>是在整个mini-batch上进行计算，但是在测试时，不能将一个mini-batch中的6428或2056个样本同时处理，因此需要用其它方式来得到μ和σ<sup>2</sup>，而且如果只有一个样本，一个样本的均值和方差没有意义。那么实际上，为了将神经网络运用于测试，就需要单独估算μ和σ<sup>2</sup>，在典型的Batch归一化运用中，需要用一个指数加权平均来估算，这个平均数涵盖了所有mini-batch，接下来是具体解释。</p><p>选择 l 层，假设我们用mini-batch，X<sup>[1]</sup>,X<sup>[2]</sup>,X<sup>[3]</sup>……以及对应的y值等等，那么在 l 层训练X<sup>{1}</sup>时，就得到了μ<sup>{1}[ l ]</sup>。当我们训练第二个mini-batch，我们就得到了μ<sup>{2}[ l ]</sup>，第三个mini-batch，就得到了μ<sup>{3}[1]</sup>值。正如我们之前用的指数加权平均来计算&theta;<sub>1</sub>，&theta;<sub>2</sub>，&theta;<sub>3</sub>的均值，当时是试着计算当前气温的指数加权平均，你会这样来追踪你看到的这个均值向量的最新平均值，于是这个指数加权平均就成了你对这一隐藏层的z均值的估值。同样的，你可以用指数加权平均来追踪你在这一层的第一个mini-batch中所见的的σ<sup>2</sup>值，以及第二个mini-batch中所见的的σ<sup>2</sup>值等等。因此在用不同的mini-batch训练神经网络的同时，能够得到你所查看的每一层的μ和σ<sup>2</sup>的平均数的实时数值。最后在测试时，我们只需要z值来计算$z^{(i)}_{norm}$，用μ和σ<sup>2</sup>的指数加权平均，用手头的最新数值来做调整，然后就可以用刚算出来的z<sub>norm</sub>和在神经网络训练过程中得到的&beta;和&gamma;参数来计算那个测试样本的z~值。</p><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>前面的讲过逻辑回归属于二分类，如果我们有多种可能的类型呢？</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015163607.png" alt=""></p><p>假设我们不止需要识别猫，而是想识别猫、狗和小鸡。把猫当作类1，把狗当作类2，把小鸡当作类3。如果不属于以上任何一类，就分到“其它”或“以上均不符合”这一类，我把它叫做类0。所以上面图中第一张图为3类，第二张图为1类，第三张图为2类，第四张图为0类，依次类推。我们使用C来表示输入会被分入的类别总个数。在这个例子中，我们有四种可能的类别。当有四个分类时，指示类别的数字0-C-1。就是0、1、2、3。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015171121.png" alt=""></p><p>最后一层的隐藏单元为4，为所分的类的数目。输出的值表示属于每个类的概率。它们加起来等于一。</p><p>Softmax的具体步骤如下：</p><p>在神经网络的最后一层，我们将会想往常一样计算各层的线性部分，z<sup>[ l ]</sup>是最后一层的z变量。z<sup>[ l ]</sup>=W<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>，算出了z后，我们需要应用Softmax激活函数。它的作用是这样的： 我们要计算一个临时变量，我们把它叫做t。它等于e<sup>z<sup>[ l ]</sup>&lt;/sup&gt;，这适用于每个元素。而这里的z<sup>[ l ]</sup>，在上面这个例子中，维度是4x1的。四维向量t=e<sup>z[ l ]</sup>，这是对所有元素求幂，t也是一个4x1维向量，然后输出a<sup>[ l ]</sup>，基本上就是向量t，但是会归一化，使和为1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015174838.png" alt=""></p><p>换句话说，a<sup>[ l ]</sup>也是一个4x1维向量。而这个思维向量的第 i 个元素：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175116.png" alt=""></p><p>讲个例子：假设我们算出了z<sup>[ l ]</sup>，这是一个四维向量。假设为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175309.png" alt=""></p><p>我们要做的就是用这个元素取幂方法来计算t，所以：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175348.png" alt=""></p><p>接着利用计算器计算得到以下值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175424.png" alt=""></p><p>如果把t的元素都加起来，把这四个数字加起来，得到176.3。最终：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175556.png" alt=""></p><p>看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015172242.png" alt=""></p><p>例如第一个节点，会输出e<sup>5</sup>/176.3=0.842。这样来说，对于这张图片，它是0类的概率就是84.2%。下个节点输出0.042，也就是4.2%的几率是1类。其它类别以这种规律推出。</p><p>神经网络的输出a<sup>[ l ]</sup>，也就是y帽。是一个4x1维向量，如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015180202.png" alt=""></p><p>所以这种算法通过向量z计算出总和为1的四个概率。</p><p>之前，我们的激活函数都是接受单行数值输入，例如Sigmoid和ReLu激活函数，输入一个实数，输出一个实数。Softmax激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。</p><p>下面是分类的几个例子：</p><p>这是一个没有隐藏层的神经网络。他所做的就是z<sup>[1]</sup>=W<sup>[1]</sup> * x+b<sup>[1]</sup>，而输出的a<sup>[ l ]</sup>=g(z<sup>[1]</sup>)，就是Softmax激活函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015193021.png" alt=""></p><p>上面三张图的C=3，下面三张图从左到右的C等于4、5、6。</p><p>这显示了Softmax分类器在没有隐藏层的情况下能够做到的事情，当然更深的神经网络会有x，然后是一些隐藏单元，以及更多隐藏单元等等，你就可以学习更复杂的非线性决策边界，来区分多种不同分类。</p><h2 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h2><p>回忆我们之前举得例子，输出层计算的z<sup>[ l ]</sup>。我们有四个分类，z<sup>[ l ]</sup>可以是4x1维向量，我们计算了临时变量t。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015222626.png" alt=""></p><p>如果我们的激活函数是Softmax，那么输出是这样的:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015222734.png" alt=""></p><p>简单来说就是用临时变量t将它归一化，使总和为1。于是这就变成了a<sup>[ l ]</sup>。在这个向量z中，最大的元素是5。最大的概率是0.842。</p><p>Softmax这个是与所谓的<strong>hardmax</strong>对比，hardmax会把z变量变为[1 0  0  0]<sup>T</sup>，hardmax会观察z的元素，然后在z中最大元素的位置放上1，其它的输出都放0。</p><p>Softmax回归或Softmax激活函数将logistic激活函数推广到C类，而不仅仅是两类。而当C=2，那么的Softmax实际上变回了logistic回归，那么输出层将会输出两个数字。如果C=2的话，也许输出0.842和0.158，对吧？这两个数字加起来要等于1，因为它们的和必须为1，其实它们是冗余的，也许你不需要计算两个，而只需要计算其中一个，结果就是你最终计算那个数字的方式又回到了logistic回归计算单个输出的方式。</p><p>接下来我们来看怎样训练带有Softmax输出层的神经网络，具体而言，我们先定义训练神经网络使会用到的损失函数。举个例子，我们来看看训练集中某个样本的目标输出，真实标签是[0 1 0 0]<sup>T</sup>，用上一个视频中讲到过的例子，这表示这是一张猫的图片，因为它属于类1，现在我们假设你的神经网络输出的是 y帽，y帽是一个包括总和为1的概率的向量，y帽=[0.3 0.2 0.1 0.4]<sup>T</sup>，可以看到总和为1，这就是a<sup>[ l ]</sup>。对于这个样本神经网络的表现不佳，这实际上是一只猫，但却只分配到20%是猫的概率，所以在本例中表现不佳。</p><p>那么我们使用什么损失函数来训练这个神经网络？看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015224734.png" alt=""></p><p>注意在这个样本中，y<sub>1</sub>=y<sub>2</sub>=y<sub>3</sub>=0，因为这些都是0，只有y<sub>2</sub>=1，如果你看这个求和，所有含有值为0的y<sub>i</sub>的项都等于0，最后只剩下 -y<sub>2</sub>logy<sub>2</sub>帽，因为当你按照下标 j 全部加起来，所有的项都为0，除了j=2，因为y<sub>2</sub>=1，所以它就等于 -logy<sub>2</sub>帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015225125.png" alt=""></p><p>这就意味着，如果你的学习算法试图将它变小，因为梯度下降法是用来减少训练集的损失的，要使它变小的唯一方式就是使 -logy<sub>2</sub>帽变小，要想做到这一点，就需要使 y<sub>2</sub>帽 尽可能大。也就是[0.3 0.2 0.1 0.4]<sup>T</sup>中的第二个元素。</p><p>接下来看看，我们在有Softmax的输出层如何实现梯度下降法。输出层会计算z<sup>[1]</sup>，它是C x 1维的。在上个例子中，是4 x1维的。然后用Softmax激活函数来得到a<sup>[ l ]</sup>或者y帽。然后计算出损失。反向传播的关键步骤是这个表达式dz<sup>[ l ]</sup>=y帽-y。我们用y帽这个4x1向量减去y这个4x1向量。这是对z<sup>[ l ]</sup>的损失函数的偏导数dz<sup>[ l ]</sup>=∂J/∂z<sup>[ l ]</sup>，然后开始反向传播的过程，计算整个神经网络中所需要的所有导数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;调试处理&quot;&gt;&lt;a href=&quot;#调试处理&quot; class=&quot;headerlink&quot; title=&quot;调试处理&quot;&gt;&lt;/a&gt;调试处理&lt;/h2&gt;&lt;p&gt;关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentu
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（2）</title>
    <link href="http://yoursite.com/2019/10/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(2)/"/>
    <id>http://yoursite.com/2019/10/11/优化算法(2)/</id>
    <published>2019-10-11T05:13:53.000Z</published>
    <updated>2019-10-21T08:42:28.795Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mini-batch-梯度下降"><a href="#Mini-batch-梯度下降" class="headerlink" title="Mini-batch 梯度下降"></a>Mini-batch 梯度下降</h2><p>我们之前学过，向量化能够让我们有效地对所有m个样本进行计算，所以我们要把训练样本放到巨大的矩阵X中。</p><p>X=[x<sup>(1)</sup> x<sup>(2)</sup>   x<sup>(3)</sup>   x<sup>(4)</sup>  ……  x<sup>(n)</sup> ]。Y也是如此，Y=[y<sup>(1)</sup> y<sup>(2)</sup>  y<sup>(3)</sup>   y<sup>(4)</sup>  ……  y<sup>(n)</sup> ]。所以X的维度是(n<sub>x</sub>,m)，Y的维度是(1,m)。向量化能够让我们相对较快地处理所有样本，如果m很大的话，处理速度仍然缓慢。</p><p>相比于mini-batch梯度下降法，我们大家更熟悉的应该是batch梯度下降法，即梯度下降法。那batch梯度下降法和mini-batch梯度下降法有什么区别吗？其实它俩的区别就存在于名字中，一个是batch，即进行梯度下降训练时，使用全部的训练集，而mini-batch，表示比batch小一些，就是指在进行梯度下降训练时，并不使用全部的训练集，只使用其中一部分数据集。</p><p>我们知道，不论是梯度下降法还是mini-batch梯度下降法，我们都可以通过向量化(vectorization)更加有效地计算所有样本。既然已经有了梯度下降法，我们为什么还要提出mini-batch梯度下降法呢？在实际计算中，我们可能会遇到特别大的数据集，这时再使用梯度下降法，每次迭代都要计算所有的数据集，计算量太大，效率低下，而mini-batch梯度下降法允许我们拿出一小部分数据集来运行梯度下降法，能够大大提高计算效率。</p><p>我们可以把训练集分割成小一点的子集训练，这些子集被取名为mini-batch，假设每一个子集中只要1000个样本，那么把x<sup>(1)</sup>到x<sup>(1000)</sup>取出来，将其称为第一个子训练集，也叫做mini-batch，然后我们再取出接下来的1000个样本，从x<sup>(1001)</sup>到x<sup>(2000)</sup>，然后再取1000个样本，以此类推。</p><p>接下来是吴恩达老师的一个新的符号，把x<sup>(1)</sup>到x<sup>(1000)</sup>称为X<sup>{1}</sup>，x<sup>(1001)</sup>到x<sup>(2000)</sup>称为X<sup>{2}</sup>，如果我们的训练样本一共有500万个，每个mini-batch都有1000个样本。也就是说，你有5000个mini-batch，因为5000乘以1000就是500万。最后得到的是X<sup>{5000}</sup>，对y进行相同的处理。</p><p>mini-batch的数量t组成X<sup>{t}</sup>和Y<sup>{t}</sup>，这就是10000个训练样本。包括相应的输入输出对。如果X<sup>{1}</sup>是一个有1000个样本的训练集，X<sup>{1}</sup>的维度应该是(n<sub>x</sub>,1000)，X<sup>[2]</sup>的应该也是(n<sub>x</sub>，1000)，以此类推，所有的子集维数都是(n<sub>x</sub>,1000)，对于Y也是一样的。</p><p>之前我们执行前向传播，就是执行z<sup>[1]</sup>=W<sup>[1]</sup>X+b<sup>[1]</sup>。变成mini-batch后呢，把X替换成X<sup>[t]</sup>，即z<sup>[1]</sup>=W<sup>[1]</sup>X<sup>{t}</sup>+b<sup>[1]</sup>，然后执行A<sup>[1]k</sup>=g<sup>[1]</sup>(Z<sup>[1]</sup>)，依次类推，直到A<sup>[L]</sup>=g<sup>[L]</sup>(Z<sup>[L]</sup>)，这就是我们的预测值。注意：这里一次性处理的是1000个样本不是500万个样本。接着计算成本函数，因为子集规模是1000，所以J=1/1000$\sum_{i=1}^{1000}$L(y<sup>(i)</sup> 帽,y<sup>(i)</sup>)。</p><p>如果我们使用了正则化，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011191909.png" alt=""></p><p>你也会注意到，我们做的一切似曾相识，其实跟之前我们执行梯度下降法如出一辙，除了现在的对象不是X，Y，而是X<sup>{t}</sup>和Y<sup>{t}</sup>。接下来，执行反向传播来计算J<sup>{t}</sup>的梯度，你只是使用X<sup>{t}</sup>和Y<sup>{t}</sup>，然后更新加权值，W实际上是W<sup>[l]</sup>，更新为W<sup>[l]</sup>=W<sup>[l]</sup>-adW<sup>[l]</sup>。对b做相同处理，b<sup>[l]</sup>=b<sup>[l]</sup>-adb<sup>[l]</sup>。这是使用mini-batch梯度下降法训练样本的一步。被称为进行“一代”（<strong>1 epoch</strong>）的训练。一代这个词意味着只是遍历了一次训练集。我们可以在外围加一个for循环，从1到5000，因为我们有5000个各有1000个样本的组。</p><p>使用batch梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用mini-batch梯度下降法，一次遍历训练集，能让你做5000个梯度下降。</p><h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p>使用batch梯度下降法时，每次迭代都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许学习率太大。</p><p>使用mini-batch梯度下降法，如果你作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是X<sup>{t}</sup>和Y<sup>{t}</sup>，如果要作出成本函数J<sup>{t}</sup>的图，而J<sup>{t}</sup>只和X<sup>{t}</sup>，Y<sup>{t}</sup>有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的mini-batch。</p><p>我们需要决定的变量之一是mini-batch的大小，m就是训练集的大小。其中一个极端情况下，mini-batch梯度下降法就是batch梯度下降法。另一个极端情况，假设mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch，当你看第一个mini-batch，也就是X<sup>{1}</sup>和Y<sup>{1}</sup>，如果mini-batch大小为1，它就是你的第一个训练样本。接着再看第二个mini-batch，也就是第二个训练样本，采取梯度下降步骤，然后是第三个训练样本，以此类推，一次只处理一个。</p><p>实际上你选择的mini-batch大小在二者之间，大小在1和m之间，而1太小了，m太大了。如果取n，每个迭代需要处理大量训练样本，单次迭代耗时太长。如果训练样本不大，batch梯度下降法运行地很好。相反，如果使用随机梯度下降法，如果你只要处理一个样本，那这个方法很好，这样做没有问题，通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的mini-batch尺寸，实际上学习率达到最快。</p><p>首先，如果训练集较小，直接使用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，可以快速处理整个训练集，所以使用batch梯度下降法也很好，这里的少是说小于2000个样本，这样比较适合使用batch梯度下降法。不然，样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的n次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。所以我经常把mini-batch大小设成2的次方。</p><p>最后需要注意的是在你的mini-batch中，要确保X<sup>{t}</sup>和Y<sup>{t}</sup>要符合<strong>CPU</strong>/<strong>GPU</strong>内存，取决于你的应用方向以及训练集的大小。如果不符合内存，无论采取什么方法处理数据，结果变得惨不忍睹。</p><h2 id="指数加权平均数"><a href="#指数加权平均数" class="headerlink" title="指数加权平均数"></a>指数加权平均数</h2><p>指数加权平均也叫指数加权移动平均，是一种常用的序列数据处理方式。</p><p>它的计算公式是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222214.png" alt=""></p><p>其中，</p><ul><li>θ_t：为第 t 天的实际观察值，</li><li>V_t: 是要代替 θ_t 的估计值，也就是第 t 天的指数加权平均值，</li><li>β： 为 V_{t-1} 的权重，是可调节的超参。( 0 &lt; β &lt; 1 )</li></ul><p>下面是一组气温数据，图中横轴为一年中的第几天，纵轴为气温：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222352.png" alt=""></p><p>直接看上面的数据图会发现噪音很多，</p><p>这时，我们<strong>可以用 指数加权平均 来提取这组数据的趋势，</strong></p><p>按照前面的公式计算：</p><p>这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t，将计算后得到的V_t表示出来，得到下图红色线：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222623.png" alt=""></p><p>可以看出，红色的数据比蓝色的原数据更加平滑，少了很多噪音，并且刻画了原数据的趋势。</p><p>指数加权平均，作为原数据的估计值，不仅可以 <strong>1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来</strong>。</p><p>我们可以改变&beta;值，当&beta;=0.98时，得出下图的绿线。当&beta;=0.5，结果是下图的黄线。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011223152.png" alt=""></p><h2 id="理解指数加权平均数"><a href="#理解指数加权平均数" class="headerlink" title="理解指数加权平均数"></a>理解指数加权平均数</h2><p>上个小节，我们得出下面公式。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222214.png" alt=""></p><p>我们进一步分析，来理解如何计算出每日温度的平均值</p><p>使&beta;=0.9，得出下面公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012113654.png" alt=""></p><p>将第二个公式带到第一个，第三个公式带到第二个公式，一次类推，把这些展开：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012113917.png" alt=""></p><p>我们可以将v<sub>0</sub>，v<sub>1</sub>，v<sub>2</sub>等等写成明确的变量，不过在实际中执行的话，你要做的是，一开始将v<sub>0</sub>初始化为0，然后在第一天使v=&beta;v+(1-&beta;)&theta;<sub>1</sub>，然后第二天，更新v值，v=&beta;v+(1-&beta;)&theta;<sub>2</sub>，以此类推，有些人会把v加下标，来表示v是用来计算数据的指数加权平均数。</p><p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。</p><h2 id="指数加权平均的偏差修正"><a href="#指数加权平均的偏差修正" class="headerlink" title="指数加权平均的偏差修正"></a>指数加权平均的偏差修正</h2><p>偏差修正可以让平均数运算更加准确</p><p>在前几节中，如上图，红色曲线对应&beta;为0.9，绿色曲线对应的&beta;=0.98。吴恩达老师说执行下面这个公式：</p><blockquote><p>v<sub>t</sub>=&beta;v<sub>t-1</sub>+(1-&beta;)&theta;<sub>t</sub></p></blockquote><p>得到的就是紫色曲线，而不是绿色曲线。后面紫色和绿色有大部分重合。PS：为什么？同时0.98，紫色曲线的起点低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012160618.png" alt=""></p><p>计算移动平均数时，初始化v<sub>0</sub>，v<sub>1</sub>=0.98v<sub>0</sub>+0.02&theta;<sub>1</sub>，因为v<sub>0</sub>=0，所以0.98v<sub>0</sub>=0。所以v<sub>1</sub>=0.02&theta;<sub>1</sub>，如果第一天温度时40，v<sub>1</sub>=0.02 x 40=8。因此得到的值会小很多，所以第一天的温度估测不准。</p><p>v<sub>1</sub>=0.98v<sub>0</sub>+0.02&theta;<sub>1</sub>，如果带入v<sub>1</sub>，然后相乘，v<sub>2</sub>=0.98 x 0.02&theta;<sub>1</sub> + 0.02&theta;<sub>2</sub>，假如&theta;<sub>1</sub>和&theta;<sub>2</sub>都是正数，计算后的v<sub>2</sub>要远小于&theta;<sub>2</sub>和&theta;<sub>1</sub>，所以不能很好预估这一年前两天的温度。</p><p>有个办法可以修改这一估测，让估测变得更好，更准确，特别是在估测初期，也就是不用v<sub>t</sub>，而是用v<sub>t</sub>/1-&beta;<sup>t</sup>，t就是现在的天数。举个具体例子，当t=2时，1-&beta;<sup>t</sup>=1-0.98<sup>2</sup>=0.0396，因此对第二天温度的估测变成了</p><blockquote><p>v<sub>2</sub>/0.0396=(0.0196&theta;<sub>1</sub>+0.02&theta;<sub>2</sub>)/0.0396</p></blockquote><p>也就是和的加权平均数，并去除了偏差。你会发现随着t增加，&beta;<sup>t</sup>接近于0，所以当t很大的时候，偏差修正几乎没有作用.因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p><h2 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h2><p>还有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法。基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新的权重。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012173500.png" alt=""></p><p>如果优化成本函数，函数形状如图，红点代表最小值的位置，假设从这里（蓝色点）开始梯度下降法，如果进行梯度下降法的一次迭代，无论是batch或mini-batch下降法，也许会指向这里，现在在椭圆的另一边，计算下一步梯度下降，结果或许如此，然后再计算一步，再一步，计算下去。我们发现梯度下降法要很多计算步骤。</p><p>慢慢摆动到最小值，这种上下波动减慢了梯度下降法的速度，就无法使用更大的学习率，如果要用较大的学习率（紫色箭头），结果可能会偏离函数的范围，为了避免摆动过大，就要用一个较小的学习率。</p><p>另一个看待问题的角度是，在竖直方向上，我们希望学习慢一点，因为我们不想要这些摆动。但在水平方向，我们希望快速从左向右移动，移向最小值，移向红点。所以使用动力梯度下降法，在每次迭代中，都会计算dW，db。我们要做的是计算v<sub>dW</sub>=&beta;v<sub>dW</sub>+(1-&beta;)dW，接着同样计算v<sub>db</sub>=&beta;v<sub>db</sub>+(1-&beta;)db。然后重新赋值权重，W=W-av<sub>dW</sub>，同样b=b-av<sub>dW</sub>，从而减缓梯度下降的幅度。</p><p>想象有一个碗，再拿一个球，微分项给了这个球一个加速度，此时球正向碗中间滚，球因为加速度越滚越快，而因为&beta;稍小于1，表现出一些摩擦力，所以球不会无限加速下去，所以不像梯度下降法，每一步都独立于之前的步骤，球可以向下滚，获得动量，可以从碗向下加速获得动量。</p><p>所以我们有两个超参数，学习率&alpha;以及&beta;参数。&beta;是指数加权平均数，常用值为0.9。</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>前面我们知道动力梯度下降法可以加快梯度下降，还有一个叫做<strong>RMSprop</strong>的算法，全称是<strong>root mean square prop</strong>算法，它也可以加速梯度下降，我们来看看它是如何运作的。</p><p>复习前面的内容，如果我们执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅动摆动。所以，我们想减缓纵轴方向的学习，同时加快横轴方向的学习(至少不减慢)。RMSprop算法可以实现这个目标。</p><p>在第t次迭代中，该算法会找出计算mini-batch的微分dW，db。这里我们不用v<sub>dW</sub>，而是用到新符号S<sub>dW</sub>。因此：</p><blockquote><p>S<sub>dW</sub>=&beta;S<sub>dW</sub>+(1-&beta;)(dW)<sup>2</sup>，这里的平方是对整个dW平方。</p><p>S<sub>db</sub>=&beta;S<sub>db</sub>+(1-&beta;)db<sup>2</sup>，这里的平方也是对整个db平方。</p></blockquote><p> 接着，RMSprop会更新参数值。如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012200635.png" alt=""></p><p>我们希望，S<sub>dW</sub>相对较少，S<sub>db</sub>较大。因为垂直方向的微分要比水平方向的大得多，所以斜率在垂直方向特别大。所以db较大，dW较小。db的平方较大，所以S<sub>db</sub>的平方较大。dW会小，S<sub>dW</sub>也会小。结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012224237.png" alt=""></p><p><strong>RMSprop</strong>的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率a，然后加快学习，而无须在纵轴上垂直方向偏离。</p><h2 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h2><p><strong>Adam</strong>优化算法基本上就是将<strong>Momentum</strong>和<strong>RMSprop</strong>结合在一起，那么来看看如何使用Adam算法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013142056.png" alt=""></p><p>本算法有很多超参数，分别有a、&beta;<sub>1</sub>、&beta;<sub>2</sub>、ξ四个超参数。a需要调试，尝试一系列的值，然后看哪个有效。&beta;<sub>1</sub>常用的值为0.9。至于&beta;<sub>2</sub>，推荐使用0.999，ξ为10<sup>-8</sup>。</p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013143740.png" alt=""></p><p>假设使用mini-batch梯度下降法，mini-batch数量不大，大概64或者128个样本，在迭代过程中会有噪音（蓝色线），下降朝向这里的最小值，但是不会精确地收敛，所以你的算法最后在附近摆动，并不会真正收敛，因为你用的a是固定值，不同的mini-batch中有噪音。</p><p>但是如果要减少学习率的话，在初期的时候，a学习率还比较大，我们的学习还是相对较快，但随着a变小，我们的步伐也会变慢。而不是在训练过程中，大幅度在最小值附近摆动。</p><p>所以慢慢减少a的本质在于，在学习初期，承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p><p>我们应该拆分成不同的mini-batch。第一次遍历训练集叫做第一代，第二代就是第二代。以此类推，我们得出公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144516.png" alt=""></p><p>decay-rate称为衰减率，epoch-num为代数，&alpha;<sub>0</sub>为初始学习率），注意这个衰减率是另一个你需要调整的超参数。</p><p>当然还有其它衰减：</p><p>方法二：指数衰减</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144713.png" alt=""></p><p>方法三：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144738.png" alt=""></p><p>方法四：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144804.png" alt=""></p><p>方法五：手动衰减</p><p>如果一次只训练一个模型，如果花上数小时或数天来训练，看看自己的模型训练，耗上数日，学习速率变慢了，然后把&alpha;调小一点。但这种方法只是在模型数量小的时候有用。</p><h2 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h2><p>也许我们想优化一些参数，我们把它们称之为W<sub>1</sub>和W<sub>2</sub>，平面的高度就是损失函数。在图中似乎各处都分布着局部最优。梯度下降法或者某个算法可能困在一个局部最优中，而不会抵达全局最优。如果要作图计算一个数字，比如说这两个维度，就容易出现有多个不同局部最优的图，而这些低维的图曾经影响了我们的理解，但是这些理解并不正确。事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013150349.png" alt=""></p><p>在高维度空间，我们更可能碰到鞍点。</p><p>首先，你不太可能困在极差的局部最优中，条件是你在训练较大的神经网络，存在大量参数，并且成本函数J被定义在较高的维度空间。</p><p>第二点，平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或是RMSprop，Adam这样的算法，能够加速学习算法的地方。在这些情况下，更成熟的优化算法，如Adam算法，能够加快速度，让你尽早往下走出平稳段。</p><p><strong>名词解释</strong>：平稳段是一段区域，其中导数长时间接近于0。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Mini-batch-梯度下降&quot;&gt;&lt;a href=&quot;#Mini-batch-梯度下降&quot; class=&quot;headerlink&quot; title=&quot;Mini-batch 梯度下降&quot;&gt;&lt;/a&gt;Mini-batch 梯度下降&lt;/h2&gt;&lt;p&gt;我们之前学过，向量化能够让我们有效地
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（1）</title>
    <link href="http://yoursite.com/2019/10/08/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(1)/"/>
    <id>http://yoursite.com/2019/10/08/优化算法(1)/</id>
    <published>2019-10-08T04:00:46.000Z</published>
    <updated>2019-10-21T08:42:32.813Z</updated>
    
    <content type="html"><![CDATA[<p>在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。</p><h2 id="训练、验证和测试集"><a href="#训练、验证和测试集" class="headerlink" title="训练、验证和测试集"></a>训练、验证和测试集</h2><p>我们通常会将这些数据划分成三部分，一部分作为训练集（<strong>train set</strong>），一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念。最后一部分则作为测试集（<strong>test set</strong>）。</p><p>接下来，我们开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，我们选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况。</p><p>在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。</p><p>在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被人们称为训练集，而验证集则被称为测试集。</p><p>那么验证集和测试集有什么区别呢？为什么要划分训练集、验证集和测试集？</p><p>训练集用于训练模型参数，测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。</p><h2 id="偏差（Bias）、方差（Variance）"><a href="#偏差（Bias）、方差（Variance）" class="headerlink" title="偏差（Bias）、方差（Variance）"></a>偏差（Bias）、方差（Variance）</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123133.png" alt=""></p><p>假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（<strong>high bias</strong>）的情况，我们称为“欠拟合”（<strong>underfitting</strong>）。</p><p>相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（<strong>high variance</strong>），数据过度拟合（<strong>overfitting</strong>）。</p><p>在两者之间，可能还有复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（<strong>just right</strong>）是介于过度拟合和欠拟合中间的一类。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123354.png" alt=""></p><p>下面举例子：</p><p>假定训练集误差是1%，为了方便论证，假定验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，<strong>验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。</strong></p><p>通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有高方差。<strong>也就是说衡量训练集和验证集误差就可以得出不同结论。</strong></p><p>假设训练集误差是15%，我们把训练集误差写在首行，验证集误差是16%，假设该案例中人的错误率几乎为0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，所以这种算法偏差高，因为它甚至不能拟合训练集，这与上一张图最左边的图片相似。</p><p>上面的分析都是基于假设预测，假设人眼辨别的错误率接近0%。最优误差也被称为贝叶斯误差。如果最优误差或贝叶斯误差非常高，比如15%。我们再看看上面这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt=""></p><h2 id="正则化（Rugularization）"><a href="#正则化（Rugularization）" class="headerlink" title="正则化（Rugularization）"></a>正则化（Rugularization）</h2><p>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差，下面我们就来讲讲正则化的作用原理。</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>我们用逻辑回归讲解原理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008191606.png" alt=""></p><p>&lambda;/2m乘以&omega;范数的平方。&omega;是欧几里得范数，&omega;的平方等于&omega;<sub>j</sub> (j值从1到n<sub>x</sub>)，也可以表示为&omega;<sup>T</sup>&omega;。此方法称为L2正则化。这里的&lambda;就是正则化参数。因为这里使用了欧几里得法线，被称为向量参数&omega;的L2范式。</p><p>为什么只正则化参数&omega;呢？我们可以加上参数b吗？我们可以这么做，但是一般习惯省略不写。因为&omega;通常是一个高维参数矢量，已经可以表达高偏差问题，&omega;可能包含很多参数，我们不可能拟合所有参数，而b只是单个数字，所以&omega;几乎涵盖所有参数，而不是b。如果加了参数b，没什么影响。因为b也是众多参数的一个，想加就加，没有问题。</p><p>L2正则化是最常见的正则化类型，还有L1正则化。L1正则化加的不是L2范式，而是正则项 &lambda;/m乘以$\sum_{j=1}^n $ |&omega;|，这也被称为参数&omega;向量的L1范数。(这里的n就是项数，和L2范式的nx一样)</p><p>如果用的L1是正则化，最终会是稀疏的，也就是说&omega;向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是L1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用L2正则化。</p><h3 id="为什么正则化有利于预防过拟合？"><a href="#为什么正则化有利于预防过拟合？" class="headerlink" title="为什么正则化有利于预防过拟合？"></a>为什么正则化有利于预防过拟合？</h3><p>如果正则化&lambda;设置得足够大，权重矩阵W被设置为接近于0的值（我也没看懂）。实际上不会发生这种情况的。直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近高偏差状态。</p><p>假设我们用tanh作为我们的激活函数，用g(z)表示tanh(z)。那么我们发现，只要z非常小，z只涉及少量参数。这里我们利用双曲正切函数的线性状态，只要z可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。</p><p>总结一下，如果正则化参数变得很大，参数W很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上，z的取值范围很小，这个激活函数，也就是曲线函数tanh会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p><h3 id="dropout正则化"><a href="#dropout正则化" class="headerlink" title="dropout正则化"></a>dropout正则化</h3><p>除了L2正则化，还有一个非常使用的正则化方法——Dropout(随机失活)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008212828.png" alt=""></p><p>假设你在训练上图这样的神经网络，它存在过拟合，这就是<strong>dropout</strong>所要处理的，我们复制这个神经网络，<strong>dropout</strong>会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008213026.png" alt=""></p><p>这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。不过可想而知，我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p><h3 id="如何实施dropout？"><a href="#如何实施dropout？" class="headerlink" title="如何实施dropout？"></a>如何实施dropout？</h3><p>吴恩达老师讲了最常用的方法。就是inverted dropout(随机失活)。</p><p>首先定义变量d，d3表示一个三层的dropout向量:</p><blockquote><p>d3 = np.random.rand(a3.shape[0],a3.shape[1])</p></blockquote><p>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，keep-prob是一个具体数字，上个示例中它是0.5，而假设在本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2。keep-prob的作用是生成随机矩阵，如果对a3进行因子分解，效果也是一样的。d3是一个矩阵，其中d3中的值为1的概率都是0.8，对应为0的概率是0.2。</p><p>接下来要做的就是从第三层中获取激活函数，这里我们叫它a3，a3含有要计算的激活函数，等于上面的a3乘以d3</p><blockquote><p>a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为a3*=d3。</p></blockquote><p>它的作用就是让过滤d3中所有等于0的元素，而各个元素等于0的概率只有20%，乘法运算最终把d3中相应元素归零，即让d3中0元素与a3中相对元素归零。</p><p>最后，我们向外扩展a3。用它除以0.8，或者除以keep-prob参数。</p><blockquote><p>a3/= (keep-prob)</p></blockquote><p>解释一下最后一步，我们假设第三隐藏层上有50个神经元，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%）个。现在我们看下z<sup>[4]</sup>，z<sup>[4]</sup>=&omega;<sup>[4]</sup>*a<sup>[3]</sup>+b<sup>[4]</sup>，我们的预期是，a<sup>[3]</sup>减少20%，也就是说a<sup>[3]</sup>中有20%的元素被归零。为了不影响z<sup>[4]</sup>的期望值，我们需要用&omega;<sup>[4]</sup>a<sup>[3]</sup>/0.8，它将会修正或弥补我们所需的那20%，a<sup>[3]</sup>的期望值不会变。</p><p>它的功能是，不论keep-prop的值是多少0.8，0.9甚至是1，如果keep-prop设置为1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以keep-prob，确保a<sup>[3]</sup>的期望值不变。</p><p>代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = relu(Z2)</span><br><span class="line"><span class="comment"># 随机生成和 A2 相同维度的矩阵</span></span><br><span class="line">D2 = np.random.rand(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>])    </span><br><span class="line"><span class="comment"># 保留部分元素: 小于keep_prob的设为 True, 否则设为 0 </span></span><br><span class="line">D2 = D2 &lt; keep_prob                             </span><br><span class="line"><span class="comment"># 对应位置的元素相乘, 可以把 True 看做 1, False 看做 0. </span></span><br><span class="line">A2 = np.multiply(A2, D2)                        </span><br><span class="line"><span class="comment"># 如果没有下面这行就是普通的 Dropout</span></span><br><span class="line">A2 = A2/keep_prob</span><br></pre></td></tr></table></figure><p>要注意的是, 我们不希望在测试的时候, 得到的结果也是随机的, 因此Dropout <strong>在测试过程不使用</strong>。</p><h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><p><strong>Dropout</strong>可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？</p><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，所以不愿意给任何一个输入加上太多权重。因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果。和之前讲的L2正则化类似，实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009170758.png" alt=""></p><p>实施dropout的另一个细节是，这是一个拥有三个输入特征的网络，其中一个要选择的参数是keep-prob，它代表每一层上保留单元的概率。所以不同层的keep-prob也可以变化。第一层，矩阵W<sup>[1]</sup>是7×3，第二个权重矩阵W<sup>[2]</sup>是7×7，第三个权重矩阵W<sup>[3]</sup>是3×7，以此类推，W<sup>[2]</sup>是最大的权重矩阵，因为W<sup>[2]</sup>拥有最大参数集，即7×7，为了预防矩阵的过拟合，对于这一层，它的keep-prob值应该相对较低，假设是0.5。对于其它层，过拟合的程度可能没那么严重，它们的keep-prob值可能高一些，可能是0.7甚至更高，假设这里是0.7。如果在某一层，我们不必担心其过拟合的问题，那么keep-prob可以为1。</p><p><strong>总结：</strong>如果我们担心某些层比其他层更容易发生过拟合，可以把某些层的keep-prob值设置得比其他层更低，缺点是为了使用交叉验证，我们要搜索更多得超级参数。另一种方案是在一些层上应用dropout，而有些层不用dropout应用。dropout的层只含有keep-prob这一个超参数。</p><p>dropout一大缺点就是代价函数J不再被明确定义，因为每次迭代都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。所以吴恩达老师推荐通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。</p><h3 id="其它正则化方法"><a href="#其它正则化方法" class="headerlink" title="其它正则化方法"></a>其它正则化方法</h3><h4 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h4><p>假设我们正在拟合猫咪图片分类器，如果我们想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。</p><p>除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。</p><p>通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009193402.png" alt=""></p><p>对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字4看起来是波形的，其实不用对数字4做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个4看起来有点扭曲。所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。</p><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009203859.png" alt=""></p><p>因为在训练过程中，我们希望训练误差，代价函数J都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升。early stopping 代表提早停止训练神经网络。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009200928.png" alt=""></p><p>当我们还未在神经网络上运行太多迭代过程的时候，参数&omega;接近0，因为随机初始化&omega;值时，它的值可能都是较小的随机值，所以在我们长期训练神经网络之前&omega;依然很小，在迭代过程和训练过程中&omega;的值会变得越来越大，比如在这儿，神经网络中参数&omega;的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个&omega;值中等大小的弗罗贝尼乌斯范数，与L2正则化相似，选择参数&omega;范数较小的神经网络，但愿那时的神经网络过度拟合不严重。</p><p>early stopping 有一个缺点，接下来了解一下。</p><p>在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。在重点优化代价函数时，你只需要留意&omega;和b，J(&omega;,b)的值越小越好，你只需要想办法减小这个值，其它的不用关注。然后，预防过拟合还有其他任务，就是减少方差”。</p><p>缺点就是我们不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p><h2 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h2><p>假设我们有一个数据集，它有两个输入特征，所以数据是二维的。下图即为数据集散点图:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212210.png" alt=""></p><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：</p><ol><li>零均值</li><li>归一化方差；</li></ol><p><strong>我们希望无论是训练集还是测试集都是通过相同的μ和&sigma;<sup>2</sup>定义的数据转换。</strong></p><p>第一步是零均值。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$x<sup>(i)</sup></p></blockquote><p>μ是一个向量，x等于每个训练数据x减去μ，意思是移动数据集，直到完成零均值化。</p><p>输入数据经过零均值后，得出下面的散点图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212709.png" alt=""></p><p>第二步是归一化方差，注意特征x<sub>1</sub>的方差比特征x<sub>2</sub>的方差要大得多。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$(x<sup>(i)</sup>)<sup>2</sup></p></blockquote><p> &sigma;<sup>2</sup>是一个向量，它的每个特征都有方差。经过归一化方差后，得出下面结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009214916.png" alt=""></p><p>如果你用它来调整训练数据，那么用相同的μ和σ<sup>2</sup>来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论的μ值是什么，也不论的σ<sup>2</sup>值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估μ和σ<sup>2</sup>。因为我们希望不论是训练数据还是测试数据，都是通过相同μ和σ<sup>2</sup>定义的相同数据转换，其中μ和σ<sup>2</sup>是由训练集数据计算得来的。</p><p>为什么我们需要归一化输入特征？回想一下代价函数</p><blockquote><p>J(&omega;,b)=1/m x $\sum_{i=1}^m$L(y<sup>(i)</sup> hat , y<sup>(i)</sup>)</p></blockquote><p>如果输入未归一化的输入特征，代价函数如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009221916.png" alt=""></p><p>然而如果你归一化特征，代价函数平均起来看更对称，如果你在上图这样的代价函数上运行梯度下降法，你必须使用一个非常小的学习率。因为如果是在这个位置，梯度下降法可能需要多次迭代过程，直到最后找到最小值。但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，你可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行。</p><p>下图即为输入归一化输入特征的代价函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009222132.png" alt=""></p><p>如果输入特征处于不同范围内，可能有些特征值从0到1，有些从1到1000，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。</p><h2 id="梯度消失-梯度爆炸"><a href="#梯度消失-梯度爆炸" class="headerlink" title="梯度消失/梯度爆炸"></a>梯度消失/梯度爆炸</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p><p>吴恩达老师给我们的理解是：权重W只比1略大一点（或者说比单位矩阵略大一点），深度神经网络的激活函数将爆炸式增长，如果W比单位矩阵略小一点，激活函数将以指数级递减。</p><h2 id="神经网络的权重初始化"><a href="#神经网络的权重初始化" class="headerlink" title="神经网络的权重初始化"></a>神经网络的权重初始化</h2><p>为了避免上述的梯度爆炸，权重的初始化很重要。首先，我们来看看只有一个神经元的情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021163626.png" alt=""></p><p>假设b=0，可以得到：</p><p>Z=W<sub>1</sub>x<sub>1</sub>+W<sub>2</sub>x<sub>2</sub>+W<sub>3</sub>x<sub>3</sub>+……+W<sub>n</sub>x<sub>n</sub>，b=0。</p><p>为了不让Z那么大，我们尽量让W<sub>i</sub>小一些。实际做法如下：</p><p>​    使用Relu函数，W的初始化方法：</p><p>​    <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021163946.png" alt=""></p><p>​    使用tanh函数，W的初始化方法：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021164016.png" alt=""></p><p>其中, n<sup>[l−1]</sup> 表示第l层的输入特征的个数, 也就是第 l - 1 层神经元的个数.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。&lt;/p&gt;
&lt;h2 id=&quot;训练、验证和测试集&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>人脸验证和神经风格转换</title>
    <link href="http://yoursite.com/2019/10/05/%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2/"/>
    <id>http://yoursite.com/2019/10/05/人脸验证和神经风格转换/</id>
    <published>2019-10-05T01:39:06.000Z</published>
    <updated>2019-10-19T03:30:22.988Z</updated>
    
    <content type="html"><![CDATA[<h2 id="One—Shot学习"><a href="#One—Shot学习" class="headerlink" title="One—Shot学习"></a>One—Shot学习</h2><p>人脸识别所面临的一个挑战就是需要解决一次学习问题，这意味着在大多数人脸识别应用中，需要通过单单一张图片或者单单一个人脸样例就能去识别这个人。而在我们的学习过程中，发现深度学习只有一个训练样例时，它的表现并不好。接下来解决一下这个问题。</p><p>假设我们的数据库有以下四张照片：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005095438.png" alt=""></p><p>有一种办法是，将人的照片放进卷积神经网络中，使用softmax单元来输出4种，或者说5种标签，分别对应这4个人，或者4个都不是，所以softmax里我们会有5种输出。但实际上这样效果并不好，因为如此小的训练集不足以去训练一个稳健的神经网络。而且，假如有新人加入团队，我们现在将会有5个组员需要识别，所以输出就变成了6种，这时你要重新训练神经网络吗？这听起来实在不像一个好办法。</p><p>所以要让人脸识别能够做到一次学习，为了能有更好的效果，现在要做的应该是学习<strong>Similarity</strong>函数。详细地说，想要神经网络学习这样一个用d表示的函数：d(img1,img2)=degree of difference between images。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005100413.png" alt=""></p><p>它以两张图片作为输入，然后输出这两张图片的差异值。如果你放进同一个人的两张照片，你希望它能输出一个很小的值，如果放进两个长相差别很大的人的照片，它就输出一个很大的值。所以在识别过程中，如果这两张图片的差异值小于某个阈值τ，它是一个超参数，那么这时就能预测这两张图片是同一个人，如果差异值大于τ，就能预测这是不同的两个人，这就是解决人脸验证问题的一个可行办法。</p><h2 id="Siamese网络"><a href="#Siamese网络" class="headerlink" title="Siamese网络"></a>Siamese网络</h2><p>通过上一小节我们知道我们该怎么去做人脸识别，通过输入两张图片。它将让你解决一次学习问题。接下来我们学习如何训练我们的神经网络学会这个函数d。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005102040.png" alt=""></p><p>看上图，我们经常看到这样的卷积网络，输入图片，然后通过一些列卷积，池化和全连接层，最终得到编号1这样的特征向量。有时这个会被送进<strong>softmax</strong>单元来做分类。但是我们关注的重点是编号1，假如它有128个数，它是由网络深层的全连接层计算出来的，我们要给这128个数命个名字，把它叫做f(x<sup>(1)</sup>)。可以把f(x(<sup>(1)</sup>)看作是输入图像x<sup>(1)</sup>的编码，取这个输入图像（编号2），然后表示成128维的向量。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005102947.png" alt=""></p><p>建立一个人脸识别系统的方法就是，如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我们把第二张图片的编码叫做f(x<sup>(2)</sup>)。这里我用x<sup>(1)</sup>和x<sup>(2)</sup>仅仅代表两个输入图片，它们没必要非是第一个和第二个训练样本，可以是任意两个图片。最后如果相信这些编码很好地代表了这两个图片，我们要做的就是定义d，将x<sup>(1)</sup>和x<sup>(2)</sup>的距离定义为这两幅图片的编码之差的范数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005104620.png" alt=""></p><p>那么怎么训练这个Siamese神经网络呢？不要忘了这两个网络有相同的参数，所以实际要做的就是训练一个网络，它计算得到的编码可以用于函数d，它可以告诉我们两张图片是否是同一个人。更准确地说，神经网络的参数定义了一个编码函数，如果给定输入图像，这个网络会输出的128维的编码。你要做的就是学习参数，使得如果两个图片和是同一个人，那么你得到的两个编码的距离就小。相反，如果和是不同的人，那么我们会想让它们之间的编码距离大一点。</p><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>训练神经网络两种方法：Triplet 损失 和 人脸识别二分类</p><h3 id="Triplet损失"><a href="#Triplet损失" class="headerlink" title="Triplet损失"></a>Triplet损失</h3><p>要想学习神经网络参数来得到优质的人脸图片编码，方法之一就是定义三元组损失函数然后应用梯度下降。看到这个的时候，我也一脸懵逼。这是什么啊？接下来我们一起看下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005110336.png" alt=""></p><p>我们看下这是什么意思，为了应用三元组损失函数，你需要比较成对的图像，比如这个图片，为了学习网络的参数，你需要同时看几幅图片，比如这对图片（编号1和编号2），你想要它们的编码相似，因为这是同一个人。然而假如是这对图片（编号3和编号4），你会想要它们的编码差异大一些，因为这是不同的人。</p><p>用三元组损失的术语来说，我们要做的通常是看一个 Anchor 图片，想让Anchor图片和Positive图片（Positive意味着是同一个人）的距离很接近。然而，当Anchor图片与Negative图片（Negative意味着是非同一个人）对比时，我们会想让他们的距离离得更远一点。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005111313.png" alt=""></p><p>这就是为什么叫做三元组损失，它代表你通常会同时看三张图片，你需要看Anchor图片、Postive图片，还有Negative图片，我要把Anchor图片、Positive图片和Negative图片简写成A、P、N。</p><p>把以上内容写成公式的话，d= ||f(A)-f(P)||<sup>2</sup>。我们希望||f(A)-f(P)||<sup>2</sup> &lt;= ||f(A)-f(N)||<sup>2</sup>。||f(A)-f(P)||<sup>2</sup> 就是d(A,P)，||f(A)-f(N)||<sup>2</sup>时d(A,N)，我们可以把d看为距离函数。</p><p>对表达式修改一下，因为有一种情况满足这个表达式，但是没有用处，就是把所有的东西都学成0，如果总是输出0，即0-0≤0，这就是0减去0还等于0，如果所有图像的都是一个零向量，那么总能满足这个方程f。所以为了确保网络对于所有的编码不会总是输出0，也为了确保它不会把所有的编码都设成互相相等的。另一种方法能让网络得到这种没用的输出，就是如果每个图片的编码和其他图片一样，这种情况，还是得到0-0。</p><p>为了阻止网络出现上述状况，我们需要修改表达式。也就是||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>不能刚好小于等于0。应该是比0还要小，所以这个应该小于一个-a，也就是||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup> &lt;= -a。这里的a是一个超参数，是为了阻止网络输出无用的结果。</p><p>举个例子，假如间隔设置成0.2，如果在这个例子中，d(A,P)=0.5，如果 Anchor和 Negative图片的，即只大一点，比如说0.51，条件就不能满足。虽然0.51也是大于0.5的，但还是不够好，我们想要比大很多。你会想让这个值d(A,N)至少是0.7或者更高，这样间距至少达到0.2，你可以把这项调大或者这个调小。超参数a至少是0.2，在d(A,P)和d(A,N)之间至少相差0.2，这就是间隔参数的作用。</p><p>接下来定义损失函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005114357.png" alt=""></p><p>这个max函数的作用就是，只要这个||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a &lt;= 0，那么损失函数就是0。另一方面如果||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a&gt;=0，然后取最大值。最后得到||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a，这样就会得到一个正的损失值。通过最小化这个损失函数达到的效果就是使这部分||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a小于等于0，只要这个损失函数小于等于0，网络不会关心它负值有多大。</p><p>这是一个三元组定义的损失，整个网络的代价函数应该是训练集中这些单个三元组损失的总和。假如你有一个10000个图片的训练集，里面是1000个不同的人的照片，你要做的就是取这10000个图片，然后生成这样的三元组，然后训练你的学习算法，对这种代价函数用梯度下降，这个代价函数就是定义在你数据集里的这样的三元组图片上。</p><p>注意，为了定义三元组的数据集你需要成对的A和P，即同一个人的成对的图片，为了训练你的系统你确实需要一个数据集，<strong>里面有同一个人的多个照片</strong>。这也是为什么在这个例子中，我说假设你有1000个不同的人的10000张照片，也许是这1000个人平均每个人10张照片，组成了你整个数据集。如果你只有每个人一张照片，那么根本没法训练这个系统。当然，训练完这个系统之后，你可以应用到你的一次学习问题上，对于你的人脸识别系统，可能你只有想要识别的某个人的一张照片。但对于训练集，你需要确保有同一个人的多个图片，至少是你训练集里的一部分人，这样就有成对的Anchor和Positive图片了。</p><p>那么我们该怎么选择这些三元组来形成训练集呢？一个问题是如果从训练集中，随机地选择A、P和N，遵守A和P是同一个人，而 A 和 N 是不同的人这一原则。有个问题就是，如果随机的选择它们，那么这个约束条件 d(A,P)+a&lt;=d(A,N) 很容易达到，因为随机选择的图片，A和N 比 A和P差别很大的概率很大。所以有很大的可能性|| f(A)-f(N) ||会比||(f(A) -f(P) )||大，而且差距远大于a，这样网络并不能从中学到什么。</p><p>所以为了构建一个数据集，要做的就是尽可能选择难训练的三元组A、P和N。具体而言，你想要所有的三元组都满足这个条件 d(A,P)+a&lt;=d(A,N)。难训练的三元组就是，你的A、P和N的选择使得很接近，即d(A,P)约等于d(A,N)。这样的学习算法会竭尽全力使d(A,N)式子变大，或者使d(A,P)变小，这样左右两边至少有一个的间隔。并且选择这样的三元组还可以增加你的学习算法的计算效率，如果随机的选择这些三元组，其中有太多会很简单，梯度算法不会有什么效果，因为网络总是很容易就得到了正确的结果，只有选择难的三元组梯度下降法才能发挥作用，使得这两边离得尽可能远。</p><h3 id="人脸验证与二分类"><a href="#人脸验证与二分类" class="headerlink" title="人脸验证与二分类"></a>人脸验证与二分类</h3><p>上述的Triplet loss是一个学习人脸识别卷积网络参数的好方法。另一个训练神经网络的方法是选取一对神经网络Siamese网络，使其同时计算这些嵌入，比如说128维的嵌入（编号1），或者更高维，然后将其输入到逻辑回归单元，然后进行预测，如果是相同的人，那么输出是1，若是不同的人，输出是0。这就把人脸识别问题转换为一个二分类问题，训练这种系统时可以替换Triplet loss的方法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005162142.png" alt=""></p><p>最后的逻辑回归单元输出 y帽：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005162852.png" alt=""></p><p>f(x<sup>(i)</sup>)<sub>k</sub> 代表图片x<sup>(i)</sup> 的编码，下标 k代表选择这个向量中的第k个元素。| f(x<sup>(i)</sup>)<sub>k</sub> -f(x<sup>(i)</sup>)<sub>k</sub> |是对着两个编码取元素差的绝对值。把这128个元素当作特征，然后把他们放入逻辑回归中，最后的逻辑回归可以增加参数&omega;<sub>i</sub> 和 b，就像普通的逻辑回归一样。你将在这128个单元上训练合适的权重，用来预测两张图片是否是一个人，这是一个很合理的方法来学习预测0或者1，即是否是同一个人。</p><p>   <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005165455.png" alt=""></p><p>但是在这个学习公式中，输入是一对图片，这是你的训练输入x（编号1、2），输出y是0或者1，取决于你的输入是相似图片还是非相似图片。与之前类似，你正在训练一个Siamese网络，意味着上面这个神经网络拥有的参数和下面神经网络的相同（编号3和4所示的网络），两组参数是绑定的，这样的系统效果很好。</p><p>如果这是一张新图片（编号1），当员工走进门时，希望门可以自动为他们打开，这个（编号2）是在数据库中的图片，不需要每次都计算这些特征（编号6），不需要每次都计算这个嵌入，你可以提前计算好，那么当一个新员工走近时，你可以使用上方的卷积网络来计算这些编码（编号5），然后使用它，和预先计算好的编码进行比较，然后输出预测值。因此不需要存储原始图像，如果你有一个很大的员工数据库，你不需要为每个员工每次都计算这些编码。这个预先计算的思想，可以节省大量的计算。</p><h2 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h2><p>这是卷积神经网络最有趣的应用。什么是神经风格迁移？</p><p>看例子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005170737.png" alt=""></p><p>为了描述如何实现神经网络迁移，我将使用来C表示内容图像，S表示风格图像，G表示生成的图像。这只是一个提出，在深入了解如何实现神经风格迁移之前，我们先看神经网络不同层之间的具体运算。</p><h2 id="CNN特征可视化"><a href="#CNN特征可视化" class="headerlink" title="CNN特征可视化"></a>CNN特征可视化</h2><p>其实我一直觉得神经网络的解释性真的蛮差的。让我们接下来看一下，深度卷积网络到底在学什么？</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005193429.png" alt=""></p><p>看不懂，之后再补。。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;One—Shot学习&quot;&gt;&lt;a href=&quot;#One—Shot学习&quot; class=&quot;headerlink&quot; title=&quot;One—Shot学习&quot;&gt;&lt;/a&gt;One—Shot学习&lt;/h2&gt;&lt;p&gt;人脸识别所面临的一个挑战就是需要解决一次学习问题，这意味着在大多数人脸识别应
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="人脸验证" scheme="http://yoursite.com/tags/%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>目标检测组件</title>
    <link href="http://yoursite.com/2019/10/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%84%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/10/03/目标检测组件/</id>
    <published>2019-10-03T13:16:58.000Z</published>
    <updated>2019-10-05T01:29:03.958Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个问题，即定位分类问题。这意味着，我们不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来，这就是定位分类问题。其中“定位”的意思是判断汽车在图片中的具体位置。之后，我们再讲讲当图片中有多个对象时，应该如何检测它们，并确定出位置。比如，你正在做一个自动驾驶程序，程序不但要检测其它车辆，还要检测其它对象，如行人、摩托车等等，稍后我们再详细讲。</p><p>本节我们要研究的分类定位问题，通常只有一个较大的对象位于图片中间位置，我们要对它进行识别和定位。而在对象检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。因此，图片分类的思路可以帮助学习分类定位，而对象定位的思路又有助于学习对象检测，我们先从分类和定位开始讲起。</p><p>图片分类问题你已经并不陌生了，例如，输入一张图片到多层卷积神经网络。这就是卷积神经网络，它会输出一个特征向量，并反馈给<strong>softmax</strong>单元来预测图片类型。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004134535.png" alt=""></p><p>如果你正在构建汽车自动驾驶系统，那么对象可能包括以下几类：行人、汽车、摩托车和背景，这意味着图片中不含有前三种对象，也就是说图片中没有行人、汽车和摩托车，输出结果会是背景对象，这四个分类就是softmax函数可能输出的结果。</p><p>下面是如何为监督学习任务定义目标标签y，请注意，这有四个分类，神经网络输出的是这四个数字和一个分类标签，或分类标签出现的概率。目标标签y的定义如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004140243.png" alt=""></p><p>它是一个向量，第一个组件p<sub>c</sub>表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则p<sub>c</sub>=1，如果是背景，则图片中没有要检测的对象，则p<sub>c</sub>=0 。我们可以这样理解p<sub>c</sub>，它表示被检测对象属于某一分类的概率，背景分类除外。</p><p>如果检测到对象，就输出被检测对象的边界框参数b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>和b<sub>w</sub>。最后，如果存在某个对象，那么p<sub>c</sub>，同时输出c<sub>1</sub>、c<sub>2</sub> 和c<sub>3</sub>，表示该对象属于1-3类中的哪一类，是行人，汽车还是摩托车。鉴于我们所要处理的问题，我们假设图片中只含有一个对象，所以针对这个分类定位问题，图片最多只会出现其中一个对象。</p><p>我们再看几个例子，假如下图是一张训练集图片。在y当中，第一个元素p<sub>c</sub>，因为图中有一辆车，b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>和b<sub>w</sub>会指明边界框的位置，所以标签训练集需要标签的边界框。图片中是一辆车，所以结果属于分类2，因为定位目标不是行人或摩托车，而是汽车，所以c<sub>1</sub>=0，c<sub>2</sub>=1，c<sub>3</sub>=0，c<sub>1</sub>、c<sub>2</sub>和c<sub>3</sub>中最多只有一个等于1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143054.png" alt=""></p><p>上图是只有一个检测对象的情况，如果图片中没有检测对象呢？看下图</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143246.png" alt=""></p><p>这种情况下，p<sub>c</sub>=0，y的其它参数将变得毫无意义。这里我全部写成问号，表示”毫无意义“的参数，因为图片中不存在检测对象，所以不用考虑网络输出中边界框的大小。也不用考虑图片中的对象是属于c<sub>1</sub>、c<sub>2</sub>、c<sub>3</sub>中的哪一类。</p><p>最后，我们介绍一下神经网络的损失函数，其参数为类别y和网络输出y帽，如果采用平方误差策略，则</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143820.png" alt=""></p><p>损失值等于每个元素相应差值的平方和。</p><p>如果图片中存在定位对象，那么y<sub>1</sub> = 1，所以y<sub>1 </sub>= p<sub>c</sub>，同样地，如果图片中存在定位对象，p<sub>c</sub> = 1，损失值就是不同元素的平方和。</p><p>另一种情况是，y<sub>1</sub>=0，也就是p<sub>c</sub>=0，损失值是(y<sub>1</sub>帽 - y<sub>1</sub>)^2，因为对于这种情况，我们不用考虑其它元素，只需要关注神经网络输出p<sub>c</sub>的准确度。</p><h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>假设我们正在构建一个人脸识别应用，如下图。出于某种原因，我们希望算法可以给出眼角的具体位置。眼角坐标为(x,y)，你可以让神经网络的最后一层多输出两个数字 l<sub>x</sub> 和 l<sub>y</sub>，作为眼角的坐标值。如果你想知道两只眼睛的四个眼角的具体位置，那么从左到右，依次用四个特征点来表示这四个眼角。对神经网络稍做些修改，输出第一个特征点（l<sub>1x</sub>，l<sub>1y</sub>），第二个特征点（l<sub>2x</sub>，l<sub>2y</sub>），依此类推，这四个脸部特征点的位置就可以通过神经网络输出了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004145802.png" alt=""></p><p>也许除了这四个特征点，你还想得到更多的特征点输出值，这些（图中眼眶上的红色特征点）都是眼睛的特征点，你还可以根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。如下图，为了便于说明，你可以设定特征点的个数，假设脸部有64个特征点，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004145926.png" alt=""></p><p>具体做法是，准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1或0，1表示有人脸，0表示没有人脸，然后输出（l<sub>1x</sub>，l<sub>1y</sub>）……直到（l<sub>64x</sub>，l<sub>64y</sub>）。这里我用 l 代表一个特征，这里有129个输出单元，其中1表示图片中有人脸，因为有64个特征，64×2=128，所以最终输出128+1=129个单元。</p><p>最后一个例子，看下图。如果你对人体姿态检测感兴趣，你还可以定义一些关键特征点，假设有32个，如胸部的中点，左肩，左肘，腰等等。然后通过神经网络标注人物姿态的关键特征点，再输出这些标注过的特征点，就相当于输出了人物的姿态动作。当然，要实现这个功能，你需要设定这些关键特征点，从胸部中心(l<sub>1x</sub>，l<sub>1y</sub>)一直向下，直到（l<sub>32x</sub>，l<sub>32y</sub>）。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004150615.png" alt=""></p><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。接下来，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151006.png" alt=""></p><p>假如你想构建一个汽车检测算法，步骤是，首先创建一个标签训练集，也就是x和y表示适当剪切的汽车图片样本，这张图片（编号1）x是一个正样本，因为它是一辆汽车图片，这几张图片（编号2、3）也有汽车，但这两张（编号4、5）没有汽车。出于我们对这个训练集的期望，你一开始可以使用适当剪切的图片，就是整张图片x几乎都被汽车占据，你可以照张照片，然后剪切，剪掉汽车以外的部分，使汽车居于中间位置，并基本占据整张图片。有了这个标签训练集，你就可以开始训练卷积网络了，输入这些适当剪切过的图片（编号6），卷积网络输出y，0或1表示图片中有汽车或没有汽车。训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004172940.png" alt=""></p><p>假设上图这是一张测试图片，首先选定一个特定大小的窗口，比如图片下方这个窗口，将这个红色小方块输入卷积神经网络，卷积网络开始进行预测，即判断红色方框内有没有汽车。</p><p>滑动窗口目标检测算法接下来会继续处理第二个图像，即红色方框稍向右滑动之后的区域，并输入给卷积网络，因此输入给卷积网络的只有红色方框内的区域，再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151258.png" alt=""></p><p>接着我们重复上述过程，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或1。如下图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151527.png" alt=""></p><p>再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151813.png" alt=""></p><p>这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。</p><p>滑动窗口目标检测算法也有很明显的缺点，就是计算成本，因为你在图片中剪切出太多小方块，卷积网络要一个个地处理。如果你选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本。</p><h2 id="滑动窗口的卷积实现"><a href="#滑动窗口的卷积实现" class="headerlink" title="滑动窗口的卷积实现"></a>滑动窗口的卷积实现</h2><p>为了构建滑动窗口的卷积应用，首先要知道如何把<strong>神经网络的全连接层转化为卷积层</strong>。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004152353.png" alt=""></p><p>假设对象检测算法输入一个14×14×3的图像，图像很小，不过演示起来方便。在这里过滤器大小为5×5，数量是16，14×14×3的图像在过滤器处理之后映射为10×10×16。然后通过参数为2×2的最大池化操作，图像减小到5×5×16。然后添加一个连接400个单元的全连接层，接着再添加一个全连接层，最后通过<strong>softmax</strong>单元输出。为了跟下图区分开，我先做一点改动，用4个数字来表示，它们分别对应softmax单元所输出的4个分类出现的概率。这4个分类可以是行人、汽车、摩托车和背景或其它对象。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004152537.png" alt=""></p><p>画一个这样的卷积网络，它的前几层和之前的一样，而对于下一层，也就是这个全连接层，我们可以用5×5的过滤器来实现，数量是400个（编号1所示），输入图像大小为5×5×16，用5×5的过滤器对它进行卷积操作，过滤器实际上是5×5×16，因为在卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致，输出结果为1×1。假设应用400个这样的5×5×16过滤器，输出维度就是1×1×400，我们不再把它看作一个含有400个节点的集合，而是一个1×1×400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5×5×16维度的过滤器，所以每个值都是上一层这些5×5×16激活值经过某个任意线性函数的输出结果。接着我们再添加另外一个卷积层（编号2所示），这里用的是1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，它其实就是上个网络中的这一全连接层。最后经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，我们最终得到这个1×1×4的输出层，而不是这4个数字（编号3所示）。以上就是用卷积层代替全连接层的过程，结果这几个单元集变成了1×1×400和1×1×4的维度。</p><p>掌握了卷积知识，我们再看看如何通过卷积实现滑动窗口对象检测算法。</p><p>假设向滑动窗口卷积网络输入14×14×3的图片，为了简化演示和计算过程，这里我们依然用14×14的小图片。和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4，我画得比较简单，严格来说，14×14×3应该是一个长方体，第二个10×10×16也是一个长方体。为了方便，立体部分随便画了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154252.png" alt=""></p><p>上图是输入给卷积网络的图片大小是14×14×3，测试集图片是16×16×3，如下图。现在给这个输入图片加上黄色条块，在最初的滑动窗口算法中，你会把这片蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。我们在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154529.png" alt=""></p><p>把这两个图放一起：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154910.png" alt=""></p><p>结果发现，这4次卷积操作中很多计算都是重复的。所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在这一步操作中（编号1），卷积网络运行同样的参数，使得相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化（编号2），输出结果6×6×16。照旧应用400个5×5的过滤器（编号3），得到一个2×2×400的输出层，现在输出层为2×2×400，而不是1×1×400。应用1×1过滤器（编号4）得到另一个2×2×400的输出层。再做一次全连接的操作（编号5），最终得到2×2×4的输出层，而不是1×1×4。最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），也就是这个14×14区域经过卷积网络处理后的结果，同样，右下角这个方块是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果。</p><p>具体的计算步骤是以绿色方块为例，假设你剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6）</p><p>所以该卷积操作的原理是我们不需要把输入图像分割成四个子集，分别执行前向传播，而是把<strong>它们作为一张图片输入给卷积网络进行计算</strong>，其中的公共区域可以共享很多计算。</p><p>以上就是在卷积层上应用滑动窗口算法的内容，它提高了整个算法的效率。不过这种算法仍然存在一个缺点，就是边界框的位置可能不够准确。</p><h2 id="Bounding-Box预测"><a href="#Bounding-Box预测" class="headerlink" title="Bounding Box预测"></a>Bounding Box预测</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004161102.png" alt=""></p><p>看上面的预测结果，这些边界框没有一个能完美匹配汽车的位置。</p><p>其中一个能得到更精准边框的算法是YOLO算法。它是这么做的，比如你的输入图像是100×100的，然后在图像上放一个网格。为了介绍起来简单一些，我用3×3网格，实际实现时会用更精细的网格，可能是19×19，可能更精细。基本思路是：采用图像分类和定位算法，逐一应用到图像的九个格子中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004162024.png" alt=""></p><p>所以对于这里9个格子中任何一个，你都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。因为这里有3×3格子，然后对于每个格子，你都有一个8维向量，所以目标输出尺寸是3×3×8。</p><p>注意：把对象分配到一个格子的过程是，<strong>你观察对象的中点，然后将这个对象分配到其中点所在的格子</strong>。所以即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，就是3×3网络的其中一个格子，或者19×19网络的其中一个格子。如果我们在现实实践时，采用19x19的网格会更精细。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。</p><h2 id="交并比（IOU）"><a href="#交并比（IOU）" class="headerlink" title="交并比（IOU）"></a>交并比（IOU）</h2><p>在对象检测任务中，你希望能够同时定位对象，所以如果实际边界框是这样的，你的算法给出这个紫色的边界框，那么这个结果是好还是坏？所以交并比（lOU）函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域（绿色阴影表示区域），而交集就是这个比较小的区域（橙色阴影表示区域），那么交并比就是交集的大小，这个橙色阴影面积，然后除以绿色阴影的并集面积。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004162947.png" alt=""></p><p>一般约定，在计算机检测任务中，如果IOU&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，lOU就是1，因为交集就等于并集。但一般来说只要IOU&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将IOU定得更高，比如说大于0.6或者更大的数字，但IOU越高，边界框越精确。</p><h2 id="非极大值抑制（NMS）"><a href="#非极大值抑制（NMS）" class="headerlink" title="非极大值抑制（NMS）"></a>非极大值抑制（NMS）</h2><p>到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004165505.png" alt=""></p><p>假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上右边这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004165617.png" alt=""></p><p>图中的绿色点和黄色点分别为两辆车的中点。</p><p>实践中当你运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。对于左边的车子也一样，所以不仅仅是这个格子，如果这是你们以前见过的图像，不仅这个格（编号4）子会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。</p><p>我们分步介绍一下非极大抑制是怎么起效的，因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的p<sub>c</sub>，我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后可能会对同一个对象做出多次检测，所以非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。</p><p>所以具体上，这个算法做的是，首先看看每次报告每个检测结果相关的概率p<sub>c</sub>，首先看概率最大的那个，这个例子（右边车辆）中是0.9，然后就说这是最可靠的检测，所以我们就用高亮标记，就说我这里找到了一辆车。这么做之后，非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制。所以这两个矩形分别是0.6和0.7，这两个矩形和淡蓝色矩形重叠程度很高，所以会被抑制，变暗，表示它们被抑制了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004170554.png" alt=""></p><p>接下来，逐一审视剩下的矩形，找出概率最高p<sub>c</sub>= 0.8，我们就认为这里检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他lOU值很高的矩形。所以现在每个矩形都会被高亮显示或者变暗，如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些，这就是最后得到的两个预测结果。</p><p>所以这就是非极大值抑制，非最大值意味着你只输出概率最大的分类结果，但抑制很接近，但不是最大的其他预测结果，所以这方法叫做非极大值抑制。</p><p>在这里只介绍了算法检测单个对象的情况，如果你尝试同时检测三个对象，比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。</p><h2 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h2><p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果你想让一个格子检测出多个对象，你可以这么做，就是使用<strong>anchor box</strong>这个概念。</p><p>假设有这样一张图片，对于这个例子，我们继续使用3x3网格。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004175438.png" alt=""></p><p>注意人的中点和汽车的中点几乎在同一个地方，两者都落入同一个格子。对于那个同一个格子，y输出</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004140243.png" alt=""></p><p>我们可以检测着三个类别：行人、汽车、摩托车。它将无法输出检测结果，所以我必须从两个检测结果中选一个。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004185914.png" alt=""></p><p>而anchor box的思路是，这样子，预先定义两个不同形状的anchor box，或者anchor box形状，你要做的是把预测结果和这两个anchor box关联起来。一般来说，你可能会用更多的anchor box，可能要5个甚至更多，但对于这个视频，我们就用两个anchor box，这样介绍起来简单一些。</p><p>我们要做的是定义类别标签。用的向量不再是上面那个y=[p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>]^T，而是重复两次，y=[p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>  p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>]^T。前面的p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub> 是和anchor box1关联的8个参数，后面的8个参数适合anchor box2相关联。</p><p>因为行人的形状更类似于anchor box 1的形状，而不是anchor box 2的形状，所以你可以用这8个数值（前8个参数），这么编码p<sub>c</sub>=1代表有个行人，用b<sub>x</sub>,b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>编码包住行人的边界框，然后用c<sub>1</sub>=1,c<sub>2</sub>=0,c<sub>3</sub>=0来说明这个对象是个行人。然后是车子，因为车子的边界框比起anchor box 1更像anchor box 2的形状。这样编码，p<sub>c</sub>=1,b<sub>x</sub>,b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>, c<sub>1</sub>=0,c<sub>2</sub>=1,c<sub>3</sub>=0。</p><p>假设车子的边界框形状是这样，更像anchor box 2，如果这里只有一辆车，行人走开了，那么anchor box 2分量还是一样的，要记住这是向量对应anchor box 2的分量和anchor box 1对应的向量分量，你要填的就是，里面没有任何对象，所以 ，然后剩下的就是？。编码：y=[0  ?  ?  ?  ?  ?  ?  ?  1 b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  0  1  0]。</p><p>最后，你应该怎么选择anchor box呢？人们一般手工指定anchor box形状，你可以选择5到10个anchor box形状，覆盖到多种不同的形状，可以涵盖你想要检测的对象的各种形状。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目标定位&quot;&gt;&lt;a href=&quot;#目标定位&quot; class=&quot;headerlink&quot; title=&quot;目标定位&quot;&gt;&lt;/a&gt;目标定位&lt;/h2&gt;&lt;p&gt;图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度卷积网络</title>
    <link href="http://yoursite.com/2019/10/02/%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/10/02/深度卷积网络/</id>
    <published>2019-10-02T04:25:39.000Z</published>
    <updated>2019-10-03T12:56:52.847Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet - 5"></a>LeNet - 5</h3><p>LeNet-5是专门为灰度图训练的。所以他的图像样本的深度都是1。</p><p>LeNet-5的网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002154958.png" alt=""></p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002155614.png" alt=""></p><p>实际上论文的原文是使用224x224x3的，但经过试验发现227x227x3效果更好。</p><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p>网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002161015.png" alt=""></p><p>从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。</p><p>可以看到 VGG16 是13个卷积层+3个全连接层叠加而成。</p><h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>网络越深越难训练，因为存在梯度消失和梯度爆炸的问题。本小节学习跳远连接。跳远连接可从某一个网络层激活，然后迅速反馈给另外一层甚至是神经网络的更深层。我们可以利用跳远连接构建能够训练深度网络的ResNets。</p><p>ResNets是由残差块构建的。那什么是残差块？看下图</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002163255.png" alt=""></p><p>这是一个两层的神经网络，它在L层进行激活，得到a[ l +1]再次进行激活，两层之后得到a[ l +2]。下图中的黑色部分即为计算过程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/img_0007.png" alt=""></p><p>而在残差网络中，我们直接将a^[ l ]拷贝到蓝色箭头所指位置。在线性激活之后、Relu非线性激活之前加上a[ l ]，不在沿着原来的主路径传递。这就意味者我们主路径过程的第四个式子替换为蓝色式子，也正是这个蓝色式子中加上的a[ l ]产生了一个残差块。</p><h3 id="为什么残差网络有用？"><a href="#为什么残差网络有用？" class="headerlink" title="为什么残差网络有用？"></a>为什么残差网络有用？</h3><p>一个网络深度越深，它在训练集上训练网络的效率会有所减弱，但对于ResNets就不完全是这样了。</p><p>上一张图已经说过a^[ l +2]=g(z^[ l +2]+a^[ l ])，添加项a^[ l ]是刚添加的跳远连接的输入。解开这个式子，得：</p><p>a^[ l+2]=g(w^[ l+2] * a^[ l+1]+b^[ l+2]+a^[ l ])，这里w和b为关键值。如果w和b均为0，那a^[ l+2]=g(a^[ l ])=a^[ l ](假设这里得激活函数是Relu)。结果表明，残差块学习这个恒等式函数残差块并不难，跳远连接让我们很容易的得到a^[ l+2]=a^[ l ]</p><p>这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络。因为对它来说，学习恒等函数对它来说很简单，尽管它多了两层，也只是把a^[ l ]的值赋给a^[ l+2]。</p><p><strong>所以，残差网络有用的原因是这些函数残差块学习恒等函数非常容易。</strong></p><h2 id="1x1的卷积"><a href="#1x1的卷积" class="headerlink" title="1x1的卷积"></a>1x1的卷积</h2><p>看到这个标题，你也许会迷惑，1x1的卷积能做什么呢？不就是乘以数字吗？结果并非如此</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002205234.png" alt=""></p><p>看上图，你会觉得这个1x1的过滤器没什么用，只是对输入矩阵乘以某个数字。但这个1x1的过滤器仅仅是对于6x6x1的信道图片效果不好 。如果是一张6x6x32的图片就不一样了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002205913.png" alt=""></p><p>具体来说，1x1卷积所实现的功能是遍历这36个单元格。计算输入图中32个数字和过滤器中32个数字的乘积，然后应用Relu函数。我们以一个单元格为例，用着36个数字乘以这个输入层上1x1的切片，得到一个实数画在下面图中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002211057.png" alt=""></p><p>这个1x1x32的过滤器中的32可以这样理解，一个神经元的输入是32个数字，乘以相同高度和宽度上某个切片上的32个数字。这三十二个数字具有不同信道，乘以32个权重，然后应用Relu非线性函数。一般来说，如果过滤器不止一个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6x6x过滤器数量。所以1x1卷积可以从根本上理解为这32个单元都应用了一个全连接神经网络。全连接层的作用是输入32个数字和过滤器数量，标记为nc^[ l+1]。在36个单元上重复此过程，输出结果是6x6x过滤器数量。这种方法通常称为1x1卷积，也成为Network in Network。</p><p>下面是一个1x1卷积的应用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002213518.png" alt=""></p><p>假设这是一个28x28x192的输入层，我们可以利用池化层压缩它的高度和宽度。但是如果信道数量很大，我们该如何把它压缩为28x28x32维度的层呢？我们可以用32个大小为1x1的过滤器，每个过滤器的大小都是1x1x192维，因为过滤器中信道的数量必须与输入层中信道的数量一致。因此过滤器数量为32，输出层为28x28x32。这就是压缩nc的方法。</p><h2 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h2><p>构建卷积层是，我们需要决定过滤器的大小是3x3还是5x5或者其它大小？或者要不要添加池化层？而我们接下来要讲的Inception就是代替我们来做决定的。虽然网络结构会变得非常复杂，但网络表现得非常好。我们来看一下原理。</p><p>基本思想：不需要人为的决定使用哪个过滤器，或者是否需要池化，而是由网络自行确定这些参数。人们只需给出这些参数的所有可能值，然后把这些输出连起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合。</p><p>举个例子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003102559.png" alt=""></p><p>如果我们直接计算上图，我们的计算成本为28x28x32x5x5x192</p><p>但是我们用1x1卷积后：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003103150.png" alt=""></p><p>我们的计算成本变为28x28x16x192+28x28x32x5x5x16，使用1x1卷积后计算成本是没使用前的1/10。</p><p>下面再举一个例子：</p><p>例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=”SAME”)，输出数据的大小为100x100x256。其中，卷积层的参数为5x5x128x256。假如上一层输出先经过具有32个输出的1x1卷积层，这时得图片输出为100x100x32，接着再经过具有256个输出的5x5卷积层，那么最终的输出数据的大小仍为100x100x256，但卷积参数量已经减少为1x1x128x32 + 5x5x32x256，大约减少了4倍。</p><p>在inception结构中，大量采用了1x1的矩阵，主要是两点作用：1）对数据进行降维；2）引入更多的非线性，提高泛化能力，因为卷积后要经过ReLU激活函数。</p><h3 id="搭建inception网络："><a href="#搭建inception网络：" class="headerlink" title="搭建inception网络："></a>搭建inception网络：</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003111139.png" alt=""></p><p>inception就是将这些模块都组合到一起。</p><h3 id="Inception-network："><a href="#Inception-network：" class="headerlink" title="Inception network："></a>Inception network：</h3><p>下面这个图就是Inception的网络：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003111653.png" alt=""></p><p>看起来很复杂，但我们截取其中一个环节，如上图的红色框。就会发现这不正是我们搭建的inception吗。另外，再一些inception模块前网络会使用最大池化层来修改高和宽的维度。</p><p>其实上图的Inception并不完整，看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003112235.png" alt=""></p><p>多出了红色框住的部分。这些分支是做什么的呢?这些分支是通过隐藏层做出预测。</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>我们在做计算机视觉的应用时，相对于从头训练权重，下载别人训练好的网络结构的权重作为我们预训练，然后转换到感兴趣的任务上。别人的训练过程可能是需要花费好几周，并且需要很多的GPU找最优的过程，这就意味着我们可以下载别人开源的权重参数并把它当做一个很好的初始化，用在我们的神经网络上。这就是迁移学习。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003130518.png" alt=""></p><p>假如我们在做一个识别任务，却没有很多的训练集。我们就可以把别人的网络下载，冻结所有层的参数。我们只需要训练和我们Softmax层有关的参数，然后把别人Softmax改成我们自己Softmax。通过别人的训练的权重，我们可能会得到一个好的结果，即使我们的训练集并不多。</p><p>由于前面的层都冻结了，相当于一个固定函数，不需要改变，因为我们不训练它。</p><p><strong>网络层数越多，需要冻结的层数越少，需要训练的层数就越多。</strong></p><h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><p> 数据扩充也叫数据增强。因为计算机视觉相对于机器学习，数据较少。所以数据增强为了增加数据的数量。下面我们讲一下数据增强的办法：</p><h3 id="垂直镜像对称"><a href="#垂直镜像对称" class="headerlink" title="垂直镜像对称"></a>垂直镜像对称</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003200138.png" alt=""></p><h3 id="随机裁剪"><a href="#随机裁剪" class="headerlink" title="随机裁剪"></a>随机裁剪</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003200328.png" alt=""></p><h3 id="色彩转换"><a href="#色彩转换" class="headerlink" title="色彩转换"></a>色彩转换</h3><p>给RGB三个通道加上不同的失真值</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003201235.png" alt=""></p><p>这些可以轻易改变图像的颜色，但是对目标的识别还是保持不变的。所以使用这种数据增强方法使得模型对照片的颜色更改更具鲁棒性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;经典网络&quot;&gt;&lt;a href=&quot;#经典网络&quot; class=&quot;headerlink&quot; title=&quot;经典网络&quot;&gt;&lt;/a&gt;经典网络&lt;/h2&gt;&lt;h3 id=&quot;LeNet-5&quot;&gt;&lt;a href=&quot;#LeNet-5&quot; class=&quot;headerlink&quot; title=&quot;LeN
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>CNN详解</title>
    <link href="http://yoursite.com/2019/10/01/CNN%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2019/10/01/CNN详解/</id>
    <published>2019-10-01T09:00:13.000Z</published>
    <updated>2019-10-07T12:03:04.374Z</updated>
    
    <content type="html"><![CDATA[<h2 id="全连接神经网络的局限性"><a href="#全连接神经网络的局限性" class="headerlink" title="全连接神经网络的局限性"></a>全连接神经网络的局限性</h2><p>过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为每张图片都有 3 个颜色通道，它的数据量是 64×64×3=12288。</p><p>现代计算机中，64×64 真的是很小的图片，如果想处理大一点的图片，比如1000×1000，那么最终输入层的数据量是300万，假设我们有一个隐藏层，含有1000个神经元构成的全连接网络，那么数据量将达到 1000*300万，也就是30亿。在这样的情况下很难获得足够的数据防止过拟合，并且需要的内存大小很难得到满足。</p><p>本篇讲解的卷积神经网络（也称作 <strong>ConvNets</strong> 或 <strong>CNN</strong>）不仅可以达到减少参数的作用，而且在图像分类任务中还有其他优于全连接神经网络的特性。</p><h2 id="卷积神经网络概览"><a href="#卷积神经网络概览" class="headerlink" title="卷积神经网络概览"></a>卷积神经网络概览</h2><p>一个图像的相互靠近的局部具有联系性，卷积神经网络的思想就是用不同的神经元去学习图片的局部特征，<strong>每个神经元用来感受局部的图像区域</strong>，如下图不同颜色的圆圈所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001172911.png" alt=""></p><p>然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。</p><p>卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了由<strong>卷积层</strong>和<strong>子采样层</strong>构成的特征抽取器，并且在最后有几个<strong>全连接层</strong>用于对提取的特征进行分类。一个简单的卷积神经网络模型如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173024.png" alt=""></p><p>接下来我们讲解以下这几个层究竟在做的是什么。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层主要是做卷积运算。本文用 “*” 表示卷积操作。</p><h4 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h4><p>假如我们拥有一个6<em>6的灰度图，矩阵如下图表示，在矩阵右边还有一个3\</em>3的矩阵。我们称之为过滤器(filter)或核。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173735.png" alt=""></p><p>卷积的第一步就是将过滤器覆盖在灰度图的左上角的数字就是过滤器对应位置上的数字，如下图蓝色部分：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174317.png" alt=""></p><p>蓝色矩阵每个小个子的左上角的数字就是过滤器对应位置上的数字，将每个格子的两个数字相乘：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174505.png" alt=""></p><p>然后将得到的矩阵所有元素相加，即3+1+2+0+0+0+(-1)+(-8)+(-2)= -5，然后将这个值放到新的矩阵的左上角，最后的新的矩阵是一个4*4的矩阵。<strong>规律：n*n维的矩阵经过一个f*f过滤器后，得到一个（n-f+1）*（n-f+1）维度的新矩阵。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174907.png" alt=""></p><p>我们以1为步长(stride)向右移动一格，再进行类似的计算：</p><p>0 × 1 + 5 × 1 + 7 × 1 + 1 × 0 + 8 × 0 + 2 × 0 + 2 × (-1) + 9 × (-1) + 5 × (-1) = -4 写在刚刚得到的-5的右边。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174952.png" alt=""></p><p>重复向右移动，直到第一行都计算完成：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175037.png" alt=""></p><p>然后将过滤器向下移动一格，从左边开始继续运算，得到-10，写在新的矩阵第二行第一个位置上：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175208.png" alt=""></p><p>不断的向右、向下移动，直到计算出所有的数据，这样就完成了卷积运算，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175359.png" alt=""></p><p>下面这张图也可以帮助我们理解卷积运算(黄色的矩阵式过滤器)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/动态.gif" alt=""></p><p>最终经过卷积得到的图像，我们称之为<strong>特征图(Feature Map)</strong></p><p>对于上述过程，也许你有很多疑问，比如过滤器是什么，为什么是3×3的，作用是什么，滑动的步长stride为什么是1，为什么得到4×4的矩阵，卷积有什么作用等，我们一一解答。</p><h4 id="过滤器-filter"><a href="#过滤器-filter" class="headerlink" title="过滤器(filter)"></a>过滤器(filter)</h4><p>我们来看看下面的例子，本例子中我们用正数表示白色，0表示灰色，负数表示黑色。</p><p>左边 6×6 的矩阵是一个灰度图，左半部分是白色，右半部分是灰色。下面的3×3的过滤器是一种垂直边缘过滤器。</p><p>对二者进行卷积得到新的矩阵，这个矩阵的中间部分数值大于0，显示为白色，而两边为 0， 显示为灰色。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203230.png" alt=""></p><p>上图中三个红色箭头都是我们过滤得到的边缘。在这里，你可能会疑问为什么会有这么大的边缘，那是因为我们的原始图片太小，如果我们把原始灰度图放大一点：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203434.png" alt=""></p><p>再进行卷积，就可以得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203509.png" alt=""></p><p>这样就能大致看出过滤器确实过滤出了垂直边缘。如果想得到水平的边缘，将过滤器转置一下即可。而如果想得到其他角度的边缘，则需要在过滤器中设置不同的数值。下图是水平边缘过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203557.png" alt=""></p><p>当然很难手动设置过滤器的值去过滤某个特殊角度（比如31°）的边缘，我们通常是<strong>将过滤器中的9个数字当成9个参数</strong>。随机初始化这些参数后，通过神经网络的学习来获得一组值，用于过滤某个角度的边缘。</p><p>目前主流网络的过滤器的大小一般是1x1、3x3、5x5等，数值一般不会太大，因为我们希望这些过滤器可以获得一些更细微的边缘特征。另外，一般将长和宽都取奇数，具体原因我们讲到Padding的时候再解释。</p><h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>Padding是填充。什么是填充呢？</p><p>在前面，我们的例子一个6*6的矩阵经过一个3*3的过滤器后，得到一个4*4的新矩阵。但是这样的话会有两个缺点，第一个缺点就是我们每次做卷积运算的时候，我们的图像都会缩小。可能我们在做完几次卷积之后，我们的图像就会变得很小。第二个缺点是角落边的像素点只会被使用一次，像下图的绿色。但是如果是中间的像素点(类似图中的红色框)，就会被使用很多次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001210310.png" alt=""></p><p>所以，那些边缘或者角落的像素点在输出时使用较少。这意味者，我们将丢掉图片边缘区域。如果这里有重要信息，那我们就得不偿失了。</p><p>那我们该怎么解决这个问题呢？这时候就用到Padding了。我们可以在卷积操作之前填充这个图像。我们可以沿着图像的边缘再填充一层像素(通常为0，因为用0填充不影响原来的图片特征)。这样做的话，我们的原始图片像素就有6<em>6变成了8\</em>8。接着我们对这幅8*8的图像进行卷积，得到的新矩阵就不是4*4的，而是6*6的。<strong>规律：n*n维的矩阵填充了P个像素点后再经过一个f*f过滤器后，得到一个（n+2p-f+1）*（n+2p-f+1）维度的新矩阵。</strong></p><p>由上面这个规律：我们得出p=(f-1)/2，为了使p是对称填充，所以我们的f通常都是奇数。还有一个原因，是因为当我们有一个奇数的过滤器，我们会有一个中间像素点。这就是为什么前面我们说过滤器通常都是奇数维度。</p><p>那我们Padding填充的时候到底选择多少呢？1还是2还是更大的数字？</p><p>其实Padding有两个选择：一个Valid，一个是Same。</p><p>Valid：意味不填充。也就是之前的6<em>6矩阵经过一个3\</em>3的矩阵后，得出一个4*4的矩阵。</p><p>Same：意味我们的图片输出大小和输入大小是一样的。也就是上面的6<em>6矩阵在Padding=1之后经过一个3\</em>3的矩阵后，得出一个6*6的矩阵。图片输出大小和输入大小一样。</p><h4 id="卷积步长"><a href="#卷积步长" class="headerlink" title="卷积步长"></a>卷积步长</h4><p>我们先用 3x3 的过滤器以2为步长卷积 7x7 的图像，看看会得到什么。</p><p>第一步，对左上角的区域进行计算：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214116.png" alt=""></p><p>第二步，向右移动，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214201.png" alt=""></p><p>再继续向右向下移动过滤器，就可以得到下面的结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214424.png" alt=""></p><p><strong>规律：我们用一个f<em>f的过滤器卷积一个n\</em>n的图像，Padding=p，步长为s，我们得到一个(n+2p-f)/s+1*(n+2p-f)/s+1</strong></p><p>最后一个问题：如果我们的商不是整数呢？这时我们采用向下求整的策略。</p><h4 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h4><p>上面的讲解你已经知道如何对灰度图进行卷积运算了，现在看看如何在有三个通道(RGB)的图像上进行卷积。</p><p>假设彩色图像尺寸 6×6×3，这里的 3 指的是三个颜色通道。<strong>我们约定图像的通道数必须和过滤器的通道数匹配</strong>，所以需要增加过滤器的个数到3个。将他们叠在一起，进行卷积操作。6*6*3的图像经过一个3<em>3\</em>3过滤器得出一个新的4*4*1图像。</p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223726.png" alt=""></p><p>具体过程就是，首先，把这个 3×3×3 的过滤器先放到最左上角的位置，依次取原图和过滤器对应位置上的各27 个数相乘后求和，得到的数值放到新矩阵的左上角。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223830.png" alt=""></p><p><strong>注意，由于我们是用三维的过滤器对三维的原图进行卷积操作，所以最终得到的矩阵只有一维。</strong></p><p>然后不断向右向下进行类似操作，直到“遍历”完整个原图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223939.png" alt=""></p><p>至于效果是怎么样的。结合之前的知识，我们假设了一组垂直边界过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002081928.png" alt=""></p><p>它可以完成RGB三个通道上垂直边缘的检测工作。这也解释了为什么过滤器使用3个通道：也就是说如果你想获得RGB三个通道上的特征，那么你使用的过滤器也得是3个通道的。</p><p>如果你除了垂直边界，还想要检测更多的边界怎么办？可以使用更多 3维 甚至更多的过滤器，如下图框住的部分，我们增加了一组过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002082118.png" alt=""></p><p>这样，两组过滤器就得到了两组边界，我们一般会将卷积后得到的图叠加在一起，得到上图左边的 4x4x2 的图像。如果你还想得到更多的边界特征，使用更多的3维的过滤器即可。</p><p>现在我们对维度进行总结：</p><blockquote><p>首先我们假设我们的没有padding且步长为1，这时候使用 n × n × nc 原图和 nc’ 个 f × f × fc 的过滤器进行卷积，得到 (n-f+1) × (n-f+1) × nc’ 的图像</p></blockquote><p>nc 是 原图的通道数(3)，过滤器的通道数和原图的通道数必须一样，都是 nc。</p><p>nc’ 是最终得到的图像的通道数(在上面的例子为2)，由使用的过滤器个数决定(而 nc’ 又是下一层的输入图像的通道数)</p><h4 id="卷积层全貌"><a href="#卷积层全貌" class="headerlink" title="卷积层全貌"></a>卷积层全貌</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002083956.png" alt=""></p><p>对于卷积层，我们同样需要进行以下操作，这也是前向传播的操作：</p><blockquote><p>z^[1] = w^[1] * a^[0]+b^[1]</p><p>a^[1]=g(z^[1])</p></blockquote><p>上面所用到的变量解释：</p><blockquote><p>a^[0] 是我们原图的数据 X ，也就是上图 6x6x3 的RGB图。</p><p>w^[1] 是这一层的权重矩阵，由 2 个 3x3x3 的过滤器组成。</p><p>b^[1] 是第一层的偏置项，不同的过滤器对应不同的偏置值</p><p>g() 是激活函数，本例子中使用 ReLu</p></blockquote><p>卷积层的工作说明：</p><p>式子中 w^[1] <em> a^[0] 是代表进行<strong>卷积运算</strong>得到两个特征图(也就是2个4x4的矩阵)，如下图的绿色框所示，接着再对两个特征图<em>*加上不同的偏置</em></em>就得到了z^[1]，如下图红色框所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002084726.png" alt=""></p><p>进一步<strong>应用激活函数</strong>就得到了两个处理过的图像，将他们叠加在一起，就得到了这一层的输出 a^[1](上图右下角)。</p><p><strong>注意</strong>：上图中的加偏置(b1,b2)均使用了python的广播</p><p>简单来说，卷积层的工作就是：<strong>将输入的数据a^[l - 1]进行卷积操作，加上偏置，再经过激活函数非线性化处理得到a^[l]</strong>。到这里卷积层的任务就完成了。</p><h4 id="为什么要使用卷积？"><a href="#为什么要使用卷积？" class="headerlink" title="为什么要使用卷积？"></a>为什么要使用卷积？</h4><p>卷积层的主要优势在稀疏连接就和权值共享</p><h5 id="稀疏连接"><a href="#稀疏连接" class="headerlink" title="稀疏连接"></a>稀疏连接</h5><p>假设我们输入的是32×32×3的RGB图像。</p><p>如果是<strong>全连接网络</strong>，对于隐藏层的某个神经元，它不得不和前一层的所有输入（32×32×3）都保持连接。</p><p>但是，对<strong>卷积神经网络</strong>而言，神经元只和输入图像的局部相连接，如下图圆圈所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002115607.png" alt=""></p><p>这个局部连接的区域，我们称之为“感受野”，大小等同于过滤器的大小(3*3*3)。</p><p>看下面的图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002121345.png" alt=""></p><p>右边的绿色或者红色的输出单元仅仅与36个输入特征中的九个相连接，其它像素值对输出不会产生任何影响。</p><p>相比之下，卷积神经网络的神经元与图像的连接要稀疏的多，我们称之为<strong>稀疏连接</strong>。</p><h5 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h5><p>权值共享其实就是过滤器共享。特征检测如垂直边缘检测过滤器如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，如果我们用垂直过滤器扫描整张图片，就可以得到整张图片的所有垂直边缘，而换做水平过滤器就可以扫描出图片的水平边缘。</p><p>在CNN中，我们<strong>用不同的神经元(过滤器)去学习整张图片的不同特征</strong>，而不是利用不同的神经元 学习图片不同的局部特征。因为图像的不同部分也可能有相同的特征。每个特征检测过滤器都可以在输入图片的不同区域中使用同样的参数(即方格内的9个数字)，以便提取垂直边缘或其它特征。右边两张图每个位置是被<strong>同样</strong>的过滤器扫描而得的，所以<strong>权重</strong>是一样的，也就是<strong>共享</strong>。假设有100个神经元，全连接神经网络就需要32×32×3×100=307200个参数。</p><p>因为共享了权值，提取一个特征只需要 f×f 个参数，上图右边两张图的每个像素只与使用 3x3 的过滤器相关，顶多再加上一个偏置项。在本例子中，得到一个feature map 只用到了 3*3+1=10 个参数。如果想得到100个特征，也不过是用去1000个参数。这就解释了为什么利用卷积神经网络可以减少参数个数了。</p><p>当你对提取到的 水平垂直或者其他角度 的特征，再进行卷积操作，就可以得到更复杂的特征，比如一个曲线。如果对得到的曲线再进行卷积操作，又能得到更高维度的特征，比如一个车轮，如此往复，最终可以提取到整个图像全局的特征。</p><p>到这里卷积层的内容就结束了。</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>对于一个32x32像素的图像，假设我们使用400个3x3的过滤器提取特征，每一个过滤器和图像卷积都会得到含有 (32-3 + 1) × (32 - 3 + 1) = 900 个特征的feature map，由于有 400 个过滤器，所以每个样例 (example) 都会得到 900 × 400 = 360000 个特征。而如果是96x96的图像，使用400个8x8的过滤器提取特征，最终得到一个样本的特征个数为 3168400。对于如此大的输入，训练难度大，也容易发生过拟合。</p><p>为了减少下一个卷积层的输入，引入了采样层(也叫池化层)，采样操作分为最大值采样(Max pooling)，平均值采样(Mean pooling)。</p><p><strong>最大值采样</strong></p><p>举个例子，我们用3x3的过滤器以1为步长去扫描图像，每次将重叠的部分的最大值提取出来，作为这部分的特征，过程如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093125.png" alt=""></p><p>我们也可以使用2x2的过滤器。以2为步长进行特征值的提取。最终得到的图像长度和宽度都缩小一半：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093418.png" alt=""></p><p><strong>平均值采样</strong></p><p>与最大值采样不同的是 将覆盖部分特征的平均值作为特征，其他过程都一样。经过采样处理，可以提取更为抽象的特征，减少数据处理量的同时保留有效信息。</p><p>下图是过滤器为2x2，步长为2进行特征值的提取：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093447.png" alt=""></p><p><strong>注意</strong>：池化层没有需要学习的参数。且padding大多数时候为0</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>在经过几次卷积和池化的操作之后，卷积神经网络的最后一步是全连接层，全连接层的个数可能不止一个：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173024.png" alt=""></p><p><strong>最后一个采样层到全连接层的过渡是通过卷积来实现的</strong>，比如前层输出为 3x3x5 的feature map，那我们就用 3x3x5 的过滤器对前层的这些feature map进行卷积，于是得到了一个神经元的值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002101140.png" alt=""></p><p>全连接层有1024个神经元，那我们就用1024个3x3x5 的过滤器进行卷积即可。</p><p><strong>第一个全连接层到第二个全连接层的过渡也是通过卷积实现的</strong>，若前层有1024个神经元，这层有512个，那可以看做前层有1024x1x1个feature map，然后用1x1的卷积核对这些feature map进行卷积，则得到100x1x1个feature map。</p><p>卷积神经网络要做的事情，大致分为两步：</p><ol><li>卷积层、采样层负责提取特征</li><li>全连接层用于对提取到的特性进行分类</li></ol><p>最后一步就和普通的神经网络没有什么区别，充当一个分类器的作用，输入是不同特征的特征值，输出是分类。在全连接层后，就可以根据实际情况用softmax对图片进行分类了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;全连接神经网络的局限性&quot;&gt;&lt;a href=&quot;#全连接神经网络的局限性&quot; class=&quot;headerlink&quot; title=&quot;全连接神经网络的局限性&quot;&gt;&lt;/a&gt;全连接神经网络的局限性&lt;/h2&gt;&lt;p&gt;过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深层神经网络</title>
    <link href="http://yoursite.com/2019/09/29/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/29/深层神经网络/</id>
    <published>2019-09-29T13:11:55.000Z</published>
    <updated>2019-10-21T03:41:27.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h2><p>深层神经网络样子也许是下面这样的，也有可能是更多层的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190930110158.png" alt=""></p><p>上图是一个四层的神经网络(输入层一般记为零层或不记住)，我们用L表示神经网络的层数，n^[l] 表示第 l 层的神经元数量</p><p>由于神经网络具有许多层，在下图中用方框代表一层，每个层都要完成各自的任务，流程图大致如下所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward.png" alt=""></p><p>蓝框中的部分完成正向传播：</p><blockquote><p>该过程的输入是 X 也就是 A<sup>[0]</sup>，一层一层向后计算，最后得到A<sup>[L]</sup>。</p><p>并且，在各层 l 计算出 A^[l] 的同时，<strong>缓存</strong>各层的输出值 A^[l] 到变量 cache 中，因为反向传播将会用上。<strong>图中这里写错了，应该是A<sup>[1]</sup>、A<sup>[2]</sup>、A<sup>[3]</sup>的。</strong></p></blockquote><p>绿框中的部分完成反向传播：</p><blockquote><p>该过程的输入是dA<sup>[L]</sup>，并根据dA<sup>[L]</sup>一步步向前计算，得到各层对应的dZ<sup>[l]</sup>，dW<sup>[l]</sup>，db<sup>[l]</sup></p><p>此外，还要计算出 dA<sup>[l - 1]</sup> 作为前一层的输入</p></blockquote><p>具体过程请看<a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的正向传播/#more">浅层神经网络的正向传播</a>) 和 <a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的反向传播/#more">浅层神经网络的反向传播</a>)</p><h2 id="核对矩阵的维数"><a href="#核对矩阵的维数" class="headerlink" title="核对矩阵的维数"></a>核对矩阵的维数</h2><p>为了在写代码的过程中减少出现bug，尤其是在进行反向传播的时候，我们要注意核对矩阵的维数，这个知识点我们前面将 <a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的反向传播/#more">浅层神经网络的反向传播</a>)的文章中也提到了，大致规律如下：</p><blockquote><p>W^[l] ： (n<sup>[ l ]</sup>, n<sup>[l - 1]</sup>)</p><p>Z^[l] ： (n<sup>[ l ]</sup>, m),</p><p>A^[l] ： (n<sup>[ l ]</sup>, m)</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward4.png" alt=""></p><p>另外，还有一个规律：</p><blockquote><p>对任意层而言：</p><p>dW 的维度和 W 的维度相同</p><p>dZ 的维度和 Z 的维度相同</p><p>dA 的维度和 A 的维度相同</p></blockquote><h2 id="为什么要使用深度神经网络？"><a href="#为什么要使用深度神经网络？" class="headerlink" title="为什么要使用深度神经网络？"></a>为什么要使用深度神经网络？</h2><p>我们都知道深度神经网络可以解决很多问题，网络并不一定要很大，但一定要有深度，需要比较多的隐藏层。这到底为什么呢？</p><p>如果我们在建立一个人脸识别或者检测系统，当我们输入一张脸部的图片时，我们可以把深度神经网络第一层当成一个特征探测器或者边缘检测器。在这个例子中，我们建立一个大概有20个隐藏单元的深度神经网络。看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190930164522.png" alt=""></p><p>隐藏单元就是这个图里这些小方块。每个小方块都是一个隐藏单元，它会去寻找一张图的边缘方向。它可能在水平方向找、也可能在竖直方向找。这个东西在卷积神经网络我们会细讲(别问，问就是当时不会)。这里就告诉你们我们可以先把神经网络的第一层当作看图，然后去找这张图片的各个边缘。接着我们把组成图片边缘的像素放在一起看。它可以把探测到的边缘组合成面部的不同部分。比如说，可能有一个神经元回去找眼睛的部分，另外还有找鼻子或其它的部分。然后把这许多边缘结合在一起，就可以开始检测人脸的不同部分，最后再把这些部分放在一起，就可以识别或检测不同的人脸。我们把前几层的神经网络当作简单的检测函数，例如：边缘检测等，之后把它们跟后几层结合在一起。注意：边缘探测器其实相对来说都是针对图片中非常小块的面积。</p><p>具体原因移步网易云课堂的吴恩达的深度学习。<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=613c87c1215241179940b8e203431a3d#/learn/content?type=detail&amp;id=2001701022&amp;cid=2001694285" target="_blank" rel="noopener">链接</a></p><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>在学习算法中还有其它参数，需要输入到学习算法中，比如学习率&alpha;、隐藏层的数量、使用的激活函数种类等都是超参数。因为它们影响着最终W和b的值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;深层神经网络&quot;&gt;&lt;a href=&quot;#深层神经网络&quot; class=&quot;headerlink&quot; title=&quot;深层神经网络&quot;&gt;&lt;/a&gt;深层神经网络&lt;/h2&gt;&lt;p&gt;深层神经网络样子也许是下面这样的，也有可能是更多层的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://r
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>上传项目到GitHub</title>
    <link href="http://yoursite.com/2019/09/28/%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E5%88%B0GitHub/"/>
    <id>http://yoursite.com/2019/09/28/上传项目到GitHub/</id>
    <published>2019-09-28T10:05:42.000Z</published>
    <updated>2019-10-21T08:14:30.290Z</updated>
    
    <content type="html"><![CDATA[<p>首先登录GitHub，没有账号的申请一个。很简单，跳过。</p><p>新建一个仓库：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193118.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193719.png" alt=""></p><p>记住这个网址，之后用到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928194701.png" alt=""></p><p>进入项目的目录，点击空白处，选择git Bash。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192144.png" alt=""></p><p>输入git init，会发现当前目录下多了一个.git文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928191933.png" alt=""></p><p>接着输入git add .   这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把.换成这个特定的文件名即可。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192510.png" alt=""></p><p>输入git commit -m “first commit”，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192735.png" alt=""></p><p>输入git remote add origin 自己的仓库url地址（上面有说到） 将本地的仓库关联到github上，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928194818.png" alt=""></p><p>输入git push -u origin master，这是把代码上传到github仓库的意思</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928195326.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先登录GitHub，没有账号的申请一个。很简单，跳过。&lt;/p&gt;
&lt;p&gt;新建一个仓库：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Github的使用</title>
    <link href="http://yoursite.com/2019/09/28/Github%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/09/28/Github的使用/</id>
    <published>2019-09-28T08:10:43.000Z</published>
    <updated>2019-09-28T10:01:01.820Z</updated>
    
    <content type="html"><![CDATA[<p>github的账户创建和仓库创建比较简单，就不赘述了</p><h2 id="github添加ssh账户"><a href="#github添加ssh账户" class="headerlink" title="github添加ssh账户"></a>github添加ssh账户</h2><p>首先，点击账户，选择setting</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928164421.png" alt=""></p><p>直接添加就完事了。那我们怎么生成ssh公钥呢？</p><p>先回到用户的主目录下，编辑文件.gitconfig，修改某台机器的git配置。修改为注册github的邮箱，填写用户名</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928165020.png" alt=""></p><p>接着执行命令，ssh-keygen -t rsa -C “邮箱地址”，一路yes。最后生成三个文件：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928165739.png" alt=""></p><p>id_rsa是机器私钥，自己保留。id_rsa.pub就是我们的公钥。把公钥内容复制到第一张图New SSH Key后的位置。名字可以随便取。</p><h2 id="克隆项目"><a href="#克隆项目" class="headerlink" title="克隆项目"></a>克隆项目</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928170507.png" alt=""></p><p>接着git clone SSH</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928172714.png" alt=""></p><p>这个再cmd下执行同样有效。</p><p>如果出错了，执行以下代码：先执行eval “$(ssh-agent -s)”，再执行ssh-add</p><p>不只是可以使用SSH，也可以使用HTTPS。</p><p>git  clone  网址         </p><p>在cmd上同样是有效的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928172950.png" alt=""></p><h2 id="推送代码"><a href="#推送代码" class="headerlink" title="推送代码"></a>推送代码</h2><p>推送前：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928174030.png" alt=""></p><p>图中的红框是分支。</p><p>推送分支就是把给分支上的所有本地提交库推送到远程库，推送时要指定本地分支，这样，git就会把该分支推送到远程库对应的远程分支上。</p><p>git push origin 分支名称</p><h2 id="本地分支跟踪远程分支"><a href="#本地分支跟踪远程分支" class="headerlink" title="本地分支跟踪远程分支"></a>本地分支跟踪远程分支</h2><p>git branch —set-upstream-to=origin/远程分支名词  本地分支名称</p><p>跟踪后，如果本地分支和远程分支的进度不一样，使用命令 git status 会提醒。</p><h2 id="拉取代码"><a href="#拉取代码" class="headerlink" title="拉取代码"></a>拉取代码</h2><p>git pull orgin 分支名称</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;github的账户创建和仓库创建比较简单，就不赘述了&lt;/p&gt;
&lt;h2 id=&quot;github添加ssh账户&quot;&gt;&lt;a href=&quot;#github添加ssh账户&quot; class=&quot;headerlink&quot; title=&quot;github添加ssh账户&quot;&gt;&lt;/a&gt;github添加ssh账户
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>git的分支管理</title>
    <link href="http://yoursite.com/2019/09/27/git%E7%9A%84%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/27/git的分支管理/</id>
    <published>2019-09-27T10:01:21.000Z</published>
    <updated>2019-09-28T08:11:44.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分支原理"><a href="#分支原理" class="headerlink" title="分支原理"></a>分支原理</h2><p>git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。HEAD严格来说不是指向提交，而是指向master。master才是指向提交的版本，所以，HEAD指向的就是当前分支。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927191539.png" alt=""></p><p>每次提交，master分支都会向前移动一步。这样，随着不断提交，master分支的线也越来越长。</p><p>当我们创建新的分支dev，git创建了一个指针叫dev。指向master相同的提交。再把HEAD指向dev，就表示当前分支在dev上。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927191917.png" alt=""></p><p>git创建一个分支很快，因为除了增加一个dev指针，改变HEAD的指向，工作区的文件没有任何变化。</p><p>假如我们在dev上的工作完成了，就可以把dev合并到master上。怎么合并的呢？git直接把master指向dev的当前提交，就完成了合并。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928102253.png" alt=""></p><p>合并分支也很快，就改改指针。工作区的内容不变。</p><p>合并完分支后，甚至可以删除dev分支，删除dev分支就是把dev指针给删掉。删掉后，我们只剩下一条master指针。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928102344.png" alt=""></p><h2 id="分支作用"><a href="#分支作用" class="headerlink" title="分支作用"></a>分支作用</h2><p>分支在实际中有很大用处。假设你准备开发一个新功能，但是需要两周才能完成，第一周只写了一半的代码。如果立刻提交，由于代码还没写完，不完整的代码库会导致别人无法干活，如果等代码全部写完，又存在丢失每天进度的巨大风险。有了分支，就不用怕这些事情。你创建自己的一个分支，别人看不到，也可以在原来的分支上工作。而你还在自己的分支上干活，想提交就提交。直到全部开发完，一次性合并到<strong>主分支</strong>。这样既安全，又不影响别人工作。</p><h2 id="分支基本操作"><a href="#分支基本操作" class="headerlink" title="分支基本操作"></a>分支基本操作</h2><h3 id="查看当前几个分支且能看到在哪个分支工作"><a href="#查看当前几个分支且能看到在哪个分支工作" class="headerlink" title="查看当前几个分支且能看到在哪个分支工作"></a>查看当前几个分支且能看到在哪个分支工作</h3><p>git branch</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928103803.png" alt=""></p><h3 id="创建分支"><a href="#创建分支" class="headerlink" title="创建分支"></a>创建分支</h3><p>git branch 分支名</p><h3 id="创建分支并切换到其上工作"><a href="#创建分支并切换到其上工作" class="headerlink" title="创建分支并切换到其上工作"></a>创建分支并切换到其上工作</h3><p>git checkout -b dev</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928104927.png" alt=""></p><h3 id="切换回master分支"><a href="#切换回master分支" class="headerlink" title="切换回master分支"></a>切换回master分支</h3><p>git checkoout master</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928110538.png" alt=""></p><h3 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h3><p>git merge 分支名字</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928114835.png" alt=""></p><p>注意上面的Fast-forward，也就是红色框。git告诉我们，这次合并是快速合并，也就是直接把master指向dev的当前提交，所以合并速度非常快</p><h4 id="解决冲突"><a href="#解决冲突" class="headerlink" title="解决冲突"></a>解决冲突</h4><p>合并冲突并不是一番风顺的。在两个分支上修改同一个文件并提交，就会起冲突。解决办法：手动解决冲突，再提交一次</p><p>看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928130118.png" alt=""></p><p>git告诉我们，git_test2.txt文件存在冲突，必须手动解决冲突再提交。</p><p>打开刚才修改的文件，发现文件修改了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928131316.png" alt=""></p><p>将文件中新增加的&lt;、= 和 &gt;手动删掉，再提交一次</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928131626.png" alt=""></p><h3 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h3><p>git branch -d 分支名字</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928115530.png" alt=""></p><p>这个操作也是非常快的，直接把dev这个指针删了就好了。</p><h2 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h2><p>通常，合并分支时，如果可能，git会用fast forward模式。但是有些快速合并并不能合并但不会起冲突，合并之后并做一次新的提交。在这种模式下，删除分支后，会丢掉部分信息。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928141314.png" alt=""></p><p>如上图，merge并不会起冲突(因为不是同一个文件)。但是，会出来一个框：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928142350.png" alt=""></p><p>为什么会出现这个框？前面我们说了，合并分支无法合并但不会起冲突且做一次提交。在这次提交中，需要输入描述信息。就在弹窗输入描述信息</p><p><strong>PS：我太难了，我接下来一直退不出这个框。所以这个例子就这样吧。可以的跟我说一下，拜托了。</strong></p><p>禁用快速合并：</p><p>git merge —no-ff  -m  “描述”  分支名</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928152949.png" alt=""></p><h2 id="Bug分支"><a href="#Bug分支" class="headerlink" title="Bug分支"></a>Bug分支</h2><p>软件开发中，bug就像家常便饭，有了bug就要修复。在git中，由于分支是如此强大，所以，每个bug都可以通过一个新的临时分支来修复。修复后，合并分支，然后将临时分支删除。这个例子不难，就不贴图了。</p><p>假如你正在写代码，突然老大让你修改一个bug。你需要先保存一下工作现场，修复完bug后，再恢复工作现场。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928154851.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分支原理&quot;&gt;&lt;a href=&quot;#分支原理&quot; class=&quot;headerlink&quot; title=&quot;分支原理&quot;&gt;&lt;/a&gt;分支原理&lt;/h2&gt;&lt;p&gt;git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。H
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>git介绍和基本操作</title>
    <link href="http://yoursite.com/2019/09/26/git%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2019/09/26/git介绍和基本操作/</id>
    <published>2019-09-26T08:02:15.000Z</published>
    <updated>2019-09-28T08:11:56.423Z</updated>
    
    <content type="html"><![CDATA[<p>最近经常用到git和GitHub，但是命令很多都忘记了。故写一篇博客回顾并总结一下</p><h2 id="git介绍"><a href="#git介绍" class="headerlink" title="git介绍"></a>git介绍</h2><p>git是目前世界上最先进的分布式版本系统。</p><p>git的两大特点</p><p>版本控制：可以解决多人同时开发的代码问题，也可以解决找回历史代码的问题</p><p>分布式：git是分布式版本控制系统，同一个git仓库，可以分布到不同的机器上，首先找一台电脑充当服务器的角色。每天24小时开机，其它每个人都从这个“服务器”仓库克隆一份自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交，可以自己搭建服务器，也可以使用github网站。</p><p>git的安装就不赘述了，直接下好安转包。一路next就完事了(不用改安装路径)。</p><h2 id="git基本操作"><a href="#git基本操作" class="headerlink" title="git基本操作"></a>git基本操作</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>通过git我们可以管理一个目录下的代码，首先我们通过<strong>git init</strong>创建一个版本库</p><p>随机切到一个目录，点击空白区域。点击Git Bash Here，如图中所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926172454.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926172852.png" alt=""></p><p>这样我们就有一个格式为 .git 文件</p><h3 id="版本创建"><a href="#版本创建" class="headerlink" title="版本创建"></a>版本创建</h3><p>首先，git add 文件名.文件格式：添加某个特定的文件</p><p>或</p><p> git add . ：将目录上所有的文件添加到仓库 </p><p>接着，git commit -m “对这个版本的说明信息”</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926175439.png" alt=""></p><h3 id="查看版本记录"><a href="#查看版本记录" class="headerlink" title="查看版本记录"></a>查看版本记录</h3><p>使用 git log</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926175826.png" alt=""></p><p>图中的commit 12c6a……代表版本序列号</p><p>随便放了一张图片进项目，第二次提交项目</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926181404.png" alt=""></p><p>如果版本记录过程，以简短形式显示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927174449.png" alt=""></p><h3 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h3><p>在git中，有个HEAD指针一直指向当前的版本。用HEAD^代表HEAD前一个版本，HEAD^^代表前两个版本。其它版本类推。那前一百个要写100个^吗？答案当然是不用的，用HEAD~100。HEAD~1 等价于  HEAD^。</p><p>git reset —hard HEAD</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926195030.png" alt=""></p><p>git reset —hard 版本号：这里的版本号就是commit后的版本序列号</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926195243.png" alt=""></p><p>版本号不需要写完，写完前几个号码就好了。</p><h3 id="查看操作记录"><a href="#查看操作记录" class="headerlink" title="查看操作记录"></a>查看操作记录</h3><p>git reflog</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926195734.png" alt=""></p><h3 id="工作区和暂存区"><a href="#工作区和暂存区" class="headerlink" title="工作区和暂存区"></a>工作区和暂存区</h3><p>工作区就相当于我们的目录</p><p>在工作区中，有一个隐藏目录 .git，这个不是工作区。而是git的版本库。其中存了很多东西，其中最重要的是称为stage(或者叫index)的暂存区，还有git为我们自动创建的第一个分支master，以及指向master的一个指针HEAD。</p><p>前面讲了我们把文件往git版本库里添加进去，是分两步执行的：</p><p>第一步是git add把文件添加进去，实际上就是把文件添加到暂存区</p><p>第二步是git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支</p><p>用git status查看当前的状态。如果有新的文件或有文件在修改之后、在add之前，会出现以下情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926205054.png" alt=""></p><p>把新的文件add之后，得出以下情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926205425.png" alt=""></p><p>经历add和commit后使用status：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926205807.png" alt=""></p><p>git管理文件的修改，<strong>它只会提交暂存区的修改来创建版本</strong>。也就是说，在修改或创建一个新文件后，不add到暂存区而直接commit，git也会说文件被修改。</p><h3 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h3><p>git checkout —文件名.文件格式</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926223148.png" alt=""></p><p>如果是add后，先利用reset取消暂存的变更重新回到工作区：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190926223758.png" alt=""></p><p>接着用git checkout —文件名.文件格式 丢弃工作区的改动</p><p>如果是commit了，就版本回退</p><h3 id="对比文件的不同"><a href="#对比文件的不同" class="headerlink" title="对比文件的不同"></a>对比文件的不同</h3><p><strong>对比工作区和某个版本中文件的不同</strong></p><p>git diff 某个版本库  — 比较的文件</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927164855.png" alt=""></p><p><strong>对比两个版本间的不同</strong></p><p>git diff 某个版本库  某个版本库 — 比较的文件</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927170025.png" alt=""></p><p>两个版本交换位置，得出的效果是不同的</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927170142.png" alt=""></p><h3 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927171813.png" alt=""></p><p>如果是add到暂存区，也是和之前一样，先reset，再checkout。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927174247.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近经常用到git和GitHub，但是命令很多都忘记了。故写一篇博客回顾并总结一下&lt;/p&gt;
&lt;h2 id=&quot;git介绍&quot;&gt;&lt;a href=&quot;#git介绍&quot; class=&quot;headerlink&quot; title=&quot;git介绍&quot;&gt;&lt;/a&gt;git介绍&lt;/h2&gt;&lt;p&gt;git是目前世界上
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Python实现浅层神经网络</title>
    <link href="http://yoursite.com/2019/09/24/Python%E5%AE%9E%E7%8E%B0%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/24/Python实现浅层神经网络/</id>
    <published>2019-09-24T03:06:10.000Z</published>
    <updated>2019-09-25T14:16:03.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开始前的准备"><a href="#开始前的准备" class="headerlink" title="开始前的准备"></a>开始前的准备</h2><p>本文用到的库：numpy、sklearn、matplotlib</p><p>另外，我们还要借助一下吴恩达老师的工具函数testCases.py 和 planar_utils.py。<a href="[https://github.com/Brickexperts/superficial-net/tree/master/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C](https://github.com/Brickexperts/superficial-net/tree/master/浅层神经网络">地址</a>)</p><p>​    testCases：提供测试样本来评估我们的模型。</p><p>​    planar_utils：提供各种有用的函数。</p><p>这两个文件的函数就不具体阐述了，有兴趣的自己研究去吧。</p><h2 id="导入必要的库"><a href="#导入必要的库" class="headerlink" title="导入必要的库"></a>导入必要的库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br></pre></td></tr></table></figure><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#<span class="keyword">X</span>是训练集，<span class="keyword">Y</span>是测试集</span><br><span class="line"><span class="keyword">X</span>,<span class="keyword">Y</span>=load_planar_dataset()</span><br></pre></td></tr></table></figure><p>数据集的说明：</p><ul><li>X是维度为(2, 400)的训练集，两个维度分别代表平面的两个坐标。</li><li>Y是为(1, 400)的测试集，每个元素的值是0（代表红色），或者1（代表蓝色）</li></ul><p>利用 matplotlib 进行数据的可视化操作，这是一个由红点和蓝色的点构成的类似花状的图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment"># np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">    <span class="comment"># 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23</span></span><br><span class="line">    <span class="comment"># 目的是确保 cost 是一个浮点数</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924125032.png" alt=""></p><h2 id="构建神经网络模型"><a href="#构建神经网络模型" class="headerlink" title="构建神经网络模型"></a>构建神经网络模型</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130208.png" alt=""></p><p>对于某个样本x^[i]：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130601.png" alt=""></p><p>成本函数J：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130811.png" alt=""></p><p>接下来我们要用代码实现神经网络的结构，步骤大致如下： 1. 定义神经网络结构 2. 随机初始化参数 3. 不断迭代: - 正向传播 - 计算成本函数值 - 利用反向传播得到梯度值 - 更新参数（梯度下降）</p><h3 id="定义神经网络的结构"><a href="#定义神经网络的结构" class="headerlink" title="定义神经网络的结构"></a>定义神经网络的结构</h3><p>主要任务是描述网络各个层的节点个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>]  <span class="comment"># 输入层单元数量</span></span><br><span class="line">    n_h = <span class="number">4</span>  <span class="comment"># 隐藏层单元数量，在这个模型中，我们设置成4即可</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>]  <span class="comment"># 输出层单元数量</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="随机初始化参数"><a href="#随机初始化参数" class="headerlink" title="随机初始化参数"></a>随机初始化参数</h3><p>参数W的随机初始化是很重要的，如果W和b都初始化成0，那么这一层的所有单元的输出值都一样，导致反向传播时，所有的单元的参数都做出完全相同的调整，这样多个单元的效果和只有一个单元的效果是相同的。那么多层神经网络就没有任何意义。为了避免这样的情况发生，我们会把参数W初始化成非零。</p><p>另外，随机初始化W之后，我们还会乘上一个较小的常数，比如 0.01 ，这样是为了保证输出值 Z 数值都不会太大。为什么这么做？设想我们用的激活函数是 sigmoid 或者 tanh ，那么，太大的 Z 会导致最终 A 会落在函数图像中比较平坦的区域内，这样的地方梯度都接近于0，会降低梯度下降的速度，因此我们会将权重初始化为较小的随机值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">  <span class="comment">#随机初始化</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="循环迭代"><a href="#循环迭代" class="headerlink" title="循环迭代"></a>循环迭代</h3><h4 id="实现正向传播"><a href="#实现正向传播" class="headerlink" title="实现正向传播"></a>实现正向传播</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="comment"># 从parameters中取出参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment"># 执行正向传播操作</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">#如果不相等，直接报错</span></span><br><span class="line">    <span class="keyword">assert</span> (A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h4 id="计算成本函数J"><a href="#计算成本函数J" class="headerlink" title="计算成本函数J"></a>计算成本函数J</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924130811.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    <span class="comment"># 下一行的结果是 (1, m)的矩阵</span></span><br><span class="line">    logprobs = np.multiply(Y, np.log(A2)) + np.multiply(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - A2))</span><br><span class="line">    <span class="comment"># 将 (1, m)的矩阵求和后取平均值</span></span><br><span class="line">    cost = - <span class="number">1</span> / m * np.sum(logprobs)</span><br><span class="line">    <span class="comment"># np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">    <span class="comment"># 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23</span></span><br><span class="line">    <span class="comment"># 目的是确保 cost 是一个浮点数</span></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>利用正向传播函数返回的cache，我们可以实现反向传播了。</p><h4 id="实现反向传播"><a href="#实现反向传播" class="headerlink" title="实现反向传播"></a>实现反向传播</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924133512.png" alt=""></p><blockquote><p>说明：</p><p>我们使用的 g^[1] () 是 tanh ，并且 A1 = g^[1] ( Z^[1] ) ,那么求导变形后可以得到 g‘ ^[1] ( Z^[1] ) = 1-( A^[1] )^2 用python表示为： 1 - np.power(A1, 2)</p><p>我们使用的 g^[2] () 是 sigmoid ，并且 A2 = g^[2] ( Z^[2] ) ,那么求导变形后可以得到 g‘ ^[2] ( Z^[2] ) =A^2 - Y</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># 获取样本的数量</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 从 parameters 和 cache 中取得参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment"># 计算梯度 dW1, db1, dW2, db2.</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span> / m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), <span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span> / m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><blockquote><p>说明</p><p>1、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。</p><p>2、 keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。</p></blockquote><h4 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h4><p>利用上面反向传播的代码段获得的梯度去更新 (W1, b1, W2, b2)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924134345.png" alt=""></p><p>式子中 α 是学习率，较小的学习率的值会降低梯度下降的速度，但是可以保证成本函数 J 可以在最小值附近收敛，而较大的学习率让梯度下降速度较快，但是可能会因为下降的步子太大而越过最低点，最终无法在最低点附近出收敛。</p><p>本篇博客选择1.2作为学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    W1 -= learning_rate * dW1</span><br><span class="line">    b1 -= learning_rate * db1</span><br><span class="line">    W2 -= learning_rate * dW2</span><br><span class="line">    b2 -= learning_rate * db2</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="构建完整的神经网络模型"><a href="#构建完整的神经网络模型" class="headerlink" title="构建完整的神经网络模型"></a>构建完整的神经网络模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="comment"># 输入层单元数</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>] </span><br><span class="line">    <span class="comment"># 输出层单元数</span></span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y) </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 10000次梯度下降的迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 正向传播，得到Z1、A1、Z2、A2</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本函数的值</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 反向传播，得到各个参数的梯度值</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate = <span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每迭代1000下就输出一次cost值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回最终训练好的参数</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h4 id="对样本进行预测"><a href="#对样本进行预测" class="headerlink" title="对样本进行预测"></a>对样本进行预测</h4><p>我们约定 预测值大于0.5的之后取1，否则取0：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190924135028.png" alt=""></p><p>对一个矩阵M而言，如果你希望它的每个元素 如果大于某个阈值 threshold 就用1表示，小于这个阈值就用 0 表示，那么，在python中可以这么实现：M_new = (M &gt; threshold)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h2 id="利用模型预测"><a href="#利用模型预测" class="headerlink" title="利用模型预测"></a>利用模型预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用含有4个神经元的单隐藏层的神经网络构建分类模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h=<span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 可视化分类结果</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 输出准确率</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line">print(<span class="string">'准确率: %d'</span> % float((np.dot(Y, predictions.T) + np.dot(<span class="number">1</span> - Y, <span class="number">1</span> - predictions.T)) / float(Y.size) * <span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><h3 id="尝试换一下隐藏层的单元个数"><a href="#尝试换一下隐藏层的单元个数" class="headerlink" title="尝试换一下隐藏层的单元个数"></a>尝试换一下隐藏层的单元个数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><h3 id="换数据集"><a href="#换数据集" class="headerlink" title="换数据集"></a>换数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"><span class="comment"># 在这里选择你要选用的数据集</span></span><br><span class="line">dataset = <span class="string">"noisy_circles"</span></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 测试代码如下：</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 画出边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;开始前的准备&quot;&gt;&lt;a href=&quot;#开始前的准备&quot; class=&quot;headerlink&quot; title=&quot;开始前的准备&quot;&gt;&lt;/a&gt;开始前的准备&lt;/h2&gt;&lt;p&gt;本文用到的库：numpy、sklearn、matplotlib&lt;/p&gt;
&lt;p&gt;另外，我们还要借助一下吴恩达老
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅层神经网络的反向传播</title>
    <link href="http://yoursite.com/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/09/23/浅层神经网络的反向传播/</id>
    <published>2019-09-23T08:11:03.000Z</published>
    <updated>2019-09-30T14:29:04.720Z</updated>
    
    <content type="html"><![CDATA[<h2 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h2><p>首先看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923161512.png" alt=""></p><p>我们假设n[l] 表示第 l 层的神经元的数量，那么这个单隐层的神经网络的输入层、隐含层和输出层的维度分别是：n[0]= n_x =3、n[1]=4、n[2]=1。</p><p>那么根据<a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的正向传播/#more">浅层神经网络的正向传播</a>)的分析，如果一次性输入带有 m 个样本的矩阵 X 我们可以得到：</p><blockquote><p>W^[1] = (4, 3), Z^[1] = (4, m), A^[1] = (4, m)</p><p>W^[2] = (1, 4), Z^[2] = (1, m), A^[2] = (1, m)</p></blockquote><p>可以总结出如下规律，对第 l 层的单元而言：</p><blockquote><p>W^[l] = (n^[l], n^[l-1]),</p><p>Z^[l] = (n^[l], m),</p><p>A^[l] = (n^[l], m).</p></blockquote><p>另外，还有一个规律：</p><blockquote><p>对任意层而言：</p><p>dW 的维度和 W 的维度相同</p><p>dZ 的维度和 Z 的维度相同</p><p>dA 的维度和 A 的维度相同</p></blockquote><p>注意：以上出现的符号都是将m个样本向量化后的表达。</p><h2 id="单个样本的梯度计算"><a href="#单个样本的梯度计算" class="headerlink" title="单个样本的梯度计算"></a>单个样本的梯度计算</h2><p>讲反向传播前，我们先回顾一下单个样本的正向传播的过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923162211.png" alt=""></p><p>其中计算z^[1]时用到的 x 可以看做是 a^[0]，σ( ) 是激活函数中一种，一般情况下，我们更习惯用用 g( ) 来表示激活函数。于是将上述式子一般化可以得到第 l 层的公式为：</p><blockquote><p>z^[l] = W^[l]a^[l-1] + b^[l] ————- ①</p><p>a^[l] = g^[l] ( z^[l] ) —————————- ②</p></blockquote><p>由于我们输入的样本只有一个，所以各个向量的维度如下：</p><blockquote><p>W^[l] = (n^[l], n^[l-1]),</p><p>z^[l] = (n^[l], 1),</p><p>a^[1] = (n^[l], 1).</p></blockquote><p>我们现在根据式子 ① 和 ② 来讨论反向传播的过程。</p><p>因为正向传播的时候我们依次计算了z^[1], a^[1], z^[2], a^[2]最终得到了损失函数L。所以反向传播的时候，我们要从L向前计算梯度。</p><p>第一步计算dz^[2]和da^[2]进而算出 dW^[2] 和 db^[2]：</p><blockquote><p>da^[2]=dL/da^[2]=  -y/a^[2]+(1-y)/(1-a^[2])</p><p>dz^[2]= dL/dz^[2] = dL/da^[2]*da^[2]/dz^[2]=-y(1-a^[2])+(1-y)a^[2] = a^[2] - y</p><p>说明1：</p><p>dz^[2] 的维度和z^[2]相同，da^[2] 的维度和a^[2]相同，为(1, 1)</p><p>g’( z^[2] ) 的维度与 z^[2]维度相同，为(1, 1)</p><p>在第二层中，a^[2] 与 z^[2]的维度也相同，为(1, 1)</p><p>实际上，dz^[2] 应该等于da^[2] 与 g’( z^[2] ) 的内积的结果，理由我们我们先向下看</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203151.png" alt=""></p><blockquote><p>说明2 ：</p><p>上图中我们可以得到 dW^[2] = dz^[2]乘上 a^[1] 的转置，这里是因为 :</p><p>dW^[2] 和 W^[2] 的形状是一样的(n^[2], n^[1])也就是(1, 4)，</p><p>dz^[2] 和 z^[2] 的形状是一样的(n^[2], 1)也就是(1, 1)，</p><p>a^[1] 的形状是 (n^[1], 1) 也就是(4, 1)，可以得到 a^[1] 的转置 a^( [1]T ) 的形状是 (1, 4)，</p><p>这样 dz^[2] 和 a^[1] 的转置 的乘积的形状才能是 dW^[2] 的形状 (1, 4)；</p><p>因此 dW^[2] = dz^[2]乘上 a^[1] 的转置。</p><p>这里给我们的启发是：<strong>向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行转置操作</strong>。</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203936.png" alt=""></p><p>第二步计算 da^[1] 和 dz^[1]进而算出 dW^[1] 和 db^[1]</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923203959.png" alt=""></p><blockquote><p>检查矩阵形状是否匹配：（4，1）* （1，1），匹配</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204207.png" alt=""></p><blockquote><p>说明3：</p><p>dz^[1] 的维度和z^[1]相同，da^[1] 的维度和a^[1]相同，均为(4, 1)</p><p>W^[2]T 的维度是 (4, 1)，dz^[2] 的维度是 (1, 1) ，二者的乘积与da^[1] 维度相同。</p><p>g’( z^[1] ) 的维度与 z^[1]维度相同，也与da^[1] 维度相同，为(4, 1)。</p><p>所以想得到维度为 (4, 1) 的dz^[1] ，da^[1] 与 g’( z^[1] ) 直接的关系为<strong>内积</strong></p><p>这里给我们的启发是：<strong>向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行内积操作</strong>。这里就解释了说明（1）留下的问题。</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204516.png" alt=""></p><blockquote><p>检查维度：（4，3）=（4，1）*（1，3），正确</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923204639.png" alt=""></p><p>公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward4.png" alt=""></p><h2 id="多个样本的梯度运算"><a href="#多个样本的梯度运算" class="headerlink" title="多个样本的梯度运算"></a>多个样本的梯度运算</h2><p>用m个样本作为输入，并进行<strong>向量化</strong>后正向传播的公式：</p><blockquote><p>Z^[1] = W^[1]X + b^[1]</p><p>A^[1] = g^[1] ( Z^[1] )</p><p>Z^[2] = W^[2]A^[1] + b^[2]</p><p>A^[2] = g^[2] ( Z^[2] )</p></blockquote><p>由于引入了m个样本，我们也要有个成本函数来衡量整体的表现情况，我们的目的是让J达到最小值</p><p>下图中左边列举了我们上面所推导的各种式子，其中图中所用的激活函数 g^[2] () 为sigmoid函数，因而dz^[2] = a^[2] - y。<strong>这些左边的式子都是针对单个样本而言的，而右边则是将m个样本作为输入并向量化后的公式的表达形式</strong>。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923210356.png" alt=""></p><blockquote><p>上图的说明：</p><p>1、由于成本函数 J 是所有的 损失函数 L 的值求和后的<strong>平均值</strong>， <strong>那么 J 对 W^[1], b^[1], W^[2], b^[2] 的导数 等价于 各个 L 对 W^[1], b^[1], W^[2], b^[2] 的导数求和后的平均值</strong>。所以dW^[1], db^[1], dW^[2], db^[2]的式子中需要乘上 1 / m。</p><p>2、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。</p><p>3、keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。</p></blockquote><h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><p>最后，不要忘了要更新参数（式子中的&alpha;是学习率）</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923210037.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;符号约定&quot;&gt;&lt;a href=&quot;#符号约定&quot; class=&quot;headerlink&quot; title=&quot;符号约定&quot;&gt;&lt;/a&gt;符号约定&lt;/h2&gt;&lt;p&gt;首先看下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bric
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浅层神经网络的正向传播</title>
    <link href="http://yoursite.com/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://yoursite.com/2019/09/23/浅层神经网络的正向传播/</id>
    <published>2019-09-23T07:02:03.000Z</published>
    <updated>2019-09-29T11:32:04.164Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络概览"><a href="#神经网络概览" class="headerlink" title="神经网络概览"></a>神经网络概览</h2><p>在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150442.png" alt=""></p><p>如果把上面的图片抽象化，可以得到以下模型：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150528.png" alt=""></p><p>再进一步简化得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923150605.png" alt=""></p><p>上图里，小圆圈是sigmoid单元，它要完成的工作就是先计算 z 然后计算 a 最终作为 y帽 输出。</p><p>进而，我们看看下图，这是较为简单的神经网络的样子，它无非就是将多个 sigmoid 单元组合在一起，形成了更复杂的结构。图中每个单元都需要接收数据的输入，并完成数据的输出，其每个单元的计算过程与 logistic回归 的正向传播类似。可以看到，图片里给神经网络分了层次，最左边的是<strong>输入层</strong>，也就是第0层；中间的是<strong>隐藏层</strong>，也就是第一层；最右边的是<strong>输出层</strong>，是第二层。通常，我们不将输入层看做神经网络的一层，因而下图是一个2层的神经网络。 另外要清楚的是，本图中隐藏层只有一个，但实际上，隐藏层可以有多个。由于对用户而言，隐藏层计算得到的数据用户不可预见，也没有太大必要知道，所以称之为隐藏层。也正是因为如此，神经网络的解释器很差。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151156.png" alt=""></p><p>再进一步认识下这张图片里的标记，隐藏层的每个单元都需要接收输入层的数据，并且各个单元都需要计算 z , 并经过 sigmoid 函数得到各自的 a ，为了便于区分不同层的不同单元的 a，我们做如下约定：</p><p>a 的右上角有个角标[i]，表示这是第i层的单元；a 的右下角有个角标 j 用于区分这是该层自上向下的第 j 个单元。例如我们用 a^[1]_3 表示这是第一层的第三个单元。输入层的 x1, x2, … xn 可以看做是 a^[0]_1, a^[0]_2 … a^[0]_n.</p><p>前一层a[ i ] 的输出，便是后一层 a[ i+1 ] 的<strong>所有单元</strong>输入。除了输入层外的其它层，也就是隐藏层和输出层的单元都有各自的参数 w 和 b 用于计算 z ，同样是用w^[i]_j, b^[i]_j, z^[i]_j 来区分他们；得到 z^[i]_j 后用sigmoid函数计算 a^[i]_j ，再将本层n个单元计算得到的n个 a 作为下一层的输入，最终在输出层得到预测值 y帽。其计算过程大致如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151444.png" alt=""></p><h2 id="计算一个样本的神经网络输出"><a href="#计算一个样本的神经网络输出" class="headerlink" title="计算一个样本的神经网络输出"></a>计算一个样本的神经网络输出</h2><p>下图是输入单个样本(该样本含有特征x1, x2, x3)的神经网络图，隐藏层有4个单元：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151606.png" alt=""></p><p>根据上面的说明，要计算该神经网络的输出，我们首先要计算隐藏层4个单元的输出</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151804.png" alt=""></p><p>第一步就是计算第一层各个单元的 z^[1]_j ，第二步是计算出各个单元的 a^[i]_j，不难想到可以用<strong>向量化计算</strong>来化简上述操作，我们将所有 w^[1]_j 的转置纵向叠加得到下图的内容，我们将这个大的矩阵记为W^[1]，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923151938.png" alt=""></p><p>由于输入的 x 是三维的列向量，所以每个分量x1, x2, x3 都需要一个 w1, w2, w3 对应，因此 w^[1]_j 的转置是一个(1, 3) 的矩阵，又因为隐藏层有4个单元，即 j 的取值为1, 2, 3, 4，故 <strong>W^[1] 是 (4, 3) 的矩阵</strong>。</p><p>同理，第二层，也就是输出层的 W^[2] 由于有4个输入的特征，1个单元，所以 <strong>W^[2] 是 (1, 4)的矩阵</strong>。</p><p>对于只有一个样本的情况，我们不难得到如下式子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152132.png" alt=""></p><p>z^[1] 是个 (4, 1) 的矩阵。可以再进一步通过 sigmoid 函数得到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152206.png" alt=""></p><p>至此，第一层神经网络得任务已完成</p><p>第二层也就是输出层的工作，无非就是把第一层的 a^[1] 作为输入，继续用 W^[2] 与 a^[1] 相乘 再加上 b^[2] 得到 z^[2]，再通过 sigmoid 函数得到 a^[2] 也就是最终的预测结果 y帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152532.png" alt=""></p><p>至此，我们就完成了一个样本的输出。接下来看看如何用矩阵实现多样本的正向输出。</p><h2 id="计算多样本的输出"><a href="#计算多样本的输出" class="headerlink" title="计算多样本的输出"></a>计算多样本的输出</h2><p>假设我们有m个样本，每个样本有3个特征，并且隐藏层也是4个单元。</p><p>那么，通常我们需要使用一个 for 循环将 从 1 遍历至 m 完成以下操作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923152921.png" alt=""></p><p>角标 ^(i) 表示第 i 个样本。我们可以构造这样一个矩阵 x：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153223.png" alt=""></p><p>它将所有样本的特征，按列叠加在一起，构成 (3, m) 的矩阵。</p><p>如果我们替换上面计算 z^[1] 的过程中使用的 单个样本 x^(1) 为(3, m) 的矩阵 x (也就是下图的绿色框)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153359.png" alt=""></p><p>就可以得到下面的式子（为了方便表达，下面的公式中假设 b 等于0）：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190923153641.png" alt=""></p><p>到这里，我们求出了 Z^[1] ，并且由于 Z^[1] 是(4, 3)的矩阵W^[1]乘以(3, m)的矩阵x 再加上b，所以它是 (4, m) 的矩阵。再经过 sigmoid 函数即可得到同样是 (4, m) 的矩阵 A^[1]，到此隐藏层的工作完成了。</p><h2 id="输出数据"><a href="#输出数据" class="headerlink" title="输出数据"></a>输出数据</h2><p>矩阵 A^[1]，作为下一层（也就是输出层）的输入参数，经过类似的计算也可以得到 Z^[2] = W^[2] × A^[1] + b^[2]，上面我们分析到 W^[2] 是 (1, 4)的矩阵，所以得到的Z^[2]是 (1, m) 的矩阵，同样经过sigmoid函数处理得到的 A^[2] 也是 (1, m) 的矩阵，A^[2]的每个元素，代表一个样本输出的预测值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络概览&quot;&gt;&lt;a href=&quot;#神经网络概览&quot; class=&quot;headerlink&quot; title=&quot;神经网络概览&quot;&gt;&lt;/a&gt;神经网络概览&lt;/h2&gt;&lt;p&gt;在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关系数据库</title>
    <link href="http://yoursite.com/2019/09/19/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://yoursite.com/2019/09/19/关系数据库/</id>
    <published>2019-09-19T12:54:22.000Z</published>
    <updated>2019-09-20T09:22:57.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="域"><a href="#域" class="headerlink" title="域"></a>域</h2><p>域是一组具有相同数据类型的值的集合。基数是域中数据的个数</p><h2 id="笛卡儿积"><a href="#笛卡儿积" class="headerlink" title="笛卡儿积"></a>笛卡儿积</h2><p>笛卡尔积直观意义是诸集合各元素间一切可能的组合，可表示为一个二维表。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/数据库5.png" alt=""></p><h2 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h2><p>关系是笛卡尔积的有限集合</p><p>关系可以有三种类型：</p><p>​    基本表：实际存储数据的逻辑表示</p><p>​    查询表：查询结果对应的标</p><p>​    视图表：是虚表，由基本表或其他试图导出，不对应实际存储的数据</p><p>关系的基本性质：</p><p>​    列是同质的，每一列中的分量是同一类型的数据，来自同一个域。</p><p>​    不同的列可出自同一个域，其中的每一列称为一个属性，不同的属性要给予不同的属性名。</p><p>​    列的顺序无所谓，列的次序可以任意交换。</p><p>​    行的顺序无所谓，行的次序可以任意交换</p><p>​    任意两个元祖不能完全相同</p><p>​    分量必须取原子值，每一个分量都必须是不可分的数据项。这是规范条件中最基本的一条</p><h2 id="几个术语："><a href="#几个术语：" class="headerlink" title="几个术语："></a>几个术语：</h2><p>若关系中的某一属性组的值能唯一识别一个元祖，则称该属性为候选码。</p><p>若一个关系有多个候选码，则选定其中一个作为主码。</p><p>候选码的诸属性称为非码属性</p><p>不包含在任何候选码中的属性称为非码属性</p><p>若关系模式的所有属性组是这个关系模式的候选码，则称为全码</p><h2 id="关系模式"><a href="#关系模式" class="headerlink" title="关系模式"></a>关系模式</h2><p>关系模式是型，关系是值。关系模式是对关系的描述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;域&quot;&gt;&lt;a href=&quot;#域&quot; class=&quot;headerlink&quot; title=&quot;域&quot;&gt;&lt;/a&gt;域&lt;/h2&gt;&lt;p&gt;域是一组具有相同数据类型的值的集合。基数是域中数据的个数&lt;/p&gt;
&lt;h2 id=&quot;笛卡儿积&quot;&gt;&lt;a href=&quot;#笛卡儿积&quot; class=&quot;head
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib画图总结</title>
    <link href="http://yoursite.com/2019/09/18/matplotlib%E7%94%BB%E5%9B%BE%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/09/18/matplotlib画图总结/</id>
    <published>2019-09-18T07:00:59.000Z</published>
    <updated>2019-09-19T11:13:24.152Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧</p><p>没有这个库的，pip安装一下。</p><p>首先将matplotlib中的pyplot导入，如下：</p><p>import matplotlib.pyplot as plt，所以以下的plt都是pyplot。</p><h4 id="plt-scatter"><a href="#plt-scatter" class="headerlink" title="plt.scatter"></a>plt.scatter</h4><p>画散点图</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = list(range(1,1001))</span><br><span class="line">y = [x*<span class="number">*2</span> <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line"><span class="comment">#s代表点的面积</span></span><br><span class="line"><span class="comment">#c代表颜色</span></span><br><span class="line">plt.scatter(x,y,<span class="attribute">c</span>=<span class="string">"green"</span>,s=20)</span><br><span class="line"><span class="comment">#设置标题并加上轴标签</span></span><br><span class="line">plt.title(<span class="string">"Squares Numbers"</span>,<span class="attribute">fontsize</span>=24)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line">plt.xlabel(<span class="string">"Square of Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line"><span class="comment">#设置刻度标记的大小</span></span><br><span class="line">plt.tick_params(<span class="attribute">axis</span>=<span class="string">'both'</span>,which='major',labelsize=14)</span><br><span class="line"><span class="comment">#设置每个坐标的取值范围</span></span><br><span class="line">plt.axis([0,1100,0,1100000])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>上面的代码运行后，是一条线。那是因为点太多了。绘制很多点的时候，轮廓会连在一起。要删除数据点的轮廓可用scatter，传递参数edgecolor=”none”</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/matplotlib1.png" alt=""></p><p>模块pyplot内置了一组颜色映射，要使用这些颜色映射，你需要告诉pyplot该如何设置数据集中每个点的颜色。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = list(range(1,1001))</span><br><span class="line">y = [x*<span class="number">*2</span> <span class="keyword">for</span> x <span class="keyword">in</span> x]</span><br><span class="line"><span class="comment">#s代表点的面积</span></span><br><span class="line"><span class="comment">#cmap设置颜色映射</span></span><br><span class="line">plt.scatter(x,y,<span class="attribute">s</span>=20,c=y,cmap=plt.cm.Greens)</span><br><span class="line"><span class="comment">#设置标题并加上轴标签</span></span><br><span class="line">plt.title(<span class="string">"Squares Numbers"</span>,<span class="attribute">fontsize</span>=24)</span><br><span class="line">plt.xlabel(<span class="string">"Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line">plt.xlabel(<span class="string">"Square of Value"</span>,<span class="attribute">fontsize</span>=14)</span><br><span class="line"><span class="comment">#设置刻度标记的大小</span></span><br><span class="line">plt.tick_params(<span class="attribute">axis</span>=<span class="string">'both'</span>,which='major',labelsize=14)</span><br><span class="line"><span class="comment">#设置每个坐标的取值范围</span></span><br><span class="line">plt.axis([0,1100,0,1100000])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>保存散点图：savefig</p><p>保存图片之前，一定要把plt.show()注释掉，否则会保存一张空白的图片</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#plt</span><span class="selector-class">.show</span>()</span><br><span class="line"><span class="selector-tag">plt</span><span class="selector-class">.savefig</span>(<span class="string">"save.jpg"</span>,bbox_inches=<span class="string">"tight"</span>)</span><br></pre></td></tr></table></figure><h4 id="plt-bar"><a href="#plt-bar" class="headerlink" title="plt.bar"></a>plt.bar</h4><p>画柱状图，默认是竖直条形图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/matplotlib2.png" alt=""></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="keyword">import</span> numpy as np</span><br><span class="line">y = [<span class="number">20</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">25</span>, <span class="number">15</span>]</span><br><span class="line">x = np.arange(<span class="number">5</span>)</span><br><span class="line">p1 = plt.bar(x, height=y, width=<span class="number">0.5</span>, )</span><br><span class="line"># 展示图形</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>水平条形图：需要把orientation改为horizontal，然后x与y的数据交换</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">x = [20, 10, 30, 25, 15]</span><br><span class="line">y = np.arange(5)</span><br><span class="line"><span class="comment"># x= 起始位置，bottom= 水平条的底部(左侧), y轴， height：水平条的宽度</span></span><br><span class="line"><span class="comment">#width：水平条的长度</span></span><br><span class="line">p1 = plt.bar(<span class="attribute">x</span>=0, <span class="attribute">bottom</span>=y, <span class="attribute">height</span>=0.5, <span class="attribute">width</span>=x, <span class="attribute">orientation</span>=<span class="string">"horizontal"</span>)</span><br><span class="line"><span class="comment"># 展示图形</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="plt-plot-x-y-color-’red’-linewidth-2-5-linestyle-’-‘"><a href="#plt-plot-x-y-color-’red’-linewidth-2-5-linestyle-’-‘" class="headerlink" title="plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘)"></a>plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘)</h4><p> 画线，也可以用来画折线图。x是横坐标的值。y是纵坐标的值。color参数设置曲线颜色，linewidth设置曲线宽度，linestyle设置曲线风格。</p><p>linestyle的可选参数：</p><p>‘-‘       solid line style<br> ‘—‘      dashed line style<br> ‘-.’      dash-dot line style<br> ‘:’       dotted line style</p><h4 id="plt-figure"><a href="#plt-figure" class="headerlink" title="plt.figure()"></a>plt.figure()</h4><p>自定义画布大小，画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小。</p><p>figure语法说明</p><p>figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True)</p><p>num:图像编号或名称，数字为编号 ，字符串为名称</p><p>figsize:指定figure的宽和高，单位为英寸；</p><p>dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80</p><p>facecolor:背景颜色</p><p>edgecolor:边框颜色</p><p>frameon:是否显示边框</p><h4 id="plt-xticks"><a href="#plt-xticks" class="headerlink" title="plt.xticks()"></a>plt.xticks()</h4><p>设置x轴刻度的表现方式</p><h4 id="plt-yticks"><a href="#plt-yticks" class="headerlink" title="plt.yticks()"></a>plt.yticks()</h4><p>设置y轴刻度的表现方式</p><h4 id="plt-xlim"><a href="#plt-xlim" class="headerlink" title="plt.xlim()"></a>plt.xlim()</h4><p>设置x轴刻度的取值范围</p><h4 id="plt-ylim"><a href="#plt-ylim" class="headerlink" title="plt.ylim()"></a>plt.ylim()</h4><p>设置y轴刻度的取值范围</p><h4 id="plt-text-1-2-“I’m-a-text”"><a href="#plt-text-1-2-“I’m-a-text”" class="headerlink" title="plt.text(1, 2, “I’m a text”)"></a>plt.text(1, 2, “I’m a text”)</h4><p>前两个参数表示文本坐标, 第三个参数为要添加的文本</p><h4 id="plt-legend"><a href="#plt-legend" class="headerlink" title="plt.legend()"></a>plt.legend()</h4><p>函数实现了图例功能, 他有两个参数, 第一个为样式对象, 第二个为描述字符,都可以为空</p><h4 id="plt-xlabel"><a href="#plt-xlabel" class="headerlink" title="plt.xlabel()"></a>plt.xlabel()</h4><p>添加x轴名字</p><h4 id="plt-ylabel"><a href="#plt-ylabel" class="headerlink" title="plt.ylabel()"></a>plt.ylabel()</h4><p>添加y轴名字</p><h4 id="plt-tight-layout"><a href="#plt-tight-layout" class="headerlink" title="plt.tight_layout()"></a>plt.tight_layout()</h4><p>tight_layout自动调整subplot(s)参数，以便subplot(s)适应图形区域</p><h4 id="plt-subplot"><a href="#plt-subplot" class="headerlink" title="plt.subplot()"></a>plt.subplot()</h4><p>设置画布划分以及图像在画布上输出的位置，将figure设置的画布大小分成几个部分，参数‘221’表示2(row)x2(colu),即将画布分成2x2，两行两列的4块区域，1表示选择图形输出的区域在第一块，图形输出区域参数必须在“行x列”范围</p><h4 id="plt-subplots"><a href="#plt-subplots" class="headerlink" title="plt.subplots()"></a>plt.subplots()</h4><p>subplots(nrows,ncols,sharex,sharey,squeeze,subplot_kw,gridspec_kw,**fig_kw) :创建画布和子图</p><p>1、nrows,ncols表示将画布分割成几行几列。shares，sharey表示坐标轴的属性是否相同，可选的参数：True、False、row、col。默认值为False。</p><p>2、 squeeze  bool</p><p>a.默认参数为True：额外的维度从返回的Axes(轴)对象中挤出，对于N<em>1或1</em>N个子图，返回一个1维数组，对于N*M，N&gt;1和M&gt;1返回一个2维数组。</p><p>b.为False，不进行挤压操作：返回一个元素为Axes实例的2维数组，即使它最终是1x1。</p><p>3、subplot_kw:字典类型，可选参数。把字典的关键字传递给add_subplot()来创建每个子图。</p><p>4、gridspec_kw:字典类型，可选参数。把字典的关键字传递给GridSpec构造函数创建子图放在网格里(grid)。</p><p>5、**fig_kw：把所有详细的关键字参数传给figure()函数。</p><h4 id="plt-grid"><a href="#plt-grid" class="headerlink" title="plt.grid()"></a>plt.grid()</h4><p>是否开启方格，True为开，False为不显示。默认为False</p><h4 id="plt-gca"><a href="#plt-gca" class="headerlink" title="plt.gca()"></a>plt.gca()</h4><p>获取当前的子图</p><h4 id="plt-xcale"><a href="#plt-xcale" class="headerlink" title="plt.xcale()"></a>plt.xcale()</h4><p>改变x轴的比例。pyplot不仅支持线性轴刻度，还支持对数和logit刻度。如果数据跨越许多数量级，则通常使用此方法</p><p>plt.xcale(“logit”)，还有log、symmlog等选择。还可以添加自己的比例。</p><h4 id="plt-yscale"><a href="#plt-yscale" class="headerlink" title="plt.yscale()"></a>plt.yscale()</h4><p>改变y轴的比例。用法同plt.xcale一样</p><p>随便写了个例子，其他具体的用法还是要自己去练习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter([<span class="number">1</span>,<span class="number">3</span>,<span class="number">9</span>],[<span class="number">5</span>,<span class="number">8</span>,<span class="number">2</span>], label=<span class="string">"Example one"</span>,color=<span class="string">"red"</span>,s=<span class="number">25</span>,marker=<span class="string">"o"</span>)</span><br><span class="line">plt.plot([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>],[<span class="number">8</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">6</span>], label=<span class="string">"Example two"</span>, color=<span class="string">'g'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xticks([<span class="number">1</span>,<span class="number">10</span>])</span><br><span class="line">plt.yticks([<span class="number">1</span>,<span class="number">15</span>])</span><br><span class="line">plt.xlabel(<span class="string">'bar number'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'bar height'</span>)</span><br><span class="line">plt.title(<span class="string">'test'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧&lt;/p&gt;
&lt;p&gt;没有这个库的，pip安装一下。&lt;/p&gt;
&lt;p&gt;首先将matplotlib中的pyplot导入，如下：&lt;/p&gt;
&lt;p&gt;import matplotlib.pyplot a
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="matplotlib" scheme="http://yoursite.com/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(4)</title>
    <link href="http://yoursite.com/2019/09/17/SVM%E8%A7%A3%E8%AF%BB(4)/"/>
    <id>http://yoursite.com/2019/09/17/SVM解读(4)/</id>
    <published>2019-09-17T07:13:10.000Z</published>
    <updated>2019-09-19T10:21:17.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用SVC时的其他考虑"><a href="#使用SVC时的其他考虑" class="headerlink" title="使用SVC时的其他考虑"></a>使用SVC时的其他考虑</h2><h3 id="SVC处理多分类问题"><a href="#SVC处理多分类问题" class="headerlink" title="SVC处理多分类问题"></a>SVC处理多分类问题</h3><p>之前的所有的SVM的(1)-(3)内容，全部是基于二分类的情况来说明的。因为支持向量机是天生二分类的模型。但是，它也可以做多分类。但是SVC在多分类情况的推广是很难的。因为要研究透彻多分类状况下的SVC，就必须研究透彻多分类时所需要的决策边界个数，每个决策边界所需要的支持向量个数，以及这些支持向量如何组合起来计算拉格朗日乘数。本小节只说一小部分。支持向量机是天在生二分类的模型，所以支持向量机在处理多分类问题的时候，是把多分类问题转换成了二分类问题来解决。这种转换有两种模式，一种叫做“一对一”模式（one vs one），一种叫做“一对多”模式(one vs rest)。 </p><p>在ovo模式下(一对一模式)上，标签中的所有类别都会被两两组合，每两个类别建立一个SVC模型，每个模型生成一个决策边界，分别进行二分类，这种模式下，对于n_class个标签类别的数据来说，SVC会生成总共$C^2_{n_class}$个模型，即会生成总共$C^2_{n_class}$个超平面，其中：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM74.png" alt=""></p><p>ovo模式下，二维空间的三分类状况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM75.png" alt=""></p><p>首先让提出紫色点和红色点作为一组，然后求解出两个类之间的SVC和绿色决策边界。然后让绿色点和红色点作为一组，求解出两个类之间的SVC和灰色边界。最后让绿色和紫色组成一组，组成两个类之间的SVC和黄色边界。然后基于三个边界，分别对三个类别进行分类。</p><p>ovr模式下，标签中所有的类别会分别于其他类别进行组合，建立n_class个模型，每个模型生成一个决策边界。分别进行二分类。ovr模式下，则会生成以下的决策边界：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM76.png" alt=""></p><p>紫色类 vs 剩下的类，生成绿色的决策边界。红色类 vs 剩下的类，生成黄色的决策边界。绿色类 vs 剩下的类，生成灰色的决策边界，当类别更多的时候，如此类推下去，我们永远需要n_class个模型。</p><p>当类别更多的时候，无论是ovr还是ovo模式需要的决策边界都会越来越多，模型也会越来越复杂，不过ovo模式下的模型计算会更加复杂，因为ovo模式中的决策边界数量增加更快，但相对的，ovo模型也会更加精确。ovr模型计算更快，但是效果往往不是很好。在硬件可以支持的情况下，还是建议选择ovo模式。 </p><p>模型和超平面的数量变化了，SVC的很多计算、接口、属性都会发生变化，而参数decision_function_shape决定我们究竟使用哪一种分类模式。</p><h4 id="decision-function-shape"><a href="#decision-function-shape" class="headerlink" title="decision_function_shape"></a>decision_function_shape</h4><p>可输入“ovo”，”ovr”，默认”ovr”，对所有分类器，选择使用ovo或者ovr模式。<br>选择ovr模式，则返回的decision_function结构为(n_samples，n_class)。但是当二分类时，尽管选用ovr模式，却会返回<br>(n_samples，)的结构。 </p><p>选择ovo模式，则使用libsvm中原始的，结构为(n_samples,n_class<em>(n_class-1)/2)的decision_function接口。在ovo模式并且核函数为线性核的情况下，属性coef_和intercepe_会分别返回(n_class\</em>(n_class-1)/2,n_features) 和(n_class*(n_class-1)/2,)的结构，每行对应一个生成的二元分类器。ovo模式只在多分类的状况下使用。</p><p>SVC的其它参数、属性和接口的列表在<a href="[https://brickexperts.github.io/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2">SVM解读(2)</a>/#more](<a href="https://brickexperts.github.io/2019/09/15/SVM解读(2)/#more" target="_blank" rel="noopener">https://brickexperts.github.io/2019/09/15/SVM解读(2)/#more</a>)</p><h2 id="线性支持向量机类LinearSVC"><a href="#线性支持向量机类LinearSVC" class="headerlink" title="线性支持向量机类LinearSVC"></a>线性支持向量机类LinearSVC</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM77.png" alt=""></p><p>线性支持向量机其实与SVC类中选择”linear”作为核函数的功能类似，但是其背后的实现库是liblinear而不是libsvm，这使得在线性数据上，linearSVC的运行速度比SVC中的“linear”核函数要快，不过两者的运行结果相似。在现实中，许多数据都是线性的，因此我们可以依赖计算得更快得LinearSVC类。除此之外，线性支持向量可以很容易地推广到大样本上，还可以支持稀疏矩阵，多分类中也支持ovr方案。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM79.png" alt=""></p><p>和SVC一样，LinearSVC也有C这个惩罚参数，但LinearSVC在C变大时对C不太敏感，并且在某个阈值之后就不能再改善结果了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用SVC时的其他考虑&quot;&gt;&lt;a href=&quot;#使用SVC时的其他考虑&quot; class=&quot;headerlink&quot; title=&quot;使用SVC时的其他考虑&quot;&gt;&lt;/a&gt;使用SVC时的其他考虑&lt;/h2&gt;&lt;h3 id=&quot;SVC处理多分类问题&quot;&gt;&lt;a href=&quot;#SVC处理多分
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(3)</title>
    <link href="http://yoursite.com/2019/09/16/SVM%E8%A7%A3%E8%AF%BB(3)/"/>
    <id>http://yoursite.com/2019/09/16/SVM解读(3)/</id>
    <published>2019-09-16T11:00:53.000Z</published>
    <updated>2019-09-17T07:18:46.550Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参数C的理解进阶"><a href="#参数C的理解进阶" class="headerlink" title="参数C的理解进阶"></a>参数C的理解进阶</h2><p>在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，即无法让训练误差为0，这样的数据被我们称为“存在软间隔的数据”。此时此刻，我们需要让我们决策边界能够忍受一小部分训练误差，我们就不能单纯地寻求最大边际了。 因为对于软间隔地数据来说，边际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡。因此，我们引入松弛系数ζ和松弛系数的系数C作为一个惩罚项，来惩罚我们对最大边际的追求。 </p><p>那我们的参数C如何影响我们的决策边界呢？在硬间隔的时候，我们的决策边界完全由两个支持向量和最小化损失函数（最大化边际）来决定，而我们的支持向量是两个标签类别不一致的点，即分别是正样本和负样本。然而在软间隔情况下我们的边际依然由支持向量决定，但此时此刻的支持向量可能就不再是来自两种标签类别的点了，而是分布在决策边界两边的，同类别的点。回忆一下我们的图像： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt=""></p><p>此时我们的虚线超平面&omega;*x+b=1-ζ<sub>i</sub>是由混杂在红色点中间的紫色点来决定的，所以此时此刻，这个蓝色点就是我们的支持向量了。所以软间隔让决定两条虚线超平面的支持向量可能是来自于同一个类别的样本点，而硬间隔的时候两条虚线超平面必须是由来自两个不同类别的支持向量决定的。而C值会决定我们究竟是依赖红色点作为支持向量（只追求最大边界），还是我们要依赖软间隔中，混杂在红色点中的紫色点来作为支持向量（追求最大边界和判断正确的平衡）。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，尽量将掉落在决策边界另一方的样本点预测正确，决策功能会更简单，但代价是训练的准确度，因为此时会有更多红色的点被分类错误。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">10</span>,<span class="number">16</span>))</span><br><span class="line">    <span class="comment">#第一层循环：在不同的数据集中循环</span></span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">                   , zorder=<span class="number">10</span></span><br><span class="line">                   , cmap=plt.cm.Paired, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">100</span>,</span><br><span class="line">                   facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'white'</span>)</span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">                   levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">                , size=<span class="number">15</span></span><br><span class="line">                , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">                <span class="comment"># 为分数添加一个白色的格子作为底色</span></span><br><span class="line">                , transform=ax.transAxes  <span class="comment"># 确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">                , horizontalalignment=<span class="string">'right'</span>  <span class="comment"># 位于坐标轴的什么方向</span></span><br><span class="line">                )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>白色圈圈出的就是我们的支持向量，大家可以看到，所有在两条虚线超平面之间的点，和虚线超平面外，但属于另一个类别的点，都被我们认为是支持向量。并不是因为这些点都在我们的超平面上，而是因为我们的超平面由所有的这些点来决定，我们可以通过调节C来移动我们的超平面，让超平面过任何一个白色圈圈出的点。参数C就是这样影响了我们的决策，可以说是彻底改变了SVM的决策过程。</p><h2 id="样本不均衡问题"><a href="#样本不均衡问题" class="headerlink" title="样本不均衡问题"></a>样本不均衡问题</h2><p>分类问题永远逃不过的痛点是样本不均衡问题。样本不均衡是指在一组数据集中，标签的一类天生占有很大的比例，但我们有着捕捉出某种特定的分类的需求的状况。</p><p>分类问题天生会倾向于多数的类，让多数类更容易被判断准确，少数类被牺牲掉。因此对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。如果我们希望捕获少数类，模型就会失败。其次，模型评估指标会失去意义。 </p><h3 id="class-weight"><a href="#class-weight" class="headerlink" title="class_weight"></a>class_weight</h3><p>可输入字典或者”balanced”，可不填，默认None 对SVC，将类i的参数C设置为class_weight [i] <em> C。如果没有给出具体的class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如{“标签的值1”：权重1，”标签的值2”：权重2}的字典，则参数C将会自动被设为：标签的值1的C：权重1 </em> C，标签的值2的C：权重2<em>C或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为n_samples/(n_classes </em> np.bincount(y)) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line"><span class="comment">#其中红色点是少数类，紫色点是多数类</span></span><br><span class="line">clf = svm.SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">1.0</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"><span class="comment">#设定class_weight</span></span><br><span class="line">wclf = svm.SVC(kernel=<span class="string">'linear'</span>, class_weight=&#123;<span class="number">1</span>: <span class="number">10</span>&#125;)</span><br><span class="line">wclf.fit(X, y)</span><br><span class="line"><span class="comment">#给两个模型分别打分看看，这个分数是accuracy准确度</span></span><br><span class="line">print(<span class="string">"没设定class_weight："</span>,clf.score(X,y))</span><br><span class="line">print(<span class="string">"设定class_weight："</span>,wclf.score(X,y))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">ax = plt.gca() <span class="comment">#获取当前的子图，如果不存在，则创建新的子图</span></span><br><span class="line">xlim = ax.get_xlim()</span><br><span class="line">ylim = ax.get_ylim()</span><br><span class="line">xx = np.linspace(xlim[<span class="number">0</span>], xlim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">yy = np.linspace(ylim[<span class="number">0</span>], ylim[<span class="number">1</span>], <span class="number">30</span>)</span><br><span class="line">YY, XX = np.meshgrid(yy, xx)</span><br><span class="line">xy = np.vstack([XX.ravel(), YY.ravel()]).T</span><br><span class="line"><span class="comment">#第二步：找出我们的样本点到决策边界的距离</span></span><br><span class="line">Z_clf = clf.decision_function(xy).reshape(XX.shape)</span><br><span class="line">a = ax.contour(XX, YY, Z_clf, colors=<span class="string">'black'</span>, levels=[<span class="number">0</span>], alpha=<span class="number">0.5</span>, linestyles=[<span class="string">'-'</span>])</span><br><span class="line">Z_wclf = wclf.decision_function(xy).reshape(XX.shape)</span><br><span class="line">b = ax.contour(XX, YY, Z_wclf, colors=<span class="string">'red'</span>, levels=[<span class="number">0</span>], alpha=<span class="number">0.5</span>, linestyles=[<span class="string">'-'</span>])</span><br><span class="line"><span class="comment">#第三步：画图例 a.collections调用这个等高线对象中画的所有线，返回一个惰性对象</span></span><br><span class="line">plt.legend([a.collections[<span class="number">0</span>], b.collections[<span class="number">0</span>]], [<span class="string">"non weighted"</span>, <span class="string">"weighted"</span>],</span><br><span class="line">loc=<span class="string">"upper right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从图像上可以看出，灰色是我们做样本平衡之前的决策边界。灰色线上方的点被分为一类，下方的点被分为另一类。可以看到，大约有一半少数类（红色）被分错，多数类（紫色点）几乎都被分类正确了。红色是我们做样本平衡之后的决策边界，同样是红色线上方一类，红色线下方一类。可以看到，做了样本平衡后，少数类几乎全部都被分类正确了，但是多数类有许多被分错了。</p><p>从准确率的角度来看，不做样本平衡的时候准确率反而更高，做了样本平衡准确率反而变低了，这是因为做了样本平衡后，为了要更有效地捕捉出少数类，模型误伤了许多多数类样本，而多数类被分错的样本数量 &gt; 少数类被分类正确的样本数量，使得模型整体的精确性下降。现在，如果我们的目的是模型整体的准确率，那我们就要拒绝样本平衡，使用class_weight被设置之前的模型。而在现实情况中，我们往往都在捕捉少数类。因为有些情况是宁愿误伤，也要尽量的捕捉少数类。</p><h2 id="SVC的模型评估指标"><a href="#SVC的模型评估指标" class="headerlink" title="SVC的模型评估指标"></a>SVC的模型评估指标</h2><p>上面说了，我们往往都在捕捉少数类的。但是单纯地追求捕捉出少数类，就会成本太高，而不顾及少数类，又会无法达成模型的效果。所以在现实中，我们往往在寻找捕获少数类的能力和将多数类判错后需要付出的成本的平衡。如果一个模型在能够尽量捕获少数类的情况下，还能够尽量对多数类判断正确，则这个模型就非常优秀了。为了评估这样的能力，我们将引入新的模型评估指标：混淆矩阵和ROC曲线来帮助我们 </p><h3 id="混淆矩阵-Confusion-Matrix"><a href="#混淆矩阵-Confusion-Matrix" class="headerlink" title="混淆矩阵(Confusion Matrix)"></a>混淆矩阵(Confusion Matrix)</h3><h4 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h4><p>精确度Precision，又叫查准率，表示所有被我们预测为是少数类的样本中，真正的少数类所占的比例 </p><p>召回率Recall，又被称为敏感度(sensitivity)，真正率，查全率，表示所有真实为1的样本中，被我们预测正确的样<br>本所占的比例。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt=""></p><p>如果我们希望不计一切代价，找出少数类，那我们就会追求高召回率，相反如果我们的目标不是尽量捕获少数类，那我们就不需要在意召回率。 注意召回率和精确度的分子是相同的，只是分母不同。而召回率和精确度是此消彼长的，两者之间的平衡代表了捕捉少数类的需求和尽量不要误伤多数类的需求的平衡。究竟要偏向于哪一方，取决于我们的业务需求：<br>究竟是误伤多数类的成本更高，还是无法捕捉少数类的代价更高。</p><p>为了同时兼顾精确度和召回率，我们创造了两者的调和平均数作为考量两者平衡的综合性指标，称之为F1measure。两个数之间的调和平均倾向于靠近两个数中比较小的那一个数，因此我们追求尽量高的F1 measure，能够保证我们的精确度和召回率都比较高。F1 measure在[0,1]之间分布，越接近1越好：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM68.png" alt=""></p><h4 id="假负率、特异度-真负率-、假正率"><a href="#假负率、特异度-真负率-、假正率" class="headerlink" title="假负率、特异度(真负率)、假正率"></a>假负率、特异度(真负率)、假正率</h4><p>从Recall延申出来的另一个评估指标叫做假负率（False Negative Rate），它等于 1 - Recall，用于衡量所有真实为1的样本中，被我们错误判断为0的，通常用得不多。 </p><p>特异度(Specificity)，也叫做真负率。表示所有真实为0的样本中，被正确预测为0的样本所占的比例。在支持向量机中，可以形象地表示为，决策边界下方的点占所有蓝色点的比例。特异度衡量了一个模型将多数类判断正确的能力，而1 - specificity就是一个模型将多数类判断错误的能力，叫做假正率。</p><p>sklearn当中提供了大量的类来帮助我们了解和使用混淆矩阵：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM69.png" alt=""></p><h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>ROC曲线，全称The Receiver Operating Characteristic Curve，译为受试者操作特性曲线。这是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。让我们先从概率和阈值开始看起。 </p><h4 id="阈值-threshold"><a href="#阈值-threshold" class="headerlink" title="阈值(threshold)"></a>阈值(threshold)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">0.5</span>, <span class="number">1</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LogiR</span><br><span class="line">clf_lo=LogiR().fit(X,y)</span><br><span class="line">prob=clf_lo.predict_proba(X)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">prob=pd.DataFrame(prob)</span><br><span class="line">prob.columns=[<span class="string">"0"</span>,<span class="string">"1"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(prob.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="comment">#将0.5设立为阈值</span></span><br><span class="line">    <span class="keyword">if</span> prob.loc[i,<span class="string">"1"</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">    <span class="comment">#创建了一个新的列名为pred的列</span></span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#创建了名为y_true的列，且将y赋值为该列</span></span><br><span class="line">prob[<span class="string">"y_true"</span>] = y</span><br><span class="line"><span class="comment">#通过“1”列拍序，ascending表示正序or逆序</span></span><br><span class="line">prob = prob.sort_values(by=<span class="string">"1"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM, precision_score <span class="keyword">as</span> P, recall_score <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score <span class="keyword">as</span> A</span><br><span class="line">print(<span class="string">"混淆矩阵："</span>,CM(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"精确度："</span>,P(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"召回率："</span>,R(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"准确率："</span>,A(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>]))</span><br></pre></td></tr></table></figure><p>我们可以设置不同阈值来实验模型的结果，在不同阈值下，我们的模型评估指标会发生变化。我们正利用这一点来观察Recall和FPR之间如何互相影响。但是注意，并不是升高阈值，就一定能够增加或者减少Recall，一切要根据数据的实际分布来进行判断。</p><h4 id="概率-probability"><a href="#概率-probability" class="headerlink" title="概率(probability)"></a>概率(probability)</h4><p>我们在画等高线，也就是决策边界的时候曾经使用SVC的接口decision_function，它返回我们输入的特征矩阵中每个样本到划分数据集的超平面的距离。我们在SVM中利用超平面来判断我们的样本，本质上来说，当两个点的距离是相同的符号的时候，越远离超平面的样本点归属于某个标签类的概率就很大。比如说，一个距离超平面0.1的点，和一个距离超平面100的点，明显是距离为0.1的点更有可能是负类别的点混入了边界。同理，一个距离超平面距离为-0.1的点，和一个离超平面距离为-100的点，明显是-100的点的标签更有可能是负类接口decision_function返回的值也因此被我们认为是SVM中的置信度(confidence)。</p><p>不过，置信度始终不是概率，它没有边界，可以无限大。大部分时候不是也小数的形式呈现，而SVC的判断过程又不像决策树一样求解出一个比例。为了解决这个矛盾，SVC有重要参数probability。</p><p><strong>probability</strong>：是否启用概率估计，布尔值，可不填，默认时False。必须在调用fit之前使用它，启用此功能会减慢SVM的计算速度。设置为True会启动，SVC的接口predict_proba和predict_log_proba将生效。在二分类情况下，SVC将使用Platt缩放生成概率，即在decision_function生成的距离上进行Sigmoid压缩，并附加训练数据的交叉验证拟合，来生成类逻辑回归的SVM分数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#其中红色点是少数类，紫色点是多数类</span></span><br><span class="line">clf_proba = svm.SVC(kernel=<span class="string">"linear"</span>,C=<span class="number">1.0</span>,probability=<span class="literal">True</span>).fit(X,y)</span><br><span class="line">print(<span class="string">"返回预测某标签的概率："</span>,clf_proba.predict_proba(X))</span><br><span class="line">print(<span class="string">"行数和列数："</span>,clf_proba.predict_proba(X).shape)</span><br><span class="line">print(<span class="string">"每个样本到划分数据集的超平面的距离："</span>,clf_proba.decision_function(X))</span><br></pre></td></tr></table></figure><h4 id="绘制SVM的ROC曲线"><a href="#绘制SVM的ROC曲线" class="headerlink" title="绘制SVM的ROC曲线"></a>绘制SVM的ROC曲线</h4><p>现在，我们理解了什么是阈值（threshold），了解了不同阈值会让混淆矩阵产生变化，也了解了如何从我们的分类算法中获取概率。现在，我们就可以开始画我们的ROC曲线了。<strong>ROC是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。</strong>简单地来说，只要我们有数据和模型，我们就可以在python中绘制出我们的ROC曲线。思考一下，我们要绘制ROC曲线，就必须在我们的数据中去不断调节阈值，不断求解混淆矩阵，然后不断获得我们的横坐标和纵坐标，最后才能够将曲线绘制出来。接下来，我们就来执行这个过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">class_1 = <span class="number">500</span> <span class="comment">#类别1有500个样本</span></span><br><span class="line">class_2 = <span class="number">50</span> <span class="comment">#类别2只有50个</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">1.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差，通常来说，样本量比较大的类别会更加松散</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#看看数据集长什么样</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=<span class="string">"rainbow"</span>,s=<span class="number">10</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LogiR</span><br><span class="line">clf_lo=LogiR().fit(X,y)</span><br><span class="line">prob=clf_lo.predict_proba(X)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">prob=pd.DataFrame(prob)</span><br><span class="line">prob.columns=[<span class="string">"0"</span>,<span class="string">"1"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(prob.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="comment">#将0.5设立为阈值</span></span><br><span class="line">    <span class="keyword">if</span> prob.loc[i,<span class="string">"1"</span>] &gt; <span class="number">0.5</span>:</span><br><span class="line">    <span class="comment">#创建了一个新的列名为pred的列</span></span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prob.loc[i,<span class="string">"pred"</span>] = <span class="number">0</span></span><br><span class="line"><span class="comment">#创建了名为y_true的列，且将y赋值为该列</span></span><br><span class="line">prob[<span class="string">"y_true"</span>] = y</span><br><span class="line">prob = prob.sort_values(by=<span class="string">"1"</span>,ascending=<span class="literal">False</span>)</span><br><span class="line">cm = CM(prob.loc[:,<span class="string">"y_true"</span>],prob.loc[:,<span class="string">"pred"</span>],labels=[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment">#开始绘图</span></span><br><span class="line">recall = []</span><br><span class="line">FPR = []</span><br><span class="line">clf_proba = svm.SVC(kernel=<span class="string">"linear"</span>,C=<span class="number">1.0</span>,probability=<span class="literal">True</span>).fit(X,y)</span><br><span class="line">probrange = np.linspace(clf_proba.predict_proba(X)</span><br><span class="line">[:,<span class="number">1</span>].min(),clf_proba.predict_proba(X)[:,<span class="number">1</span>].max(),num=<span class="number">50</span>,endpoint=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="keyword">as</span> CM, recall_score <span class="keyword">as</span> R</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plot</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> probrange:</span><br><span class="line">    y_predict = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> clf_proba.predict_proba(X)[j,<span class="number">1</span>] &gt; i:</span><br><span class="line">            y_predict.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_predict.append(<span class="number">0</span>)</span><br><span class="line">    cm = CM(y,y_predict,labels=[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    recall.append(cm[<span class="number">0</span>,<span class="number">0</span>]/cm[<span class="number">0</span>,:].sum())</span><br><span class="line">    FPR.append(cm[<span class="number">1</span>,<span class="number">0</span>]/cm[<span class="number">1</span>,:].sum())</span><br><span class="line"><span class="comment">#排序</span></span><br><span class="line">recall.sort()</span><br><span class="line">FPR.sort()</span><br><span class="line">plt.plot(FPR,recall,c=<span class="string">"red"</span>)</span><br><span class="line">plt.plot(probrange+<span class="number">0.05</span>,probrange+<span class="number">0.05</span>,c=<span class="string">"black"</span>,linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行后，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM70.png" alt=""></p><p>我们建立ROC曲线的根本目的是找寻Recall和FPR之间的平衡，让我们能够衡量模型在尽量捕捉少数类的时候，误伤多数类的情况会如何变化。横坐标是FPR，代表着模型将多数类判断错误的能力，纵坐标Recall，代表着模型捕捉少数类的能力，所以ROC曲线代表着，随着Recall的不断增加，FPR如何增加。我们希望随着Recall的不断提升，FPR增加得越慢越好，这说明我们可以尽量高效地捕捉出少数类，而不会将很多地多数类判断错误。所以，我们希望看到的图像是，纵坐标急速上升，横坐标缓慢增长，也就是在整个图像左上方的一条弧线。这代表模型的效果很不错，拥有较好的捕获少数类的能力。 </p><p>中间的虚线代表着，当recall增加1%，我们的FPR也增加1%，也就是说，我们每捕捉出一个少数类，就会有一个多数类被判错，这种情况下，模型的效果就不好，这种模型捕获少数类的结果，会让许多多数类被误伤，从而增加我们的成本。ROC曲线通常都是凸型的。对于一条凸型ROC曲线来说，曲线越靠近左上角越好，越往下越糟糕，曲线如果在虚线的下方，则证明模型完全无法使用。但是它也有可能是一条凹形的ROC曲线。对于一条凹型ROC曲线来说，应该越靠近右下角越好，凹形曲线代表模型的预测结果与真实情况完全相反，那也不算非常糟糕，只要我们手动将模型的结果逆转，就可以得到一条左上方的弧线了。最糟糕的就是，无论曲线是凹形还是凸型，曲线位于图像中间，和虚线非常靠近，那我们拿它无能为力。</p><p>现在，我们虽然拥有了这条曲线，但是还是没有具体的数字帮助我们理解ROC曲线和模型效果。接下来将AUC面加，它代表了ROC曲线下方的面积，这个面积越大，代表ROC曲线越接近左上角，模型就越好。</p><h3 id="AUC面积"><a href="#AUC面积" class="headerlink" title="AUC面积"></a>AUC面积</h3><p>sklearn中，我们有帮助我们计算ROC曲线的横坐标假正率FPR，纵坐标Recall和对应的阈值的类<br>sklearn.metrics.roc_curve。同时，我们还有帮助我们计算AUC面积的类sklearn.metrics.roc_auc_score。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM71.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM72.png" alt=""></p><p>AUC面积的分数使用以上类来进行计算，输入的参数也比较简单，就是真实标签，和与roc_curve中一致的置信度分数或者概率值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">FPR, recall, thresholds = roc_curve(y,clf_proba.decision_function(X), pos_label=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#阈值可以也为负</span></span><br><span class="line">print(FPR,recall,thresholds)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score <span class="keyword">as</span> AUC</span><br><span class="line">area = AUC(y,clf_proba.decision_function(X))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(FPR, recall, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">'ROC curve (area = %0.2f)'</span> % area)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'Receiver operating characteristic example'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="利用ROC曲线找出最佳阈值"><a href="#利用ROC曲线找出最佳阈值" class="headerlink" title="利用ROC曲线找出最佳阈值"></a>利用ROC曲线找出最佳阈值</h2><p>对ROC曲线的理解来：ROC曲线反应的是recall增加的时候FPR如何变化，也就是当模型捕获少数类的能力变强的时候，会误伤多数类的情况是否严重。我们的希望是，模型在捕获少数类的能力变强的时候，尽量不误伤多数类，也就是说，随着recall的变大，FPR的大小越小越好。所以我们希望找到的最有点，其实是Recall和FPR差距最大的点。这个点，又叫做约登指数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">maxindex=(recall-FPR).tolist().index(max(recall-FPR))</span><br><span class="line"><span class="comment">#得出最佳阈值</span></span><br><span class="line">print(<span class="string">"最佳阈值："</span>,thresholds[maxindex])</span><br><span class="line">plt.scatter(FPR[maxindex],recall[maxindex],c=<span class="string">"black"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(FPR, recall, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">'ROC curve (area = %0.2f)'</span> % area)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'black'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.scatter(FPR[maxindex],recall[maxindex],c=<span class="string">"black"</span>,s=<span class="number">30</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">'False Positive Rate'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'Receiver operating characteristic example'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这样，最佳阈值和最好的点都找了出来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参数C的理解进阶&quot;&gt;&lt;a href=&quot;#参数C的理解进阶&quot; class=&quot;headerlink&quot; title=&quot;参数C的理解进阶&quot;&gt;&lt;/a&gt;参数C的理解进阶&lt;/h2&gt;&lt;p&gt;在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(2)</title>
    <link href="http://yoursite.com/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/"/>
    <id>http://yoursite.com/2019/09/15/SVM解读(2)/</id>
    <published>2019-09-15T05:47:15.000Z</published>
    <updated>2019-09-16T10:54:44.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="非线性SVM与核函数"><a href="#非线性SVM与核函数" class="headerlink" title="非线性SVM与核函数"></a>非线性SVM与核函数</h2><h3 id="SVC在非线性数据上的推广"><a href="#SVC在非线性数据上的推广" class="headerlink" title="SVC在非线性数据上的推广"></a>SVC在非线性数据上的推广</h3><p>为了能够找出非线性数据的线性决策边界，我们需要将数据从原始的空间x投射到新空间Φ(x)中，Φ是一个映射函数，它代表了某种非线性的变换，如同我们之前所做过的使用r来升维一样，这种非线性变换看起来是一种非常有效的方式。使用这种变换，线性SVM的原理可以被很容易推广到非线性情况下，其推到过程核逻辑都与线性SVM一摸一样，只不过在第一决策边界之前，我们必须先对数据进行升维，即将原始的x转换成Φ。</p><h3 id="重要参数kernel"><a href="#重要参数kernel" class="headerlink" title="重要参数kernel"></a>重要参数kernel</h3><p>这种变换非常巧妙，但也带有一些实现问题。首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。其次，已知适当的映射函数，我们想要计算类似于Φ(x)<sub>i</sub>* Φ(x<sub>test</sub>)这样的点积，计算量无法巨大。要找出超平面所付出的代价是非常昂贵的。</p><p><strong>关键概念</strong>：核函数</p><p>而解决上面这些问题的数学方式，叫做”核技巧”。是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。具体体现为，K(u,v)= Φ(u)*Φ(v)。而这个原始空间中的点积函数K(u,v)，就被叫做“核函数”。</p><p>核函数能够帮助我们解决三个问题：</p><p>第一，有了核函数之后，我们无需去担心$\Phi$究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数(positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量的点积一定可以被低维空间中的这两个向量的某种计算来表示。</p><p>第二，使用核函数计算低维度中的向量关系比计算原来的Φ(x<sub>i</sub>) * Φ(x<sub>test</sub>)要简单太多了。</p><p>第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。 </p><p>选用不同的核函数，就可以解决不同数据分布下的寻找超平面问题。在SVC中，这个功能由参数“kernel”和一系列与核函数相关的参数来进行控制。之前的代码中我们一直使用这个参数并输入”linear”，但却没有给大家详细讲解，也是因为如果不先理解核函数本身，很难说明这个参数到底在做什么。参数“kernel”在sklearn中可选以下几种选项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt=""></p><p>可以看出，除了选项”linear”之外，其他核函数都可以处理非线性问题。多项式核函数有次数d，当d为1的时候它就是再处理线性问题，当d为更高次项的时候它就是在处理非线性问题。我们之前画图时使用的是选项“linear”，自然不能处理环形数据这样非线性的状况。而刚才我们使用的计算r的方法，其实是高斯径向基核函数所对应的功能，在参数”kernel“中输入”rbf“就可以使用这种核函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line">X,y = make_circles(<span class="number">100</span>, factor=<span class="number">0.1</span>, noise=<span class="number">.1</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(model,ax=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ax = plt.gca()</span><br><span class="line">    xlim = ax.get_xlim()</span><br><span class="line">    ylim = ax.get_ylim()</span><br><span class="line">    x = np.linspace(xlim[<span class="number">0</span>],xlim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    y = np.linspace(ylim[<span class="number">0</span>],ylim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    Y,X = np.meshgrid(y,x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    ax.contour(X, Y, P,colors=<span class="string">"k"</span>,levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>],alpha=<span class="number">0.5</span>,linestyles=[<span class="string">"--"</span>,<span class="string">"-"</span>,<span class="string">"--"</span>])</span><br><span class="line">    ax.set_xlim(xlim)</span><br><span class="line">    ax.set_ylim(ylim)</span><br><span class="line">    plt.show()</span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">r = np.exp(-(X**<span class="number">2</span>).sum(<span class="number">1</span>))</span><br><span class="line">rlim = np.linspace(min(r),max(r),<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3D</span><span class="params">(elev=<span class="number">30</span>,azim=<span class="number">30</span>,X=X,y=y)</span>:</span></span><br><span class="line">    ax = plt.subplot(projection=<span class="string">"3d"</span>)</span><br><span class="line">    ax.scatter3D(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],r,c=y,s=<span class="number">50</span>,cmap=<span class="string">'rainbow'</span>)</span><br><span class="line">    ax.view_init(elev=elev,azim=azim)</span><br><span class="line">    ax.set_xlabel(<span class="string">"x"</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">"y"</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">"r"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_3D()</span><br><span class="line">clf = SVC(kernel = <span class="string">"rbf"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plot_svc_decision_function(clf)</span><br></pre></td></tr></table></figure><p>运行后，从效果图可以看到，决策边界被完美的找了出来。</p><h3 id="探索核函数在不同数据集上的表现"><a href="#探索核函数在不同数据集上的表现" class="headerlink" title="探索核函数在不同数据集上的表现"></a>探索核函数在不同数据集上的表现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>,<span class="string">"poly"</span>,<span class="string">"rbf"</span>,<span class="string">"sigmoid"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">20</span>,<span class="number">16</span>))</span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line"><span class="comment">#在图像中的第一列，放置原数据的分布</span></span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="comment">#第二层循环：在不同的核函数中循环</span></span><br><span class="line">    <span class="comment">#从图像的第二列开始，一个个填充分类结果</span></span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">    <span class="comment">#定义子图位置</span></span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        <span class="comment">#建模</span></span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        <span class="comment">#绘制图像本身分布的散点图</span></span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">        ,zorder=<span class="number">10</span></span><br><span class="line">        ,cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制支持向量</span></span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">50</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制决策边界</span></span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        <span class="comment">#np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法</span></span><br><span class="line">        <span class="comment">#一次性使用最大值和最小值来生成网格</span></span><br><span class="line">        <span class="comment">#表示为[起始值：结束值：步长]</span></span><br><span class="line">        <span class="comment">#如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        <span class="comment">#np.c_，类似于np.vstack的功能</span></span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        <span class="comment">#填充等高线不同区域的颜色</span></span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        <span class="comment">#绘制等高线</span></span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">        levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#设定坐标轴为不显示</span></span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="comment">#将标题放在第一行的顶上</span></span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        <span class="comment">#为每张图添加分类的分数</span></span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">        , size=<span class="number">15</span></span><br><span class="line">        , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">        <span class="comment">#为分数添加一个白色的格子作为底色</span></span><br><span class="line">        , transform=ax.transAxes <span class="comment">#确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">        , horizontalalignment=<span class="string">'right'</span> <span class="comment">#位于坐标轴的什么方向</span></span><br><span class="line">        )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。 </p><h3 id="选取与核函数相关的参数"><a href="#选取与核函数相关的参数" class="headerlink" title="选取与核函数相关的参数"></a>选取与核函数相关的参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt=""></p><p>在知道如何选取核函数后，我们还要观察一下除了kernel之外的核函数相关的参数。对于线性核函数，”kernel”是唯一能够影响它的参数，但是对于其他三种非线性核函数，他们还受到参数gamma，degree以及coef0的影响。参数gamma就是表达式中的&gamma;，degree就是多项式核函数的次数d，参数coef0就是常数项&gamma;。其中，高斯径向基核函数受到gamma的影响，而多项式核函数受到全部三个参数的影响。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM54.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用交叉验证得出最好的参数和准确率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">gamma_range = np.logspace(<span class="number">-10</span>,<span class="number">1</span>,<span class="number">20</span>)</span><br><span class="line">coef0_range = np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">param_grid = dict(gamma = gamma_range</span><br><span class="line">,coef0 = coef0_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid = GridSearchCV(svm.SVC(kernel = <span class="string">"poly"</span>,degree=<span class="number">1</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid, cv=cv)</span><br><span class="line">grid.fit(X, y)</span><br><span class="line">print(<span class="string">"The best parameters are %s with a score of %0.5f"</span> % (grid.best_params_,</span><br><span class="line">grid.best_score_))</span><br></pre></td></tr></table></figure><h3 id="重要参数C"><a href="#重要参数C" class="headerlink" title="重要参数C"></a>重要参数C</h3><p><strong>关键概念</strong>：硬件隔与软件隔</p><p>当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在“硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt=""></p><p>看上图，原来的决策边界&omega;<em>x+b=0，原本的平行于决策边界的两个虚线超平面&omega;\</em>x+b=1和&omega;*x+b=-1都依然有效。我们的原始判别函数是:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM56.png" alt=""></p><p>不过，这些超平面现在无法让数据上的训练误差等于0了，因为此时存在了一个混杂在红色点中的紫色点。于是，我们需要放松我们原始判别函数中的不等条件，来让决策边界能够适用于我们的异常点。于是我们引入松弛系数Φ来帮助我们优化原始的判别函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM57.png" alt=""></p><p>其中ζ<sub>i</sub>&gt;0。可以看的出，这其实是将原来的虚线超平面向图像的上方和下方平移。松弛系数其实蛮好理解的，看上图，在红色点附近的蓝色点在原本的判别函数中必定会被分为红色，所有一定会判断错。我们现在做一条与决策边界平行，且过蓝色点的直线&omega;<em>x<sub>i</sub>+b=1- ζ<sub>i</sub>(图中的蓝色虚线)。这条直线是由&omega;\</em>x<sub>i</sub>+b=1平移得到，所以两条直线在纵坐标上的差异就是ζ<sub>i</sub>。而点&omega;x<sub>i</sub>+b=1的距离就可以表示为 ζ<sub>i</sub>*&omega;/||&omega;||，即ζ<sub>i</sub>在&omega;方向上的投影。由于单位向量是固定的，所以ζ<sub>i</sub>可以作为蓝色点在原始的决策边界上的分类错误的程度的表示。隔得越远，分的越错。<strong>注意：ζ<sub>i</sub>并不是点到决策超平面的距离本身。</strong></p><p>不难注意到，我们让&omega;*x<sub>i</sub>+b&gt;=1-ζ<sub>i</sub>作为我们的新决策超平面，是有一定的问题的。虽然我们把异常的蓝色点分类正确了，但我们同时也分错了一系列红色的点。所以我们必须在我们求解最大边际的损失函数中加上一个惩罚项，用来惩罚我们具有巨大松弛系数的决策超平面。我们的拉格朗日函数，拉格朗日对偶函数，也因此都被松弛系数改变。现在，我们的损失函数为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM58.png" alt=""></p><p>C是用来控制惩罚力度的系数</p><p>我们的拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM59.png" alt=""></p><p>需要满足的KKT条件为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM60.png" alt=""></p><p>拉格朗日对偶函数为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM61.png" alt=""></p><p>sklearn类SVC背后使用的最终公式。公式中现在唯一的新变量，松弛系数的惩罚力度C，由我们的参数C来进行控制。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM62.png" alt=""></p><p>在实际使用中，C和核函数的相关参数（gamma，degree等等）们搭配，往往是SVM调参的重点。与gamma不同，C没有在对偶函数中出现，并且是明确了调参目标的，所以我们可以明确我们究竟是否需要训练集上的高精确度来调整C的方向。默认情况下C为1，通常来说这都是一个合理的参数。 如果我们的数据很嘈杂，那我们往往减小C。当然，我们也可以使用网格搜索或者学习曲线来调整C的值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">C1_range = np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C2_range=np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C3_range=np.linspace(<span class="number">5</span>,<span class="number">7</span>,<span class="number">50</span>)</span><br><span class="line">param_grid1 = dict(C=C1_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid1 = GridSearchCV(svm.SVC(kernel = <span class="string">"linear"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid1, cv=cv)</span><br><span class="line">grid1.fit(X, y)</span><br><span class="line">print(<span class="string">"linear  The best parameters are %s with a score of %0.5f"</span> % (grid1.best_params_,</span><br><span class="line">grid1.best_score_))</span><br><span class="line">param_grid2=dict(C=C2_range)</span><br><span class="line">grid2 = GridSearchCV(svm.SVC(kernel = <span class="string">"rbf"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid2, cv=cv)</span><br><span class="line">grid2.fit(X, y)</span><br><span class="line">print(<span class="string">"rbf(0.01,30,50)   The best parameters are %s with a score of %0.5f"</span> % (grid2.best_params_,</span><br><span class="line">grid2.best_score_))</span><br><span class="line">param_grid3=dict(C=C3_range)</span><br><span class="line">grid3=GridSearchCV(svm.SVC(kernel=<span class="string">"rbf"</span>,gamma=<span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),param_grid=param_grid3)</span><br><span class="line">grid3.fit(X,y)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_params_)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_score_)</span><br></pre></td></tr></table></figure><h2 id="SVC的参数、属性和接口"><a href="#SVC的参数、属性和接口" class="headerlink" title="SVC的参数、属性和接口"></a>SVC的参数、属性和接口</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM63.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM64.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM65.png" alt=""></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM66.png" alt=""></p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM67.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;非线性SVM与核函数&quot;&gt;&lt;a href=&quot;#非线性SVM与核函数&quot; class=&quot;headerlink&quot; title=&quot;非线性SVM与核函数&quot;&gt;&lt;/a&gt;非线性SVM与核函数&lt;/h2&gt;&lt;h3 id=&quot;SVC在非线性数据上的推广&quot;&gt;&lt;a href=&quot;#SVC在非线性数
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
</feed>
