<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-27T14:50:41.730Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>tensorflow中如何预防过拟合</title>
    <link href="http://yoursite.com/2019/11/26/tensorflow%E4%B8%AD%E5%A6%82%E4%BD%95%E9%A2%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>http://yoursite.com/2019/11/26/tensorflow中如何预防过拟合/</id>
    <published>2019-11-26T14:26:29.000Z</published>
    <updated>2019-11-27T14:50:41.730Z</updated>
    
    <content type="html"><![CDATA[<h2 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h2><p>避免过拟合的方法有很多，常用的方法有early stopping、数据集扩增、正则化、dropout。下面就概述一下，具体请参照<a href="[https://brickexperts.github.io/2019/10/08/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(1">网址</a>/#more](<a href="https://brickexperts.github.io/2019/10/08/优化算法(1)/#more)。" target="_blank" rel="noopener">https://brickexperts.github.io/2019/10/08/优化算法(1)/#more)。</a></p><p>early stoping：在发生过拟合之前提前结束。理论上是可以的，但是这个点不好把握。</p><p>数据集扩增：就是让模型见到更多的情况，可以最大化地满足全样本，但实际应用中对于未来事件的预测不理想。</p><p>正则化：通过映入范数，增强模型的泛化能力。包括L1、L2.</p><p>dropout：是网络模型中的一种方法。每次训练时舍去一些节点来增强泛化能力。</p><p>下面我们来具体看看如何实现后两种方法。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>所谓的正则化， 其实就是在神经网络计算损失值的过程中， 在损失后面再加一项。 这样损失值所代表的输出与标准结果间的误差就会受到干扰， 导致学习参数w和b无法按照目标方向来调整， 实现模型无法与样本完全拟合的结果， 从而达到防止过拟合的效果  </p><p>这个干扰项一定要有下面这样的特性：</p><p>1、当欠拟合时，希望它对模型误差的影响越小越好，以便让模型快速拟合实际</p><p>2、当过拟合时，希望他对模型误差的影响越大越好，以便让模型不要产生过拟合的情况。</p><p>由上面两个特性，引入两个范数：L1和L2</p><p>L1：所有学习参数w的绝对值的和。</p><p>L2：所有学习参数w的平方和然后求平方根</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191127210328.png" alt=""></p><p>上图中的第一个式子是L1范式，另一个就是L2范式。loss为等式左边的结果，less(0)代表真实的loss值，less(0)后面的那一项就代表正则化，&lambda;为一个可以调整的参数，用来控制正则化对loss的影响。对于L2，将其乘以1/2是为了反向传播时对其求导可以将数据规整。</p><h2 id="tensorflow中的正则化"><a href="#tensorflow中的正则化" class="headerlink" title="tensorflow中的正则化"></a>tensorflow中的正则化</h2><p>L2的正则化函数为:</p><p>tf.nn.l2_loss(t,name=None)</p><p>L1的正则化函数在tensorflow是没有自己组装的，可以自己写：</p><p>tf.reduce_sum(tf.abs(w))</p><h2 id="通过正则化改善过拟合"><a href="#通过正则化改善过拟合" class="headerlink" title="通过正则化改善过拟合"></a>通过正则化改善过拟合</h2><p>使用正则化非常简单，只需要在计算损失值时加上loss的正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line">reg=<span class="number">0.01</span></span><br><span class="line">loss=tf.reduce_mean((y_pred-y)**<span class="number">2</span>)+tf.nn.l2_loss(weights[<span class="string">'h1'</span>])*reg+tf.nn.l2_loss(weight[<span class="string">'h2'</span>])*reg</span><br><span class="line">……</span><br></pre></td></tr></table></figure><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>还有一种防止过拟合的方法是dropout。这个方法的做法是：在训练过程中，每次随机选择一部分节点不要去“学习”</p><p>因为从样本数据的分析来看，数据本身是不可能很纯净的，也就是任何一个模型不能100%把数据完全分开，在某一类中一定会有一些异常数据，过拟合的问题恰恰是把这些异常数据当初规律来学习了。我们希望把异常数据过滤掉，只关心有用的规律数据。</p><p>异常数据的特点是，它与主流样本中的规律都不同，但是量非常少，相当于在一个样本中出现的概率比主流数据出现的概率低很多。我们可以利用这一点，通过在每次模型中忽略一些节点的数据学习，将小概率的异常数据获得学习的机会降低，这样这些异常数据对模型的影响就会更小了。</p><p><strong>注意</strong>：由于dropout让一部分节点不去“学习”，所以在增加模型的泛化能力的同时，会使学习速度降低，使模型不太容易学成。所以在使用的过程中需要合理地调节到底丢弃多少节点，并不是丢弃的节点越多越好。</p><h2 id="tensorflow中的dropout"><a href="#tensorflow中的dropout" class="headerlink" title="tensorflow中的dropout"></a>tensorflow中的dropout</h2><p>tf.nn.dropout(x,keep_prob,noise_shape=None,seed=None,name=None)</p><p>x： 输入的模型节点。<br>keep_prob： 保持率。 如果为1， 则代表全部进行学习； 如果为0.8， 则代表丢弃20%的节点， 只让80%的节点参与学习。<br>noise_shape： 代表指定x中， 哪些维度可以使用dropout技术。 为None时， 表示所有维度都使用dropout技术。 也可以将某个维度标志为1， 来代表该维度使用dropout技术。 例如： x的形状为[n， len， w， ch]， 使用noise_shape为[n， 1， 1，ch]， 这表明会对x中的第二维度len和第三维度w进行dropout。<br>seed： 随机选取节点的过程中随机数的种子值。</p><h2 id="全连接网络的深浅关系"><a href="#全连接网络的深浅关系" class="headerlink" title="全连接网络的深浅关系"></a>全连接网络的深浅关系</h2><p>在实际中，如果想使用浅层神经网络拟合复杂非线性函数，就需要靠增加的神经元个数来实现。神经元过多意味着需要训练的参数过多，这会增加网络的学习难度，并影响网络的泛化能力。因此，在增加网络结构时，一般倾向于使用更多的模型，来减少网络中所需要神经元的数量，使网络有更好的泛化能力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;方法概述&quot;&gt;&lt;a href=&quot;#方法概述&quot; class=&quot;headerlink&quot; title=&quot;方法概述&quot;&gt;&lt;/a&gt;方法概述&lt;/h2&gt;&lt;p&gt;避免过拟合的方法有很多，常用的方法有early stopping、数据集扩增、正则化、dropout。下面就概述一下，具体请
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>numpy(2)</title>
    <link href="http://yoursite.com/2019/11/24/numpy(2)/"/>
    <id>http://yoursite.com/2019/11/24/numpy(2)/</id>
    <published>2019-11-24T06:58:11.000Z</published>
    <updated>2019-11-24T11:31:45.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="np-concatenate"><a href="#np-concatenate" class="headerlink" title="np.concatenate"></a>np.concatenate</h2><p>数组拼接函数，concatenate((a1,a2,……),axis=0)</p><p>参数a1，a2……为要拼接的数组，axis为在哪个维度上进行拼接。默认为0</p><p>>&gt;&gt;import numpy as np<br>>&gt;&gt;a=np.array([[1,2],[3,4]])<br>>&gt;&gt;b=np.array([[5,6]])</p><p>>&gt;&gt;np.concatenate((a,b),axis=0)<br>array([[1, 2],<br>       [3, 4],<br>       [5, 6]])</p><p>>&gt;&gt;np.concatenate((a,b.T),axis=1)<br>array([[1, 2, 5],<br>       [3, 4, 6]])</p><h2 id="np-eye"><a href="#np-eye" class="headerlink" title="np.eye"></a>np.eye</h2><p>生成对角矩阵</p><p>numpy.eye(N,M=None, k=0, dtype=<type 'float'>)</p><p>第一个参数：输出方阵（行数=列数）的规模，即行数或列数</p><p>第三个参数：默认情况下输出的是对角线全“1”，其余全“0”的方阵，如果k为正整数，则在右上方第k条对角线全“1”其余全“0”，k为负整数则在左下方第k条对角线全“1”其余全“0”。</p><p>>&gt;&gt;import numpy as np<br>>&gt;&gt;np.eye(1,3,dtype=int)<br>array([[1, 0, 0]])<br>>&gt;&gt;np.eye(2,3,dtype=int)<br>array([[1, 0, 0],<br>       [0, 1, 0]])<br>>&gt;&gt;np.eye(2,2,dtype=int)<br>array([[1, 0],<br>       [0, 1]])<br>>&gt;&gt;np.eye(4,4,dtype=int)<br>array([[1, 0, 0, 0],<br>       [0, 1, 0, 0],<br>       [0, 0, 1, 0],<br>       [0, 0, 0, 1]])</p><p>>&gt;&gt;np.eye(4,4,k=2,dtype=int)<br>array([[0, 0, 1, 0],<br>       [0, 0, 0, 1],<br>       [0, 0, 0, 0],<br>       [0, 0, 0, 0]])</p><h2 id="np-random-multivariate-normal"><a href="#np-random-multivariate-normal" class="headerlink" title="np.random.multivariate_normal"></a>np.random.multivariate_normal</h2><p>multivariate_normal(mean,cov,size=None,check_valid=None,tol=None)：用于根据实际情况生成一个多元正态分布矩阵</p><p>其中mean和cov为必要的传参而size，check_valid以及tol为可选参数。</p><p>mean：mean是多维分布的均值维度为1；</p><p>cov：协方差矩阵，注意：协方差矩阵必须是对称的且需为半正定矩阵；</p><p>size：指定生成的正态分布矩阵的维度（例：若size=(1, 1, 2)，则输出的矩阵的shape即形状为 1X1X2XN（N为mean的长度））。</p><p>check_valid：这个参数用于决定当cov即协方差矩阵不是半正定矩阵时程序的处理方式，它一共有三个值：warn，raise以及ignore。当使用warn作为传入的参数时，如果cov不是半正定的程序会输出警告但仍旧会得到结果；当使用raise作为传入的参数时，如果cov不是半正定的程序会报错且不会计算出结果；当使用ignore时忽略这个问题即无论cov是否为半正定的都会计算出结果。3种情况的console打印结果如下：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;np-concatenate&quot;&gt;&lt;a href=&quot;#np-concatenate&quot; class=&quot;headerlink&quot; title=&quot;np.concatenate&quot;&gt;&lt;/a&gt;np.concatenate&lt;/h2&gt;&lt;p&gt;数组拼接函数，concatenate((a1
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
      <category term="数学计算" scheme="http://yoursite.com/categories/python/%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="numpy" scheme="http://yoursite.com/categories/python/%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97/numpy/"/>
    
    
      <category term="python的数学计算模块" scheme="http://yoursite.com/tags/python%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9D%97/"/>
    
  </entry>
  
  <entry>
    <title>Maxout网络</title>
    <link href="http://yoursite.com/2019/11/23/Maxout%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/11/23/Maxout网络/</id>
    <published>2019-11-23T08:18:18.000Z</published>
    <updated>2019-11-23T11:12:54.214Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Maxout介绍"><a href="#Maxout介绍" class="headerlink" title="Maxout介绍"></a>Maxout介绍</h2><p>Maxout网络可以理解为单个神经元的扩展，主要是扩展单个神经元里面的激活函数。Maxout是将激活函数变成一个网络选择器，原理就是将多个神经元并列地放在一起，从它们的输出结果中找到最大的那个，代表对特征响应最敏感，然后取这个神经元的结束参与后面的运算。</p><p>下图是单个神经元和Maxout网络的区别：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/6.jpg" alt=""></p><p>Maxout的公式可以理解为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123163141.png" alt=""></p><p>这个的做法就是相当于同时使用多个神经元放在一起， 哪个有效果就用哪个。 所以这样的网络会有更好的拟合效果。  </p><h2 id="Maxout网络实现MNIST分类"><a href="#Maxout网络实现MNIST分类" class="headerlink" title="Maxout网络实现MNIST分类"></a>Maxout网络实现MNIST分类</h2><p>Maxout网络的构建方法：通过reduce_max函数对多个神经元的输出来计算Max值，将Max值当作输入按照神经元正反传播方向进行计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据:'</span>,mnist.train.images)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据打shape:'</span>,mnist.train.images.shape)</span><br><span class="line"><span class="keyword">import</span> pylab </span><br><span class="line">im = mnist.train.images[<span class="number">1</span>]</span><br><span class="line">im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">pylab.imshow(im)</span><br><span class="line">pylab.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据打shape:'</span>,mnist.test.images.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据打shape:'</span>,mnist.validation.images.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment">#导入tensorflow库</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>]) <span class="comment"># mnist data维度 28*28=784</span></span><br><span class="line">y = tf.placeholder(tf.int32, [<span class="literal">None</span>]) <span class="comment"># 0-9 数字=&gt; 10 classes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">z= tf.matmul(x, W) + b</span><br><span class="line">cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=z))</span><br><span class="line">maxout=tf.reduce_max(z,axis=<span class="number">1</span>,keep_dims=<span class="literal">True</span>)</span><br><span class="line">W2=tf.Variable(tf.truncated_normal([<span class="number">1</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2=tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">pred=tf.nn.softmax(tf.matmul(maxout,W2)+b2)</span><br><span class="line">learning_rate = <span class="number">0.04</span></span><br><span class="line"><span class="comment"># 使用梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">training_epochs = <span class="number">200</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())<span class="comment"># Initializing OP</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启动循环开始训练</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># 遍历全部数据集</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># 显示训练中的详细信息</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print( <span class="string">" Finished!"</span>)</span><br></pre></td></tr></table></figure><p>Maxout的拟合功能很强大，但是也会有节点过多，参数过多，训练过慢的缺点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Maxout介绍&quot;&gt;&lt;a href=&quot;#Maxout介绍&quot; class=&quot;headerlink&quot; title=&quot;Maxout介绍&quot;&gt;&lt;/a&gt;Maxout介绍&lt;/h2&gt;&lt;p&gt;Maxout网络可以理解为单个神经元的扩展，主要是扩展单个神经元里面的激活函数。Maxout
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow的学习率退化和随机初始化</title>
    <link href="http://yoursite.com/2019/11/23/Tensorflow%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%80%E5%8C%96%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://yoursite.com/2019/11/23/Tensorflow的学习率退化和随机初始化/</id>
    <published>2019-11-23T03:47:08.000Z</published>
    <updated>2019-11-23T08:15:12.753Z</updated>
    
    <content type="html"><![CDATA[<h2 id="退化学习率"><a href="#退化学习率" class="headerlink" title="退化学习率"></a>退化学习率</h2><p>设置学习率的大小，是在精度和速度之间找到一个平衡。如果学习率的值比较大，则训练速度快，但结果的精度不够。如果学习率的值比较小，精度虽然提升了，但训练会花太多时间。</p><p>退化学习率又叫学习率衰减， 它的本意是希望在训练过程中对于学习率大和小的优点都能够为我们所用， 也就是当训练刚开始时使用大的学习率加快速度， 训练到一定程度后使用小的学习率来提高精度， 这时可以使用学习率衰减的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exponential_decay</span><span class="params">(learning_rate,global_step,decay_rate,staircase=False,name=None)</span></span></span><br></pre></td></tr></table></figure><p>学习率的衰减速度是由global_step和decay_steps来决定的。计算公式如下：</p><p>decayed_learning_rate=learning_rate*decay_rate^(global_step/decay_step)。staircase值默认为False。当为True，将没有衰减功能，只是使用上面的公式初始化一个学习率的值而已。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=tf.train.exponential_decay(starter_learning_rate,global_step,<span class="number">10000</span>,<span class="number">0.96</span>)</span><br></pre></td></tr></table></figure><p>这种方式定义的学习率就是退化学习率， 它的意思是当前迭代到global_step步， 学习率每一步都按照每10万步缩小到0.96%的速度衰退。  </p><p>通过增大批次处理样本的数量也可以起到退化学习率的效果。但是这种方法要求训练时的最小批次要与实际应用中的最小批次一致。一旦满足该条件，建议优先选择增大批次数量的方法。因为这样会省去一些开发量和训练中的计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">global_step=tf.Variable(<span class="number">0</span>,trainable=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#初始学习率为0.1</span></span><br><span class="line">initial_learning_rate=<span class="number">0.1</span></span><br><span class="line"><span class="comment">#每十次衰减0.9</span></span><br><span class="line">learning_rate=tf.train.exponential_decay(initial_learning_rate,global_step=global_step,</span><br><span class="line">                                        decay_steps=<span class="number">10</span>,</span><br><span class="line">                                        decay_rate=<span class="number">0.9</span>)</span><br><span class="line">opt=tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line"><span class="comment">#相当于global_step+1</span></span><br><span class="line">add_global=global_step.assign_add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  print(sess.run(learning_rate))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    g,rate=sess.run([add_global,learning_rate])</span><br><span class="line">    print(g,rate)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：  这是一种常用的训练策略， 在训练神经网络时， 通常在训练刚开始时使用较大的learning rate， 随着训练的进行， 会慢慢减小learning rate。 在使用时， 一定要把当前迭代次数global_step传进去， 否则不会有退化的功能。  </p><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在定义学习参数时，可以通过get_variable和Variable两个方式。在使用get_variable时，tf.get_variable(name,shape,initializer)，当然还有其它参数，可以自己上网找一下。参数initializer就是初始化参数。可以去下表中列出的相关函数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123153950.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123154010.png" alt=""></p><h3 id="初始化为常量"><a href="#初始化为常量" class="headerlink" title="初始化为常量"></a>初始化为常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">value = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">init = tf.constant_initializer(value)</span><br><span class="line">x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">8</span>], initializer=init)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  <span class="comment">#sess.run(tf.global_variables_initializer())</span></span><br><span class="line"><span class="comment">#print(sess.run(x))</span></span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line"><span class="comment">#[ 0.  1.  2.  3.  4.  5.  6.  7.]</span></span><br></pre></td></tr></table></figure><h3 id="初始化为正态分布"><a href="#初始化为正态分布" class="headerlink" title="初始化为正态分布"></a>初始化为正态分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_random = tf.random_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=<span class="literal">None</span>, dtype=tf.float32)</span><br><span class="line">init_truncated = tf.truncated_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=<span class="literal">None</span>, dtype=tf.float32)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_random)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_truncated)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line"> </span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br></pre></td></tr></table></figure><h3 id="初始化为均匀分布"><a href="#初始化为均匀分布" class="headerlink" title="初始化为均匀分布"></a>初始化为均匀分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">init_uniform = tf.random_uniform_initializer(minval=<span class="number">0</span>, maxval=<span class="number">10</span>, seed=<span class="literal">None</span>, dtype=tf.float32</span><br><span class="line">x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_uniform)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># [ 6.93343639  9.41196823  5.54009819  1.38017178  1.78720832  5.38881063</span></span><br><span class="line"><span class="comment">#   3.39674473  8.12443542  0.62157512  8.36026382]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;退化学习率&quot;&gt;&lt;a href=&quot;#退化学习率&quot; class=&quot;headerlink&quot; title=&quot;退化学习率&quot;&gt;&lt;/a&gt;退化学习率&lt;/h2&gt;&lt;p&gt;设置学习率的大小，是在精度和速度之间找到一个平衡。如果学习率的值比较大，则训练速度快，但结果的精度不够。如果学习率的
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的损失函数和梯度下降</title>
    <link href="http://yoursite.com/2019/11/22/Tensorflow%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://yoursite.com/2019/11/22/Tensorflow中的损失函数和梯度下降/</id>
    <published>2019-11-22T12:52:58.000Z</published>
    <updated>2019-11-23T03:34:10.889Z</updated>
    
    <content type="html"><![CDATA[<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="均值平方差"><a href="#均值平方差" class="headerlink" title="均值平方差"></a>均值平方差</h3><p>均值平方差(Mean Squared Error，MSE)，也称”均方误差”，在神经网络中主要是表达预测值与真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的期望值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122210450.png" alt=""></p><p>均方误差的值越小，表明模型越好。类似的损失算法还有均方误差RMSE(将MSE开平方)，平均绝对值误差MAD(对一个真实值与预测值相减的绝对值取平均值)。</p><p><strong>注意</strong>：在神经网络计算时，预测值要与真实值控制在同样的数据分布内，假设将预测值经过Sigmoid激活函数得到取值范围在0~1之间，那么真实值也归一化成0~1之间。</p><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>交叉熵(crossentropy)也是loss算法的一种，一般用于分类问题，表达的意思为预测输入样本属于某一类的概率。其中y代表真实值分类(0或1)，a代表预测值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122211858.png" alt=""></p><p>交叉熵也只值越小，代表预测结果越准。</p><p><strong>注意</strong>：这里用于计算的a也是通过分布同一化处理的（或者是经过Sigmoid函数激活的），取值范围0~1。</p><h3 id="损失算法的选取"><a href="#损失算法的选取" class="headerlink" title="损失算法的选取"></a>损失算法的选取</h3><p>损失函数的选取取决于输入标签数据的类型： 如果输入的是实数、 无界的值， 损失函数使用平方差； 如果输入标签是位矢量（分类标志） ， 使用交叉熵会更适合。  </p><h3 id="Tensorflow的loss函数"><a href="#Tensorflow的loss函数" class="headerlink" title="Tensorflow的loss函数"></a>Tensorflow的loss函数</h3><h4 id="均值平方差-1"><a href="#均值平方差-1" class="headerlink" title="均值平方差"></a>均值平方差</h4><p>在Tensorflow没有单独的MSE函数，不过由于公式比较简单，往往都是自己写函数。也有多种写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MSE=tf.reduce_mean(tf.pow(tf.sub(logits,outputs),<span class="number">2.0</span>))</span><br><span class="line">MSE=tf.reduce_mean(tf.square(tf.sub(logits,outputs)))</span><br><span class="line">MSE=tf.reduce_mean(tf.square(logits-outputs))</span><br></pre></td></tr></table></figure><p>logits代表标签值，outputs代表预测值。同样也可以组合其它类似loss，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rmse=tf.sqrt(tf.reduce_mean(tf.pow(tf.sub(logits,outputs),<span class="number">2.0</span>)))</span><br><span class="line">mad=tf.reduce_mean(tf.complex_abs(logits,outputs))</span><br></pre></td></tr></table></figure><h4 id="交叉熵-1"><a href="#交叉熵-1" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>在tensorflow中常见的交叉熵函数有：Sigmoid交叉熵、softmax交叉熵、Sparse交叉熵、加权Sigmoid交叉熵</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122215154.png" alt=""></p><p>当然，我们也可以像MSE那样使用自己组合的公式计算交叉熵。对于softmax后的结果logits我们可以对其使用公式-tf.reduce_sum(labels*tf.log(logis),1)，就等同于softmax_cross_entropy_with_logits得到结果。（注意有个负号）</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="softmax交叉熵"><a href="#softmax交叉熵" class="headerlink" title="softmax交叉熵"></a>softmax交叉熵</h4><p>标签是one-hot编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">labels=[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],]</span><br><span class="line">logits=[[<span class="number">2</span>,<span class="number">0.5</span>,<span class="number">6</span>],[<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">3</span>]]</span><br><span class="line"><span class="comment">#进行第一次softmax</span></span><br><span class="line">logits_scaled=tf.nn.softmax(logits)</span><br><span class="line"><span class="comment">#进行第二次softmax</span></span><br><span class="line">logits_scaled2=tf.nn.softmax(logits_scaled)</span><br><span class="line"><span class="comment">#用第一次的softmax进行交叉熵计算</span></span><br><span class="line">result1=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)</span><br><span class="line"><span class="comment">#用第二次的softmax进行交叉熵计算</span></span><br><span class="line">result2=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits_scaled)</span><br><span class="line"><span class="comment">#用自建公式实验</span></span><br><span class="line">result3=-tf.reduce_sum(labels*tf.log(logits_scaled),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  print(<span class="string">"logits_scaled："</span>,sess.run(logits_scaled))</span><br><span class="line">  print(<span class="string">"logits_scaled2"</span>,sess.run(logits_scaled2))</span><br><span class="line">  print(<span class="string">"result1："</span>,sess.run(result1))</span><br><span class="line">  print(<span class="string">"result2："</span>,sess.run(result2))</span><br><span class="line">  print(<span class="string">"result3："</span>,sess.run(result3))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123095531.png" alt=""></p><p>从结果看，logits里面的值原本加和都是大于1的，但是经过softmax之后，总和变成了1。logits中的第一个是跟标签分类相符的，第二个与标签分类不符，所以第一个的交叉熵比较小，是0.02215518。第二个交叉熵比较大，是3.09967351。</p><p><strong>总结</strong>：</p><blockquote><p>比较scaled和scaled2可以看到： 经过第二次的softmax后， 分布概率会有变化， 而scaled才是我们真实转化的softmax值。 比较rel1和rel2可以看到： 传入softmax_cross_entropy_with_logits的logits是不需要进行softmax的。 如果将softmax后的值scaled传入softmax_cross_entropy_with_ logits就相当于进行了两次的softmax转换。</p></blockquote><p>对于已经用softmax转换过的scaled，在计算loss的时候不能再使用softmax_cross_entropy_with_logits了。应该自己写一个函数，如上面代码的result3。</p><p>下面用一组总和为1但是数组中每个值都不等于0或1的数组来代替标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">labels2=[[<span class="number">0.4</span>,<span class="number">0.1</span>,<span class="number">0.5</span>],[<span class="number">0.3</span>,<span class="number">0.6</span>,<span class="number">0.1</span>]]</span><br><span class="line">result4=tf.nn.softmax_cross_entropy_with_logits(labels=labels2,logits=logits)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"result4："</span>,result4)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">result4 [<span class="number">2.1721554</span> <span class="number">2.7696736</span>]</span><br></pre></td></tr></table></figure><p>与前面的result1对比发现，标准的one-hot的结果比较明显。</p><h4 id="sparse交叉熵"><a href="#sparse交叉熵" class="headerlink" title="sparse交叉熵"></a>sparse交叉熵</h4><p>使用sparse_softmax_cross_entropy_with_logits函数的用法，他需要使用非one-hot的标签，所以要把前面的标签换成具体的数值[2,1]。PS：这个labels能不能换成[1,2]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">labels3=[<span class="number">2</span>,<span class="number">1</span>]<span class="comment">#表明labels共有3个类，0、1、2。[2,1]等价于one-hot编码的001与010</span></span><br><span class="line">result5=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels3,logits=logits)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"result5："</span>,result5)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">result5 [<span class="number">0.02215516</span> <span class="number">3.0996735</span> ]</span><br></pre></td></tr></table></figure><p>result5与result1完全一样。</p><h3 id="计算loss值"><a href="#计算loss值" class="headerlink" title="计算loss值"></a>计算loss值</h3><p>对于softmax_cross_entropy_with_logits后的结果求loss直接取均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss=tf.reduce_mean(result1)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"loss"</span>,sess.run(loss))</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="comment">#loss 1.5609143</span></span><br></pre></td></tr></table></figure><p>对于softmax后的结果，先使用-tf.reduce_sum(labels*tf.log(logits_scaled))，接着求均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss2=tf.reduce_mean(-tf.reduce_sum(labels*tf.log(logits_scaled),<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"loss2:"</span>,loss2)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="comment">#loss 1.5609144</span></span><br></pre></td></tr></table></figure><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降法是一个最优化算法， 通常也称为最速下降法， 常用于机器学习和人工智能中递归性地逼近最小偏差模型， 梯度下降的方向也就是用负梯度方向为搜索方向， 沿着梯度下降的方向求解极小值。  </p><p>在训练过程中，每次的正向传播后都会得到输出值与真实值的损失值。这个损失值越小越好，代表模型越好。于是梯度下降的算法就用在这里，帮助寻找最小的那个损失值，从而可以反推出对应的学习参数b和w，达到优化模型的效果。</p><p>常用的梯度下降方法可以分为：批量梯度下降、随机梯度下降、小批量梯度下降。</p><p>批量梯度下降：  遍历全部数据集算一次损失函数， 然后算函数对各个参数的梯度和更新梯度。 这种方法每更新一次参数， 都要把数据集里的所有样本看一遍， 计算量大， 计算速度慢， 不支持在线学习，称为batch gradient descent。</p><p>随机梯度下降：每看一个数据就算一下损失函数，然后求梯度更新参数。  这称为stochastic gradient descent， 随机梯度下降。 这个方法速度比较快， 但是收敛性能不太好， 可能在最优点附近晃来晃去， 命中不到最优点。 两次参数的更新也有可能互相抵消， 造成目标函数震荡比较剧烈  </p><p>小批量梯度下降：为了克服上面两种方法的缺点， 一般采用一种折中手段——小批的梯度下降。 这种方法把数据分为若干个批， 按批来更新参数， 这样一批中的一组数据共同决定了本次梯度的方向， 下降起来就不容易跑偏， 减少了随机性。 另一方面因为批量的样本数与整个数据集相比小了很多， 计算量也不是很大。  </p><h3 id="tensorflow中的梯度下降函数"><a href="#tensorflow中的梯度下降函数" class="headerlink" title="tensorflow中的梯度下降函数"></a>tensorflow中的梯度下降函数</h3><p>在tensorflow中是通过一个叫做Optimizer的优化器进行训练优化的。对于不同的优化器，在tensorflow会有不同的类：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123112610.png" alt=""></p><p>在训练过程中，先实例化一个优化函数，并基于一定的学习率进行梯度优化训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer=tf.train.GradientDescentOptimizer(learning_rate)</span><br></pre></td></tr></table></figure><p>接着使用minimize()操作，接着传入损失值loss到这个操作。优化器就会按照循环的次数一次次沿着loss最小值的方向优化参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;损失函数&quot;&gt;&lt;a href=&quot;#损失函数&quot; class=&quot;headerlink&quot; title=&quot;损失函数&quot;&gt;&lt;/a&gt;损失函数&lt;/h2&gt;&lt;h3 id=&quot;均值平方差&quot;&gt;&lt;a href=&quot;#均值平方差&quot; class=&quot;headerlink&quot; title=&quot;均值平方差&quot;&gt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的激活函数和分类函数</title>
    <link href="http://yoursite.com/2019/11/22/Tensorflow%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%92%8C%E5%88%86%E7%B1%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/11/22/Tensorflow中的激活函数和分类函数/</id>
    <published>2019-11-22T08:36:54.000Z</published>
    <updated>2019-11-22T12:49:46.543Z</updated>
    
    <content type="html"><![CDATA[<p>关于激活函数，我已经在一篇博客上讲解了它的常见种类和作用，详情点击<a href="[https://brickexperts.github.io/2019/09/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/](https://brickexperts.github.io/2019/09/03/激活函数/">激活函数</a>)。这篇博客一起来看下在tensorflow下的激活函数，并补充一些激活函数。顺提一下分类函数。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数的作用就是用来加入非线性因素的，以解决线性模型表达能力不足的缺陷。常用的激活函数有sigmoid，tanh，relu。</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>sigmoid在tensorflow下的对应函数为：</p><p>tf.nn.sigmoid(x,name=None)。从sigmoid的图像来看，随着x趋近正负无穷大，y对应的值越来越接近1或-1，这种情况叫做饱和。处于饱和态的激活函数意味着，当x=100和x=1000时的反映都是一样的，这样的特性转换相当于将1000大于100十倍这个信息丢失了。</p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>tanh在tensorflow下的对应函数为：</p><p>tf.nn.tanh(x,name=None)。  x取值也是从正无穷到负无穷， 但其对应的y值变为-1～1之间， 相对于Sigmoid函数有更广的值域。  但同样也拥有饱和问题。</p><h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><p>relu在tensorflow下的对应函数为：</p><p>tf.nn.relu(x,name=None)。该函数非常简单，大于0的留下，否则一律为0。relu函数运算简单，大大提升了机器的运行效率。还有tf.nn.relu6(x,name=None)，这是以6为阈值的relu函数。与relu函数类似的还有softplus函数，二者的区别是：Softplus函数会更加平滑，但是计算量很大。</p><blockquote><p>softplus的函数公式：f(x)=ln(1+e<sup>x</sup>)。在tensorflow中，Softplus函数对应的函数是tf.nn.softplus(x,name=None)</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122191433.png" alt=""></p><p>虽然ReLU函数在信号响应上有很多优势， 但这仅仅在正向传播方面。 由于其对负值的全部舍去， 因此很容易使模型输出全零从而无法再进行训练。 例如， 随机初始化的w加入值中有个值是负值， 其对应的正值输入值特征也就被全部屏蔽了， 同理， 对应负值输入值反而被激活了。 这显然不是我们想要的结果。 于是在基于ReLU的基础上又演化出了一些变种函数， 举例如下：  </p><blockquote><p>Noise relus：为max中的x加了一个高斯分布的噪声</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122192702.png" alt=""></p><p>Leaky relus：在relu的基础上，保留一部分负值，让x为负时乘a，a小于等于1。也就是Leaky relus对负信号不是一昧的拒绝，而是缩小。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122192303.png" alt=""></p><p>在TensorFlow中， Leaky relus公式没有专门的函数， 不过可以利用现有函数组成而得到：</p><p>tf.maximum(x,leak*x,name=name) #leakl为传入的参数，可以设为0.01等。</p><p>Elus：当x小于0时，做了更复杂的变换。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122192444.png" alt=""></p><p>在tensorflow中，Elus函数对应的函数，tf.nn.elu(x,name=None)</p></blockquote><h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><p>Swish函数是谷歌公司发现的一个效果更优于Relu的激活函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122194816.png" alt=""></p><p>其中&beta;为x的缩放参数，一般情况默认为1即可。在tensorflow的低版本中，没有单独的Swish函数，可以手动封装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Swish</span><span class="params">(x,beta=<span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> x*tf.nn.sigmoid(x*beta)</span><br></pre></td></tr></table></figure><h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><p>对于上面讲的激活函数，其输出值只有两种（0、1，或-1、1，0、x），而现实生活中需要对某一问题进行某种分类，这时就需要使用softmax算法。</p><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>softmax， 就是如果判断输入属于某一个类的概率大于属于其他类的概率， 那么这个类对应的值就逼近于1， 其他类的值就逼近于0。 该算法的主要应用就是多分类， 而且是互斥的， 即只能属于其中的一个类。 与sigmoid类的激活函数不同的是， 一般的激活函数只能分两类，所以可以理解成Softmax是Sigmoid类的激活函数的扩展。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122201749.png" alt=""></p><p> 把所有值用e的n次方计算出来， 求和后算每个值占的比率， 保证总和为1。一般就可以认为softmax得出的就是概率。</p><p>举个例子， 训练的模型可能推测一张包含9的图片代表数字9的概率是80%， 但是判断它是8的概率是5%（因为8和9都有上半部分相似的小圆） ，判断它代表其他数字的概率值更小。 于是取最大    概率的对应数值， 就是这个图片的分类了。 这是一个softmax回归。  </p><h3 id="常用的分类函数"><a href="#常用的分类函数" class="headerlink" title="常用的分类函数"></a>常用的分类函数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122203519.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于激活函数，我已经在一篇博客上讲解了它的常见种类和作用，详情点击&lt;a href=&quot;[https://brickexperts.github.io/2019/09/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/](https://brick
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>识别图中模糊的手写数字</title>
    <link href="http://yoursite.com/2019/11/21/%E8%AF%86%E5%88%AB%E5%9B%BE%E4%B8%AD%E6%A8%A1%E7%B3%8A%E7%9A%84%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97/"/>
    <id>http://yoursite.com/2019/11/21/识别图中模糊的手写数字/</id>
    <published>2019-11-21T08:57:06.000Z</published>
    <updated>2019-11-25T12:25:16.033Z</updated>
    
    <content type="html"><![CDATA[<p>MNIST是一个入门级的计算机视觉数据集。MNIST数据集的官网是<a href="http://yann.lecun.com/exdb/mnist/，我们可以手动下载数据集。" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/，我们可以手动下载数据集。</a></p><h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><p>除了上面的手动下载数据集，tensorflow提供了一个库，可以直接用来自动下载MNIST。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>运行上面的代码，会自动下载数据集并将文件解压到当前代码所在同级目录下的MNIST_data文件夹下。</p><p><strong>注意</strong>：代码中的one_hot=True，表示将样本标签转化为one_hot编码。解释one_hot编码，假如一共10类。0的one_hot为1000000000，1的one_hot编码为0100000000，2的one_hot编码为0010000000，等等。只有一个位是1，1所在的位置就代表第几类，从零开始数。</p><p>MNIST数据集中的图片是28x28Pixel，所以，每一幅图就是1行784列的数据，每一个值代表一个像素。</p><p>如果是黑白的图片，图片中的黑色的地方数值为0，有图案的地方数值为0~255之间的数字，代表其颜色的深度。</p><p>如果是彩色的图片，一个像素由三个值表示RGB（红，黄，蓝）。</p><h2 id="显示数据集信息"><a href="#显示数据集信息" class="headerlink" title="显示数据集信息"></a>显示数据集信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"输入数据为："</span>,mnist.train.images)</span><br><span class="line">print(<span class="string">"输入数据打印shape："</span>,mnist.train.images.shape)</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line">im=mnist.train.images[<span class="number">1</span>]</span><br><span class="line">im=im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">pylab.imshow(im)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure><p>运行代码得出结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191121201128.png" alt=""></p><p>这是一个55000行、784列的矩阵，即：这个数据集有55000张图片。</p><h2 id="MNIST数据集组成"><a href="#MNIST数据集组成" class="headerlink" title="MNIST数据集组成"></a>MNIST数据集组成</h2><p>在MNIST训练数据集中，mnist.train.images是一个形状为[55000,784]的张量。其中，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0~255之间。</p><p>MNIST数据集里包含三个数据集：训练集、测试集、验证集。训练集用于训练，测试集用于评估训练过程中的准确度，验证集用于评估最终模型的准确度。可使用以下命令查看里面的数据信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"测试集的shape："</span>,mnist.test.images.shape)</span><br><span class="line">print(<span class="string">"验证集的shape："</span>,mnist.validation.images.shape)</span><br></pre></td></tr></table></figure><p>运行完上面代码，可以发现在测试数据集里有10000条样本图片，验证集有5000个图片。</p><p>三个数据集还有分别对应的三个标签文件，用来标注每个图片上的数字是几。把图片和标签放在一起，称为“样本”。</p><p>MNIST数据集的标签是介于0～9之间的数字， 用来描述给定图片里表示的数字。标签数据是“one-hot vectors”： 一个one-hot向量，除了某一位的数字是1外， 其余各维度数字都是0。 例如， 标签0将表示为（[1， 0， 0， 0， 0， 0，0， 0， 0， 0， 0]） 。 因此， mnist.train.labels是一个[55000， 10]的数字矩阵。  </p><h2 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a>定义变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">x=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line"><span class="comment">#标签</span></span><br><span class="line">y=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>代码中的None，代表此张量的第一个维度可以是任意长度的。</p><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><h3 id="定义学习参数"><a href="#定义学习参数" class="headerlink" title="定义学习参数"></a>定义学习参数</h3><p>模型也需要权重值和偏置值，它们被统一称为学习参数。在Tensorflow，使用Variable来定义学习参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W=tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b=tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><h3 id="定义正向传播"><a href="#定义正向传播" class="headerlink" title="定义正向传播"></a>定义正向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred=tf.nn.softmax(tf.matual(x,W)+b)</span><br></pre></td></tr></table></figure><h3 id="定义反向传播"><a href="#定义反向传播" class="headerlink" title="定义反向传播"></a>定义反向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#定义超参数</span></span><br><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)</span><br></pre></td></tr></table></figure><p>首先，将正向传播生成的pred与样本标签y进行一次交叉熵的运算，然后取平均值。接着将cost作为一次正向传播的误差，通过梯度下降的优化方法找到能够使这个误差最小化的W和b。整个过程就是不断让损失值变小。因为损失值越小，才能表明输出的结果跟标签数据越接近。</p><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#要把整个训练样本集迭代25次</span></span><br><span class="line">train_epochs=<span class="number">25</span></span><br><span class="line"><span class="comment">#代表在训练过程中一次取100条数据进行训练</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line"><span class="comment">#每训练一次就把具体的中间状态显示出来</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment">#初始化op</span></span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    avg_cost=<span class="number">0</span></span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">      batch_xs,batch_ys=mnist.train.next_batch(batch_size)</span><br><span class="line">      _,c=sess.run([optimizer,cost],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">      avg_cost+=c/total_batch</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%display_step==<span class="number">0</span>:</span><br><span class="line">      print(<span class="string">"Epoch:"</span>,<span class="string">"%04d"</span> % (epoch+<span class="number">1</span>),<span class="string">"cost="</span>,<span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">  print(<span class="string">"~~~~finish~~~~"</span>)</span><br></pre></td></tr></table></figure><h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><p>测试错误率的算法是：直接判断预测的结果与真实的标签是否相同，如是相同的就表示是正确的，如是不相同的，就表示是错误的。然后将正确的个数除以总个数，得到的值即为正确率。由于是one-hot编码，这里使用了tf.argmax函数返回one-hot编码中数值为1的哪个元素的下标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试 model</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><p>在代码的两处区域加入以下代码：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191121221831.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191121221920.png" alt=""></p><h2 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"开始第二个会话"</span>)</span><br><span class="line">print(<span class="string">"Starting 2nd session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># Restore model weights from previously saved model</span></span><br><span class="line">    saver.restore(sess, model_path)</span><br><span class="line">    </span><br><span class="line">     <span class="comment"># 测试 model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    </span><br><span class="line">    output = tf.argmax(pred, <span class="number">1</span>)</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">2</span>)</span><br><span class="line">    outputval,predv = sess.run([output,pred], feed_dict=&#123;x: batch_xs&#125;)</span><br><span class="line">    print(outputval,predv,batch_ys)</span><br><span class="line"></span><br><span class="line">    im = batch_xs[<span class="number">0</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    <span class="comment">#pylab.imshow(im)</span></span><br><span class="line">    <span class="comment">#pylab.show()</span></span><br><span class="line">    plt.imshow(im)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    im = batch_xs[<span class="number">1</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    pylab.imshow(im)</span><br><span class="line">    pylab.show()</span><br></pre></td></tr></table></figure><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">x=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">y=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W=tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b=tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line">pred=tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),))</span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"><span class="comment">#模型保存</span></span><br><span class="line">saver=tf.train.Saver()</span><br><span class="line">model_path=<span class="string">"./model.ckpt"</span></span><br><span class="line">train_epochs=<span class="number">50</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment">#初始化op</span></span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    avg_cost=<span class="number">0</span></span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">      batch_xs,batch_ys=mnist.train.next_batch(batch_size)</span><br><span class="line">      <span class="comment">#c代表每一个batch_size总的损失</span></span><br><span class="line">      _,c=sess.run([optimizer,cost],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">      avg_cost+=c/total_batch</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%display_step==<span class="number">0</span>:</span><br><span class="line">      print(<span class="string">"Epoch:"</span>,<span class="string">"%04d"</span> % (epoch+<span class="number">1</span>),<span class="string">"cost="</span>,<span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">  print(<span class="string">"~~~~finish~~~~"</span>)</span><br><span class="line">  <span class="comment"># 测试 model</span></span><br><span class="line">  correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">  <span class="comment"># 计算准确率，cast函数用于类型转换</span></span><br><span class="line">  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">  <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">  <span class="comment">#模型保存</span></span><br><span class="line">  save_path = saver.save(sess, model_path)</span><br><span class="line">  print(<span class="string">"Model saved in file: %s"</span> % save_path)</span><br><span class="line">print(<span class="string">"开始第二个会话"</span>)</span><br><span class="line">print(<span class="string">"Starting 2nd session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># Restore model weights from previously saved model</span></span><br><span class="line">    saver.restore(sess, model_path)</span><br><span class="line">    </span><br><span class="line">     <span class="comment"># 测试 model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    </span><br><span class="line">    output = tf.argmax(pred, <span class="number">1</span>)</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">2</span>)</span><br><span class="line">    outputval,predv = sess.run([output,pred], feed_dict=&#123;x: batch_xs&#125;)</span><br><span class="line">    print(outputval,predv,batch_ys)</span><br><span class="line"></span><br><span class="line">    im = batch_xs[<span class="number">0</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    <span class="comment">#pylab.imshow(im)</span></span><br><span class="line">    <span class="comment">#pylab.show()</span></span><br><span class="line">    plt.imshow(im)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    im = batch_xs[<span class="number">1</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    pylab.imshow(im)</span><br><span class="line">    pylab.show()</span><br></pre></td></tr></table></figure><p>第70、71行的代码可以用68、69行的代码代替。pylab库结合了pyplot模块和numpy模块。</p><h2 id="把模糊数字换成我们自己的图片"><a href="#把模糊数字换成我们自己的图片" class="headerlink" title="把模糊数字换成我们自己的图片"></a>把模糊数字换成我们自己的图片</h2><p>研究以下，补。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MNIST是一个入门级的计算机视觉数据集。MNIST数据集的官网是&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/，我们可以手动下载数据集。&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://yann.lecu
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="识别数字" scheme="http://yoursite.com/tags/%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的eval用法</title>
    <link href="http://yoursite.com/2019/11/20/tensorflow%E7%9A%84eval%E7%94%A8%E6%B3%95/"/>
    <id>http://yoursite.com/2019/11/20/tensorflow的eval用法/</id>
    <published>2019-11-20T13:23:03.000Z</published>
    <updated>2019-11-20T13:26:25.219Z</updated>
    
    <content type="html"><![CDATA[<p>eval()其实就是tf.Tensor的session.run()的另一种写法。</p><p>1、eval()也是启动计算的一种方式。基于tensorflow基本原理，首先需要定义图，然后计算图，其中计算图的函数有常见的run()函数，如sess.run()，eval()也是类似。</p><p>2、eval()只能用于tf.tensor类对象，也就是有输出的operaton。没有输出的operation，使用session.run()。<br>t.eval() 等价于 tf.get_default_session().run(t)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;eval()其实就是tf.Tensor的session.run()的另一种写法。&lt;/p&gt;
&lt;p&gt;1、eval()也是启动计算的一种方式。基于tensorflow基本原理，首先需要定义图，然后计算图，其中计算图的函数有常见的run()函数，如sess.run()，eval()
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下安装qt5</title>
    <link href="http://yoursite.com/2019/11/11/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85qt5/"/>
    <id>http://yoursite.com/2019/11/11/ubuntu下安装qt5/</id>
    <published>2019-11-11T11:45:29.000Z</published>
    <updated>2019-11-19T06:49:11.077Z</updated>
    
    <content type="html"><![CDATA[<p>在官网下载相关文件，<a href="http://download.qt.io/archive/qt/" target="_blank" rel="noopener">官网</a></p><p>我下载的是qt5.11.1版本。点击进入，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119142332.png" alt=""></p><p>下载安装程序时需注意，只能下载qt-opensource-linux-x64-5.11.1.run，因为只有这个是在linux环境下的安装程序。新建一个名为Qt5.11.1的文件夹，将这个文件放进去。</p><p>在文件目录下打开终端，输入./run进行安装。接着点击next。可能会有要求填邮箱。点击skip跳过输入邮箱步骤。接下来更改程序安装目录，按照自己的需求修改。之后在select components步骤时，建议Select All。在License Agreement步骤时，选择Qt Installer LGPL Agreement。接着等待。进入安装目录，发现如下内容即为成功安装。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119144311.png" alt=""></p><p>此时安装完后，仍就无法运行。我们需要安装相应的工具，以使得程序正常运行。</p><blockquote><p>sudo apt-get install gcc g++</p><p>sudo apt-get install libqt4-dev</p><p>sudo apt-get install build-essential</p></blockquote><p>以上内容全部安装完毕，进入目录下的Tools/Qtcreator/bin目录下，在终端输入./qtcreator</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119144742.png" alt=""></p><p>接下来的使用就要靠各位读者自己摸索和学习了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在官网下载相关文件，&lt;a href=&quot;http://download.qt.io/archive/qt/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我下载的是qt5.11.1版本。点击进入，&lt;/p&gt;
&lt;p&gt;&lt;img src=
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>用darknet训练自己的数据</title>
    <link href="http://yoursite.com/2019/11/09/%E7%94%A8darknet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2019/11/09/用darknet训练自己的数据/</id>
    <published>2019-11-09T13:53:32.000Z</published>
    <updated>2019-11-19T07:05:43.063Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客采用darknet训练自己的数据，那么在训练自己的数据之前，我们得先拥有自己数据，怎么得到呢?只能自己做了</p><h2 id="安装labelImg"><a href="#安装labelImg" class="headerlink" title="安装labelImg"></a>安装labelImg</h2><p>我用过精灵标注助手和labelImg两款标注工具，标注后得出得XML不一样。本篇博客采用labelImg工具标注图片。</p><p>环境：python3、ubuntu18.04</p><p><code>sudo apt-get install pyqt5-dev-tools</code><br><code>sudo pip3 install lxml</code></p><p>下载labelImg源码</p><p><code>git clone https://github.com/tzutalin/labelImg.git</code></p><p>进入labelImg目录下</p><p>cd labelImg </p><p>再make qt5py3，建议不要make all。出现下面这种结果即为成功</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110101542.png" alt=""></p><p>然后python3 labelImg.py。出现界面即为成功。woc，这是我最顺利的一次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110101733.png" alt=""></p><h2 id="制作自己的数据集"><a href="#制作自己的数据集" class="headerlink" title="制作自己的数据集"></a>制作自己的数据集</h2><p>首先进入darknet目录下，再目录下新建文件夹VOC2019，并在VOC2019下新建Annotations，ImageSets，JPEGImages三个文件夹。在ImageSets新建Main文件夹。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110104912.png" alt=""></p><p>将自己的数据集图片放到JPEGImages目录下，将标注文件放到Annotations目录下。接着开始标注数据。过程就随便说以下。[Open Dir]或Ctrl+u选择要标注的图片所在的根目录，[Create\nRectBox]或w开始标注，鼠标框选目标区域后选择对应的标签类别,按空格或Ctrl+s保存，[Next Image]或d切换到下一张图片，标注错误的选框可选中后按[Delete]删除。要注意的是，如果不是使用原有的目标检测物体的类别，我们要打开data/predefined_classes.txt，修改默认类别为要检测的类别。</p><p>接着再VOC2019下新建test.py文件，将以下代码拷贝进去。在ImageSets的Maxin文件夹下将生成四个文件：train.txt，val.txt，test.txt，trainval.txt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">trainval_percent = <span class="number">0.1</span></span><br><span class="line">train_percent = <span class="number">0.9</span></span><br><span class="line">xmlfilepath = <span class="string">'Annotations'</span></span><br><span class="line">txtsavepath = <span class="string">'ImageSets\Main'</span></span><br><span class="line">total_xml = os.listdir(xmlfilepath)</span><br><span class="line"></span><br><span class="line">num = len(total_xml)</span><br><span class="line">list = range(num)</span><br><span class="line">tv = int(num * trainval_percent)</span><br><span class="line">tr = int(tv * train_percent)</span><br><span class="line">trainval = random.sample(list, tv)</span><br><span class="line">train = random.sample(trainval, tr)</span><br><span class="line"></span><br><span class="line">ftrainval = open(<span class="string">'ImageSets/Main/trainval.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">ftest = open(<span class="string">'ImageSets/Main/test.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">ftrain = open(<span class="string">'ImageSets/Main/train.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">fval = open(<span class="string">'ImageSets/Main/val.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">    name = total_xml[i][:<span class="number">-4</span>] + <span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">in</span> trainval:</span><br><span class="line">        ftrainval.write(name)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> train:</span><br><span class="line">            ftest.write(name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fval.write(name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ftrain.write(name)</span><br><span class="line"></span><br><span class="line">ftrainval.close()</span><br><span class="line">ftrain.close()</span><br><span class="line">fval.close()</span><br><span class="line">ftest.close()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110123328.png" alt=""></p><p>YOLOV3的label标注的一行五个数分别代表类别（从 0 开始编号）， BoundingBox 中心 X 坐标，中心 Y 坐标，宽，高。这些坐标都是 0～1 的相对坐标。和我们刚才标注的label不同，因此我们需要下面的py文件帮我们转换label。</p><p><code>wget  https://pjreddie.com/media/files/voc_label.py</code></p><p>也可以在windows下好了拷到ubuntu下。总之把这个文件放到darknet文件夹下。打开voc_label.py文件，修改sets和classes。sets如下，classes根据自己的类别需要修改。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110222155.png" alt=""></p><p>打开终端输入<code>python voc_label.py</code>，于是在当前目录生成三个txt文件2019_train.txt，2019_val.txt，2019_test.txt。在VOCdevkit文件夹下的VOC2019也会多生成一个文件夹labels。点开里面的文件就会发现以及转化成YOLOv3需要的格式了。数据集的制作完成，bingo！！！</p><h2 id="局部修改"><a href="#局部修改" class="headerlink" title="局部修改"></a>局部修改</h2><p>1、打开darknet下的cfg文件夹，修改voc.data。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110222652.png" alt=""></p><p>根据自己的需要修改classes类别个数，train和valid的地址。names和backup不用修改。</p><p>2、修改data/voc.names和coco.names。打开对应的文件发现都是原本数据集里的类别，改成自己需求的类别就行。</p><p>3、修改参数文件cfg/yolov3-voc.cfg，用ctrl+f搜 yolo, 总共会搜出3个含有yolo的地方。每个地方都必须要改2处， filters：3*（5+len（classes））和classes类别数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110223739.png" alt=""></p><p>可修改：random，原本是1，显存小改为0。（是否要多尺度输出。）</p><h2 id="报错-amp-训练"><a href="#报错-amp-训练" class="headerlink" title="报错&amp;训练"></a>报错&amp;训练</h2><p>首先下载darknet53的预训练模型：</p><p><code>wget https://pjreddie.com/media/files/darknet53.conv.74</code></p><p>开始训练：</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code></p><p>你以为就这样结束了吗？我就知道没怎么简单。又报错了，报错信息如下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110144312.png" alt=""></p><p>检查文件和路径，完全正确。</p><p>上网找了解决方案，如下</p><p>下载一个notepad++，打开文件。</p><p>选择 视图 -&gt; 显示符号 -&gt; 显示所有符号；</p><p>选择 编辑 -&gt; 文档格式转换 -&gt; 转换为UNIX（LF）格式；</p><p>转换完成后的格式如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119145931.png" alt=""></p><p>注：</p><p>1.注意检查最后一行是否有LF标志。</p><p>2.为保证不出错，将所有训练过程中使用到的相关文件都修改。</p><p>我使用了上面的方法，发现我的文件格式本来就是对的。不需要改。那是什么问题呢？后来和一位同学一起瞎改cfg目录下voc.data文件，将train和valid的路径改成如下这样：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119150418.png" alt=""></p><p>才开始运行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇博客采用darknet训练自己的数据，那么在训练自己的数据之前，我们得先拥有自己数据，怎么得到呢?只能自己做了&lt;/p&gt;
&lt;h2 id=&quot;安装labelImg&quot;&gt;&lt;a href=&quot;#安装labelImg&quot; class=&quot;headerlink&quot; title=&quot;安装label
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="darknet" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/darknet/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下安装darknet</title>
    <link href="http://yoursite.com/2019/11/09/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85darknet/"/>
    <id>http://yoursite.com/2019/11/09/ubuntu下安装darknet/</id>
    <published>2019-11-09T12:36:33.000Z</published>
    <updated>2019-11-19T04:57:42.547Z</updated>
    
    <content type="html"><![CDATA[<p>首先下载源码</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/pjreddie/darknet.git</span><br><span class="line">cd darknet</span><br></pre></td></tr></table></figure><p>进入darknet目录后，打开Makefile。用到那个将对应的0改为1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109204621.png" alt=""></p><p>比如我没有GPU，就不用修改GPU为1。但我用到了opencv，将opencv的0改为1。</p><p>编译源码</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">make</span></span><br></pre></td></tr></table></figure><p>测试是否安装成功</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">./darknet</span></span><br></pre></td></tr></table></figure><p>此时看到如下信息即为安装成功。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109205410.png" alt=""></p><p>我在执行这步的时候报错了，报错信息如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109205513.png" alt=""></p><p>解决方案如下，在终端执行下面命令：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="string">/bin/bash</span> -c '<span class="keyword">echo</span> <span class="string">"/usr/local/lib"</span> &gt; <span class="string">/etc/ld.so.conf.d/opencv.conf</span>'</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p>安装成功后，可以先下载预训练模型测试效果。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> wget https:<span class="comment">//pjreddie.com/media/files/yolov3.weights </span></span><br><span class="line">./darknet detect cfg/yolov3<span class="selector-class">.cfg</span> yolov3<span class="selector-class">.weights</span> data/dog.jpg</span><br></pre></td></tr></table></figure><p>我在这里又报错了，报错信息如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109211403.png" alt=""></p><p>解决方案：sudo apt-get install libcanberra-gtk-module</p><p>再运行一次</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109212322.png" alt=""></p><p>可以看到YOLO的detection图。到这里，YOLOV3已经走通了，是时候加入自己的数据了。 请看下回分解。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先下载源码&lt;/p&gt;
&lt;figure class=&quot;highlight crmsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="darknet" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/darknet/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下编译安装opencv</title>
    <link href="http://yoursite.com/2019/11/09/ubuntu%E4%B8%8B%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85opencv/"/>
    <id>http://yoursite.com/2019/11/09/ubuntu下编译安装opencv/</id>
    <published>2019-11-09T11:37:17.000Z</published>
    <updated>2019-11-19T05:21:00.124Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客使用的ubuntu版本是18.04和opencv3.2</p><p>首先安装CMAKE</p><p><code>sudo apt-get install cmake</code></p><p>接着安装一些依赖项</p><p><code>sudo apt-get install build-essential pkg-config</code></p><p>安装视频I/O包：</p><p><code>sudo apt-get install libgtk2.0-dev  libavcodec-dev libavformat-dev libswscale-dev</code></p><p>安装gtk2.0：</p><p><code>sudo apt-get install libgtk2.0-dev</code></p><p>安装常用图像工具包：</p><p><code>sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev libdc1394-22-dev</code></p><p>如果没有报错，那是最好的。但是我报错了。报错信息如下：</p><p><code>E: Package &#39;libpng12-dev&#39; has no installation candidate</code><br><code>E: Unable to locate package libjasper-dev</code></p><p>上网找了解决方案，首先先解决第一个错：</p><p><code>libpng12-dev</code>在Ubuntu16.04之后就被丢弃了，所以放弃用这个吧。把 <code>libpng12-dev</code> 换成 <code>libpng-dev</code> 就行了</p><p>接着是第二个错误：</p><p><code>sudo add-apt-repository &quot;deb http://security.ubuntu.com/ubuntu xenial-security main&quot;</code><br><code>sudo apt update</code><br><code>sudo apt install libjasper1 libjasper-dev</code></p><p>然后从官网下载源码，直接git clone。也可以从windows拷到虚拟机（要装VM tools）。<a href="https://github.com/opencv/opencv/tree/3.2.0" target="_blank" rel="noopener">网址</a></p><p><code>git clone https://github.com/opencv/opencv.git</code></p><p>在解压后的文件夹中新建build文件夹，用来存放编译文件。</p><p><code>mkdir build</code><br><code>cd build</code></p><p>在电脑上有多个版本的python时，可以通过-D PYTHON_DEFAULT_EXECUTABLE=$(which python3)来确定安装在哪个版本python上。</p><p><code>cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON_DEFAULT_EXECUTABLE=​\$(which python3) ..</code></p><p>如果执行上面代码没有问题，直接make。接着再执行sudo make install。</p><p>我又报错了。。。。报错信息如下：</p><p> — ICV: Downloading ippicv_linux_20151201.tgz…<br>CMake Error at 3rdparty/ippicv/downloader.cmake:73 (file):<br>file DOWNLOAD HASH mismatch</p><p>for file: [/root/library/opencv/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/ippicv_linux_20151201.tgz]<br>expected hash: [808b791a6eac9ed78d32a7666804320e]<br>actual hash: [d41d8cd98f00b204e9800998ecf8427e]<br>status: [1;”Unsupported protocol”]</p><p>Call Stack (most recent call first):<br>3rdparty/ippicv/downloader.cmake:110 (_icv_downloader)<br>cmake/OpenCVFindIPP.cmake:243 (include)<br>cmake/OpenCVFindLibsPerf.cmake:37 (include)<br>CMakeLists.txt:558 (include)</p><p>CMake Error at 3rdparty/ippicv/downloader.cmake:77 (message):<br>ICV: Failed to download ICV package: ippicv_linux_20151201.tgz.<br>Status=1;”Unsupported protocol”<br>Call Stack (most recent call first):<br>3rdparty/ippicv/downloader.cmake:110 (_icv_downloader)<br>cmake/OpenCVFindIPP.cmake:243 (include)<br>cmake/OpenCVFindLibsPerf.cmake:37 (include)<br>CMakeLists.txt:558 (include)</p><p>— Configuring incomplete, errors occurred!<br>See also “/root/library/opencv/opencv-3.2.0/build/CMakeFiles/CMakeOutput.log”.<br>See also “/root/library/opencv/opencv-3.2.0/build/CMakeFiles/CMakeError.log”. </p><p>百度一下，这是因为我们在编译opencv的时候需要下载ippicv_linux_20151201.tgz，但是由于网络的原因，经常下载失败。解决方案：</p><p>手动下载ippicv_linux_20151201.tgz，网上都有资源。读者找一下。接着放入相关路径，我的路径是</p><p>home/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/。读者根据自己的路径放。</p><p>在重新执行cmake。</p><p>接着执行完cmake后，make编译，sudo make install 安装。</p><p>到这里还没装完我们要编译的文件，还有一些模块保留在opencv_contrib的资源库中。所以这个我们也要编译。</p><p>将opencv_contrib下到build的同级目录下，在build目录下打开终端或者在终端进入build目录，</p><p><code>cd  build</code><br><code>cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON_DEFAULT_EXECUTABLE=$(which python3) -D OPENCV_EXTRA_MODULES_PATH=../opencv_contrib-3.2.0/modules/ ..</code><br><code>make</code><br><code>sudo make install</code></p><p>在终端运行python3，import cv2。没有报错，opencv安装就成功了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109203044.png" alt=""></p><p>我装过好几次opencv，这是我最顺的一次。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇博客使用的ubuntu版本是18.04和opencv3.2&lt;/p&gt;
&lt;p&gt;首先安装CMAKE&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo apt-get install cmake&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;接着安装一些依赖项&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo apt-ge
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下装python3和库</title>
    <link href="http://yoursite.com/2019/11/09/ubuntu%E4%B8%8B%E8%A3%85python3/"/>
    <id>http://yoursite.com/2019/11/09/ubuntu下装python3/</id>
    <published>2019-11-09T02:35:36.000Z</published>
    <updated>2019-11-19T05:52:22.773Z</updated>
    
    <content type="html"><![CDATA[<p>ubuntu下的环境配得我要吐了，全都是坑。一定要写博客避坑。后面还有编译opencv、tensorflow等环境。</p><p>本片博客安装的python版本是python3.5。<strong>不用卸载python2，不用卸载python2，不用卸载python2</strong></p><p>首先更新软件包：</p><p><code>sudo apt-get update</code></p><p>接着执行一下命令：</p><p><code>sudo apt-get install python3.5</code></p><p>安装pip3：</p><p><code>sudo apt-get install python3-pip</code></p><p>在这里我就报错了：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109115010.png" alt=""></p><p>上网找了解决方案：</p><p>第一种情况：进程中存在与apt相关的正在运行的进程</p><p>首先检查是否在运行apt，apt-get相关的进程。</p><p> <code>ps aux | grep -i apt</code> </p><p>如果存在与apt相关的正在运行的进程，kill掉</p><p><code>sudo kill -9 &lt;进程号&gt;</code></p><p>或者简单粗暴的直接kill掉：</p><p><code>sudo killall apt apt-get</code></p><p>再执行一次<code>sudo apt-get install python3-pip</code>。如果这还不行，那就是第二种情况了</p><p>第二种情况：</p><p>产生错误的根本原因是lock file。 loack file用于防止两个或多个进程使用相同的数据。 当运行apt或apt-commands时，它会在几个地方创建lock files。 当前一个apt命令未正确终止时，lock file未被删除，因此它们会阻止任何新的apt / apt-get命令实例，比如正在执行apt-get upgrade，在执行过程中直接ctrl+c取消了该操作，很有可能就会造成这种情况。要解决此问题，首先要删除lock file。</p><p>首先使用lsof命令获取持有lock file的进程的ID：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109121205.png" alt=""></p><p>如果三个命令都没有返回值，则说明没有正在运行的进程。如果返回了相应的进程，则需要kill掉。</p><p>接着删除所有的lock file：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109121413.png" alt=""></p><p>最后重新配置一下dpkg：</p><p><code>sudo dpkg --configure -a</code></p><p>到了这一步，没有报错的话，完事大吉。再执行<code>sudo apt-get python3-pip</code></p><p>但是，屋漏偏逢连阴雨。太难了。又报错了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109121954.png" alt=""></p><p>接着找出正在锁定lock file的进程：</p><p><code>lsof /var/lib/dpkg/lock-frontend</code></p><p>如果上述命令返回进程，kill掉输出的进程。</p><p><code>sudo kill -9 进程号</code></p><p>删除lock file 并重新配置dpkg：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /var/<span class="class"><span class="keyword">lib</span>/<span class="title">dpkg</span>/<span class="title">lock</span>-<span class="title">frontend</span></span></span><br><span class="line">sudo dpkg --configure -a</span><br></pre></td></tr></table></figure><p>再重新配置一下dpkg。</p><p>后面几步的命令集合：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109124103.png" alt=""></p><p>到了这里，就全部完成了。</p><p>接下来安装一些库：</p><p>先安装build依赖包：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109132649.png" alt=""></p><p>接着就可以安装python库了。</p><p><code>sudo pip3 install numpy</code></p><p>等等一些库</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ubuntu下的环境配得我要吐了，全都是坑。一定要写博客避坑。后面还有编译opencv、tensorflow等环境。&lt;/p&gt;
&lt;p&gt;本片博客安装的python版本是python3.5。&lt;strong&gt;不用卸载python2，不用卸载python2，不用卸载python2&lt;/
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Yolo算法实战</title>
    <link href="http://yoursite.com/2019/10/31/Yolo%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2019/10/31/Yolo算法实战/</id>
    <published>2019-10-31T12:27:47.000Z</published>
    <updated>2019-11-19T05:00:39.562Z</updated>
    
    <content type="html"><![CDATA[<h2 id="coco介绍"><a href="#coco介绍" class="headerlink" title="coco介绍"></a>coco介绍</h2><p>本次实战采用coco数据集。这个数据集是由微软团队提供的。<a href="http://cocodataset.org/#download" target="_blank" rel="noopener">下载网址</a></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031203750.png" alt=""></p><p>annotations是采用json格式标注的，包含三个信息：object instances(物体当前的实例信息)、object keypoints(关键点信息)、image caption(图像信息)</p><p>下图是标注文件的整体结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031210251.png" alt=""></p><p>info是基本信息，image是图像信息，annotations是针对于图像数据的标注信息。下图是除了annotations字段其它字段展开后的具体信息：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031210505.png" alt=""></p><p>我们要具体关注annotations的信息。annotations信息如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031211132.png" alt=""></p><p>对于annotations，如果我们标注当前的图片为单个物体或多个物体时我们需要修改iscrowd和segmentation。</p><p>单个object时：</p><blockquote><p>iscrowd=0</p><p>segmentation=polygon</p></blockquote><p>多个objects时：</p><blockquote><p>iscrowd=1</p><p>segmentation=RLE</p></blockquote><h2 id="检测模型的搭建"><a href="#检测模型的搭建" class="headerlink" title="检测模型的搭建"></a>检测模型的搭建</h2><h3 id="Darknet的搭建"><a href="#Darknet的搭建" class="headerlink" title="Darknet的搭建"></a>Darknet的搭建</h3><p>Darknet是一个较为轻型的完全基于C与CUDA的开源深度学习框架。支持CPU和GPU两种计算方式。容易安装，且没有任何依赖项。</p><p> 在windows上我是看这篇博客配好darknet的。<a href="https://blog.csdn.net/lvsehaiyang1993/article/details/81032826" target="_blank" rel="noopener">https://blog.csdn.net/lvsehaiyang1993/article/details/81032826</a> </p><p>注意在使用darknet的时候最后如果出现Couldn’t open file: D:/darknet/data/coco.names。重新下一次zip文件、解压再编译一次。博客中说会有predictions.png文件，我并没有找到。百度是说，电脑内存不够运算。</p><p>后面训练的时候，有报错</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191103104157.png" alt=""></p><p>在网上没有找到解决方案。所以更换了系统再配一次。</p><p>在ubuntu上我配了一次。配得我都吐了，最后还把opencv装到python2上。过程不写了，网上都是。也都不是。坑踩踩就好了。我的博客有教程</p><h3 id="Darknet解读"><a href="#Darknet解读" class="headerlink" title="Darknet解读"></a>Darknet解读</h3><p>下面的部分图片是windows。</p><p>Darknet文件结构：</p><p>src、include、obj：存放了darknet的源码和编译后的文件</p><p>cfg：存放了作者提供好的各种各样的网络配置文件</p><p>打开cfg文件夹下的yolov3.cfg文件，下图网络配置信息的含义。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102204940.png" alt=""></p><p>接下去的就是主干网络。一组中括号加上网络层的名字定义当前的网络属于哪一层。接着是训练过程中用到的信息。然后就是中括号加上convolutional定义卷积层和卷积层的参数，同样定义了池化层和池化层参数。略过，我们直接看Yolo层</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102211807.png" alt=""></p><p>接着打开coco.data文件，配置如下：</p><p>在windows下，我没有找到train和valid两个文件列表。linux也没有找到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102213118.png" alt=""></p><p>data：存放了接下来用到的数据</p><p>script：存放了一些脚本</p><p>python：存放了darknet编译出来针对于python的接口</p><p>examples：存放了我们可能会用到的函数接口</p><p>backup：存放了模型训练时中间结果。比如说，在训练1000次的时候存一个model，10000次的时候存一个model。这个训练多少次保存一次模型，也是可以修改的。在examples文件夹下的detector.c文件的138行修改。<img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102164144.png" alt=""></p><p><strong>注意：每次修改darknet配置，都要重新编译。</strong></p><h3 id="Darknet的使用"><a href="#Darknet的使用" class="headerlink" title="Darknet的使用"></a>Darknet的使用</h3><p>安装完darknet后，在终端运行./darknet，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191103191816.png" alt=""></p><p>在window上运行darknet就行了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102164403.png" alt=""></p><p>这句话告诉我们调用darknet来进行模型训练，我们需要采用的格式为./darknet 函数来完成调用。</p><p>在ubuntu进行数据增强：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191103192537.png" alt=""></p><p>在windows上显示不出来。我怀疑是因为我的windows上没有编译运行opencv，编译过后应该可以运行的。大家可以试一下。</p><p>训练命令：</p><p>./darknet detector train cfg/coco.data  cfg/yolov3.cfg  </p><p>采用预训练的模型：</p><p>./darknet detector train cfg/coco.data  cfg/yolov3.cfg  cfg/yolov3_20000.weights</p><p>测试命令：</p><p>./darknet detector test cfg/coco.data  cfg/yolov3.cfg  backup/yolov3_20000.weights data/giraffe.jpg -thresh 0.4</p><p>-thresh用于阈值筛选</p><p>在example下，darknet有提供python的接口。我们也可以利用这个接口完成模型测试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;coco介绍&quot;&gt;&lt;a href=&quot;#coco介绍&quot; class=&quot;headerlink&quot; title=&quot;coco介绍&quot;&gt;&lt;/a&gt;coco介绍&lt;/h2&gt;&lt;p&gt;本次实战采用coco数据集。这个数据集是由微软团队提供的。&lt;a href=&quot;http://cocodatas
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="Yolo" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/Yolo/"/>
    
    
      <category term="Yolo算法" scheme="http://yoursite.com/tags/Yolo%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Yolo系列算法</title>
    <link href="http://yoursite.com/2019/10/29/Yolo-%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/10/29/Yolo-系列算法/</id>
    <published>2019-10-29T01:50:12.000Z</published>
    <updated>2019-11-19T05:57:59.823Z</updated>
    
    <content type="html"><![CDATA[<p>对于目标检测，我们最多到底能检测多少个类别呢？对于Yolo来说，是9000个。这是非常厉害的，所以，接下来看看Yolo的三代算法Yolo v1、Yolo v2、Yolo v3。</p><p>目标检测经历了一个高度的符合人类的直觉的过程。既需要识别出目标的位置，将图片划分成小图片扔进算法中去，当算法认为某物体在这个小区域上之时，那么检测完成。那我们就认为这个物体在这个小图片上了。而这个思路，正是比较早期的目标检测思路，比如R-CNN。后来的Fast R-CNN，Faster R-CNN虽有改进，比如不再是将图片一块块的传进CNN提取特征，而是整体放进CNN提取特征图后，再做进一步处理，但依旧是整体流程分为 ‘区域提取’和‘目标分类’两部分（two-stage），这样做的一个特点是虽然确保了精度，但速度非常慢。而Yolo将物体检测任务当作一个regression（回归）问题来处理，每张图像只需要“看一眼”就能得出图像中都有哪些物体和这些物体的位置。其实Yolo展开就是you only look once。Yolo是One-stage算法。</p><h2 id="Yolo-v1"><a href="#Yolo-v1" class="headerlink" title="Yolo v1"></a>Yolo v1</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>YOLO v1的核心思想在于将目标检测作为回归问题解决 ，YOLO v1首先会把原始图片放缩到448×448的尺寸，放缩到这个尺寸是为了后面整除来的方便。然后将图片划分成SxS个区域，注意这个区域的概念不同于上文提及将图片划分成N个区域扔进算法的区域不同。上文提及的区域是将图片进行剪裁，或者说把图片的某个局部的像素输入算法中，而这里的划分区域，只的是逻辑上的划分。 </p><p> 如果一个对象的中心落在某个单元格上，那么这个单元格负责预测这个物体。每个单元格需要预测B(超参数)个边界框（bbox）值(bbox值包括坐标和宽高)，同时为每个bbox值预测一个置信度(confidence scores)。此后以每个单元格为单位进行预测分析。</p><p>这个置信度并不只是该边界框是待检测目标的概率，而是该边界框是待检测目标的概率乘上该边界框和真实位置的IoU（框之间的交集除以并集）的积。通过乘上这个交并比，反映出该边界框预测位置的精度。如下式所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029112732.png" alt=""></p><p> 每个边界框对应于5个输出，分别是x，y，w，h和置信度。其中x，y代表边界框的中心离开其所在网格单元格边界的偏移。w，h代表边界框真实宽高相对于整幅图像的比例。x，y，w，h这几个参数都已经被限制到了区间[0,1]上。除此以外，每个单元格还产生C个概率(有多少类物体，C就是多少)。<strong>注意，我们不管B的大小，每个单元格只产生一组这样的概率。</strong> </p><p>然后我们让每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score: </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029143235.png" alt=""></p><p>P(Class<sub>i</sub>)就是每个网络预测的类别信息。 这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。</p><h3 id="网络模型架构"><a href="#网络模型架构" class="headerlink" title="网络模型架构"></a>网络模型架构</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029143846.png" alt=""></p><p> 网络结构借鉴了 GoogLeNet 。24个卷积层，2个全连接层。 </p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>YOLO v1全部使用了均方差（mean squared error）作为损失（loss）函数。由三部分组成：坐标误差、IOU误差和分类误差。</p><p>考虑到每种loss的贡献率，YOLO v1给坐标误差（coordErr）设置权重λcoord=5。在计算IoU误差时，包含物体的格子与不包含物体的格子（此处的‘包含’是指存在一个物体，它的中心坐标落入到格子内），二者的IOU误差对网络loss的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的置信度值近似为0，变相放大了包含物体的格子的置信度误差，在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用λnoobj（置信度误差）=0.5修正iouErr。</p><p> 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（w和h）进行求平方根来改进这个问题，但并不能完全解决这个问题。 </p><p>所以，Loss计算公式：                                                              PS：连数学符号都没认全，我太难了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029145518.png" alt=""></p><p>后面看别人的博客，才知道公式意思如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029151345.png" alt=""></p><p>在激活函数上：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029144700.png" alt=""></p><p> 在最后一层使用的是标准的线性激活函数，其他的层都使用leaky rectified 线性激活函数。 </p><h2 id="Yolo-v2-Yolo-9000"><a href="#Yolo-v2-Yolo-9000" class="headerlink" title="Yolo v2/Yolo 9000"></a>Yolo v2/Yolo 9000</h2><p>YOLO v1对于bounding box的定位不是很好，在精度上比同类网络还有一定的差距，所以YOLOv2对于速度和精度做了很大的优化，并且吸收了同类网络的优点，一步步做出尝试。</p><p>YOLO v2在v1基础上做出改进后提出。其受到Faster RCNN方法的启发，引入了anchor（先验框）。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中。并且修改了网络结构，去掉了全连接层，改成了全卷积结构。在训练时引入了世界树（WordTree）结构，将检测和分类问题做成了一个统一的框架，并且提出了一种层次性联合训练方法，将ImageNet分类数据集和COCO检测数据集同时对模型训练。</p><h3 id="预测更准确"><a href="#预测更准确" class="headerlink" title="预测更准确"></a>预测更准确</h3><h4 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h4><p>YOLOv2对每批数据都做了一个归一化预处理。通过在每一个卷积层后添加batch normalization，极大的改善了收敛速度同时减少了对其它正则方法的依赖（Yolo v2不在使用dropout），使得mAP获得了提升。（mAP：平均精度均值（mean Average Precision））</p><p> 通常，一次训练会输入一批样本（batch）进入神经网络。批规一化在神经网络的每一层，在网络（线性变换）输出后和激活函数（非线性变换）之前增加一个批归一化层（BN），BN层进行如下变换：①对该批样本的各特征量（对于中间层来说，就是每一个神经元）分别进行归一化处理，分别使每个特征的数据分布变换为均值0，方差1。从而使得每一批训练样本在每一层都有类似的分布。这一变换不需要引入额外的参数。②对上一步的输出再做一次线性变换，假设上一步的输出为Z，则Z1=γZ + β。这里γ、β是可以训练的参数。增加这一变换是因为上一步骤中强制改变了特征数据的分布，可能影响了原有数据的信息表达能力。增加的线性变换使其有机会恢复其原本的信息。 </p><h4 id="使用高分辨率图像"><a href="#使用高分辨率图像" class="headerlink" title="使用高分辨率图像"></a>使用高分辨率图像</h4><p>YOLOv1在分辨率为224×224的图片上进行预训练，在正式训练时将分辨率提升到448×448，这需要模型去适应新的分辨率。但是YOLOv2是直接使用448×448的输入， 所以YOLO2在采用 224*224 图像进行分类模型预训练后，再采用 448*448 的高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448*448 的分辨率。然后再使用 448*448 的检测样本进行训练，缓解了分辨率突然切换造成的影响。 </p><h4 id="采用先验框anchor"><a href="#采用先验框anchor" class="headerlink" title="采用先验框anchor"></a>采用先验框anchor</h4><p>在预测框的数量上，由于YOLOv2将网络的输入分辨率调整到416×416，下采样率为32，多次卷积后得到13×13的特征图（feature map）。在这上面使用9种anchor boxes，得到13×13×9=1521个，这比YOLOv1大多了。PS：我也不知道为什么突然就变416了？</p><p>YOLOv1利用全连接层的数据完成边框的预测，会导致丢失较多的空间信息，使定位不准。在YOLOv2中作者借鉴了Faster R-CNN中的anchor思想，来改善全连接层带来的影响。</p><p>Anchor是RPN（region proposal network）网络在Faster R-CNN中的一个关键步骤，是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的候选框。</p><p>为了引入anchor boxes来预测候选框，作者在网络中去掉了全连接层。并去掉了最后的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416 * 416，目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个中心框（center cell）。YOLO算法的作者观察到，大物体通常占据了图像的中间位置，可以只用中心的一个框来预测这些物体的位置，否则就要用中间的4个格子来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（采样因子为32），使得输入卷积网络的416 * 416图片最终得到13 * 13的卷积特征图（416/32=13）</p><h4 id="聚类提取先验框尺度。"><a href="#聚类提取先验框尺度。" class="headerlink" title="聚类提取先验框尺度。"></a>聚类提取先验框尺度。</h4><p>使用anchor的时候，anchor boxes的宽和高往往是人工选定的。虽然在训练过程中，网络也会调整宽和高。但一开始就选择了更好的宽和高，网络就更容易得到准确的预测位置。为了使网络更易学到准确的预测位置，作者使用了K-means聚类方法类训练bounding boxes，可以自动找到更好的框宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的框会比较小的框产生更多的误差，聚类结果可能会偏离。为此，作者采用IOU得分作为评价标准，这样的话，误差就和框的尺度无关。最终的距离函数为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029224241.png" alt=""></p><p>centroid是聚类时被选作中心的边框，box就是其它边框，d就是两者间的“距离”。IOU越大，“距离”越近。YOLO2给出的聚类分析结果如下图所示： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029224317.png" alt=""></p><p>上图左边是选择不同的聚类k值情况下，得到的k个centroid边框，计算样本中标注的边框与各centroid的Avg IOU。显然，边框数k越多，Avg IOU越大。YOLO2选择k=5作为边框数量与IOU的折中。对比手工选择的先验框，使用5个聚类框即可达到61 Avg IOU，相当于9个手工设置的先验框60.9 Avg IOU。 </p><h4 id="约束预测边框的位置"><a href="#约束预测边框的位置" class="headerlink" title="约束预测边框的位置"></a>约束预测边框的位置</h4><p> 借鉴于Faster RCNN的先验框方法，在训练的早期阶段，其位置预测容易不稳定。其位置预测公式为： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030172721.png" alt=""></p><p>其中，x，y是预测边框的中心，x<sub>a</sub>，y<sub>a</sub>是先验框(anchor)的中心点坐标，&omega;<sub>a</sub>，h<sub>a</sub>是先验框(anchor)的宽和高，t<sub>x</sub>，t<sub>y</sub>是要学习的参数。在Yolo论文中写的是x=(t<sub>x</sub>*&omega;<sub>a</sub>)-x<sub>a</sub>，根据Faster RCNN，应该是“+”。</p><p>由于t<sub>x</sub>，t<sub>y</sub>的取值没有任何约束，因此预测边框的中心可能出现在任何位置，导致训练早期阶段不容易稳定。Yolo 调整了预测公式，将预测边框的中心约束在特定的grid网格内。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030193343.png" alt=""></p><p>其中，b<sub>x</sub>、b<sub>y</sub>、b<sub>w</sub>、b<sub>h</sub>是预测边框的中心和宽高。Pr(object) * IOU(b,object)是预测边框的置信度，Yolo v1是直接预测置信度的值，这里对预测参数t<sub>0</sub>进行&sigma;(sigmoid)变换后作为置信度的值。c<sub>x</sub>，c<sub>y</sub>是当前网格左上角的距离，要先将网格大小归一化，即令一个网格的宽=1，高=1。p<sub>w</sub>，p<sub>h</sub>是先验框的宽和高。t<sub>x</sub>、t<sub>y</sub>、t<sub>w</sub>、t<sub>h</sub>、t<sub>o</sub>是要学习的参数，分别用于预测边框的中心和宽高，以及置信度。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030195154.png" alt=""></p><p>参考上图，由于σ函数将t<sub>x</sub>、t<sub>y</sub>约束在(0,1)范围内，所以根据上面的计算公式，预测边框的蓝色中心点被约束在蓝色背景的网格内。约束边框位置使得模型更容易学习，且预测更为稳定。 </p><h4 id="passthrough层检测细粒度特征"><a href="#passthrough层检测细粒度特征" class="headerlink" title="passthrough层检测细粒度特征"></a>passthrough层检测细粒度特征</h4><p>对象检测面临的一个问题是图像中对象会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLO2中输入416*416经过卷积网络下采样最后输出是13*13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。</p><p>YOLO2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26*26*512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030200424.png" alt=""></p><p>一拆4是什么拆的呢？举个例子，如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/7baff56b-42aa-31db-8db5-768957bdec3f.jpg" alt=""></p><p>还有其它拆法。就不具述了。</p><h4 id="多尺度图像训练"><a href="#多尺度图像训练" class="headerlink" title="多尺度图像训练"></a>多尺度图像训练</h4><p> 因为去掉了全连接层，YOLO2可以输入任何尺寸的图像。因为整个网络下采样倍数是32，作者采用了{320,352,…,608}等10种输入图像的尺寸，这些尺寸的输入图像对应输出的特征图宽和高是{10,11,…19}。训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。 </p><h3 id="速度更快"><a href="#速度更快" class="headerlink" title="速度更快"></a>速度更快</h3><p>大多数目标检测的框架是建立在VGG-16上的。为了进一步提升速度，Yolo v2提出了Darknet-19网络结构。Darknet-19(19层卷积层和5层池化层)比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到约1/5。</p><p> YOLO2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224*224 ，共训练160个epochs。然后第二阶段将网络的输入调整为 448*448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs。第三个阶段就是修改Darknet-19分类模型为检测模型，移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个 3*3*1024卷积层，同时增加了一个passthrough层，最后使用 1<em>1 卷积层输出预测结果，输出的channels数为：**num_anchors\</em>(5+num_classes)** ，和训练采用的数据集有关系。对于VOC数据集(20种分类对象)，假如anchor数为5，输出的channels就是125。</p><h3 id="识别对象更多"><a href="#识别对象更多" class="headerlink" title="识别对象更多"></a>识别对象更多</h3><p>论文提出了一种联合训练的机制：使用识别数据集训练模型识别相关部分，使用分类数据集训练模型分类相关部分。</p><p>众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。所以在YOLOv1中，边界框的预测其实并不依赖于物体的标签，YOLOv2实现了在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p><p>作者选择在COCO和ImageNet数据集上进行联合训练，遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的词树（WordTree）如下图所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029195823.png" alt=""></p><p>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个路径，然后计算路径上各个节点的概率之积。</p><p>在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是  ，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p><h2 id="Yolo-v3"><a href="#Yolo-v3" class="headerlink" title="Yolo v3"></a>Yolo v3</h2><p>YOLO v3没有太多的创新，主要是借鉴一些好的方案融合到YOLO里面。不过效果还是不错的，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</p><h3 id="新的网络结构"><a href="#新的网络结构" class="headerlink" title="新的网络结构"></a>新的网络结构</h3><p>在基本的图像特征提取方面，Yolo v3采用了称之为Darknet-53的网络结构(含有53个卷积层)，它借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路(shortcut connections)。</p><h3 id="利用多尺度特征进行对象预测"><a href="#利用多尺度特征进行对象预测" class="headerlink" title="利用多尺度特征进行对象预测"></a>利用多尺度特征进行对象预测</h3><p>YOLO2曾采用passthrough结构来检测细粒度特征，在YOLO3更进一步采用了3个不同尺度的特征图来进行对象检测。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030221243.png" alt=""></p><p>结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416*416的话，这里的特征图就是13*13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。</p><p>为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。</p><p>最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。</p><h3 id="9种尺度的先验框"><a href="#9种尺度的先验框" class="headerlink" title="9种尺度的先验框"></a>9种尺度的先验框</h3><p>随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。</p><p>分配上，在最小的13*13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26*26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52*52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030221659.png" alt=""></p><p>感受一下9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格。 </p><p>13*13:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030222109.png" alt=""></p><p>26*26:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030222209.png" alt=""></p><p>52*52:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030222240.png" alt=""></p><h2 id="对象分类softmax改成logistic"><a href="#对象分类softmax改成logistic" class="headerlink" title="对象分类softmax改成logistic"></a><strong>对象分类softmax改成logistic</strong></h2><p>预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象（比如一个人有Woman 和 Person两个标签）。</p><h2 id="输入映射到输出"><a href="#输入映射到输出" class="headerlink" title="输入映射到输出"></a>输入映射到输出</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030223023.png" alt=""></p><p>不考虑神经网络结构细节的话，总的来说，对于一个输入图像，YOLO3将其映射到3个尺度的输出张量，代表图像各个位置存在各种对象的概率。</p><p>我们看一下YOLO3共进行了多少个预测。对于一个416*416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13*13*3 + 26*26*3 + 52*52*3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。</p><p>对比一下，YOLO2采用13*13*5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。</p><p>本篇文章使用了较多的他人的图片，如有侵权，请联系我删掉。QQ：1171708687</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对于目标检测，我们最多到底能检测多少个类别呢？对于Yolo来说，是9000个。这是非常厉害的，所以，接下来看看Yolo的三代算法Yolo v1、Yolo v2、Yolo v3。&lt;/p&gt;
&lt;p&gt;目标检测经历了一个高度的符合人类的直觉的过程。既需要识别出目标的位置，将图片划分成
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="Yolo" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/Yolo/"/>
    
    
      <category term="Yolo算法" scheme="http://yoursite.com/tags/Yolo%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>pytorch的可视化</title>
    <link href="http://yoursite.com/2019/10/26/pytorch%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://yoursite.com/2019/10/26/pytorch的可视化/</id>
    <published>2019-10-26T03:29:54.000Z</published>
    <updated>2019-11-19T06:54:13.033Z</updated>
    
    <content type="html"><![CDATA[<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>Visdom是一个灵活的可视化工具，可以实时显示新创建的数据。Visdom的目的是促进远程数据的可视化，支持科学实验。可以发送可视化图像和文本。通过UI为实时数据创建dashboards，检查实验的结果。</p><h3 id="Panes-窗格"><a href="#Panes-窗格" class="headerlink" title="Panes(窗格)"></a>Panes(窗格)</h3><p>UI刚开始是个白板，我们可以用图像和文本填充它。这些填充的数据出现在Panes，我们可以对这些Panes进行拖放、删除、调整大小和销毁操作。Panes是保存在Environment(环境)中的，Environment(环境)的状态存储在会话之间。可以使用浏览器的放大缩小功能来调整UI的大小。</p><h3 id="Environment-环境"><a href="#Environment-环境" class="headerlink" title="Environment(环境)"></a>Environment(环境)</h3><p>可以使用Envs对可视化空间进行分区。每个用户都会有一个叫做main的Envs。可以通过编程或UI创建新的Envs，Envs的状态是长期保存的。可以通过<a href="https://localhost.com:8097/env/main访问特定的ENV。如果服务器是被托管的，那么可以将此URL分享给别人，那么其他人也会看到可视化结果，" target="_blank" rel="noopener">https://localhost.com:8097/env/main访问特定的ENV。如果服务器是被托管的，那么可以将此URL分享给别人，那么其他人也会看到可视化结果，</a></p><p>在初始化服务器的时候，Envs默认通过$HOME/.visdom/加载。也可以将自定义的路径当作命令行参数传入。如果移除了Envs文件家下的.json文件，那么相应的环境也会删除。</p><h3 id="State-状态"><a href="#State-状态" class="headerlink" title="State(状态)"></a>State(状态)</h3><p>一旦创建了一些可视化，状态是被保存的。服务器自动缓存我们的可视化，如果重新加载网页，可视化就会重新出现。</p><p>Save：可以通过点击save按钮手动保存Envs。它首先会序列化Envs的状态，然后以.json文件的形式保存到硬盘上，包括窗口的位置。</p><p>Fork：输入一个新的Envs名字，“保存”会建立一个新的Envs，有效的分割之前的状态。</p><p>当前用不到可视化，先学后面的。后面有用到就补</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;headerlink&quot; title=&quot;可视化&quot;&gt;&lt;/a&gt;可视化&lt;/h2&gt;&lt;p&gt;Visdom是一个灵活的可视化工具，可以实时显示新创建的数据。Visdom的目的是促进远程数据的可视化，支持科学实验。可以发送可视化
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/pytorch/"/>
    
    
      <category term="深度学习框架" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>pytorch(1)</title>
    <link href="http://yoursite.com/2019/10/22/pytorch-1/"/>
    <id>http://yoursite.com/2019/10/22/pytorch-1/</id>
    <published>2019-10-22T09:23:19.000Z</published>
    <updated>2019-11-20T13:24:39.796Z</updated>
    
    <content type="html"><![CDATA[<p>torch.nn包里的Functional包含convolution函数，pooling函数，非线性函数、激活函数等函数，torch.nn.optim包含各种优化算法，Momentum、RMSProp等。</p><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>torch.Tensor是一种包含单一数据类型元素的多维矩阵。</p><p>Torch定义了七种CPU tensor类型和八种 tensor类型：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191022221728.png" alt=""></p><p>torch.Tensor是默认的tensor类型（torch.FloatTensor）的简称。</p><p>一个张量tensor可以从Python的list或序列构建：</p><blockquote><p>>&gt;&gt;torch.FloatTensor([[1,2,3],[4,5,6]])<br>tensor([[1., 2., 3.],<br>       [4., 5., 6.]])</p><p>>&gt;&gt;torch.Tensor([[3,2,1],[6,5,4]])<br>tensor([[3., 2., 1.],<br>[6., 5., 4.]])</p></blockquote><p>空张量tensor可以通过规定其大小来构建：</p><blockquote><p>>&gt;&gt;torch.IntTensor(2,4).zero_()<br>tensor([[0, 0, 0, 0],<br>        [0, 0, 0, 0]], dtype=torch.int32)</p></blockquote><p>可以用python的索引和切片来获取和修改一个张量tensor中的内容：</p><blockquote><p>>&gt;&gt;x=torch.FloatTensor([[1,2,3],[4,5,6]])<br>>&gt;&gt;print(x[1][2])<br>tensor(6.)<br>>&gt;&gt;print(x[0][1])<br>tensor(2.)<br>>&gt;&gt;x[0][1]=8<br>>&gt;&gt;print(x)<br>tensor([[1., 8., 3.],<br>        [4., 5., 6.]])</p></blockquote><p>每一个张量tensor都有一个相应的tourch.Storage用来保存其数据。</p><p><strong>注意：会改变tensor的函数操作会用一个下划线后缀来表示。</strong>比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而torch.FloatTensor.abs()将会在一个新的tensor中计算结果。</p><p>创建张量时有个参数是requires_grad，如果设置 requires_grad为 True，那么将会追踪所有对于该张量的操作吗，默认False。 当完成计算后通过调用 .backward()，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 .grad属性。 </p><h2 id="torch-squeeze"><a href="#torch-squeeze" class="headerlink" title="torch.squeeze"></a>torch.squeeze</h2><p>torch.squeeze(input,dim=None,out=None)：将输入张量形状中的1去除并返回。当给定dim时，那么挤压操作只在给定维度上。</p><blockquote><p>>&gt;&gt;x=torch.zeros(2,1)<br>>&gt;&gt;x<br>tensor([[0.],<br>        [0.]])<br>>&gt;&gt;y=torch.squeeze(x)<br>>&gt;&gt;y<br>tensor([0., 0.])</p></blockquote><h2 id="torch-abs"><a href="#torch-abs" class="headerlink" title="torch.abs"></a>torch.abs</h2><p>torch.abs(input,out=None)：计算输入张量的每个元素的绝对值</p><blockquote><p>>&gt;&gt;torch.abs(torch.Tensor([-1,-2,-3]),out=z)<br>tensor([1., 2., 3.])<br>>&gt;&gt;z<br>tensor([1., 2., 3.])</p></blockquote><h2 id="torch-add"><a href="#torch-add" class="headerlink" title="torch.add"></a>torch.add</h2><p>torch.add(input,value,out=None)，对输入张量input逐元素加上标量值value，并返回结果得到一个新的张量out。</p><blockquote><p>>&gt;&gt;a=torch.rand(4)<br>>&gt;&gt;a<br>tensor([0.0694, 0.6366, 0.6938, 0.8872])<br>>&gt;&gt;torch.add(a,20)<br>tensor([20.0694, 20.6366, 20.6938, 20.8872])</p></blockquote><h2 id="torch-mean"><a href="#torch-mean" class="headerlink" title="torch.mean"></a>torch.mean</h2><p>torch.mean(input,dim,out=None)：返回输入张量给定维度dim上每行的均值。0是列，1是行。</p><blockquote><p>>&gt;&gt;a=torch.rand(4,4)<br>>&gt;&gt;a<br>tensor([[0.7413, 0.5828, 0.0070, 0.0146],<br>        [0.1668, 0.6566, 0.1865, 0.0700],<br>        [0.7603, 0.8017, 0.3065, 0.8772],<br>        [0.7918, 0.8798, 0.7355, 0.1142]])<br>>&gt;&gt;torch.mean(a,1)<br>tensor([0.3364, 0.2700, 0.6864, 0.6303])</p></blockquote><h2 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h2><h3 id="torch-eq"><a href="#torch-eq" class="headerlink" title="torch.eq"></a>torch.eq</h3><p>torch.eq(input,other,out=None)：比较元素相等性。第二个参数可以为一个数或与第一个参数同类型形状的张量。</p><blockquote><p>>&gt;&gt;torch.eq(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[ True, False],<br>        [False,  True]])</p></blockquote><p>如果两个张量有相同的形状和元素值，则返回True，否则返回False。</p><blockquote><p>>&gt;&gt;torch.equal(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>False</p><p>>&gt;&gt;torch.equal(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,2],[3,4]]))<br>True</p></blockquote><h3 id="torch-ge"><a href="#torch-ge" class="headerlink" title="torch.ge"></a>torch.ge</h3><p>torch.ge(input,other,out=None)：逐元素比较input和other。即是否 input&gt;=otherinput&gt;=other。如果两个张量有相同的形状和元素值，则返回True。否则返回False。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。</p><blockquote><p>>&gt;&gt;torch.ge(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[ True,  True],<br>        [False,  True]])</p></blockquote><h3 id="torch-gt"><a href="#torch-gt" class="headerlink" title="torch.gt"></a>torch.gt</h3><p>torch.gt(input,other,out=None)：逐元素比较input和other。即input&gt;otherinput&gt;other。如果两个张量有相同的形状和元素值，则返回True。否则返回False。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。PS:这个和ge有什么区别？</p><blockquote><p>>&gt;&gt;torch.gt(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[False,  True],<br>        [False, False]])</p></blockquote><h3 id="torch-le"><a href="#torch-le" class="headerlink" title="torch.le"></a>torch.le</h3><p>torch.le(input, other, out=None)：逐元素比较input和other ， 即是否input&lt;=otherinput&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><blockquote><p>>&gt;&gt;torch.le(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[ True, False],<br>        [ True,  True]])</p></blockquote><h3 id="torch-lt"><a href="#torch-lt" class="headerlink" title="torch.lt"></a>torch.lt</h3><p>torch.lt(input, other, out=None)：逐元素比较input和other ， 即是否 input&lt;otherinput&lt;other。第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><blockquote><p>>&gt;&gt;torch.lt(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[False, False],<br>        [ True, False]])</p></blockquote><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p>Variable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。</p><p>首先Variable是在torch.autograd.Variable中，要将tensor变成Variable，只需要Variable(a)就可以了。Variable有三个比较重要的组成属性：data、grad、grad_fn。data可以取出variable里面的tensor值，grad_fn表示的是得到这个Variable的操作(op)，grad是这个Variable的反向传播梯度。</p><blockquote><p>>&gt;&gt;from torch.autograd import Variable<br>>&gt;&gt;x=Variable(torch.Tensor([1]),requires_grad=True)<br>>&gt;&gt;w=Variable(torch.Tensor([2]),requires_grad=True)<br>>&gt;&gt;b=Variable(torch.Tensor([3]),requires_grad=True)<br>>&gt;&gt;y=w*x+b<br>>&gt;&gt;y<br>tensor([5.], grad_fn=<AddBackward0>)<br>>&gt;&gt;print(y.backward())<br>None<br>>&gt;&gt;print(x.grad)<br>tensor([2.])<br>>&gt;&gt;print(w.grad)<br>tensor([1.])<br>>&gt;&gt;print(b.grad)<br>tensor([1.])<br>>&gt;&gt;print(x.grad_fn)<br>None<br>>&gt;&gt;print(y.grad_fn)</p><AddBackward0 object at 0x000001C56569E2B0></blockquote><p>构建Variable，我们传入了一个参数requires_grad=True，这个参数表示是否对这个变量求梯度，默认的是False，也就是不对这个变量求梯度。</p><h2 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h2><p>paramerters(memo=None)：返回一个包含模型所有参数的迭代器。一般用来当作optimizer的参数。</p><h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>如果需要计算导数，可以在Tensor上调用.backward()。 如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数， 但是如果它有更多的元素，需要指定一个gradient参数来匹配张量的形状。 </p><h2 id="zero-grad"><a href="#zero-grad" class="headerlink" title="zero_grad"></a>zero_grad</h2><p>zero_grad()：清空所有被优化过的Variable的梯度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## view</span><br><span class="line"></span><br><span class="line"> 返回一个有相同数据但大小不同的tensor。 返回的tensor必须有与原tensor相同的数据和相同数目的元素，但可以有不同的大小。一个tensor必须是连续的`contiguous()`才能被查看。 </span><br><span class="line"></span><br><span class="line">&gt; \&gt;&gt;&gt;import torch</span><br><span class="line">&gt; \&gt;&gt;&gt;x=torch.randn(4,4)</span><br><span class="line">&gt; \&gt;&gt;&gt;x.size()</span><br><span class="line">&gt; torch.Size([4, 4])</span><br><span class="line">&gt; \&gt;&gt;&gt;y=x.view(16)</span><br><span class="line">&gt; \&gt;&gt;&gt;y.size()</span><br><span class="line">&gt; torch.Size([16])</span><br><span class="line">&gt; \&gt;&gt;&gt;z=x.view(-1,8)</span><br><span class="line">&gt; \&gt;&gt;&gt;z.size()</span><br><span class="line">&gt; torch.Size([2, 8])</span><br><span class="line"></span><br><span class="line">## zero_</span><br><span class="line"></span><br><span class="line">zero_(tensor)：用0填充一个tensor</span><br><span class="line"></span><br><span class="line">&gt; \&gt;&gt;&gt;torch.zero_(x)</span><br><span class="line">&gt; tensor([[0., 0., 0., 0.],</span><br><span class="line">&gt;         [0., 0., 0., 0.],</span><br><span class="line">&gt;         [0., 0., 0., 0.],</span><br><span class="line">&gt;         [0., 0., 0., 0.]])</span><br><span class="line"></span><br><span class="line">## 模型的保存和加载</span><br><span class="line"></span><br><span class="line">在pytorch里面使用torch.save来保存模型的结构和参数，有两种保存方式，当然加载模型有两种方式对应于保存模型的方式：</span><br><span class="line"></span><br><span class="line">（1）保存这个模型的结构信息和参数信息，保存的对象是模型model。加载完整的模型结构和参数信息，在网络较大的时候加载的时间比较长，同时存储空间也比较大。</span><br><span class="line"></span><br><span class="line">torch.save(the_model,path)</span><br><span class="line"></span><br><span class="line">the_model=torch.load(&quot;path&quot;)</span><br><span class="line"></span><br><span class="line">（2）保存模型的参数，保存的对象是模型的状态model.state_dict()。加载模型时加载的是参数信息。</span><br><span class="line"></span><br><span class="line">torch.save(the_model.state_dict(),path)</span><br><span class="line"></span><br><span class="line">the_model=TheModeClass(\*args,\**kwargs)</span><br><span class="line"></span><br><span class="line">the.model.load_state_dict(path)</span><br><span class="line"></span><br><span class="line">## 优化算法</span><br><span class="line"></span><br><span class="line">优化算法分两类：</span><br><span class="line"></span><br><span class="line">### 一阶优化算法</span><br><span class="line"></span><br><span class="line">这种算法使用各个参数的梯度值来更新参数，最常用的一阶优化算法是梯度下降。所谓的梯度就是导数的多变量表达式，同时也是一个方向，这个方向上方向导数最大，且等于梯度。梯度下降的功能是通过寻找最小值，控制方差，更新模型参数，最终使模型收敛。</span><br><span class="line"></span><br><span class="line">### 二阶优化算法</span><br><span class="line"></span><br><span class="line">二阶优化算法使用了二阶导数来最小化或最大化损失函数，主要基于牛顿法。但是由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。torch.optim是一个实现各种优化算法的包，大多数常见的算法都能通过这个包调用。</span><br><span class="line"></span><br><span class="line">### torch.optim</span><br><span class="line"></span><br><span class="line">为了使用torch.optim，我们需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。</span><br><span class="line"></span><br><span class="line">#### 构建optimizer</span><br><span class="line"></span><br><span class="line">为了构建一个optimizer，我们需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，可以设置optimizer的参数选项，比如学习率，权重衰减，等等。 </span><br><span class="line"></span><br><span class="line">举个例子：</span><br><span class="line"></span><br><span class="line"> ```optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)</span><br></pre></td></tr></table></figure><h4 id="单独设置参数"><a href="#单独设置参数" class="headerlink" title="单独设置参数"></a>单独设置参数</h4><p>我们还可以为每个参数单独设置选项。Optimizer也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。每一个dict都分别定 义了一组参数，并且包含一个param键，这个键对应参数的列表。其他的键应该optimizer所接受的其他参数的关键字相匹配，并且会被用于对这组参数的优化。</p><p>你仍然能够传递选项作为关键字参数。在未重写这些选项的组中，它们会被用作默认值。当你只想改动一个参数组的选项，但其他参数组的选项不变时，这是非常有用的。</p><p>例如，当我们想指定每一层的学习率时，这是非常有用的：</p><p> <code>optim.SGD([{&#39;params&#39;: model.base.parameters()}, {&#39;params&#39;: model.classifier.parameters(), &#39;lr&#39;:1e-3], lr=1e-2, momentum=0.9)</code></p><h4 id="单次优化"><a href="#单次优化" class="headerlink" title="单次优化"></a>单次优化</h4><p>使用optimizer.step()，这是大多数optimizer支持的版本，一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。</p><p><code>for input, target in dataset:</code> </p><p>​    optimizer.zero_grad()    </p><p>​    output = model(input)    </p><p>​    loss = loss_fn(output, target)   </p><p>​     loss.backward()   </p><p>​     optimizer.step()</p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>我看到优化算法时，发现中文文档好像没有调整学习率的函数。补充一下。torch.optim.lr_scheduler提供了几种方法来根据epoches的数量调整学习速率。 两种机制：LambdaLR机制和StepLR机制；</p><h4 id="LambdaLR机制："><a href="#LambdaLR机制：" class="headerlink" title="LambdaLR机制："></a>LambdaLR机制：</h4><p>class torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda,last_epoch=-1)</p><p>将每一个参数组的学习率设置为初始学习率lr的某个函数倍.当last_epoch=-1时,设置初始学习率为lr.</p><p>参数:</p><p>​    optimizer(Optimizer对象)—优化器</p><p>​    lr_lambda(是一个函数,或者列表(list))— 当是一个函数时,需要给其一个整数参数,使其计算出一个乘数因子,用于调整学习率,通常该输入参数是epoch数目或者是一组上面的函数组成的列表,</p><p>​    last_epoch(int类型):最后一次epoch的索引,默认为-1.</p><h4 id="StepLR机制"><a href="#StepLR机制" class="headerlink" title="StepLR机制"></a>StepLR机制</h4><p>class torch.optim.lr_scheduler.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1)</p><p>设置每个参数组的学习率为 lr*&lambda;<sup>n</sup>, n=epoch/step_size。当last_epoch=-1时,令lr=lr</p><p>参数:</p><p>​    optimizer(Optimizer对象)—优化器</p><p>​     step_size(整数类型): 调整学习率的步长,每过step_size次,更新一次学习率</p><p>​    gamma(float 类型):学习率下降的乘数因子</p><p>​    last_epoch(int类型):最后一次epoch的索引,默认为-1.</p><h2 id="其它API"><a href="#其它API" class="headerlink" title="其它API"></a>其它API</h2><p>其它的API不想做搬运工了，上网址。<a href="https://www.pytorchtutorial.com/docs/" target="_blank" rel="noopener">pytorch中文文档</a></p><p>看文档我发现torch.nn包里面和nn.functional都有卷积等一些函数。其实nn.functional中的函数仅仅定义了一些具体的基本操作，不能构成pytorch中的一个Layer。当我们需要自定义一些非标准Layer时，可以在其中调用nn.functional中的操作。例如，relu仅仅是一个函数，参数包括输入和计算需要的参数，返回计算的结果，它不能存储任何上下文的信息。</p><p>torch.nn包里面只是包装好了神经网络架构的类，nn.functional 与torch.nn包相比，nn.functional是可以直接调用函数的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;torch.nn包里的Functional包含convolution函数，pooling函数，非线性函数、激活函数等函数，torch.nn.optim包含各种优化算法，Momentum、RMSProp等。&lt;/p&gt;
&lt;h2 id=&quot;Tensor&quot;&gt;&lt;a href=&quot;#Tens
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/pytorch/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>超参数调试、batch正则化</title>
    <link href="http://yoursite.com/2019/10/13/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81batch%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://yoursite.com/2019/10/13/超参数调试、batch正则化/</id>
    <published>2019-10-13T07:19:36.000Z</published>
    <updated>2019-10-18T14:02:38.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="调试处理"><a href="#调试处理" class="headerlink" title="调试处理"></a>调试处理</h2><p>关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentum或Adam优化算法的参数，&beta;<sub>1</sub>，&beta;<sub>2</sub>和ξ，也许你还得选择层数，也许还得选择不同层中隐藏单元的数量，也许还想使用学习率衰减。所以，使用的不是单一的学习率a。接着，当然可能还需要选择mini-batch的大小。</p><p>&beta;<sub>1</sub>、&beta;<sub>2</sub>、ξ推荐使用0.9、0.999、10<sup>-10</sup>。a是学习速率，学习速率是需要调试的最重要的超参数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014135821.png" alt=""></p><p>现在，如果我们尝试调整一些超参数，该如何选择调试值呢？在早一代的机器学习算法中，如果有两个超参数，这里会称之为超参1，超参2，常见的做法是在网格中取样点，像这样，然后系统的研究这些数值。这里放置的是5×5的网格，实践证明，网格可以是5×5，也可多可少，但对于这个例子，我们可以尝试这所有的25个点，然后选择哪个参数效果最好。当参数的数量相对较少时，这个方法很实用。</p><p>在深度学习领域，吴恩达老师推荐我们使用随机选择点方法，所以我们可以选择同等数量的点，对吗？25个点，接着，用这些随机取的点试验超参数的效果。之所以这么做是因为，对于要解决的问题而言，你很难提前知道哪个超参数最重要，正如之前看到的，一些超参数的确要比其它的更重要。</p><p>假如我们拥有三个超参数呢？这时我们搜索的就不只是一个方格了，而是一个立方体。超参数3代表第三维，接着，在三维立方体中取值，我们会实验更多的值。</p><p>实践中，需要搜索的可能不止三个超参数有时很难预知，哪个是最重要的超参数，对于具体应用而言，随机取值而不是网格取值表明，我们要探究了更多重要超参数的潜在值，无论结果是什么。</p><p>当我们给超参数取值时，另一个惯例是采用由粗糙到精细的策略。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014141132.png" alt=""></p><p>比如在二维的那个例子中，你进行了取值，也许你会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内），然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索，如果你怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，你会知道接下来应该聚焦到更小的方格中。在更小的方格中，你可以更密集得取点。所以这种从粗到细的搜索也经常使用。</p><h2 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h2><p>讲个例子，假如我们在搜索超参数a，假设我们怀疑其值最小是0.0001，最大是1。假如我们把这个取值范围当作数轴，沿其随机均匀取值，那90%的值都会落在0.1到1之间。只有10%的搜索资源在0.0001到0.1之间。反而，用数标尺搜索超参数的方式会更合理，因此不使用线性轴。分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用。</p><p>在python中，这可以通过numpy模块实现。</p><blockquote><p>r=-4 x np.random.rand()</p><p>a=10<sup>r</sup></p></blockquote><p>对a随机取值，由上面第一行代码得出r的取值在[-4,0]之间，a的取值[10<sup>-4</sup>,10<sup>0</sup>]之间。如果我们在10<sup>a</sup>和10<sup>b</sup>之间取值，在此例中，我们可以通过0.0001算出a的值为-4，b的值为0。我们要做的就是在[a,b]区间随机均匀的给r取值，然后设置a的值。所以总结一下，在对数坐标下取值，取最小值的对数就得到a的值，取最大值的对数就得到b值，所以现在你在对数轴上的10<sup>a</sup>到10<sup>b</sup>区间取值，在a，b间随意均匀的选取值，将超参数设置为10<sup>r</sup>，这就是在对数轴上取值的过程。</p><p>最后，还有一个例子是对&beta;取值。用于计算指数的加权平均数。假设我们认为&beta;是0.9到0.999之间的某个值，这也是我们想搜索的范围。</p><p>上面哪个例子说了如果想在0.9到0.999区间搜索，那就不能用线性轴取值。不要随机均匀在此区间取值，所以考虑这个问题最好的方法就是，我们要探究的是1-&beta;，此值在0.1到0.001区间内，所以我们会给1-&beta;取值，大概是从0.1到0.001。所以我们要做的是在[-3,-1]里随机均匀的给r取值。由1-&beta;=10<sup>r</sup>推出&beta;=1-10<sup>r</sup>，然后就变成了在特定的选择范围内超参数的随机取值。</p><h2 id="超参数调试的方法"><a href="#超参数调试的方法" class="headerlink" title="超参数调试的方法"></a>超参数调试的方法</h2><p>两种方法：</p><p>一种是你照看一个模型，通常是有庞大的数据组，但没有许多计算资源或足够的CPU和GPU的前提下，基本而言，只可以一次负担起试验一个模型或一小批模型，在这种情况下，即使当它在试验时，也可以逐渐改良。这是一个人照料一个模型的方法，观察它的表现，耐心的调试学习率。</p><p>另一种方法则是同时试验多种模型，你设置了一些超参数，尽管让它自己运行，或者是一天甚至多天，然后你会获得像这样的学习曲线，这可以是损失函数J或实验误差或损失或数据误差的损失，但都是你曲线轨迹的度量。同时你可以开始一个有着不同超参数设定的不同模型。</p><p>所以这两种方式的选择，是由你拥有的计算资源决定的，如果你拥有足够的计算机去平行试验许多模型，那绝对采用第二种方式，尝试许多不同的超参数，看效果怎么样。</p><h2 id="归一化的激活函数"><a href="#归一化的激活函数" class="headerlink" title="归一化的激活函数"></a>归一化的激活函数</h2><p>在深度学习兴起后，最重要的一个思想是它的一个算法——Batch归一化。batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易。接下来看一下原理。</p><p>之前的逻辑回归，我们讲过归一化输入特征可以加快学习过程。计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化数据集。那么更深的模型呢？我们不仅输入了特征值x，而且第一层有激活值a<sup>[1]</sup>，第二层有激活层a<sup>[2]</sup>等。那么如果我们想训练&omega;<sup>[3]</sup>，b<sup>[3]</sup>，那归一化a<sup>[2]</sup>岂不是更好？便于我们训练&omega;<sup>[3]</sup>和b<sup>[3]</sup>。</p><p>那么，我们能归一化每个隐藏层的a值吗？比如a<sup>[2]</sup>，但不仅仅是a<sup>[2]</sup>，可以是任何隐藏层的。a<sup>[2]</sup>的值是下一层的输入值，所以a<sup>[2]</sup>会影响&omega;<sup>[3]</sup>，b<sup>[3]</sup>的训练。这就是batch归一化的作用。严格来说，归一化的是z<sup>[2]</sup>，并不是a<sup>[2]</sup>。</p><p>在神经网络中，假设你有一些隐藏单元值从z<sup>[1]</sup>到z<sup>[m]</sup>，这些来源于隐藏层，所以这样写会更准确，即z<sup>[ l ]( i )</sup>为隐藏层，i从1到m，所以已知这些值。如下，你要计算平均值，强调一下，所有这些都是针对 l 层，但已省略 l 及方括号,计算方法如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014182957.png" alt=""></p><p>分母中加上ε以防止δ为0的情况。</p><p>所以现在我们已把这些z值标准化，化为含平均值 0 和标准单位方差，所以z的每一个分量都含有平均值 0 和方差 1，但我们不想让隐藏单元总是含有平均值 0 和方差 1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算，我们称之为z~(i)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183405.png" alt=""></p><p>这里γ和β是模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法，比如 Momentum 或者 Nesterov， Adam，接着更新γ和β，正如更新神经网络的权重一样。</p><p>γ和β的作用是可以随意设置z~(i) 的平均值，事实上，如果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183506.png" alt=""></p><p>那么：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183533.png" alt=""></p><p>Batch 归一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏层。应用 Batch 归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，也许不想隐藏单元值必须是平均值 0 和方差 1。γ和β参数控制使得均值和方差可以是 0 和 1，也可以是其它值。 </p><p><strong>注意</strong>：均值不是平均值，是数学期望。</p><h2 id="将Batch-Norm拟合进神经网络"><a href="#将Batch-Norm拟合进神经网络" class="headerlink" title="将Batch Norm拟合进神经网络"></a>将Batch Norm拟合进神经网络</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014212240.png" alt=""></p><p>假设有一个这样的神经网络，之前的知识说过，我们可以认为每个单元负责计算两件事。第一，它先计算z，然后应用其到激活函数中再计算a，所以可以认为，每个圆圈代表着两步的计算过程。同样的，对于下一层而言，那就是$z^{[2]}_{1}$和$z^{[2]}_{2}$等。所以如果没有应用Batch归一化，会把输入X拟合到第一隐藏层，然后首先计算z<sup>[1]</sup>，这是由&omega;<sup>[1]</sup>和b<sup>[1]</sup>两个参数控制的。接着，通常而言，会把z<sup>[1]</sup>拟合到激活函数以计算a<sup>[1]</sup>。但Batch归一化的做法是将z<sup>[1]</sup>值进行Batch归一化，简称<strong>BN</strong>，此过程将由&beta;<sup>[1]</sup>和&gamma;<sup>[1]</sup>两参数控制，这一操作会给你一个新的规范化的z<sup>[1]</sup>值（z~<sup>[1]</sup>），然后将其输入激活函数中得到a<sup>[1]</sup>，即a<sup>[1]</sup>=g<sup>[1]</sup>(z~<sup>[1]</sup>)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014215107.png" alt=""></p><p>现在，你已在第一层进行了计算，此时Batch归一化发生在z的计算和a之间，接下来，你需要应用a<sup>[1]</sup>值来计算，此过程是由&omega;<sup>[1]</sup>和b<sup>[1]</sup>控制的。与第一层所做的类似，也会将进行Batch归一化，现在我们简称<strong>BN</strong>，这是由下一层的Batch归一化参数所管制的，即&beta;<sup>[2]</sup>和&gamma;<sup>[2]</sup>，现在你得到z~<sup>[2]</sup>，再通过激活函数计算出a<sup>[2]</sup>。</p><p>所以，得出结论的是batch归一化是发生在计算z和a之间的。与其使用没有归一化的z值，不如用归一化的z~值，也就是第一层的z~<sup>[1]</sup>。第二层同理，也是与其应用z值，不如应用z~<sup>[2]</sup>值。所以，我们以前的网络参数&omega;<sup>[1]</sup>、b<sup>[1]</sup>、&omega;<sup>[2]</sup>、b<sup>[2]</sup>将加上&beta;<sup>[1]</sup>、&beta;<sup>[2]</sup>、&gamma;<sup>[1]</sup>、&gamma;<sup>[2]</sup>等参数。<strong>注意</strong>：这里的&beta;<sup>[1]</sup>、&beta;<sup>[2]</sup>和超参数&beta;没有关系</p><p>&beta;<sup>1]</sup>、&beta;<sup>[2]</sup>、&gamma;<sup>[1]</sup>、&gamma;<sup>[2]</sup>等是算法的新参数。接下来就是使用梯度下降法来执行它。举个例子，对于给定层，计算d&beta;<sup>[1]</sup>，接着更新参数为&beta;<sup>[1]</sup>=&beta;<sup>[1]</sup>-&alpha;d&beta;<sup>[1]</sup>。你也可以使用Adam或RMSprop或Momentum，以更新参数&beta;和&gamma;，并不是只应用梯度下降法。</p><p>实践中，我们常将batch归一化和mibi-bacth一起使用。我们用第一个mini-batch{X<sup>[1]</sup>}，然后应用&omega;<sup>[1]</sup>和b<sup>[1]</sup>计算z<sup>[1]</sup>，接着用batch归一化得到z~<sup>[1]</sup>，再应用激活函数得到a<sup>[1]</sup>。然后接着用&omega;<sup>[2]</sup>和b<sup>[2]</sup>计算z<sup>[2]</sup>。</p><p>类似的工作，你会在第二个mini-batch（X<sup>{2}</sup>）上计算z<sup>[1]</sup>，然后用Batch归一化来计算，所以Batch归一化的此步中，用的是第二个mini-batch（X<sup>{2}</sup>）中的数据使归一化。然后在mini-batch（X<sup>{3}</sup>）上同样这样做，继续训练。</p><p>先前说过每层的参数是&omega;<sup>[ l ]</sup>和b<sup>[ l ]</sup>，还有&beta;<sup>[ l ]</sup>和&gamma;<sup>[ l ]</sup>，请注意计算z的方式如下，z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>，但Batch归一化做的是，要看这个mini-batch，先将z<sup>[ l ]</sup>归一化，结果为均值0和标准方差，再由&beta;和&gamma;重缩放，但这意味着，无论b<sup>[ l ]</sup>的值是多少，都是要被减去的，因为在Batch归一化的过程中，要计算z<sup>[ l ]</sup>的均值，再减去平均值，在此例中的mini-batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消。</p><p>所以，我们在使用batch归一化时，我们可以消除b<sup>[ l ]</sup>这个参数。或者也可以设置为0。那么式子将从z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>变为z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>。然后归一化z<sup>[ l ]</sup>，得z~<sup>[ l ]</sup>=&gamma;<sup>[ l ]</sup> * z<sup>[ l ]</sup>+&beta;<sup>[ l ]</sup>，所以最后我们会用&beta;<sup>[ l ]</sup>，以便决定z~<sup>[ l ]</sup>的取值。</p><p>总结一下关于如何用 Batch 归一化来应用梯度下降法：</p><p>假设使用 mini-batch梯度下降法，运行t = 1到 batch 数量的 for 循环，会在 mini-batchX<sup>{t}</sup>上应用正向 prop，每个隐藏层都应用正向 prop，用 Batch 归一化代替z<sup>[l]</sup>为z~<sup>[l]</sup>。接下来，它确保在这个 mini-batch 中，z值有归一化的均值和方差，归一化均值和方差后是z~<sup>[ l ]</sup> ，然后，你用反向 prop 计算：dw<sup>[ l ]</sup>，db[l]，dβ<sup>[ l ]</sup> ，dγ<sup>[ l ]</sup>。 尽管严格来说，因为要去掉b，这部分其实已经去掉了。最后，更新这些参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015143440.png" alt=""></p><h2 id="batch-norm为什么会管用？"><a href="#batch-norm为什么会管用？" class="headerlink" title="batch norm为什么会管用？"></a>batch norm为什么会管用？</h2><p>一个原因是，你已经看到如何归一化输入特征值x，使其均值为0，方差1，它又是怎样加速学习的，有一些从0到1而不是从1到1000的特征值，通过归一化所有的输入特征值x，以获得类似范围的值，可以加速学习。所以Batch归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值，这只是Batch归一化作用的冰山一角，还有些深层的原理，它会有助于你对Batch归一化的作用有更深的理解，让我们一起来看看吧。</p><p>Batch归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层，比如，第10层的权重相比于神经网络中前层的权重更能经受得住变化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016170223.png" alt=""></p><p>看上面的素材。假设我们已经在某个网络上训练了所有黑猫的图像，如果我们将这个网络应用于有色猫。这种情况下，我们的效果可能不是很好。因为正面的例子不止左边的黑猫，还有右边其它颜色的猫。</p><p>所以我们要使数据改变分布，想法名字叫“Covariate shift”。想法是这样的，如果你已经学习了x到y的映射，如果x的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由x到y映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016171250.png" alt=""></p><p>看上图，看第三层网络。此网络已经学习了参数&omega;<sup>[3]</sup>和b<sup>[3]</sup>。它还得到了一些值，称为$a^{[2]}_{1}$、$a^{[2]}_{2}$、$a^{[2]}_{3}$、$a^{[2]}_{4}$，但这些值也可以变为x<sub>1</sub>、x<sub>2</sub>、x<sub>3</sub>、x<sub>4</sub>。第三层隐藏层要做的是，找到一种方式使这些值映射到y帽。再看网络左边前三层(包括输入层)，这个网络还有参数&omega;<sup>[2]</sup>、b<sup>[2]</sup>、&omega;<sup>[1]</sup>、b<sup>[1]</sup>，如果这些参数改变，这些a<sup>[2]</sup>的值也会变。所以从第三层隐藏层的角度来看，这些隐藏单元的值在不断地改变，所以它就有了“Covariate shift”的问题。</p><p>Batch归一化做的，是它减少了这些隐藏值分布变化的数量。batch归一化讲的是当神经网络层更新参数时，batch归一化可以确保无论$z^{[2]}_{1}$、$z^{[2]}_{2}$、$z^{[2]}_{3}$、$z^{[2]}_{4}$怎么变化它们的均值和方差都保持不变。均值和方差是由&beta;<sup>[2]</sup>和&gamma;<sup>[2]</sup>决定的值，如果神经网络选择的话，可强制均值为0，方差为1，或其它任何均值和方差。</p><p>Batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，我们可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。</p><p>batch归一化还有一个作用，它有轻微的正则化效果。在mini-batch计算中，由均值和方差缩放的，因为是在mini-batch上计算的均值和方差，而不是在整个数据集上，它只是由一小部分数据估计得出的。所以和dropout相似，它往每个隐藏层的激活值上增加了噪音，dropout有增加噪音的方式，它使一个隐藏的单元，以一定的概率乘以0，以一定的概率乘以1，所以你的dropout含几重噪音，因为它乘以0或1。对比而言，Batch归一化含几重噪音，因为标准偏差的缩放和减去均值带来的额外噪音。这里的均值和标准差的估计值也是有噪音的，所以类似于dropout，Batch归一化有轻微的正则化效果，因为给隐藏单元添加了噪音，这迫使后部单元不过分依赖任何一个隐藏单元，类似于dropout。batch归一化给隐藏层增加了噪音，因此有轻微的正则化效果。因为添加的噪音很微小，所以并不是巨大的正则化效果。如果想得到dropout更大的正则化效果，可以将Batch归一化和dropout一起使用。</p><h2 id="测试时的batch-norm"><a href="#测试时的batch-norm" class="headerlink" title="测试时的batch norm"></a>测试时的batch norm</h2><p>batch归一化将数据以mini-batch的形式逐一处理，但在测试时，需要对每个样本逐一处理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016201010.png" alt=""></p><p>在训练时，这些就是用来执行Batch归一化的等式。在一个mini-batch中，你将mini-batch的z<sup>(i)</sup>值求和，计算均值，所以这里你只把一个mini-batch中的样本都加起来，我用m来表示这个mini-batch中的样本数量，而不是整个训练集。然后计算方差，再算$z^{(i)}_{norm}$，即用均值和标准差来调整，加上ξ是为了数值稳定性。z~<sup>(i)</sup>是用&gamma;和&beta;再次调整$z^{(i)}_{norm}$得到的。</p><p>请注意用于调节计算的μ和σ<sup>2</sup>是在整个mini-batch上进行计算，但是在测试时，不能将一个mini-batch中的6428或2056个样本同时处理，因此需要用其它方式来得到μ和σ<sup>2</sup>，而且如果只有一个样本，一个样本的均值和方差没有意义。那么实际上，为了将神经网络运用于测试，就需要单独估算μ和σ<sup>2</sup>，在典型的Batch归一化运用中，需要用一个指数加权平均来估算，这个平均数涵盖了所有mini-batch，接下来是具体解释。</p><p>选择 l 层，假设我们用mini-batch，X<sup>[1]</sup>,X<sup>[2]</sup>,X<sup>[3]</sup>……以及对应的y值等等，那么在 l 层训练X<sup>{1}</sup>时，就得到了μ<sup>{1}[ l ]</sup>。当我们训练第二个mini-batch，我们就得到了μ<sup>{2}[ l ]</sup>，第三个mini-batch，就得到了μ<sup>{3}[1]</sup>值。正如我们之前用的指数加权平均来计算&theta;<sub>1</sub>，&theta;<sub>2</sub>，&theta;<sub>3</sub>的均值，当时是试着计算当前气温的指数加权平均，你会这样来追踪你看到的这个均值向量的最新平均值，于是这个指数加权平均就成了你对这一隐藏层的z均值的估值。同样的，你可以用指数加权平均来追踪你在这一层的第一个mini-batch中所见的的σ<sup>2</sup>值，以及第二个mini-batch中所见的的σ<sup>2</sup>值等等。因此在用不同的mini-batch训练神经网络的同时，能够得到你所查看的每一层的μ和σ<sup>2</sup>的平均数的实时数值。最后在测试时，我们只需要z值来计算$z^{(i)}_{norm}$，用μ和σ<sup>2</sup>的指数加权平均，用手头的最新数值来做调整，然后就可以用刚算出来的z<sub>norm</sub>和在神经网络训练过程中得到的&beta;和&gamma;参数来计算那个测试样本的z~值。</p><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>前面的讲过逻辑回归属于二分类，如果我们有多种可能的类型呢？</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015163607.png" alt=""></p><p>假设我们不止需要识别猫，而是想识别猫、狗和小鸡。把猫当作类1，把狗当作类2，把小鸡当作类3。如果不属于以上任何一类，就分到“其它”或“以上均不符合”这一类，我把它叫做类0。所以上面图中第一张图为3类，第二张图为1类，第三张图为2类，第四张图为0类，依次类推。我们使用C来表示输入会被分入的类别总个数。在这个例子中，我们有四种可能的类别。当有四个分类时，指示类别的数字0-C-1。就是0、1、2、3。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015171121.png" alt=""></p><p>最后一层的隐藏单元为4，为所分的类的数目。输出的值表示属于每个类的概率。它们加起来等于一。</p><p>Softmax的具体步骤如下：</p><p>在神经网络的最后一层，我们将会想往常一样计算各层的线性部分，z<sup>[ l ]</sup>是最后一层的z变量。z<sup>[ l ]</sup>=W<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>，算出了z后，我们需要应用Softmax激活函数。它的作用是这样的： 我们要计算一个临时变量，我们把它叫做t。它等于e<sup>z<sup>[ l ]</sup>&lt;/sup&gt;，这适用于每个元素。而这里的z<sup>[ l ]</sup>，在上面这个例子中，维度是4x1的。四维向量t=e<sup>z[ l ]</sup>，这是对所有元素求幂，t也是一个4x1维向量，然后输出a<sup>[ l ]</sup>，基本上就是向量t，但是会归一化，使和为1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015174838.png" alt=""></p><p>换句话说，a<sup>[ l ]</sup>也是一个4x1维向量。而这个思维向量的第 i 个元素：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175116.png" alt=""></p><p>讲个例子：假设我们算出了z<sup>[ l ]</sup>，这是一个四维向量。假设为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175309.png" alt=""></p><p>我们要做的就是用这个元素取幂方法来计算t，所以：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175348.png" alt=""></p><p>接着利用计算器计算得到以下值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175424.png" alt=""></p><p>如果把t的元素都加起来，把这四个数字加起来，得到176.3。最终：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175556.png" alt=""></p><p>看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015172242.png" alt=""></p><p>例如第一个节点，会输出e<sup>5</sup>/176.3=0.842。这样来说，对于这张图片，它是0类的概率就是84.2%。下个节点输出0.042，也就是4.2%的几率是1类。其它类别以这种规律推出。</p><p>神经网络的输出a<sup>[ l ]</sup>，也就是y帽。是一个4x1维向量，如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015180202.png" alt=""></p><p>所以这种算法通过向量z计算出总和为1的四个概率。</p><p>之前，我们的激活函数都是接受单行数值输入，例如Sigmoid和ReLu激活函数，输入一个实数，输出一个实数。Softmax激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。</p><p>下面是分类的几个例子：</p><p>这是一个没有隐藏层的神经网络。他所做的就是z<sup>[1]</sup>=W<sup>[1]</sup> * x+b<sup>[1]</sup>，而输出的a<sup>[ l ]</sup>=g(z<sup>[1]</sup>)，就是Softmax激活函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015193021.png" alt=""></p><p>上面三张图的C=3，下面三张图从左到右的C等于4、5、6。</p><p>这显示了Softmax分类器在没有隐藏层的情况下能够做到的事情，当然更深的神经网络会有x，然后是一些隐藏单元，以及更多隐藏单元等等，你就可以学习更复杂的非线性决策边界，来区分多种不同分类。</p><h2 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h2><p>回忆我们之前举得例子，输出层计算的z<sup>[ l ]</sup>。我们有四个分类，z<sup>[ l ]</sup>可以是4x1维向量，我们计算了临时变量t。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015222626.png" alt=""></p><p>如果我们的激活函数是Softmax，那么输出是这样的:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015222734.png" alt=""></p><p>简单来说就是用临时变量t将它归一化，使总和为1。于是这就变成了a<sup>[ l ]</sup>。在这个向量z中，最大的元素是5。最大的概率是0.842。</p><p>Softmax这个是与所谓的<strong>hardmax</strong>对比，hardmax会把z变量变为[1 0  0  0]<sup>T</sup>，hardmax会观察z的元素，然后在z中最大元素的位置放上1，其它的输出都放0。</p><p>Softmax回归或Softmax激活函数将logistic激活函数推广到C类，而不仅仅是两类。而当C=2，那么的Softmax实际上变回了logistic回归，那么输出层将会输出两个数字。如果C=2的话，也许输出0.842和0.158，对吧？这两个数字加起来要等于1，因为它们的和必须为1，其实它们是冗余的，也许你不需要计算两个，而只需要计算其中一个，结果就是你最终计算那个数字的方式又回到了logistic回归计算单个输出的方式。</p><p>接下来我们来看怎样训练带有Softmax输出层的神经网络，具体而言，我们先定义训练神经网络使会用到的损失函数。举个例子，我们来看看训练集中某个样本的目标输出，真实标签是[0 1 0 0]<sup>T</sup>，用上一个视频中讲到过的例子，这表示这是一张猫的图片，因为它属于类1，现在我们假设你的神经网络输出的是 y帽，y帽是一个包括总和为1的概率的向量，y帽=[0.3 0.2 0.1 0.4]<sup>T</sup>，可以看到总和为1，这就是a<sup>[ l ]</sup>。对于这个样本神经网络的表现不佳，这实际上是一只猫，但却只分配到20%是猫的概率，所以在本例中表现不佳。</p><p>那么我们使用什么损失函数来训练这个神经网络？看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015224734.png" alt=""></p><p>注意在这个样本中，y<sub>1</sub>=y<sub>2</sub>=y<sub>3</sub>=0，因为这些都是0，只有y<sub>2</sub>=1，如果你看这个求和，所有含有值为0的y<sub>i</sub>的项都等于0，最后只剩下 -y<sub>2</sub>logy<sub>2</sub>帽，因为当你按照下标 j 全部加起来，所有的项都为0，除了j=2，因为y<sub>2</sub>=1，所以它就等于 -logy<sub>2</sub>帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015225125.png" alt=""></p><p>这就意味着，如果你的学习算法试图将它变小，因为梯度下降法是用来减少训练集的损失的，要使它变小的唯一方式就是使 -logy<sub>2</sub>帽变小，要想做到这一点，就需要使 y<sub>2</sub>帽 尽可能大。也就是[0.3 0.2 0.1 0.4]<sup>T</sup>中的第二个元素。</p><p>接下来看看，我们在有Softmax的输出层如何实现梯度下降法。输出层会计算z<sup>[1]</sup>，它是C x 1维的。在上个例子中，是4 x1维的。然后用Softmax激活函数来得到a<sup>[ l ]</sup>或者y帽。然后计算出损失。反向传播的关键步骤是这个表达式dz<sup>[ l ]</sup>=y帽-y。我们用y帽这个4x1向量减去y这个4x1向量。这是对z<sup>[ l ]</sup>的损失函数的偏导数dz<sup>[ l ]</sup>=∂J/∂z<sup>[ l ]</sup>，然后开始反向传播的过程，计算整个神经网络中所需要的所有导数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;调试处理&quot;&gt;&lt;a href=&quot;#调试处理&quot; class=&quot;headerlink&quot; title=&quot;调试处理&quot;&gt;&lt;/a&gt;调试处理&lt;/h2&gt;&lt;p&gt;关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentu
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（2）</title>
    <link href="http://yoursite.com/2019/10/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(2)/"/>
    <id>http://yoursite.com/2019/10/11/优化算法(2)/</id>
    <published>2019-10-11T05:13:53.000Z</published>
    <updated>2019-10-21T09:03:52.658Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mini-batch-梯度下降"><a href="#Mini-batch-梯度下降" class="headerlink" title="Mini-batch 梯度下降"></a>Mini-batch 梯度下降</h2><p>我们之前学过，向量化能够让我们有效地对所有m个样本进行计算，所以我们要把训练样本放到巨大的矩阵X中。</p><p>X=[x<sup>(1)</sup> x<sup>(2)</sup>   x<sup>(3)</sup>   x<sup>(4)</sup>  ……  x<sup>(n)</sup> ]。Y也是如此，Y=[y<sup>(1)</sup> y<sup>(2)</sup>  y<sup>(3)</sup>   y<sup>(4)</sup>  ……  y<sup>(n)</sup> ]。所以X的维度是(n<sub>x</sub>,m)，Y的维度是(1,m)。向量化能够让我们相对较快地处理所有样本，如果m很大的话，处理速度仍然缓慢。</p><p>相比于mini-batch梯度下降法，我们大家更熟悉的应该是batch梯度下降法，即梯度下降法。那batch梯度下降法和mini-batch梯度下降法有什么区别吗？其实它俩的区别就存在于名字中，一个是batch，即进行梯度下降训练时，使用全部的训练集，而mini-batch，表示比batch小一些，就是指在进行梯度下降训练时，并不使用全部的训练集，只使用其中一部分数据集。</p><p>我们知道，不论是梯度下降法还是mini-batch梯度下降法，我们都可以通过向量化(vectorization)更加有效地计算所有样本。既然已经有了梯度下降法，我们为什么还要提出mini-batch梯度下降法呢？在实际计算中，我们可能会遇到特别大的数据集，这时再使用梯度下降法，每次迭代都要计算所有的数据集，计算量太大，效率低下，而mini-batch梯度下降法允许我们拿出一小部分数据集来运行梯度下降法，能够大大提高计算效率。</p><p>我们可以把训练集分割成小一点的子集训练，这些子集被取名为mini-batch，假设每一个子集中只要1000个样本，那么把x<sup>(1)</sup>到x<sup>(1000)</sup>取出来，将其称为第一个子训练集，也叫做mini-batch，然后我们再取出接下来的1000个样本，从x<sup>(1001)</sup>到x<sup>(2000)</sup>，然后再取1000个样本，以此类推。</p><p>接下来是吴恩达老师的一个新的符号，把x<sup>(1)</sup>到x<sup>(1000)</sup>称为X<sup>{1}</sup>，x<sup>(1001)</sup>到x<sup>(2000)</sup>称为X<sup>{2}</sup>，如果我们的训练样本一共有500万个，每个mini-batch都有1000个样本。也就是说，你有5000个mini-batch，因为5000乘以1000就是500万。最后得到的是X<sup>{5000}</sup>，对y进行相同的处理。</p><p>mini-batch的数量t组成X<sup>{t}</sup>和Y<sup>{t}</sup>，这就是10000个训练样本。包括相应的输入输出对。如果X<sup>{1}</sup>是一个有1000个样本的训练集，X<sup>{1}</sup>的维度应该是(n<sub>x</sub>,1000)，X<sup>[2]</sup>的应该也是(n<sub>x</sub>，1000)，以此类推，所有的子集维数都是(n<sub>x</sub>,1000)，对于Y也是一样的。</p><p>之前我们执行前向传播，就是执行z<sup>[1]</sup>=W<sup>[1]</sup>X+b<sup>[1]</sup>。变成mini-batch后呢，把X替换成X<sup>[t]</sup>，即z<sup>[1]</sup>=W<sup>[1]</sup>X<sup>{t}</sup>+b<sup>[1]</sup>，然后执行A<sup>[1]k</sup>=g<sup>[1]</sup>(Z<sup>[1]</sup>)，依次类推，直到A<sup>[L]</sup>=g<sup>[L]</sup>(Z<sup>[L]</sup>)，这就是我们的预测值。注意：这里一次性处理的是1000个样本不是500万个样本。接着计算成本函数，因为子集规模是1000，所以J=1/1000$\sum_{i=1}^{1000}$L(y<sup>(i)</sup> 帽,y<sup>(i)</sup>)。</p><p>如果我们使用了正则化，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011191909.png" alt=""></p><p>你也会注意到，我们做的一切似曾相识，其实跟之前我们执行梯度下降法如出一辙，除了现在的对象不是X，Y，而是X<sup>{t}</sup>和Y<sup>{t}</sup>。接下来，执行反向传播来计算J<sup>{t}</sup>的梯度，你只是使用X<sup>{t}</sup>和Y<sup>{t}</sup>，然后更新加权值，W实际上是W<sup>[l]</sup>，更新为W<sup>[l]</sup>=W<sup>[l]</sup>-adW<sup>[l]</sup>。对b做相同处理，b<sup>[l]</sup>=b<sup>[l]</sup>-adb<sup>[l]</sup>。这是使用mini-batch梯度下降法训练样本的一步。被称为进行“一代”（<strong>1 epoch</strong>）的训练。一代这个词意味着只是遍历了一次训练集。我们可以在外围加一个for循环，从1到5000，因为我们有5000个各有1000个样本的组。</p><p>使用batch梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用mini-batch梯度下降法，一次遍历训练集，能让你做5000个梯度下降。</p><h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p>使用batch梯度下降法时，每次迭代都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许学习率太大。</p><p>使用mini-batch梯度下降法，如果你作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是X<sup>{t}</sup>和Y<sup>{t}</sup>，如果要作出成本函数J<sup>{t}</sup>的图，而J<sup>{t}</sup>只和X<sup>{t}</sup>，Y<sup>{t}</sup>有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的mini-batch。</p><p>我们需要决定的变量之一是mini-batch的大小，m就是训练集的大小。其中一个极端情况下，mini-batch梯度下降法就是batch梯度下降法。另一个极端情况，假设mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch，当你看第一个mini-batch，也就是X<sup>{1}</sup>和Y<sup>{1}</sup>，如果mini-batch大小为1，它就是你的第一个训练样本。接着再看第二个mini-batch，也就是第二个训练样本，采取梯度下降步骤，然后是第三个训练样本，以此类推，一次只处理一个。</p><p>实际上你选择的mini-batch大小在二者之间，大小在1和m之间，而1太小了，m太大了。如果取n，每个迭代需要处理大量训练样本，单次迭代耗时太长。如果训练样本不大，batch梯度下降法运行地很好。相反，如果使用随机梯度下降法，如果你只要处理一个样本，那这个方法很好，这样做没有问题，通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的mini-batch尺寸，实际上学习率达到最快。</p><p>首先，如果训练集较小，直接使用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，可以快速处理整个训练集，所以使用batch梯度下降法也很好，这里的少是说小于2000个样本，这样比较适合使用batch梯度下降法。不然，样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的n次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。所以我经常把mini-batch大小设成2的次方。</p><p>最后需要注意的是在你的mini-batch中，要确保X<sup>{t}</sup>和Y<sup>{t}</sup>要符合<strong>CPU</strong>/<strong>GPU</strong>内存，取决于你的应用方向以及训练集的大小。如果不符合内存，无论采取什么方法处理数据，结果变得惨不忍睹。</p><h2 id="指数加权平均数"><a href="#指数加权平均数" class="headerlink" title="指数加权平均数"></a>指数加权平均数</h2><p>指数加权平均也叫指数加权移动平均，是一种常用的序列数据处理方式。</p><p>它的计算公式是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222214.png" alt=""></p><p>其中，</p><ul><li>θ_t：为第 t 天的实际观察值，</li><li>V_t: 是要代替 θ_t 的估计值，也就是第 t 天的指数加权平均值，</li><li>β： 为 V_{t-1} 的权重，是可调节的超参。( 0 &lt; β &lt; 1 )</li></ul><p>下面是一组气温数据，图中横轴为一年中的第几天，纵轴为气温：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222352.png" alt=""></p><p>直接看上面的数据图会发现噪音很多，</p><p>这时，我们<strong>可以用 指数加权平均 来提取这组数据的趋势，</strong></p><p>按照前面的公式计算：</p><p>这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t，将计算后得到的V_t表示出来，得到下图红色线：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222623.png" alt=""></p><p>可以看出，红色的数据比蓝色的原数据更加平滑，少了很多噪音，并且刻画了原数据的趋势。</p><p>指数加权平均，作为原数据的估计值，不仅可以 <strong>1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来</strong>。</p><p>我们可以改变&beta;值，当&beta;=0.98时，得出下图的绿线。当&beta;=0.5，结果是下图的黄线。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011223152.png" alt=""></p><h2 id="理解指数加权平均数"><a href="#理解指数加权平均数" class="headerlink" title="理解指数加权平均数"></a>理解指数加权平均数</h2><p>上个小节，我们得出下面公式。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222214.png" alt=""></p><p>我们进一步分析，来理解如何计算出每日温度的平均值</p><p>使&beta;=0.9，得出下面公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012113654.png" alt=""></p><p>将第二个公式带到第一个，第三个公式带到第二个公式，一次类推，把这些展开：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012113917.png" alt=""></p><p>我们可以将v<sub>0</sub>，v<sub>1</sub>，v<sub>2</sub>等等写成明确的变量，不过在实际中执行的话，你要做的是，一开始将v<sub>0</sub>初始化为0，然后在第一天使v=&beta;v+(1-&beta;)&theta;<sub>1</sub>，然后第二天，更新v值，v=&beta;v+(1-&beta;)&theta;<sub>2</sub>，以此类推，有些人会把v加下标，来表示v是用来计算数据的指数加权平均数。</p><p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。</p><h2 id="指数加权平均的偏差修正"><a href="#指数加权平均的偏差修正" class="headerlink" title="指数加权平均的偏差修正"></a>指数加权平均的偏差修正</h2><p>偏差修正可以让平均数运算更加准确</p><p>在前几节中，如上图，红色曲线对应&beta;为0.9，绿色曲线对应的&beta;=0.98。吴恩达老师说执行下面这个公式：</p><blockquote><p>v<sub>t</sub>=&beta;v<sub>t-1</sub>+(1-&beta;)&theta;<sub>t</sub></p></blockquote><p>得到的就是紫色曲线，而不是绿色曲线。后面紫色和绿色有大部分重合。PS：为什么？同时0.98，紫色曲线的起点低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012160618.png" alt=""></p><p>计算移动平均数时，初始化v<sub>0</sub>，v<sub>1</sub>=0.98v<sub>0</sub>+0.02&theta;<sub>1</sub>，因为v<sub>0</sub>=0，所以0.98v<sub>0</sub>=0。所以v<sub>1</sub>=0.02&theta;<sub>1</sub>，如果第一天温度时40，v<sub>1</sub>=0.02 x 40=8。因此得到的值会小很多，所以第一天的温度估测不准。</p><p>v<sub>1</sub>=0.98v<sub>0</sub>+0.02&theta;<sub>1</sub>，如果带入v<sub>1</sub>，然后相乘，v<sub>2</sub>=0.98 x 0.02&theta;<sub>1</sub> + 0.02&theta;<sub>2</sub>，假如&theta;<sub>1</sub>和&theta;<sub>2</sub>都是正数，计算后的v<sub>2</sub>要远小于&theta;<sub>2</sub>和&theta;<sub>1</sub>，所以不能很好预估这一年前两天的温度。</p><p>有个办法可以修改这一估测，让估测变得更好，更准确，特别是在估测初期，也就是不用v<sub>t</sub>，而是用v<sub>t</sub>/1-&beta;<sup>t</sup>，t就是现在的天数。举个具体例子，当t=2时，1-&beta;<sup>t</sup>=1-0.98<sup>2</sup>=0.0396，因此对第二天温度的估测变成了</p><blockquote><p>v<sub>2</sub>/0.0396=(0.0196&theta;<sub>1</sub>+0.02&theta;<sub>2</sub>)/0.0396</p></blockquote><p>也就是和的加权平均数，并去除了偏差。你会发现随着t增加，&beta;<sup>t</sup>接近于0，所以当t很大的时候，偏差修正几乎没有作用.因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p><h2 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h2><p>还有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法。基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新的权重。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012173500.png" alt=""></p><p>如果优化成本函数，函数形状如图，红点代表最小值的位置，假设从这里（蓝色点）开始梯度下降法，如果进行梯度下降法的一次迭代，无论是batch或mini-batch下降法，也许会指向这里，现在在椭圆的另一边，计算下一步梯度下降，结果或许如此，然后再计算一步，再一步，计算下去。我们发现梯度下降法要很多计算步骤。</p><p>慢慢摆动到最小值，这种上下波动减慢了梯度下降法的速度，就无法使用更大的学习率，如果要用较大的学习率（紫色箭头），结果可能会偏离函数的范围，为了避免摆动过大，就要用一个较小的学习率。</p><p>另一个看待问题的角度是，在竖直方向上，我们希望学习慢一点，因为我们不想要这些摆动。但在水平方向，我们希望快速从左向右移动，移向最小值，移向红点。所以使用动力梯度下降法，在每次迭代中，都会计算dW，db。我们要做的是计算v<sub>dW</sub>=&beta;v<sub>dW</sub>+(1-&beta;)dW，接着同样计算v<sub>db</sub>=&beta;v<sub>db</sub>+(1-&beta;)db。然后重新赋值权重，W=W-av<sub>dW</sub>，同样b=b-av<sub>dW</sub>，从而减缓梯度下降的幅度。</p><p>举个例子，如果你站在一个地方不动，让你立刻向后转齐步走，你可以迅速向后转然后就向相反的方向走了起来，批梯度下降和随机梯度下降就是这样，某一时刻的梯度只与这一时刻有关，改变方向可以做到立刻就变。而如果你正在按照某个速度向前跑，再让你立刻向后转，可以想象得到吧，此时你无法立刻将速度降为0然后改变方向，你由于之前的速度的作用，有可能会慢慢减速然后转一个弯。</p><p>动量梯度下降是同理的，每一次梯度下降都会有一个之前的速度的作用，如果我这次的方向与之前相同，则会因为之前的速度继续加速；如果这次的方向与之前相反，则会由于之前存在速度的作用不会产生一个急转弯，而是尽量把路线向一条直线拉过去。</p><p>这就解决了文中第一个图的那个在普通梯度下降中存在的下降路线折来折去浪费时间的问题。</p><p>我们有两个超参数，学习率&alpha;以及&beta;参数。&beta;是指数加权平均数，常用值为0.9。</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>前面我们知道动力梯度下降法可以加快梯度下降，还有一个叫做<strong>RMSprop</strong>的算法，全称是<strong>root mean square prop</strong>算法，它也可以加速梯度下降，我们来看看它是如何运作的。</p><p>复习前面的内容，如果我们执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅动摆动。所以，我们想减缓纵轴方向的学习，同时加快横轴方向的学习(至少不减慢)。RMSprop算法可以实现这个目标。</p><p>在第t次迭代中，该算法会找出计算mini-batch的微分dW，db。这里我们不用v<sub>dW</sub>，而是用到新符号S<sub>dW</sub>。因此：</p><blockquote><p>S<sub>dW</sub>=&beta;S<sub>dW</sub>+(1-&beta;)(dW)<sup>2</sup>，这里的平方是对整个dW平方。</p><p>S<sub>db</sub>=&beta;S<sub>db</sub>+(1-&beta;)db<sup>2</sup>，这里的平方也是对整个db平方。</p></blockquote><p> 接着，RMSprop会更新参数值。如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012200635.png" alt=""></p><p>我们希望，S<sub>dW</sub>相对较少，S<sub>db</sub>较大。因为垂直方向的微分要比水平方向的大得多，所以斜率在垂直方向特别大。所以db较大，dW较小。db的平方较大，所以S<sub>db</sub>的平方较大。dW会小，S<sub>dW</sub>也会小。结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012224237.png" alt=""></p><p><strong>RMSprop</strong>的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率a，然后加快学习，而无须在纵轴上垂直方向偏离。</p><h2 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h2><p><strong>Adam</strong>优化算法基本上就是将<strong>Momentum</strong>和<strong>RMSprop</strong>结合在一起，那么来看看如何使用Adam算法。</p><p>首先，初始化</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021165915.png" alt=""></p><p>然后用当前的 mini-batch 计算出 dW 和 db</p><p>接下来计算 Momentum 指数加权平均数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021165943.png" alt=""></p><blockquote><p>上图中用到的&beta;<sub>1</sub>是为了和下面的RMSprop公式中用到的&beta;<sub>2</sub>相互区分</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170221.png" alt=""></p><p>一般运用 Adam 算法的时候，我们还要对 v 和 S 的偏差进行修正：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170044.png" alt=""></p><p>然后就是权重的更新：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170019.png" alt=""></p><p>完整的如下所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013142056.png" alt=""></p><p>本算法有很多超参数，分别有a、&beta;<sub>1</sub>、&beta;<sub>2</sub>、ξ四个超参数。a需要调试，尝试一系列的值，然后看哪个有效。&beta;<sub>1</sub>常用的值为0.9。至于&beta;<sub>2</sub>，推荐使用0.999，ξ为10<sup>-8</sup>。</p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013143740.png" alt=""></p><p>假设使用mini-batch梯度下降法，mini-batch数量不大，大概64或者128个样本，在迭代过程中会有噪音（蓝色线），下降朝向这里的最小值，但是不会精确地收敛，所以你的算法最后在附近摆动，并不会真正收敛，因为你用的a是固定值，不同的mini-batch中有噪音。</p><p>但是如果要减少学习率的话，在初期的时候，a学习率还比较大，我们的学习还是相对较快，但随着a变小，我们的步伐也会变慢。而不是在训练过程中，大幅度在最小值附近摆动。</p><p>所以慢慢减少a的本质在于，在学习初期，承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p><p>我们应该拆分成不同的mini-batch。第一次遍历训练集叫做第一代，第二代就是第二代。以此类推，我们得出公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144516.png" alt=""></p><p>decay-rate称为衰减率，epoch-num为代数，&alpha;<sub>0</sub>为初始学习率），注意这个衰减率是另一个你需要调整的超参数。</p><p>当然还有其它衰减：</p><p>方法二：指数衰减</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144713.png" alt=""></p><p>方法三：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144738.png" alt=""></p><p>方法四：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144804.png" alt=""></p><p>方法五：手动衰减</p><p>如果一次只训练一个模型，如果花上数小时或数天来训练，看看自己的模型训练，耗上数日，学习速率变慢了，然后把&alpha;调小一点。但这种方法只是在模型数量小的时候有用。</p><h2 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h2><p>也许我们想优化一些参数，我们把它们称之为W<sub>1</sub>和W<sub>2</sub>，平面的高度就是损失函数。在图中似乎各处都分布着局部最优。梯度下降法或者某个算法可能困在一个局部最优中，而不会抵达全局最优。如果要作图计算一个数字，比如说这两个维度，就容易出现有多个不同局部最优的图，而这些低维的图曾经影响了我们的理解，但是这些理解并不正确。事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013150349.png" alt=""></p><p>在高维度空间，我们更可能碰到鞍点。</p><p>首先，你不太可能困在极差的局部最优中，条件是你在训练较大的神经网络，存在大量参数，并且成本函数J被定义在较高的维度空间。</p><p>第二点，平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或是RMSprop，Adam这样的算法，能够加速学习算法的地方。在这些情况下，更成熟的优化算法，如Adam算法，能够加快速度，让你尽早往下走出平稳段。</p><p><strong>名词解释</strong>：平稳段是一段区域，其中导数长时间接近于0。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Mini-batch-梯度下降&quot;&gt;&lt;a href=&quot;#Mini-batch-梯度下降&quot; class=&quot;headerlink&quot; title=&quot;Mini-batch 梯度下降&quot;&gt;&lt;/a&gt;Mini-batch 梯度下降&lt;/h2&gt;&lt;p&gt;我们之前学过，向量化能够让我们有效地
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（1）</title>
    <link href="http://yoursite.com/2019/10/08/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(1)/"/>
    <id>http://yoursite.com/2019/10/08/优化算法(1)/</id>
    <published>2019-10-08T04:00:46.000Z</published>
    <updated>2019-10-21T12:02:20.923Z</updated>
    
    <content type="html"><![CDATA[<p>在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。</p><h2 id="训练、验证和测试集"><a href="#训练、验证和测试集" class="headerlink" title="训练、验证和测试集"></a>训练、验证和测试集</h2><p>我们通常会将这些数据划分成三部分，一部分作为训练集（<strong>train set</strong>），一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念。最后一部分则作为测试集（<strong>test set</strong>）。</p><p>接下来，我们开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，我们选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况。</p><p>在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。</p><p>在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被人们称为训练集，而验证集则被称为测试集。</p><p>那么验证集和测试集有什么区别呢？为什么要划分训练集、验证集和测试集？</p><p>训练集用于训练模型参数，测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。</p><h2 id="偏差（Bias）、方差（Variance）"><a href="#偏差（Bias）、方差（Variance）" class="headerlink" title="偏差（Bias）、方差（Variance）"></a>偏差（Bias）、方差（Variance）</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123133.png" alt=""></p><p>假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（<strong>high bias</strong>）的情况，我们称为“欠拟合”（<strong>underfitting</strong>）。</p><p>相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（<strong>high variance</strong>），数据过度拟合（<strong>overfitting</strong>）。</p><p>在两者之间，可能还有复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（<strong>just right</strong>）是介于过度拟合和欠拟合中间的一类。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123354.png" alt=""></p><p>下面举例子：</p><p>假定训练集误差是1%，为了方便论证，假定验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，<strong>验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。</strong></p><p>通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有高方差。<strong>也就是说衡量训练集和验证集误差就可以得出不同结论。</strong></p><p>假设训练集误差是15%，我们把训练集误差写在首行，验证集误差是16%，假设该案例中人的错误率几乎为0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，所以这种算法偏差高，因为它甚至不能拟合训练集，这与上一张图最左边的图片相似。</p><p>上面的分析都是基于假设预测，假设人眼辨别的错误率接近0%。最优误差也被称为贝叶斯误差。如果最优误差或贝叶斯误差非常高，比如15%。我们再看看上面这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt=""></p><h2 id="正则化（Rugularization）"><a href="#正则化（Rugularization）" class="headerlink" title="正则化（Rugularization）"></a>正则化（Rugularization）</h2><p>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差，下面我们就来讲讲正则化的作用原理。</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>我们用逻辑回归讲解原理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008191606.png" alt=""></p><p>&lambda;/2m乘以&omega;范数的平方。&omega;是欧几里得范数，&omega;的平方等于&omega;<sub>j</sub> (j值从1到n<sub>x</sub>)，也可以表示为&omega;<sup>T</sup>&omega;。此方法称为L2正则化。这里的&lambda;就是正则化参数。因为这里使用了欧几里得法线，被称为向量参数&omega;的L2范式。</p><p>为什么只正则化参数&omega;呢？我们可以加上参数b吗？我们可以这么做，但是一般习惯省略不写。因为&omega;通常是一个高维参数矢量，已经可以表达高偏差问题，&omega;可能包含很多参数，我们不可能拟合所有参数，而b只是单个数字，所以&omega;几乎涵盖所有参数，而不是b。如果加了参数b，没什么影响。因为b也是众多参数的一个，想加就加，没有问题。</p><p>L2正则化是最常见的正则化类型，还有L1正则化。L1正则化加的不是L2范式，而是正则项 &lambda;/m乘以$\sum_{j=1}^n $ |&omega;|，这也被称为参数&omega;向量的L1范数。(这里的n就是项数，和L2范式的nx一样)</p><p>如果用的L1是正则化，最终会是稀疏的，也就是说&omega;向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是L1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用L2正则化。</p><h3 id="为什么正则化有利于预防过拟合？"><a href="#为什么正则化有利于预防过拟合？" class="headerlink" title="为什么正则化有利于预防过拟合？"></a>为什么正则化有利于预防过拟合？</h3><p>如果正则化&lambda;设置得足够大，权重矩阵W被设置为接近于0的值（我也没看懂）。实际上不会发生这种情况的。直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近高偏差状态。</p><p>假设我们用tanh作为我们的激活函数，用g(z)表示tanh(z)。那么我们发现，只要z非常小，z只涉及少量参数。这里我们利用双曲正切函数的线性状态，只要z可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。</p><p>总结一下，如果正则化参数变得很大，参数W很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上，z的取值范围很小，这个激活函数，也就是曲线函数tanh会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p><h3 id="dropout正则化"><a href="#dropout正则化" class="headerlink" title="dropout正则化"></a>dropout正则化</h3><p>除了L2正则化，还有一个非常使用的正则化方法——Dropout(随机失活)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008212828.png" alt=""></p><p>假设你在训练上图这样的神经网络，它存在过拟合，这就是<strong>dropout</strong>所要处理的，我们复制这个神经网络，<strong>dropout</strong>会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008213026.png" alt=""></p><p>这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。不过可想而知，我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p><h3 id="如何实施dropout？"><a href="#如何实施dropout？" class="headerlink" title="如何实施dropout？"></a>如何实施dropout？</h3><p>吴恩达老师讲了最常用的方法。就是inverted dropout(随机失活)。</p><p>首先定义变量d，d3表示一个三层的dropout向量:</p><blockquote><p>d3 = np.random.rand(a3.shape[0],a3.shape[1])</p></blockquote><p>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，keep-prob是一个具体数字，上个示例中它是0.5，而假设在本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2。keep-prob的作用是生成随机矩阵，如果对a3进行因子分解，效果也是一样的。d3是一个矩阵，其中d3中的值为1的概率都是0.8，对应为0的概率是0.2。</p><p>接下来要做的就是从第三层中获取激活函数，这里我们叫它a3，a3含有要计算的激活函数，等于上面的a3乘以d3</p><blockquote><p>a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为a3*=d3。</p></blockquote><p>它的作用就是让过滤d3中所有等于0的元素，而各个元素等于0的概率只有20%，乘法运算最终把d3中相应元素归零，即让d3中0元素与a3中相对元素归零。</p><p>最后，我们向外扩展a3。用它除以0.8，或者除以keep-prob参数。</p><blockquote><p>a3/= (keep-prob)</p></blockquote><p>解释一下最后一步，我们假设第三隐藏层上有50个神经元，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%）个。现在我们看下z<sup>[4]</sup>，z<sup>[4]</sup>=&omega;<sup>[4]</sup>*a<sup>[3]</sup>+b<sup>[4]</sup>，我们的预期是，a<sup>[3]</sup>减少20%，也就是说a<sup>[3]</sup>中有20%的元素被归零。为了不影响z<sup>[4]</sup>的期望值，我们需要用&omega;<sup>[4]</sup>a<sup>[3]</sup>/0.8，它将会修正或弥补我们所需的那20%，a<sup>[3]</sup>的期望值不会变。</p><p>它的功能是，不论keep-prop的值是多少0.8，0.9甚至是1，如果keep-prop设置为1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以keep-prob，确保a<sup>[3]</sup>的期望值不变。</p><p>代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = relu(Z2)</span><br><span class="line"><span class="comment"># 随机生成和 A2 相同维度的矩阵</span></span><br><span class="line">D2 = np.random.rand(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>])    </span><br><span class="line"><span class="comment"># 保留部分元素: 小于keep_prob的设为 True, 否则设为 0 </span></span><br><span class="line">D2 = D2 &lt; keep_prob                             </span><br><span class="line"><span class="comment"># 对应位置的元素相乘, 可以把 True 看做 1, False 看做 0. </span></span><br><span class="line">A2 = np.multiply(A2, D2)                        </span><br><span class="line"><span class="comment"># 如果没有下面这行就是普通的 Dropout</span></span><br><span class="line">A2 = A2/keep_prob</span><br></pre></td></tr></table></figure><p>要注意的是, 我们不希望在测试的时候, 得到的结果也是随机的, 因此Dropout <strong>在测试过程不使用</strong>。</p><h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><p><strong>Dropout</strong>可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？</p><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，所以不愿意给任何一个输入加上太多权重。因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果。和之前讲的L2正则化类似，实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009170758.png" alt=""></p><p>实施dropout的另一个细节是，这是一个拥有三个输入特征的网络，其中一个要选择的参数是keep-prob，它代表每一层上保留单元的概率。所以不同层的keep-prob也可以变化。第一层，矩阵W<sup>[1]</sup>是7×3，第二个权重矩阵W<sup>[2]</sup>是7×7，第三个权重矩阵W<sup>[3]</sup>是3×7，以此类推，W<sup>[2]</sup>是最大的权重矩阵，因为W<sup>[2]</sup>拥有最大参数集，即7×7，为了预防矩阵的过拟合，对于这一层，它的keep-prob值应该相对较低，假设是0.5。对于其它层，过拟合的程度可能没那么严重，它们的keep-prob值可能高一些，可能是0.7甚至更高，假设这里是0.7。如果在某一层，我们不必担心其过拟合的问题，那么keep-prob可以为1。</p><p><strong>总结：</strong>如果我们担心某些层比其他层更容易发生过拟合，可以把某些层的keep-prob值设置得比其他层更低，缺点是为了使用交叉验证，我们要搜索更多得超级参数。另一种方案是在一些层上应用dropout，而有些层不用dropout应用。dropout的层只含有keep-prob这一个超参数。</p><p>dropout一大缺点就是代价函数J不再被明确定义，因为每次迭代都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。所以吴恩达老师推荐通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。</p><h3 id="其它正则化方法"><a href="#其它正则化方法" class="headerlink" title="其它正则化方法"></a>其它正则化方法</h3><h4 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h4><p>假设我们正在拟合猫咪图片分类器，如果我们想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。</p><p>除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。</p><p>通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009193402.png" alt=""></p><p>对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字4看起来是波形的，其实不用对数字4做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个4看起来有点扭曲。所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。</p><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009203859.png" alt=""></p><p>因为在训练过程中，我们希望训练误差，代价函数J都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升。early stopping 代表提早停止训练神经网络。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009200928.png" alt=""></p><p>当我们还未在神经网络上运行太多迭代过程的时候，参数&omega;接近0，因为随机初始化&omega;值时，它的值可能都是较小的随机值，所以在我们长期训练神经网络之前&omega;依然很小，在迭代过程和训练过程中&omega;的值会变得越来越大，比如在这儿，神经网络中参数&omega;的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个&omega;值中等大小的弗罗贝尼乌斯范数，与L2正则化相似，选择参数&omega;范数较小的神经网络，但愿那时的神经网络过度拟合不严重。</p><p>early stopping 有一个缺点，接下来了解一下。</p><p>在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。在重点优化代价函数时，你只需要留意&omega;和b，J(&omega;,b)的值越小越好，你只需要想办法减小这个值，其它的不用关注。然后，预防过拟合还有其他任务，就是减少方差”。</p><p>缺点就是我们不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p><h2 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h2><p>假设我们有一个数据集，它有两个输入特征，所以数据是二维的。下图即为数据集散点图:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212210.png" alt=""></p><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：</p><ol><li>零均值化</li><li>归一化方差；</li></ol><p><strong>我们希望无论是训练集还是测试集都是通过相同的μ和&sigma;<sup>2</sup>定义的数据转换。</strong></p><p>第一步是零均值化。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$x<sup>(i)</sup></p></blockquote><p>μ是一个向量，x等于每个训练数据x减去μ，意思是移动数据集，直到完成零均值化。</p><p>输入数据经过零均值后，得出下面的散点图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212709.png" alt=""></p><p>第二步是归一化方差，注意特征x<sub>1</sub>的方差比特征x<sub>2</sub>的方差要大得多。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$(x<sup>(i)</sup>)<sup>2</sup></p></blockquote><p> &sigma;<sup>2</sup>是一个向量，它的每个特征都有方差。经过归一化方差后，得出下面结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009214916.png" alt=""></p><p>如果你用它来调整训练数据，那么用相同的μ和σ<sup>2</sup>来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论的μ值是什么，也不论的σ<sup>2</sup>值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估μ和σ<sup>2</sup>。因为我们希望不论是训练数据还是测试数据，都是通过相同μ和σ<sup>2</sup>定义的相同数据转换，其中μ和σ<sup>2</sup>是由训练集数据计算得来的。</p><p>为什么我们需要归一化输入特征？回想一下代价函数</p><blockquote><p>J(&omega;,b)=1/m x $\sum_{i=1}^m$L(y<sup>(i)</sup> hat , y<sup>(i)</sup>)</p></blockquote><p>如果输入未归一化的输入特征，代价函数如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170801.png" alt=""></p><p>然而如果归一化特征，代价函数平均起来看更对称，如果要在上图这样的代价函数上运行梯度下降法，必须使用一个非常小的学习率。因为如果是在这个位置（蓝色箭头所在位置），梯度下降法可能需要多次迭代过程，直到最后找到最小值。但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，我们可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行。</p><p>下图即为输入归一化输入特征的代价函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009222132.png" alt=""></p><p>如果输入特征处于不同范围内，可能有些特征值从0到1，有些从1到1000，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。</p><h2 id="梯度消失-梯度爆炸"><a href="#梯度消失-梯度爆炸" class="headerlink" title="梯度消失/梯度爆炸"></a>梯度消失/梯度爆炸</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p><p>吴恩达老师给我们的理解是：权重W只比1略大一点（或者说比单位矩阵略大一点），深度神经网络的激活函数将爆炸式增长，如果W比单位矩阵略小一点，激活函数将以指数级递减。</p><h2 id="神经网络的权重初始化"><a href="#神经网络的权重初始化" class="headerlink" title="神经网络的权重初始化"></a>神经网络的权重初始化</h2><p>为了避免上述的梯度爆炸，权重的初始化很重要。首先，我们来看看只有一个神经元的情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021163626.png" alt=""></p><p>假设b=0，可以得到：</p><p>Z=W<sub>1</sub>x<sub>1</sub>+W<sub>2</sub>x<sub>2</sub>+W<sub>3</sub>x<sub>3</sub>+……+W<sub>n</sub>x<sub>n</sub>，b=0。</p><p>为了不让Z那么大，我们尽量让W<sub>i</sub>小一些。实际做法如下：</p><p>​    使用Relu函数，W的初始化方法：</p><p>​    <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021163946.png" alt=""></p><p>​    使用tanh函数，W的初始化方法：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021164016.png" alt=""></p><p>其中, n<sup>[l−1]</sup> 表示第l层的输入特征的个数, 也就是第 l - 1 层神经元的个数.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。&lt;/p&gt;
&lt;h2 id=&quot;训练、验证和测试集&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
