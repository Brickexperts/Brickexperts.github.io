<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-11T15:33:05.098Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ubuntu下安装qt5</title>
    <link href="http://yoursite.com/2019/11/11/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85qt5/"/>
    <id>http://yoursite.com/2019/11/11/ubuntu下安装qt5/</id>
    <published>2019-11-11T11:45:29.000Z</published>
    <updated>2019-11-11T15:33:05.098Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>用darknet训练自己的数据</title>
    <link href="http://yoursite.com/2019/11/09/%E7%94%A8darknet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2019/11/09/用darknet训练自己的数据/</id>
    <published>2019-11-09T13:53:32.000Z</published>
    <updated>2019-11-19T05:25:34.349Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客采用darknet训练自己的数据，那么在训练自己的数据之前，我们得先拥有自己数据，怎么得到呢?只能自己做了</p><h2 id="安装labelImg"><a href="#安装labelImg" class="headerlink" title="安装labelImg"></a>安装labelImg</h2><p>我用过精灵标注助手和labelImg两款标注工具，标注后得出得XML不一样。本篇博客采用labelImg工具标注图片。</p><p>环境：python3、ubuntu18.04</p><p><code>sudo apt-get install pyqt5-dev-tools</code><br><code>sudo pip3 install lxml</code></p><p>下载labelImg源码</p><p><code>git clone https://github.com/tzutalin/labelImg.git</code></p><p>进入labelImg目录下</p><p>cd labelImg </p><p>再make qt5py3，建议不要make all。出现下面这种结果即为成功</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110101542.png" alt=""></p><p>然后python3 labelImg.py。出现界面即为成功。woc，这是我最顺利的一次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110101733.png" alt=""></p><h2 id="制作自己的数据集"><a href="#制作自己的数据集" class="headerlink" title="制作自己的数据集"></a>制作自己的数据集</h2><p>首先进入darknet目录下，再目录下新建文件夹VOC2019，并在VOC2019下新建Annotations，ImageSets，JPEGImages三个文件夹。在ImageSets新建Main文件夹。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110104912.png" alt=""></p><p>将自己的数据集图片放到JPEGImages目录下，将标注文件放到Annotations目录下。接着开始标注数据。过程就随便说以下。[Open Dir]或Ctrl+u选择要标注的图片所在的根目录，[Create\nRectBox]或w开始标注，鼠标框选目标区域后选择对应的标签类别,按空格或Ctrl+s保存，[Next Image]或d切换到下一张图片，标注错误的选框可选中后按[Delete]删除。要注意的是，如果不是使用原有的目标检测物体的类别，我们要打开data/predefined_classes.txt，修改默认类别为要检测的类别。</p><p>接着再VOC2019下新建test.py文件，将以下代码拷贝进去。在ImageSets的Maxin文件夹下将生成四个文件：train.txt，val.txt，test.txt，trainval.txt。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">trainval_percent = <span class="number">0.1</span></span><br><span class="line">train_percent = <span class="number">0.9</span></span><br><span class="line">xmlfilepath = <span class="string">'Annotations'</span></span><br><span class="line">txtsavepath = <span class="string">'ImageSets\Main'</span></span><br><span class="line">total_xml = os.listdir(xmlfilepath)</span><br><span class="line"></span><br><span class="line">num = len(total_xml)</span><br><span class="line">list = range(num)</span><br><span class="line">tv = int(num * trainval_percent)</span><br><span class="line">tr = int(tv * train_percent)</span><br><span class="line">trainval = random.sample(list, tv)</span><br><span class="line">train = random.sample(trainval, tr)</span><br><span class="line"></span><br><span class="line">ftrainval = open(<span class="string">'ImageSets/Main/trainval.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">ftest = open(<span class="string">'ImageSets/Main/test.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">ftrain = open(<span class="string">'ImageSets/Main/train.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">fval = open(<span class="string">'ImageSets/Main/val.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">    name = total_xml[i][:<span class="number">-4</span>] + <span class="string">'\n'</span></span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">in</span> trainval:</span><br><span class="line">        ftrainval.write(name)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> train:</span><br><span class="line">            ftest.write(name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fval.write(name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ftrain.write(name)</span><br><span class="line"></span><br><span class="line">ftrainval.close()</span><br><span class="line">ftrain.close()</span><br><span class="line">fval.close()</span><br><span class="line">ftest.close()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110123328.png" alt=""></p><p>YOLOV3的label标注的一行五个数分别代表类别（从 0 开始编号）， BoundingBox 中心 X 坐标，中心 Y 坐标，宽，高。这些坐标都是 0～1 的相对坐标。和我们刚才标注的label不同，因此我们需要下面的py文件帮我们转换label。</p><p><code>wget  https://pjreddie.com/media/files/voc_label.py</code></p><p>也可以在windows下好了拷到ubuntu下。总之把这个文件放到darknet文件夹下。打开voc_label.py文件，修改sets和classes。sets如下，classes根据自己的类别需要修改。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110222155.png" alt=""></p><p>打开终端输入<code>python voc_label.py</code>，于是在当前目录生成三个txt文件2019_train.txt，2019_val.txt，2019_test.txt。在VOCdevkit文件夹下的VOC2019也会多生成一个文件夹labels。点开里面的文件就会发现以及转化成YOLOv3需要的格式了。数据集的制作完成，bingo！！！</p><h2 id="局部修改"><a href="#局部修改" class="headerlink" title="局部修改"></a>局部修改</h2><p>1、打开darknet下的cfg文件夹，修改voc.data。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110222652.png" alt=""></p><p>根据自己的需要修改classes类别个数，train和valid的地址。names和backup不用修改。</p><p>2、修改data/voc.names和coco.names。打开对应的文件发现都是原本数据集里的类别，改成自己需求的类别就行。</p><p>3、修改参数文件cfg/yolov3-voc.cfg，用ctrl+f搜 yolo, 总共会搜出3个含有yolo的地方。每个地方都必须要改2处， filters：3*（5+len（classes））和classes类别数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110223739.png" alt=""></p><p>可修改：random，原本是1，显存小改为0。（是否要多尺度输出。）</p><h2 id="报错-amp-训练"><a href="#报错-amp-训练" class="headerlink" title="报错&amp;训练"></a>报错&amp;训练</h2><p>首先下载darknet53的预训练模型：</p><p><code>wget https://pjreddie.com/media/files/darknet53.conv.74</code></p><p>开始训练：</p><p><code>./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74</code></p><p>你以为就这样结束了吗？我就知道没怎么简单。又报错了，报错信息如下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191110144312.png" alt=""></p><p>检查文件和路径，完全正确。</p><p>上网找了解决方案，如下</p><p>下载一个notepad++，打开文件。</p><p>选择 视图 -&gt; 显示符号 -&gt; 显示所有符号；</p><p>选择 编辑 -&gt; 文档格式转换 -&gt; 转换为UNIX（LF）格式；</p><p>转换完成后的格式如下：</p><p>注：</p><p>1.注意检查最后一行是否有LF标志。</p><p>2.为保证不出错，将所有训练过程中使用到的相关文件都修改。</p><p>用了解决方案也还没成功，后续会继续跟进</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇博客采用darknet训练自己的数据，那么在训练自己的数据之前，我们得先拥有自己数据，怎么得到呢?只能自己做了&lt;/p&gt;
&lt;h2 id=&quot;安装labelImg&quot;&gt;&lt;a href=&quot;#安装labelImg&quot; class=&quot;headerlink&quot; title=&quot;安装label
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="darknet" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/darknet/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下安装darknet</title>
    <link href="http://yoursite.com/2019/11/09/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85darknet/"/>
    <id>http://yoursite.com/2019/11/09/ubuntu下安装darknet/</id>
    <published>2019-11-09T12:36:33.000Z</published>
    <updated>2019-11-19T04:57:42.547Z</updated>
    
    <content type="html"><![CDATA[<p>首先下载源码</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/pjreddie/darknet.git</span><br><span class="line">cd darknet</span><br></pre></td></tr></table></figure><p>进入darknet目录后，打开Makefile。用到那个将对应的0改为1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109204621.png" alt=""></p><p>比如我没有GPU，就不用修改GPU为1。但我用到了opencv，将opencv的0改为1。</p><p>编译源码</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">make</span></span><br></pre></td></tr></table></figure><p>测试是否安装成功</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">./darknet</span></span><br></pre></td></tr></table></figure><p>此时看到如下信息即为安装成功。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109205410.png" alt=""></p><p>我在执行这步的时候报错了，报错信息如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109205513.png" alt=""></p><p>解决方案如下，在终端执行下面命令：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="string">/bin/bash</span> -c '<span class="keyword">echo</span> <span class="string">"/usr/local/lib"</span> &gt; <span class="string">/etc/ld.so.conf.d/opencv.conf</span>'</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><p>安装成功后，可以先下载预训练模型测试效果。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> wget https:<span class="comment">//pjreddie.com/media/files/yolov3.weights </span></span><br><span class="line">./darknet detect cfg/yolov3<span class="selector-class">.cfg</span> yolov3<span class="selector-class">.weights</span> data/dog.jpg</span><br></pre></td></tr></table></figure><p>我在这里又报错了，报错信息如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109211403.png" alt=""></p><p>解决方案：sudo apt-get install libcanberra-gtk-module</p><p>再运行一次</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109212322.png" alt=""></p><p>可以看到YOLO的detection图。到这里，YOLOV3已经走通了，是时候加入自己的数据了。 请看下回分解。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先下载源码&lt;/p&gt;
&lt;figure class=&quot;highlight crmsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="darknet" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/darknet/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下编译安装opencv</title>
    <link href="http://yoursite.com/2019/11/09/ubuntu%E4%B8%8B%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85opencv/"/>
    <id>http://yoursite.com/2019/11/09/ubuntu下编译安装opencv/</id>
    <published>2019-11-09T11:37:17.000Z</published>
    <updated>2019-11-19T05:21:00.124Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客使用的ubuntu版本是18.04和opencv3.2</p><p>首先安装CMAKE</p><p><code>sudo apt-get install cmake</code></p><p>接着安装一些依赖项</p><p><code>sudo apt-get install build-essential pkg-config</code></p><p>安装视频I/O包：</p><p><code>sudo apt-get install libgtk2.0-dev  libavcodec-dev libavformat-dev libswscale-dev</code></p><p>安装gtk2.0：</p><p><code>sudo apt-get install libgtk2.0-dev</code></p><p>安装常用图像工具包：</p><p><code>sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev libdc1394-22-dev</code></p><p>如果没有报错，那是最好的。但是我报错了。报错信息如下：</p><p><code>E: Package &#39;libpng12-dev&#39; has no installation candidate</code><br><code>E: Unable to locate package libjasper-dev</code></p><p>上网找了解决方案，首先先解决第一个错：</p><p><code>libpng12-dev</code>在Ubuntu16.04之后就被丢弃了，所以放弃用这个吧。把 <code>libpng12-dev</code> 换成 <code>libpng-dev</code> 就行了</p><p>接着是第二个错误：</p><p><code>sudo add-apt-repository &quot;deb http://security.ubuntu.com/ubuntu xenial-security main&quot;</code><br><code>sudo apt update</code><br><code>sudo apt install libjasper1 libjasper-dev</code></p><p>然后从官网下载源码，直接git clone。也可以从windows拷到虚拟机（要装VM tools）。<a href="https://github.com/opencv/opencv/tree/3.2.0" target="_blank" rel="noopener">网址</a></p><p><code>git clone https://github.com/opencv/opencv.git</code></p><p>在解压后的文件夹中新建build文件夹，用来存放编译文件。</p><p><code>mkdir build</code><br><code>cd build</code></p><p>在电脑上有多个版本的python时，可以通过-D PYTHON_DEFAULT_EXECUTABLE=$(which python3)来确定安装在哪个版本python上。</p><p><code>cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON_DEFAULT_EXECUTABLE=​\$(which python3) ..</code></p><p>如果执行上面代码没有问题，直接make。接着再执行sudo make install。</p><p>我又报错了。。。。报错信息如下：</p><p> — ICV: Downloading ippicv_linux_20151201.tgz…<br>CMake Error at 3rdparty/ippicv/downloader.cmake:73 (file):<br>file DOWNLOAD HASH mismatch</p><p>for file: [/root/library/opencv/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/ippicv_linux_20151201.tgz]<br>expected hash: [808b791a6eac9ed78d32a7666804320e]<br>actual hash: [d41d8cd98f00b204e9800998ecf8427e]<br>status: [1;”Unsupported protocol”]</p><p>Call Stack (most recent call first):<br>3rdparty/ippicv/downloader.cmake:110 (_icv_downloader)<br>cmake/OpenCVFindIPP.cmake:243 (include)<br>cmake/OpenCVFindLibsPerf.cmake:37 (include)<br>CMakeLists.txt:558 (include)</p><p>CMake Error at 3rdparty/ippicv/downloader.cmake:77 (message):<br>ICV: Failed to download ICV package: ippicv_linux_20151201.tgz.<br>Status=1;”Unsupported protocol”<br>Call Stack (most recent call first):<br>3rdparty/ippicv/downloader.cmake:110 (_icv_downloader)<br>cmake/OpenCVFindIPP.cmake:243 (include)<br>cmake/OpenCVFindLibsPerf.cmake:37 (include)<br>CMakeLists.txt:558 (include)</p><p>— Configuring incomplete, errors occurred!<br>See also “/root/library/opencv/opencv-3.2.0/build/CMakeFiles/CMakeOutput.log”.<br>See also “/root/library/opencv/opencv-3.2.0/build/CMakeFiles/CMakeError.log”. </p><p>百度一下，这是因为我们在编译opencv的时候需要下载ippicv_linux_20151201.tgz，但是由于网络的原因，经常下载失败。解决方案：</p><p>手动下载ippicv_linux_20151201.tgz，网上都有资源。读者找一下。接着放入相关路径，我的路径是</p><p>home/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/。读者根据自己的路径放。</p><p>在重新执行cmake。</p><p>接着执行完cmake后，make编译，sudo make install 安装。</p><p>到这里还没装完我们要编译的文件，还有一些模块保留在opencv_contrib的资源库中。所以这个我们也要编译。</p><p>将opencv_contrib下到build的同级目录下，在build目录下打开终端或者在终端进入build目录，</p><p><code>cd  build</code><br><code>cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON_DEFAULT_EXECUTABLE=$(which python3) -D OPENCV_EXTRA_MODULES_PATH=../opencv_contrib-3.2.0/modules/ ..</code><br><code>make</code><br><code>sudo make install</code></p><p>在终端运行python3，import cv2。没有报错，opencv安装就成功了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109203044.png" alt=""></p><p>我装过好几次opencv，这是我最顺的一次。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇博客使用的ubuntu版本是18.04和opencv3.2&lt;/p&gt;
&lt;p&gt;首先安装CMAKE&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo apt-get install cmake&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;接着安装一些依赖项&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo apt-ge
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下装python3和库</title>
    <link href="http://yoursite.com/2019/11/09/ubuntu%E4%B8%8B%E8%A3%85python3/"/>
    <id>http://yoursite.com/2019/11/09/ubuntu下装python3/</id>
    <published>2019-11-09T02:35:36.000Z</published>
    <updated>2019-11-19T05:52:22.773Z</updated>
    
    <content type="html"><![CDATA[<p>ubuntu下的环境配得我要吐了，全都是坑。一定要写博客避坑。后面还有编译opencv、tensorflow等环境。</p><p>本片博客安装的python版本是python3.5。<strong>不用卸载python2，不用卸载python2，不用卸载python2</strong></p><p>首先更新软件包：</p><p><code>sudo apt-get update</code></p><p>接着执行一下命令：</p><p><code>sudo apt-get install python3.5</code></p><p>安装pip3：</p><p><code>sudo apt-get install python3-pip</code></p><p>在这里我就报错了：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109115010.png" alt=""></p><p>上网找了解决方案：</p><p>第一种情况：进程中存在与apt相关的正在运行的进程</p><p>首先检查是否在运行apt，apt-get相关的进程。</p><p> <code>ps aux | grep -i apt</code> </p><p>如果存在与apt相关的正在运行的进程，kill掉</p><p><code>sudo kill -9 &lt;进程号&gt;</code></p><p>或者简单粗暴的直接kill掉：</p><p><code>sudo killall apt apt-get</code></p><p>再执行一次<code>sudo apt-get install python3-pip</code>。如果这还不行，那就是第二种情况了</p><p>第二种情况：</p><p>产生错误的根本原因是lock file。 loack file用于防止两个或多个进程使用相同的数据。 当运行apt或apt-commands时，它会在几个地方创建lock files。 当前一个apt命令未正确终止时，lock file未被删除，因此它们会阻止任何新的apt / apt-get命令实例，比如正在执行apt-get upgrade，在执行过程中直接ctrl+c取消了该操作，很有可能就会造成这种情况。要解决此问题，首先要删除lock file。</p><p>首先使用lsof命令获取持有lock file的进程的ID：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109121205.png" alt=""></p><p>如果三个命令都没有返回值，则说明没有正在运行的进程。如果返回了相应的进程，则需要kill掉。</p><p>接着删除所有的lock file：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109121413.png" alt=""></p><p>最后重新配置一下dpkg：</p><p><code>sudo dpkg --configure -a</code></p><p>到了这一步，没有报错的话，完事大吉。再执行<code>sudo apt-get python3-pip</code></p><p>但是，屋漏偏逢连阴雨。太难了。又报错了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109121954.png" alt=""></p><p>接着找出正在锁定lock file的进程：</p><p><code>lsof /var/lib/dpkg/lock-frontend</code></p><p>如果上述命令返回进程，kill掉输出的进程。</p><p><code>sudo kill -9 进程号</code></p><p>删除lock file 并重新配置dpkg：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo rm /var/<span class="class"><span class="keyword">lib</span>/<span class="title">dpkg</span>/<span class="title">lock</span>-<span class="title">frontend</span></span></span><br><span class="line">sudo dpkg --configure -a</span><br></pre></td></tr></table></figure><p>再重新配置一下dpkg。</p><p>后面几步的命令集合：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109124103.png" alt=""></p><p>到了这里，就全部完成了。</p><p>接下来安装一些库：</p><p>先安装build依赖包：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191109132649.png" alt=""></p><p>接着就可以安装python库了。</p><p><code>sudo pip3 install numpy</code></p><p>等等一些库</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;ubuntu下的环境配得我要吐了，全都是坑。一定要写博客避坑。后面还有编译opencv、tensorflow等环境。&lt;/p&gt;
&lt;p&gt;本片博客安装的python版本是python3.5。&lt;strong&gt;不用卸载python2，不用卸载python2，不用卸载python2&lt;/
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Yolo算法实战</title>
    <link href="http://yoursite.com/2019/10/31/Yolo%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2019/10/31/Yolo算法实战/</id>
    <published>2019-10-31T12:27:47.000Z</published>
    <updated>2019-11-19T05:00:39.562Z</updated>
    
    <content type="html"><![CDATA[<h2 id="coco介绍"><a href="#coco介绍" class="headerlink" title="coco介绍"></a>coco介绍</h2><p>本次实战采用coco数据集。这个数据集是由微软团队提供的。<a href="http://cocodataset.org/#download" target="_blank" rel="noopener">下载网址</a></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031203750.png" alt=""></p><p>annotations是采用json格式标注的，包含三个信息：object instances(物体当前的实例信息)、object keypoints(关键点信息)、image caption(图像信息)</p><p>下图是标注文件的整体结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031210251.png" alt=""></p><p>info是基本信息，image是图像信息，annotations是针对于图像数据的标注信息。下图是除了annotations字段其它字段展开后的具体信息：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031210505.png" alt=""></p><p>我们要具体关注annotations的信息。annotations信息如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191031211132.png" alt=""></p><p>对于annotations，如果我们标注当前的图片为单个物体或多个物体时我们需要修改iscrowd和segmentation。</p><p>单个object时：</p><blockquote><p>iscrowd=0</p><p>segmentation=polygon</p></blockquote><p>多个objects时：</p><blockquote><p>iscrowd=1</p><p>segmentation=RLE</p></blockquote><h2 id="检测模型的搭建"><a href="#检测模型的搭建" class="headerlink" title="检测模型的搭建"></a>检测模型的搭建</h2><h3 id="Darknet的搭建"><a href="#Darknet的搭建" class="headerlink" title="Darknet的搭建"></a>Darknet的搭建</h3><p>Darknet是一个较为轻型的完全基于C与CUDA的开源深度学习框架。支持CPU和GPU两种计算方式。容易安装，且没有任何依赖项。</p><p> 在windows上我是看这篇博客配好darknet的。<a href="https://blog.csdn.net/lvsehaiyang1993/article/details/81032826" target="_blank" rel="noopener">https://blog.csdn.net/lvsehaiyang1993/article/details/81032826</a> </p><p>注意在使用darknet的时候最后如果出现Couldn’t open file: D:/darknet/data/coco.names。重新下一次zip文件、解压再编译一次。博客中说会有predictions.png文件，我并没有找到。百度是说，电脑内存不够运算。</p><p>后面训练的时候，有报错</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191103104157.png" alt=""></p><p>在网上没有找到解决方案。所以更换了系统再配一次。</p><p>在ubuntu上我配了一次。配得我都吐了，最后还把opencv装到python2上。过程不写了，网上都是。也都不是。坑踩踩就好了。我的博客有教程</p><h3 id="Darknet解读"><a href="#Darknet解读" class="headerlink" title="Darknet解读"></a>Darknet解读</h3><p>下面的部分图片是windows。</p><p>Darknet文件结构：</p><p>src、include、obj：存放了darknet的源码和编译后的文件</p><p>cfg：存放了作者提供好的各种各样的网络配置文件</p><p>打开cfg文件夹下的yolov3.cfg文件，下图网络配置信息的含义。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102204940.png" alt=""></p><p>接下去的就是主干网络。一组中括号加上网络层的名字定义当前的网络属于哪一层。接着是训练过程中用到的信息。然后就是中括号加上convolutional定义卷积层和卷积层的参数，同样定义了池化层和池化层参数。略过，我们直接看Yolo层</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102211807.png" alt=""></p><p>接着打开coco.data文件，配置如下：</p><p>在windows下，我没有找到train和valid两个文件列表。linux也没有找到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102213118.png" alt=""></p><p>data：存放了接下来用到的数据</p><p>script：存放了一些脚本</p><p>python：存放了darknet编译出来针对于python的接口</p><p>examples：存放了我们可能会用到的函数接口</p><p>backup：存放了模型训练时中间结果。比如说，在训练1000次的时候存一个model，10000次的时候存一个model。这个训练多少次保存一次模型，也是可以修改的。在examples文件夹下的detector.c文件的138行修改。<img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102164144.png" alt=""></p><p><strong>注意：每次修改darknet配置，都要重新编译。</strong></p><h3 id="Darknet的使用"><a href="#Darknet的使用" class="headerlink" title="Darknet的使用"></a>Darknet的使用</h3><p>安装完darknet后，在终端运行./darknet，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191103191816.png" alt=""></p><p>在window上运行darknet就行了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191102164403.png" alt=""></p><p>这句话告诉我们调用darknet来进行模型训练，我们需要采用的格式为./darknet 函数来完成调用。</p><p>在ubuntu进行数据增强：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191103192537.png" alt=""></p><p>在windows上显示不出来。我怀疑是因为我的windows上没有编译运行opencv，编译过后应该可以运行的。大家可以试一下。</p><p>训练命令：</p><p>./darknet detector train cfg/coco.data  cfg/yolov3.cfg  </p><p>采用预训练的模型：</p><p>./darknet detector train cfg/coco.data  cfg/yolov3.cfg  cfg/yolov3_20000.weights</p><p>测试命令：</p><p>./darknet detector test cfg/coco.data  cfg/yolov3.cfg  backup/yolov3_20000.weights data/giraffe.jpg -thresh 0.4</p><p>-thresh用于阈值筛选</p><p>在example下，darknet有提供python的接口。我们也可以利用这个接口完成模型测试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;coco介绍&quot;&gt;&lt;a href=&quot;#coco介绍&quot; class=&quot;headerlink&quot; title=&quot;coco介绍&quot;&gt;&lt;/a&gt;coco介绍&lt;/h2&gt;&lt;p&gt;本次实战采用coco数据集。这个数据集是由微软团队提供的。&lt;a href=&quot;http://cocodatas
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="Yolo" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/Yolo/"/>
    
    
      <category term="Yolo算法" scheme="http://yoursite.com/tags/Yolo%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Yolo系列算法</title>
    <link href="http://yoursite.com/2019/10/29/Yolo-%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/10/29/Yolo-系列算法/</id>
    <published>2019-10-29T01:50:12.000Z</published>
    <updated>2019-11-19T05:57:59.823Z</updated>
    
    <content type="html"><![CDATA[<p>对于目标检测，我们最多到底能检测多少个类别呢？对于Yolo来说，是9000个。这是非常厉害的，所以，接下来看看Yolo的三代算法Yolo v1、Yolo v2、Yolo v3。</p><p>目标检测经历了一个高度的符合人类的直觉的过程。既需要识别出目标的位置，将图片划分成小图片扔进算法中去，当算法认为某物体在这个小区域上之时，那么检测完成。那我们就认为这个物体在这个小图片上了。而这个思路，正是比较早期的目标检测思路，比如R-CNN。后来的Fast R-CNN，Faster R-CNN虽有改进，比如不再是将图片一块块的传进CNN提取特征，而是整体放进CNN提取特征图后，再做进一步处理，但依旧是整体流程分为 ‘区域提取’和‘目标分类’两部分（two-stage），这样做的一个特点是虽然确保了精度，但速度非常慢。而Yolo将物体检测任务当作一个regression（回归）问题来处理，每张图像只需要“看一眼”就能得出图像中都有哪些物体和这些物体的位置。其实Yolo展开就是you only look once。Yolo是One-stage算法。</p><h2 id="Yolo-v1"><a href="#Yolo-v1" class="headerlink" title="Yolo v1"></a>Yolo v1</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>YOLO v1的核心思想在于将目标检测作为回归问题解决 ，YOLO v1首先会把原始图片放缩到448×448的尺寸，放缩到这个尺寸是为了后面整除来的方便。然后将图片划分成SxS个区域，注意这个区域的概念不同于上文提及将图片划分成N个区域扔进算法的区域不同。上文提及的区域是将图片进行剪裁，或者说把图片的某个局部的像素输入算法中，而这里的划分区域，只的是逻辑上的划分。 </p><p> 如果一个对象的中心落在某个单元格上，那么这个单元格负责预测这个物体。每个单元格需要预测B(超参数)个边界框（bbox）值(bbox值包括坐标和宽高)，同时为每个bbox值预测一个置信度(confidence scores)。此后以每个单元格为单位进行预测分析。</p><p>这个置信度并不只是该边界框是待检测目标的概率，而是该边界框是待检测目标的概率乘上该边界框和真实位置的IoU（框之间的交集除以并集）的积。通过乘上这个交并比，反映出该边界框预测位置的精度。如下式所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029112732.png" alt=""></p><p> 每个边界框对应于5个输出，分别是x，y，w，h和置信度。其中x，y代表边界框的中心离开其所在网格单元格边界的偏移。w，h代表边界框真实宽高相对于整幅图像的比例。x，y，w，h这几个参数都已经被限制到了区间[0,1]上。除此以外，每个单元格还产生C个概率(有多少类物体，C就是多少)。<strong>注意，我们不管B的大小，每个单元格只产生一组这样的概率。</strong> </p><p>然后我们让每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score: </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029143235.png" alt=""></p><p>P(Class<sub>i</sub>)就是每个网络预测的类别信息。 这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。</p><h3 id="网络模型架构"><a href="#网络模型架构" class="headerlink" title="网络模型架构"></a>网络模型架构</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029143846.png" alt=""></p><p> 网络结构借鉴了 GoogLeNet 。24个卷积层，2个全连接层。 </p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>YOLO v1全部使用了均方差（mean squared error）作为损失（loss）函数。由三部分组成：坐标误差、IOU误差和分类误差。</p><p>考虑到每种loss的贡献率，YOLO v1给坐标误差（coordErr）设置权重λcoord=5。在计算IoU误差时，包含物体的格子与不包含物体的格子（此处的‘包含’是指存在一个物体，它的中心坐标落入到格子内），二者的IOU误差对网络loss的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的置信度值近似为0，变相放大了包含物体的格子的置信度误差，在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用λnoobj（置信度误差）=0.5修正iouErr。</p><p> 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（w和h）进行求平方根来改进这个问题，但并不能完全解决这个问题。 </p><p>所以，Loss计算公式：                                                              PS：连数学符号都没认全，我太难了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029145518.png" alt=""></p><p>后面看别人的博客，才知道公式意思如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029151345.png" alt=""></p><p>在激活函数上：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029144700.png" alt=""></p><p> 在最后一层使用的是标准的线性激活函数，其他的层都使用leaky rectified 线性激活函数。 </p><h2 id="Yolo-v2-Yolo-9000"><a href="#Yolo-v2-Yolo-9000" class="headerlink" title="Yolo v2/Yolo 9000"></a>Yolo v2/Yolo 9000</h2><p>YOLO v1对于bounding box的定位不是很好，在精度上比同类网络还有一定的差距，所以YOLOv2对于速度和精度做了很大的优化，并且吸收了同类网络的优点，一步步做出尝试。</p><p>YOLO v2在v1基础上做出改进后提出。其受到Faster RCNN方法的启发，引入了anchor（先验框）。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中。并且修改了网络结构，去掉了全连接层，改成了全卷积结构。在训练时引入了世界树（WordTree）结构，将检测和分类问题做成了一个统一的框架，并且提出了一种层次性联合训练方法，将ImageNet分类数据集和COCO检测数据集同时对模型训练。</p><h3 id="预测更准确"><a href="#预测更准确" class="headerlink" title="预测更准确"></a>预测更准确</h3><h4 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h4><p>YOLOv2对每批数据都做了一个归一化预处理。通过在每一个卷积层后添加batch normalization，极大的改善了收敛速度同时减少了对其它正则方法的依赖（Yolo v2不在使用dropout），使得mAP获得了提升。（mAP：平均精度均值（mean Average Precision））</p><p> 通常，一次训练会输入一批样本（batch）进入神经网络。批规一化在神经网络的每一层，在网络（线性变换）输出后和激活函数（非线性变换）之前增加一个批归一化层（BN），BN层进行如下变换：①对该批样本的各特征量（对于中间层来说，就是每一个神经元）分别进行归一化处理，分别使每个特征的数据分布变换为均值0，方差1。从而使得每一批训练样本在每一层都有类似的分布。这一变换不需要引入额外的参数。②对上一步的输出再做一次线性变换，假设上一步的输出为Z，则Z1=γZ + β。这里γ、β是可以训练的参数。增加这一变换是因为上一步骤中强制改变了特征数据的分布，可能影响了原有数据的信息表达能力。增加的线性变换使其有机会恢复其原本的信息。 </p><h4 id="使用高分辨率图像"><a href="#使用高分辨率图像" class="headerlink" title="使用高分辨率图像"></a>使用高分辨率图像</h4><p>YOLOv1在分辨率为224×224的图片上进行预训练，在正式训练时将分辨率提升到448×448，这需要模型去适应新的分辨率。但是YOLOv2是直接使用448×448的输入， 所以YOLO2在采用 224*224 图像进行分类模型预训练后，再采用 448*448 的高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448*448 的分辨率。然后再使用 448*448 的检测样本进行训练，缓解了分辨率突然切换造成的影响。 </p><h4 id="采用先验框anchor"><a href="#采用先验框anchor" class="headerlink" title="采用先验框anchor"></a>采用先验框anchor</h4><p>在预测框的数量上，由于YOLOv2将网络的输入分辨率调整到416×416，下采样率为32，多次卷积后得到13×13的特征图（feature map）。在这上面使用9种anchor boxes，得到13×13×9=1521个，这比YOLOv1大多了。PS：我也不知道为什么突然就变416了？</p><p>YOLOv1利用全连接层的数据完成边框的预测，会导致丢失较多的空间信息，使定位不准。在YOLOv2中作者借鉴了Faster R-CNN中的anchor思想，来改善全连接层带来的影响。</p><p>Anchor是RPN（region proposal network）网络在Faster R-CNN中的一个关键步骤，是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的候选框。</p><p>为了引入anchor boxes来预测候选框，作者在网络中去掉了全连接层。并去掉了最后的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416 * 416，目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个中心框（center cell）。YOLO算法的作者观察到，大物体通常占据了图像的中间位置，可以只用中心的一个框来预测这些物体的位置，否则就要用中间的4个格子来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（采样因子为32），使得输入卷积网络的416 * 416图片最终得到13 * 13的卷积特征图（416/32=13）</p><h4 id="聚类提取先验框尺度。"><a href="#聚类提取先验框尺度。" class="headerlink" title="聚类提取先验框尺度。"></a>聚类提取先验框尺度。</h4><p>使用anchor的时候，anchor boxes的宽和高往往是人工选定的。虽然在训练过程中，网络也会调整宽和高。但一开始就选择了更好的宽和高，网络就更容易得到准确的预测位置。为了使网络更易学到准确的预测位置，作者使用了K-means聚类方法类训练bounding boxes，可以自动找到更好的框宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的框会比较小的框产生更多的误差，聚类结果可能会偏离。为此，作者采用IOU得分作为评价标准，这样的话，误差就和框的尺度无关。最终的距离函数为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029224241.png" alt=""></p><p>centroid是聚类时被选作中心的边框，box就是其它边框，d就是两者间的“距离”。IOU越大，“距离”越近。YOLO2给出的聚类分析结果如下图所示： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029224317.png" alt=""></p><p>上图左边是选择不同的聚类k值情况下，得到的k个centroid边框，计算样本中标注的边框与各centroid的Avg IOU。显然，边框数k越多，Avg IOU越大。YOLO2选择k=5作为边框数量与IOU的折中。对比手工选择的先验框，使用5个聚类框即可达到61 Avg IOU，相当于9个手工设置的先验框60.9 Avg IOU。 </p><h4 id="约束预测边框的位置"><a href="#约束预测边框的位置" class="headerlink" title="约束预测边框的位置"></a>约束预测边框的位置</h4><p> 借鉴于Faster RCNN的先验框方法，在训练的早期阶段，其位置预测容易不稳定。其位置预测公式为： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030172721.png" alt=""></p><p>其中，x，y是预测边框的中心，x<sub>a</sub>，y<sub>a</sub>是先验框(anchor)的中心点坐标，&omega;<sub>a</sub>，h<sub>a</sub>是先验框(anchor)的宽和高，t<sub>x</sub>，t<sub>y</sub>是要学习的参数。在Yolo论文中写的是x=(t<sub>x</sub>*&omega;<sub>a</sub>)-x<sub>a</sub>，根据Faster RCNN，应该是“+”。</p><p>由于t<sub>x</sub>，t<sub>y</sub>的取值没有任何约束，因此预测边框的中心可能出现在任何位置，导致训练早期阶段不容易稳定。Yolo 调整了预测公式，将预测边框的中心约束在特定的grid网格内。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030193343.png" alt=""></p><p>其中，b<sub>x</sub>、b<sub>y</sub>、b<sub>w</sub>、b<sub>h</sub>是预测边框的中心和宽高。Pr(object) * IOU(b,object)是预测边框的置信度，Yolo v1是直接预测置信度的值，这里对预测参数t<sub>0</sub>进行&sigma;(sigmoid)变换后作为置信度的值。c<sub>x</sub>，c<sub>y</sub>是当前网格左上角的距离，要先将网格大小归一化，即令一个网格的宽=1，高=1。p<sub>w</sub>，p<sub>h</sub>是先验框的宽和高。t<sub>x</sub>、t<sub>y</sub>、t<sub>w</sub>、t<sub>h</sub>、t<sub>o</sub>是要学习的参数，分别用于预测边框的中心和宽高，以及置信度。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030195154.png" alt=""></p><p>参考上图，由于σ函数将t<sub>x</sub>、t<sub>y</sub>约束在(0,1)范围内，所以根据上面的计算公式，预测边框的蓝色中心点被约束在蓝色背景的网格内。约束边框位置使得模型更容易学习，且预测更为稳定。 </p><h4 id="passthrough层检测细粒度特征"><a href="#passthrough层检测细粒度特征" class="headerlink" title="passthrough层检测细粒度特征"></a>passthrough层检测细粒度特征</h4><p>对象检测面临的一个问题是图像中对象会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLO2中输入416*416经过卷积网络下采样最后输出是13*13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。</p><p>YOLO2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26*26*512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030200424.png" alt=""></p><p>一拆4是什么拆的呢？举个例子，如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/7baff56b-42aa-31db-8db5-768957bdec3f.jpg" alt=""></p><p>还有其它拆法。就不具述了。</p><h4 id="多尺度图像训练"><a href="#多尺度图像训练" class="headerlink" title="多尺度图像训练"></a>多尺度图像训练</h4><p> 因为去掉了全连接层，YOLO2可以输入任何尺寸的图像。因为整个网络下采样倍数是32，作者采用了{320,352,…,608}等10种输入图像的尺寸，这些尺寸的输入图像对应输出的特征图宽和高是{10,11,…19}。训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。 </p><h3 id="速度更快"><a href="#速度更快" class="headerlink" title="速度更快"></a>速度更快</h3><p>大多数目标检测的框架是建立在VGG-16上的。为了进一步提升速度，Yolo v2提出了Darknet-19网络结构。Darknet-19(19层卷积层和5层池化层)比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到约1/5。</p><p> YOLO2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224*224 ，共训练160个epochs。然后第二阶段将网络的输入调整为 448*448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs。第三个阶段就是修改Darknet-19分类模型为检测模型，移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个 3*3*1024卷积层，同时增加了一个passthrough层，最后使用 1<em>1 卷积层输出预测结果，输出的channels数为：**num_anchors\</em>(5+num_classes)** ，和训练采用的数据集有关系。对于VOC数据集(20种分类对象)，假如anchor数为5，输出的channels就是125。</p><h3 id="识别对象更多"><a href="#识别对象更多" class="headerlink" title="识别对象更多"></a>识别对象更多</h3><p>论文提出了一种联合训练的机制：使用识别数据集训练模型识别相关部分，使用分类数据集训练模型分类相关部分。</p><p>众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。所以在YOLOv1中，边界框的预测其实并不依赖于物体的标签，YOLOv2实现了在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。</p><p>作者选择在COCO和ImageNet数据集上进行联合训练，遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的词树（WordTree）如下图所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191029195823.png" alt=""></p><p>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个路径，然后计算路径上各个节点的概率之积。</p><p>在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是  ，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p><h2 id="Yolo-v3"><a href="#Yolo-v3" class="headerlink" title="Yolo v3"></a>Yolo v3</h2><p>YOLO v3没有太多的创新，主要是借鉴一些好的方案融合到YOLO里面。不过效果还是不错的，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</p><h3 id="新的网络结构"><a href="#新的网络结构" class="headerlink" title="新的网络结构"></a>新的网络结构</h3><p>在基本的图像特征提取方面，Yolo v3采用了称之为Darknet-53的网络结构(含有53个卷积层)，它借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路(shortcut connections)。</p><h3 id="利用多尺度特征进行对象预测"><a href="#利用多尺度特征进行对象预测" class="headerlink" title="利用多尺度特征进行对象预测"></a>利用多尺度特征进行对象预测</h3><p>YOLO2曾采用passthrough结构来检测细粒度特征，在YOLO3更进一步采用了3个不同尺度的特征图来进行对象检测。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030221243.png" alt=""></p><p>结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416*416的话，这里的特征图就是13*13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。</p><p>为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。</p><p>最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。</p><h3 id="9种尺度的先验框"><a href="#9种尺度的先验框" class="headerlink" title="9种尺度的先验框"></a>9种尺度的先验框</h3><p>随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。</p><p>分配上，在最小的13*13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26*26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52*52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030221659.png" alt=""></p><p>感受一下9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格。 </p><p>13*13:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030222109.png" alt=""></p><p>26*26:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030222209.png" alt=""></p><p>52*52:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030222240.png" alt=""></p><h2 id="对象分类softmax改成logistic"><a href="#对象分类softmax改成logistic" class="headerlink" title="对象分类softmax改成logistic"></a><strong>对象分类softmax改成logistic</strong></h2><p>预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象（比如一个人有Woman 和 Person两个标签）。</p><h2 id="输入映射到输出"><a href="#输入映射到输出" class="headerlink" title="输入映射到输出"></a>输入映射到输出</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191030223023.png" alt=""></p><p>不考虑神经网络结构细节的话，总的来说，对于一个输入图像，YOLO3将其映射到3个尺度的输出张量，代表图像各个位置存在各种对象的概率。</p><p>我们看一下YOLO3共进行了多少个预测。对于一个416*416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13*13*3 + 26*26*3 + 52*52*3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。</p><p>对比一下，YOLO2采用13*13*5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。</p><p>本篇文章使用了较多的他人的图片，如有侵权，请联系我删掉。QQ：1171708687</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对于目标检测，我们最多到底能检测多少个类别呢？对于Yolo来说，是9000个。这是非常厉害的，所以，接下来看看Yolo的三代算法Yolo v1、Yolo v2、Yolo v3。&lt;/p&gt;
&lt;p&gt;目标检测经历了一个高度的符合人类的直觉的过程。既需要识别出目标的位置，将图片划分成
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="Yolo" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/Yolo/"/>
    
    
      <category term="Yolo算法" scheme="http://yoursite.com/tags/Yolo%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>可视化</title>
    <link href="http://yoursite.com/2019/10/26/pytorch%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://yoursite.com/2019/10/26/pytorch的可视化/</id>
    <published>2019-10-26T03:29:54.000Z</published>
    <updated>2019-10-28T11:01:07.696Z</updated>
    
    <content type="html"><![CDATA[<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p>Visdom是一个灵活的可视化工具，可以实时显示新创建的数据。Visdom的目的是促进远程数据的可视化，支持科学实验。可以发送可视化图像和文本。通过UI为实时数据创建dashboards，检查实验的结果。</p><h3 id="Panes-窗格"><a href="#Panes-窗格" class="headerlink" title="Panes(窗格)"></a>Panes(窗格)</h3><p>UI刚开始是个白板，我们可以用图像和文本填充它。这些填充的数据出现在Panes，我们可以对这些Panes进行拖放、删除、调整大小和销毁操作。Panes是保存在Environment(环境)中的，Environment(环境)的状态存储在会话之间。可以使用浏览器的放大缩小功能来调整UI的大小。</p><h3 id="Environment-环境"><a href="#Environment-环境" class="headerlink" title="Environment(环境)"></a>Environment(环境)</h3><p>可以使用Envs对可视化空间进行分区。每个用户都会有一个叫做main的Envs。可以通过编程或UI创建新的Envs，Envs的状态是长期保存的。可以通过<a href="https://localhost.com:8097/env/main访问特定的ENV。如果服务器是被托管的，那么可以将此URL分享给别人，那么其他人也会看到可视化结果，" target="_blank" rel="noopener">https://localhost.com:8097/env/main访问特定的ENV。如果服务器是被托管的，那么可以将此URL分享给别人，那么其他人也会看到可视化结果，</a></p><p>在初始化服务器的时候，Envs默认通过$HOME/.visdom/加载。也可以将自定义的路径当作命令行参数传入。如果移除了Envs文件家下的.json文件，那么相应的环境也会删除。</p><h3 id="State-状态"><a href="#State-状态" class="headerlink" title="State(状态)"></a>State(状态)</h3><p>一旦创建了一些可视化，状态是被保存的。服务器自动缓存我们的可视化，如果重新加载网页，可视化就会重新出现。</p><p>Save：可以通过点击save按钮手动保存Envs。它首先会序列化Envs的状态，然后以.json文件的形式保存到硬盘上，包括窗口的位置。</p><p>Fork：输入一个新的Envs名字，“保存”会建立一个新的Envs，有效的分割之前的状态。</p><p>当前用不到可视化，先学后面的。后面有用到就补</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;headerlink&quot; title=&quot;可视化&quot;&gt;&lt;/a&gt;可视化&lt;/h2&gt;&lt;p&gt;Visdom是一个灵活的可视化工具，可以实时显示新创建的数据。Visdom的目的是促进远程数据的可视化，支持科学实验。可以发送可视化
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/pytorch/"/>
    
    
      <category term="深度学习框架" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>pytorch(1)</title>
    <link href="http://yoursite.com/2019/10/22/pytorch-1/"/>
    <id>http://yoursite.com/2019/10/22/pytorch-1/</id>
    <published>2019-10-22T09:23:19.000Z</published>
    <updated>2019-10-26T13:23:47.339Z</updated>
    
    <content type="html"><![CDATA[<p>torch.nn包里的Functional包含convolution函数，pooling函数，非线性函数、激活函数等函数，torch.nn.optim包含各种优化算法，Momentum、RMSProp等。</p><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>torch.Tensor是一种包含单一数据类型元素的多维矩阵。</p><p>Torch定义了七种CPU tensor类型和八种 tensor类型：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191022221728.png" alt=""></p><p>torch.Tensor是默认的tensor类型（torch.FloatTensor）的简称。</p><p>一个张量tensor可以从Python的list或序列构建：</p><blockquote><p>>&gt;&gt;torch.FloatTensor([[1,2,3],[4,5,6]])<br>tensor([[1., 2., 3.],<br>       [4., 5., 6.]])</p><p>>&gt;&gt;torch.Tensor([[3,2,1],[6,5,4]])<br>tensor([[3., 2., 1.],<br>[6., 5., 4.]])</p></blockquote><p>空张量tensor可以通过规定其大小来构建：</p><blockquote><p>>&gt;&gt;torch.IntTensor(2,4).zero_()<br>tensor([[0, 0, 0, 0],<br>        [0, 0, 0, 0]], dtype=torch.int32)</p></blockquote><p>可以用python的索引和切片来获取和修改一个张量tensor中的内容：</p><blockquote><p>>&gt;&gt;x=torch.FloatTensor([[1,2,3],[4,5,6]])<br>>&gt;&gt;print(x[1][2])<br>tensor(6.)<br>>&gt;&gt;print(x[0][1])<br>tensor(2.)<br>>&gt;&gt;x[0][1]=8<br>>&gt;&gt;print(x)<br>tensor([[1., 8., 3.],<br>        [4., 5., 6.]])</p></blockquote><p>每一个张量tensor都有一个相应的tourch.Storage用来保存其数据。</p><p><strong>注意：会改变tensor的函数操作会用一个下划线后缀来表示。</strong>比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而torch.FloatTensor.abs()将会在一个新的tensor中计算结果。</p><p>创建张量时有个参数是requires_grad，如果设置 requires_grad为 True，那么将会追踪所有对于该张量的操作吗，默认False。 当完成计算后通过调用 .backward()，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 .grad属性。 </p><h2 id="torch-squeeze"><a href="#torch-squeeze" class="headerlink" title="torch.squeeze"></a>torch.squeeze</h2><p>torch.squeeze(input,dim=None,out=None)：将输入张量形状中的1去除并返回。当给定dim时，那么挤压操作只在给定维度上。</p><blockquote><p>>&gt;&gt;x=torch.zeros(2,1)<br>>&gt;&gt;x<br>tensor([[0.],<br>        [0.]])<br>>&gt;&gt;y=torch.squeeze(x)<br>>&gt;&gt;y<br>tensor([0., 0.])</p></blockquote><h2 id="torch-abs"><a href="#torch-abs" class="headerlink" title="torch.abs"></a>torch.abs</h2><p>torch.abs(input,out=None)：计算输入张量的每个元素的绝对值</p><blockquote><p>>&gt;&gt;torch.abs(torch.Tensor([-1,-2,-3]),out=z)<br>tensor([1., 2., 3.])<br>>&gt;&gt;z<br>tensor([1., 2., 3.])</p></blockquote><h2 id="torch-add"><a href="#torch-add" class="headerlink" title="torch.add"></a>torch.add</h2><p>torch.add(input,value,out=None)，对输入张量input逐元素加上标量值value，并返回结果得到一个新的张量out。</p><blockquote><p>>&gt;&gt;a=torch.rand(4)<br>>&gt;&gt;a<br>tensor([0.0694, 0.6366, 0.6938, 0.8872])<br>>&gt;&gt;torch.add(a,20)<br>tensor([20.0694, 20.6366, 20.6938, 20.8872])</p></blockquote><h2 id="torch-mean"><a href="#torch-mean" class="headerlink" title="torch.mean"></a>torch.mean</h2><p>torch.mean(input,dim,out=None)：返回输入张量给定维度dim上每行的均值。0是列，1是行。</p><blockquote><p>>&gt;&gt;a=torch.rand(4,4)<br>>&gt;&gt;a<br>tensor([[0.7413, 0.5828, 0.0070, 0.0146],<br>        [0.1668, 0.6566, 0.1865, 0.0700],<br>        [0.7603, 0.8017, 0.3065, 0.8772],<br>        [0.7918, 0.8798, 0.7355, 0.1142]])<br>>&gt;&gt;torch.mean(a,1)<br>tensor([0.3364, 0.2700, 0.6864, 0.6303])</p></blockquote><h2 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h2><h3 id="torch-eq"><a href="#torch-eq" class="headerlink" title="torch.eq"></a>torch.eq</h3><p>torch.eq(input,other,out=None)：比较元素相等性。第二个参数可以为一个数或与第一个参数同类型形状的张量。</p><blockquote><p>>&gt;&gt;torch.eq(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[ True, False],<br>        [False,  True]])</p></blockquote><p>如果两个张量有相同的形状和元素值，则返回True，否则返回False。</p><blockquote><p>>&gt;&gt;torch.equal(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>False</p><p>>&gt;&gt;torch.equal(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,2],[3,4]]))<br>True</p></blockquote><h3 id="torch-ge"><a href="#torch-ge" class="headerlink" title="torch.ge"></a>torch.ge</h3><p>torch.ge(input,other,out=None)：逐元素比较input和other。即是否 input&gt;=otherinput&gt;=other。如果两个张量有相同的形状和元素值，则返回True。否则返回False。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。</p><blockquote><p>>&gt;&gt;torch.ge(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[ True,  True],<br>        [False,  True]])</p></blockquote><h3 id="torch-gt"><a href="#torch-gt" class="headerlink" title="torch.gt"></a>torch.gt</h3><p>torch.gt(input,other,out=None)：逐元素比较input和other。即input&gt;otherinput&gt;other。如果两个张量有相同的形状和元素值，则返回True。否则返回False。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。PS:这个和ge有什么区别？</p><blockquote><p>>&gt;&gt;torch.gt(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[False,  True],<br>        [False, False]])</p></blockquote><h3 id="torch-le"><a href="#torch-le" class="headerlink" title="torch.le"></a>torch.le</h3><p>torch.le(input, other, out=None)：逐元素比较input和other ， 即是否input&lt;=otherinput&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><blockquote><p>>&gt;&gt;torch.le(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[ True, False],<br>        [ True,  True]])</p></blockquote><h3 id="torch-lt"><a href="#torch-lt" class="headerlink" title="torch.lt"></a>torch.lt</h3><p>torch.lt(input, other, out=None)：逐元素比较input和other ， 即是否 input&lt;otherinput&lt;other。第二个参数可以为一个数或与第一个参数相同形状和类型的张量</p><blockquote><p>>&gt;&gt;torch.lt(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))<br>tensor([[False, False],<br>        [ True, False]])</p></blockquote><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p>Variable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。</p><p>首先Variable是在torch.autograd.Variable中，要将tensor变成Variable，只需要Variable(a)就可以了。Variable有三个比较重要的组成属性：data、grad、grad_fn。data可以取出variable里面的tensor值，grad_fn表示的是得到这个Variable的操作(op)，grad是这个Variable的反向传播梯度。</p><blockquote><p>>&gt;&gt;from torch.autograd import Variable<br>>&gt;&gt;x=Variable(torch.Tensor([1]),requires_grad=True)<br>>&gt;&gt;w=Variable(torch.Tensor([2]),requires_grad=True)<br>>&gt;&gt;b=Variable(torch.Tensor([3]),requires_grad=True)<br>>&gt;&gt;y=w*x+b<br>>&gt;&gt;y<br>tensor([5.], grad_fn=<AddBackward0>)<br>>&gt;&gt;print(y.backward())<br>None<br>>&gt;&gt;print(x.grad)<br>tensor([2.])<br>>&gt;&gt;print(w.grad)<br>tensor([1.])<br>>&gt;&gt;print(b.grad)<br>tensor([1.])<br>>&gt;&gt;print(x.grad_fn)<br>None<br>>&gt;&gt;print(y.grad_fn)</p><AddBackward0 object at 0x000001C56569E2B0></blockquote><p>构建Variable，我们传入了一个参数requires_grad=True，这个参数表示是否对这个变量求梯度，默认的是False，也就是不对这个变量求梯度。</p><h2 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h2><p>paramerters(memo=None)：返回一个包含模型所有参数的迭代器。一般用来当作optimizer的参数。</p><h2 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h2><p>如果需要计算导数，可以在Tensor上调用.backward()。 如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数， 但是如果它有更多的元素，需要指定一个gradient参数来匹配张量的形状。 </p><h2 id="zero-grad"><a href="#zero-grad" class="headerlink" title="zero_grad"></a>zero_grad</h2><p>zero_grad()：清空所有被优化过的Variable的梯度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## view</span><br><span class="line"></span><br><span class="line"> 返回一个有相同数据但大小不同的tensor。 返回的tensor必须有与原tensor相同的数据和相同数目的元素，但可以有不同的大小。一个tensor必须是连续的`contiguous()`才能被查看。 </span><br><span class="line"></span><br><span class="line">&gt; \&gt;&gt;&gt;import torch</span><br><span class="line">&gt; \&gt;&gt;&gt;x=torch.randn(4,4)</span><br><span class="line">&gt; \&gt;&gt;&gt;x.size()</span><br><span class="line">&gt; torch.Size([4, 4])</span><br><span class="line">&gt; \&gt;&gt;&gt;y=x.view(16)</span><br><span class="line">&gt; \&gt;&gt;&gt;y.size()</span><br><span class="line">&gt; torch.Size([16])</span><br><span class="line">&gt; \&gt;&gt;&gt;z=x.view(-1,8)</span><br><span class="line">&gt; \&gt;&gt;&gt;z.size()</span><br><span class="line">&gt; torch.Size([2, 8])</span><br><span class="line"></span><br><span class="line">## zero_</span><br><span class="line"></span><br><span class="line">zero_(tensor)：用0填充一个tensor</span><br><span class="line"></span><br><span class="line">&gt; \&gt;&gt;&gt;torch.zero_(x)</span><br><span class="line">&gt; tensor([[0., 0., 0., 0.],</span><br><span class="line">&gt;         [0., 0., 0., 0.],</span><br><span class="line">&gt;         [0., 0., 0., 0.],</span><br><span class="line">&gt;         [0., 0., 0., 0.]])</span><br><span class="line"></span><br><span class="line">## 模型的保存和加载</span><br><span class="line"></span><br><span class="line">在pytorch里面使用torch.save来保存模型的结构和参数，有两种保存方式，当然加载模型有两种方式对应于保存模型的方式：</span><br><span class="line"></span><br><span class="line">（1）保存这个模型的结构信息和参数信息，保存的对象是模型model。加载完整的模型结构和参数信息，在网络较大的时候加载的时间比较长，同时存储空间也比较大。</span><br><span class="line"></span><br><span class="line">torch.save(the_model,path)</span><br><span class="line"></span><br><span class="line">the_model=torch.load(&quot;path&quot;)</span><br><span class="line"></span><br><span class="line">（2）保存模型的参数，保存的对象是模型的状态model.state_dict()。加载模型时加载的是参数信息。</span><br><span class="line"></span><br><span class="line">torch.save(the_model.state_dict(),path)</span><br><span class="line"></span><br><span class="line">the_model=TheModeClass(\*args,\**kwargs)</span><br><span class="line"></span><br><span class="line">the.model.load_state_dict(path)</span><br><span class="line"></span><br><span class="line">## 优化算法</span><br><span class="line"></span><br><span class="line">优化算法分两类：</span><br><span class="line"></span><br><span class="line">### 一阶优化算法</span><br><span class="line"></span><br><span class="line">这种算法使用各个参数的梯度值来更新参数，最常用的一阶优化算法是梯度下降。所谓的梯度就是导数的多变量表达式，同时也是一个方向，这个方向上方向导数最大，且等于梯度。梯度下降的功能是通过寻找最小值，控制方差，更新模型参数，最终使模型收敛。</span><br><span class="line"></span><br><span class="line">### 二阶优化算法</span><br><span class="line"></span><br><span class="line">二阶优化算法使用了二阶导数来最小化或最大化损失函数，主要基于牛顿法。但是由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。torch.optim是一个实现各种优化算法的包，大多数常见的算法都能通过这个包调用。</span><br><span class="line"></span><br><span class="line">### torch.optim</span><br><span class="line"></span><br><span class="line">为了使用torch.optim，我们需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。</span><br><span class="line"></span><br><span class="line">#### 构建optimizer</span><br><span class="line"></span><br><span class="line">为了构建一个optimizer，我们需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，可以设置optimizer的参数选项，比如学习率，权重衰减，等等。 </span><br><span class="line"></span><br><span class="line">举个例子：</span><br><span class="line"></span><br><span class="line"> ```optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)</span><br></pre></td></tr></table></figure><h4 id="单独设置参数"><a href="#单独设置参数" class="headerlink" title="单独设置参数"></a>单独设置参数</h4><p>我们还可以为每个参数单独设置选项。Optimizer也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。每一个dict都分别定 义了一组参数，并且包含一个param键，这个键对应参数的列表。其他的键应该optimizer所接受的其他参数的关键字相匹配，并且会被用于对这组参数的优化。</p><p>你仍然能够传递选项作为关键字参数。在未重写这些选项的组中，它们会被用作默认值。当你只想改动一个参数组的选项，但其他参数组的选项不变时，这是非常有用的。</p><p>例如，当我们想指定每一层的学习率时，这是非常有用的：</p><p> <code>optim.SGD([{&#39;params&#39;: model.base.parameters()}, {&#39;params&#39;: model.classifier.parameters(), &#39;lr&#39;:1e-3], lr=1e-2, momentum=0.9)</code></p><h4 id="单次优化"><a href="#单次优化" class="headerlink" title="单次优化"></a>单次优化</h4><p>使用optimizer.step()，这是大多数optimizer支持的版本，一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。</p><p><code>for input, target in dataset:</code> </p><p>​    optimizer.zero_grad()    </p><p>​    output = model(input)    </p><p>​    loss = loss_fn(output, target)   </p><p>​     loss.backward()   </p><p>​     optimizer.step()</p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>我看到优化算法时，发现中文文档好像没有调整学习率的函数。补充一下。torch.optim.lr_scheduler提供了几种方法来根据epoches的数量调整学习速率。 两种机制：LambdaLR机制和StepLR机制；</p><h4 id="LambdaLR机制："><a href="#LambdaLR机制：" class="headerlink" title="LambdaLR机制："></a>LambdaLR机制：</h4><p>class torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda,last_epoch=-1)</p><p>将每一个参数组的学习率设置为初始学习率lr的某个函数倍.当last_epoch=-1时,设置初始学习率为lr.</p><p>参数:</p><p>​    optimizer(Optimizer对象)—优化器</p><p>​    lr_lambda(是一个函数,或者列表(list))— 当是一个函数时,需要给其一个整数参数,使其计算出一个乘数因子,用于调整学习率,通常该输入参数是epoch数目或者是一组上面的函数组成的列表,</p><p>​    last_epoch(int类型):最后一次epoch的索引,默认为-1.</p><h4 id="StepLR机制"><a href="#StepLR机制" class="headerlink" title="StepLR机制"></a>StepLR机制</h4><p>class torch.optim.lr_scheduler.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1)</p><p>设置每个参数组的学习率为 lr*&lambda;<sup>n</sup>, n=epoch/step_size。当last_epoch=-1时,令lr=lr</p><p>参数:</p><p>​    optimizer(Optimizer对象)—优化器</p><p>​     step_size(整数类型): 调整学习率的步长,每过step_size次,更新一次学习率</p><p>​    gamma(float 类型):学习率下降的乘数因子</p><p>​    last_epoch(int类型):最后一次epoch的索引,默认为-1.</p><h2 id="其它API"><a href="#其它API" class="headerlink" title="其它API"></a>其它API</h2><p>其它的API不想做搬运工了，上网址。<a href="https://www.pytorchtutorial.com/docs/" target="_blank" rel="noopener">pytorch中文文档</a></p><p>看文档我发现torch.nn包里面和nn.functional都有卷积等一些函数。其实nn.functional中的函数仅仅定义了一些具体的基本操作，不能构成pytorch中的一个Layer。当我们需要自定义一些非标准Layer时，可以在其中调用nn.functional中的操作。例如，relu仅仅是一个函数，参数包括输入和计算需要的参数，返回计算的结果，它不能存储任何上下文的信息。</p><p>torch.nn包里面只是包装好了神经网络架构的类，nn.functional 与torch.nn包相比，nn.functional是可以直接调用函数的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;torch.nn包里的Functional包含convolution函数，pooling函数，非线性函数、激活函数等函数，torch.nn.optim包含各种优化算法，Momentum、RMSProp等。&lt;/p&gt;
&lt;h2 id=&quot;Tensor&quot;&gt;&lt;a href=&quot;#Tens
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/pytorch/"/>
    
    
      <category term="深度学习框架" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>超参数调试、batch正则化</title>
    <link href="http://yoursite.com/2019/10/13/%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81batch%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <id>http://yoursite.com/2019/10/13/超参数调试、batch正则化/</id>
    <published>2019-10-13T07:19:36.000Z</published>
    <updated>2019-10-18T14:02:38.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="调试处理"><a href="#调试处理" class="headerlink" title="调试处理"></a>调试处理</h2><p>关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentum或Adam优化算法的参数，&beta;<sub>1</sub>，&beta;<sub>2</sub>和ξ，也许你还得选择层数，也许还得选择不同层中隐藏单元的数量，也许还想使用学习率衰减。所以，使用的不是单一的学习率a。接着，当然可能还需要选择mini-batch的大小。</p><p>&beta;<sub>1</sub>、&beta;<sub>2</sub>、ξ推荐使用0.9、0.999、10<sup>-10</sup>。a是学习速率，学习速率是需要调试的最重要的超参数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014135821.png" alt=""></p><p>现在，如果我们尝试调整一些超参数，该如何选择调试值呢？在早一代的机器学习算法中，如果有两个超参数，这里会称之为超参1，超参2，常见的做法是在网格中取样点，像这样，然后系统的研究这些数值。这里放置的是5×5的网格，实践证明，网格可以是5×5，也可多可少，但对于这个例子，我们可以尝试这所有的25个点，然后选择哪个参数效果最好。当参数的数量相对较少时，这个方法很实用。</p><p>在深度学习领域，吴恩达老师推荐我们使用随机选择点方法，所以我们可以选择同等数量的点，对吗？25个点，接着，用这些随机取的点试验超参数的效果。之所以这么做是因为，对于要解决的问题而言，你很难提前知道哪个超参数最重要，正如之前看到的，一些超参数的确要比其它的更重要。</p><p>假如我们拥有三个超参数呢？这时我们搜索的就不只是一个方格了，而是一个立方体。超参数3代表第三维，接着，在三维立方体中取值，我们会实验更多的值。</p><p>实践中，需要搜索的可能不止三个超参数有时很难预知，哪个是最重要的超参数，对于具体应用而言，随机取值而不是网格取值表明，我们要探究了更多重要超参数的潜在值，无论结果是什么。</p><p>当我们给超参数取值时，另一个惯例是采用由粗糙到精细的策略。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014141132.png" alt=""></p><p>比如在二维的那个例子中，你进行了取值，也许你会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内），然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索，如果你怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，你会知道接下来应该聚焦到更小的方格中。在更小的方格中，你可以更密集得取点。所以这种从粗到细的搜索也经常使用。</p><h2 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h2><p>讲个例子，假如我们在搜索超参数a，假设我们怀疑其值最小是0.0001，最大是1。假如我们把这个取值范围当作数轴，沿其随机均匀取值，那90%的值都会落在0.1到1之间。只有10%的搜索资源在0.0001到0.1之间。反而，用数标尺搜索超参数的方式会更合理，因此不使用线性轴。分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用。</p><p>在python中，这可以通过numpy模块实现。</p><blockquote><p>r=-4 x np.random.rand()</p><p>a=10<sup>r</sup></p></blockquote><p>对a随机取值，由上面第一行代码得出r的取值在[-4,0]之间，a的取值[10<sup>-4</sup>,10<sup>0</sup>]之间。如果我们在10<sup>a</sup>和10<sup>b</sup>之间取值，在此例中，我们可以通过0.0001算出a的值为-4，b的值为0。我们要做的就是在[a,b]区间随机均匀的给r取值，然后设置a的值。所以总结一下，在对数坐标下取值，取最小值的对数就得到a的值，取最大值的对数就得到b值，所以现在你在对数轴上的10<sup>a</sup>到10<sup>b</sup>区间取值，在a，b间随意均匀的选取值，将超参数设置为10<sup>r</sup>，这就是在对数轴上取值的过程。</p><p>最后，还有一个例子是对&beta;取值。用于计算指数的加权平均数。假设我们认为&beta;是0.9到0.999之间的某个值，这也是我们想搜索的范围。</p><p>上面哪个例子说了如果想在0.9到0.999区间搜索，那就不能用线性轴取值。不要随机均匀在此区间取值，所以考虑这个问题最好的方法就是，我们要探究的是1-&beta;，此值在0.1到0.001区间内，所以我们会给1-&beta;取值，大概是从0.1到0.001。所以我们要做的是在[-3,-1]里随机均匀的给r取值。由1-&beta;=10<sup>r</sup>推出&beta;=1-10<sup>r</sup>，然后就变成了在特定的选择范围内超参数的随机取值。</p><h2 id="超参数调试的方法"><a href="#超参数调试的方法" class="headerlink" title="超参数调试的方法"></a>超参数调试的方法</h2><p>两种方法：</p><p>一种是你照看一个模型，通常是有庞大的数据组，但没有许多计算资源或足够的CPU和GPU的前提下，基本而言，只可以一次负担起试验一个模型或一小批模型，在这种情况下，即使当它在试验时，也可以逐渐改良。这是一个人照料一个模型的方法，观察它的表现，耐心的调试学习率。</p><p>另一种方法则是同时试验多种模型，你设置了一些超参数，尽管让它自己运行，或者是一天甚至多天，然后你会获得像这样的学习曲线，这可以是损失函数J或实验误差或损失或数据误差的损失，但都是你曲线轨迹的度量。同时你可以开始一个有着不同超参数设定的不同模型。</p><p>所以这两种方式的选择，是由你拥有的计算资源决定的，如果你拥有足够的计算机去平行试验许多模型，那绝对采用第二种方式，尝试许多不同的超参数，看效果怎么样。</p><h2 id="归一化的激活函数"><a href="#归一化的激活函数" class="headerlink" title="归一化的激活函数"></a>归一化的激活函数</h2><p>在深度学习兴起后，最重要的一个思想是它的一个算法——Batch归一化。batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易。接下来看一下原理。</p><p>之前的逻辑回归，我们讲过归一化输入特征可以加快学习过程。计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化数据集。那么更深的模型呢？我们不仅输入了特征值x，而且第一层有激活值a<sup>[1]</sup>，第二层有激活层a<sup>[2]</sup>等。那么如果我们想训练&omega;<sup>[3]</sup>，b<sup>[3]</sup>，那归一化a<sup>[2]</sup>岂不是更好？便于我们训练&omega;<sup>[3]</sup>和b<sup>[3]</sup>。</p><p>那么，我们能归一化每个隐藏层的a值吗？比如a<sup>[2]</sup>，但不仅仅是a<sup>[2]</sup>，可以是任何隐藏层的。a<sup>[2]</sup>的值是下一层的输入值，所以a<sup>[2]</sup>会影响&omega;<sup>[3]</sup>，b<sup>[3]</sup>的训练。这就是batch归一化的作用。严格来说，归一化的是z<sup>[2]</sup>，并不是a<sup>[2]</sup>。</p><p>在神经网络中，假设你有一些隐藏单元值从z<sup>[1]</sup>到z<sup>[m]</sup>，这些来源于隐藏层，所以这样写会更准确，即z<sup>[ l ]( i )</sup>为隐藏层，i从1到m，所以已知这些值。如下，你要计算平均值，强调一下，所有这些都是针对 l 层，但已省略 l 及方括号,计算方法如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014182957.png" alt=""></p><p>分母中加上ε以防止δ为0的情况。</p><p>所以现在我们已把这些z值标准化，化为含平均值 0 和标准单位方差，所以z的每一个分量都含有平均值 0 和方差 1，但我们不想让隐藏单元总是含有平均值 0 和方差 1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算，我们称之为z~(i)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183405.png" alt=""></p><p>这里γ和β是模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法，比如 Momentum 或者 Nesterov， Adam，接着更新γ和β，正如更新神经网络的权重一样。</p><p>γ和β的作用是可以随意设置z~(i) 的平均值，事实上，如果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183506.png" alt=""></p><p>那么：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014183533.png" alt=""></p><p>Batch 归一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏层。应用 Batch 归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，也许不想隐藏单元值必须是平均值 0 和方差 1。γ和β参数控制使得均值和方差可以是 0 和 1，也可以是其它值。 </p><p><strong>注意</strong>：均值不是平均值，是数学期望。</p><h2 id="将Batch-Norm拟合进神经网络"><a href="#将Batch-Norm拟合进神经网络" class="headerlink" title="将Batch Norm拟合进神经网络"></a>将Batch Norm拟合进神经网络</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014212240.png" alt=""></p><p>假设有一个这样的神经网络，之前的知识说过，我们可以认为每个单元负责计算两件事。第一，它先计算z，然后应用其到激活函数中再计算a，所以可以认为，每个圆圈代表着两步的计算过程。同样的，对于下一层而言，那就是$z^{[2]}_{1}$和$z^{[2]}_{2}$等。所以如果没有应用Batch归一化，会把输入X拟合到第一隐藏层，然后首先计算z<sup>[1]</sup>，这是由&omega;<sup>[1]</sup>和b<sup>[1]</sup>两个参数控制的。接着，通常而言，会把z<sup>[1]</sup>拟合到激活函数以计算a<sup>[1]</sup>。但Batch归一化的做法是将z<sup>[1]</sup>值进行Batch归一化，简称<strong>BN</strong>，此过程将由&beta;<sup>[1]</sup>和&gamma;<sup>[1]</sup>两参数控制，这一操作会给你一个新的规范化的z<sup>[1]</sup>值（z~<sup>[1]</sup>），然后将其输入激活函数中得到a<sup>[1]</sup>，即a<sup>[1]</sup>=g<sup>[1]</sup>(z~<sup>[1]</sup>)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191014215107.png" alt=""></p><p>现在，你已在第一层进行了计算，此时Batch归一化发生在z的计算和a之间，接下来，你需要应用a<sup>[1]</sup>值来计算，此过程是由&omega;<sup>[1]</sup>和b<sup>[1]</sup>控制的。与第一层所做的类似，也会将进行Batch归一化，现在我们简称<strong>BN</strong>，这是由下一层的Batch归一化参数所管制的，即&beta;<sup>[2]</sup>和&gamma;<sup>[2]</sup>，现在你得到z~<sup>[2]</sup>，再通过激活函数计算出a<sup>[2]</sup>。</p><p>所以，得出结论的是batch归一化是发生在计算z和a之间的。与其使用没有归一化的z值，不如用归一化的z~值，也就是第一层的z~<sup>[1]</sup>。第二层同理，也是与其应用z值，不如应用z~<sup>[2]</sup>值。所以，我们以前的网络参数&omega;<sup>[1]</sup>、b<sup>[1]</sup>、&omega;<sup>[2]</sup>、b<sup>[2]</sup>将加上&beta;<sup>[1]</sup>、&beta;<sup>[2]</sup>、&gamma;<sup>[1]</sup>、&gamma;<sup>[2]</sup>等参数。<strong>注意</strong>：这里的&beta;<sup>[1]</sup>、&beta;<sup>[2]</sup>和超参数&beta;没有关系</p><p>&beta;<sup>1]</sup>、&beta;<sup>[2]</sup>、&gamma;<sup>[1]</sup>、&gamma;<sup>[2]</sup>等是算法的新参数。接下来就是使用梯度下降法来执行它。举个例子，对于给定层，计算d&beta;<sup>[1]</sup>，接着更新参数为&beta;<sup>[1]</sup>=&beta;<sup>[1]</sup>-&alpha;d&beta;<sup>[1]</sup>。你也可以使用Adam或RMSprop或Momentum，以更新参数&beta;和&gamma;，并不是只应用梯度下降法。</p><p>实践中，我们常将batch归一化和mibi-bacth一起使用。我们用第一个mini-batch{X<sup>[1]</sup>}，然后应用&omega;<sup>[1]</sup>和b<sup>[1]</sup>计算z<sup>[1]</sup>，接着用batch归一化得到z~<sup>[1]</sup>，再应用激活函数得到a<sup>[1]</sup>。然后接着用&omega;<sup>[2]</sup>和b<sup>[2]</sup>计算z<sup>[2]</sup>。</p><p>类似的工作，你会在第二个mini-batch（X<sup>{2}</sup>）上计算z<sup>[1]</sup>，然后用Batch归一化来计算，所以Batch归一化的此步中，用的是第二个mini-batch（X<sup>{2}</sup>）中的数据使归一化。然后在mini-batch（X<sup>{3}</sup>）上同样这样做，继续训练。</p><p>先前说过每层的参数是&omega;<sup>[ l ]</sup>和b<sup>[ l ]</sup>，还有&beta;<sup>[ l ]</sup>和&gamma;<sup>[ l ]</sup>，请注意计算z的方式如下，z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>，但Batch归一化做的是，要看这个mini-batch，先将z<sup>[ l ]</sup>归一化，结果为均值0和标准方差，再由&beta;和&gamma;重缩放，但这意味着，无论b<sup>[ l ]</sup>的值是多少，都是要被减去的，因为在Batch归一化的过程中，要计算z<sup>[ l ]</sup>的均值，再减去平均值，在此例中的mini-batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消。</p><p>所以，我们在使用batch归一化时，我们可以消除b<sup>[ l ]</sup>这个参数。或者也可以设置为0。那么式子将从z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>变为z<sup>[ l ]</sup>=&omega;<sup>[ l ]</sup> * a<sup>[l-1]</sup>。然后归一化z<sup>[ l ]</sup>，得z~<sup>[ l ]</sup>=&gamma;<sup>[ l ]</sup> * z<sup>[ l ]</sup>+&beta;<sup>[ l ]</sup>，所以最后我们会用&beta;<sup>[ l ]</sup>，以便决定z~<sup>[ l ]</sup>的取值。</p><p>总结一下关于如何用 Batch 归一化来应用梯度下降法：</p><p>假设使用 mini-batch梯度下降法，运行t = 1到 batch 数量的 for 循环，会在 mini-batchX<sup>{t}</sup>上应用正向 prop，每个隐藏层都应用正向 prop，用 Batch 归一化代替z<sup>[l]</sup>为z~<sup>[l]</sup>。接下来，它确保在这个 mini-batch 中，z值有归一化的均值和方差，归一化均值和方差后是z~<sup>[ l ]</sup> ，然后，你用反向 prop 计算：dw<sup>[ l ]</sup>，db[l]，dβ<sup>[ l ]</sup> ，dγ<sup>[ l ]</sup>。 尽管严格来说，因为要去掉b，这部分其实已经去掉了。最后，更新这些参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015143440.png" alt=""></p><h2 id="batch-norm为什么会管用？"><a href="#batch-norm为什么会管用？" class="headerlink" title="batch norm为什么会管用？"></a>batch norm为什么会管用？</h2><p>一个原因是，你已经看到如何归一化输入特征值x，使其均值为0，方差1，它又是怎样加速学习的，有一些从0到1而不是从1到1000的特征值，通过归一化所有的输入特征值x，以获得类似范围的值，可以加速学习。所以Batch归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值，这只是Batch归一化作用的冰山一角，还有些深层的原理，它会有助于你对Batch归一化的作用有更深的理解，让我们一起来看看吧。</p><p>Batch归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层，比如，第10层的权重相比于神经网络中前层的权重更能经受得住变化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016170223.png" alt=""></p><p>看上面的素材。假设我们已经在某个网络上训练了所有黑猫的图像，如果我们将这个网络应用于有色猫。这种情况下，我们的效果可能不是很好。因为正面的例子不止左边的黑猫，还有右边其它颜色的猫。</p><p>所以我们要使数据改变分布，想法名字叫“Covariate shift”。想法是这样的，如果你已经学习了x到y的映射，如果x的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由x到y映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016171250.png" alt=""></p><p>看上图，看第三层网络。此网络已经学习了参数&omega;<sup>[3]</sup>和b<sup>[3]</sup>。它还得到了一些值，称为$a^{[2]}_{1}$、$a^{[2]}_{2}$、$a^{[2]}_{3}$、$a^{[2]}_{4}$，但这些值也可以变为x<sub>1</sub>、x<sub>2</sub>、x<sub>3</sub>、x<sub>4</sub>。第三层隐藏层要做的是，找到一种方式使这些值映射到y帽。再看网络左边前三层(包括输入层)，这个网络还有参数&omega;<sup>[2]</sup>、b<sup>[2]</sup>、&omega;<sup>[1]</sup>、b<sup>[1]</sup>，如果这些参数改变，这些a<sup>[2]</sup>的值也会变。所以从第三层隐藏层的角度来看，这些隐藏单元的值在不断地改变，所以它就有了“Covariate shift”的问题。</p><p>Batch归一化做的，是它减少了这些隐藏值分布变化的数量。batch归一化讲的是当神经网络层更新参数时，batch归一化可以确保无论$z^{[2]}_{1}$、$z^{[2]}_{2}$、$z^{[2]}_{3}$、$z^{[2]}_{4}$怎么变化它们的均值和方差都保持不变。均值和方差是由&beta;<sup>[2]</sup>和&gamma;<sup>[2]</sup>决定的值，如果神经网络选择的话，可强制均值为0，方差为1，或其它任何均值和方差。</p><p>Batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，我们可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。</p><p>batch归一化还有一个作用，它有轻微的正则化效果。在mini-batch计算中，由均值和方差缩放的，因为是在mini-batch上计算的均值和方差，而不是在整个数据集上，它只是由一小部分数据估计得出的。所以和dropout相似，它往每个隐藏层的激活值上增加了噪音，dropout有增加噪音的方式，它使一个隐藏的单元，以一定的概率乘以0，以一定的概率乘以1，所以你的dropout含几重噪音，因为它乘以0或1。对比而言，Batch归一化含几重噪音，因为标准偏差的缩放和减去均值带来的额外噪音。这里的均值和标准差的估计值也是有噪音的，所以类似于dropout，Batch归一化有轻微的正则化效果，因为给隐藏单元添加了噪音，这迫使后部单元不过分依赖任何一个隐藏单元，类似于dropout。batch归一化给隐藏层增加了噪音，因此有轻微的正则化效果。因为添加的噪音很微小，所以并不是巨大的正则化效果。如果想得到dropout更大的正则化效果，可以将Batch归一化和dropout一起使用。</p><h2 id="测试时的batch-norm"><a href="#测试时的batch-norm" class="headerlink" title="测试时的batch norm"></a>测试时的batch norm</h2><p>batch归一化将数据以mini-batch的形式逐一处理，但在测试时，需要对每个样本逐一处理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191016201010.png" alt=""></p><p>在训练时，这些就是用来执行Batch归一化的等式。在一个mini-batch中，你将mini-batch的z<sup>(i)</sup>值求和，计算均值，所以这里你只把一个mini-batch中的样本都加起来，我用m来表示这个mini-batch中的样本数量，而不是整个训练集。然后计算方差，再算$z^{(i)}_{norm}$，即用均值和标准差来调整，加上ξ是为了数值稳定性。z~<sup>(i)</sup>是用&gamma;和&beta;再次调整$z^{(i)}_{norm}$得到的。</p><p>请注意用于调节计算的μ和σ<sup>2</sup>是在整个mini-batch上进行计算，但是在测试时，不能将一个mini-batch中的6428或2056个样本同时处理，因此需要用其它方式来得到μ和σ<sup>2</sup>，而且如果只有一个样本，一个样本的均值和方差没有意义。那么实际上，为了将神经网络运用于测试，就需要单独估算μ和σ<sup>2</sup>，在典型的Batch归一化运用中，需要用一个指数加权平均来估算，这个平均数涵盖了所有mini-batch，接下来是具体解释。</p><p>选择 l 层，假设我们用mini-batch，X<sup>[1]</sup>,X<sup>[2]</sup>,X<sup>[3]</sup>……以及对应的y值等等，那么在 l 层训练X<sup>{1}</sup>时，就得到了μ<sup>{1}[ l ]</sup>。当我们训练第二个mini-batch，我们就得到了μ<sup>{2}[ l ]</sup>，第三个mini-batch，就得到了μ<sup>{3}[1]</sup>值。正如我们之前用的指数加权平均来计算&theta;<sub>1</sub>，&theta;<sub>2</sub>，&theta;<sub>3</sub>的均值，当时是试着计算当前气温的指数加权平均，你会这样来追踪你看到的这个均值向量的最新平均值，于是这个指数加权平均就成了你对这一隐藏层的z均值的估值。同样的，你可以用指数加权平均来追踪你在这一层的第一个mini-batch中所见的的σ<sup>2</sup>值，以及第二个mini-batch中所见的的σ<sup>2</sup>值等等。因此在用不同的mini-batch训练神经网络的同时，能够得到你所查看的每一层的μ和σ<sup>2</sup>的平均数的实时数值。最后在测试时，我们只需要z值来计算$z^{(i)}_{norm}$，用μ和σ<sup>2</sup>的指数加权平均，用手头的最新数值来做调整，然后就可以用刚算出来的z<sub>norm</sub>和在神经网络训练过程中得到的&beta;和&gamma;参数来计算那个测试样本的z~值。</p><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>前面的讲过逻辑回归属于二分类，如果我们有多种可能的类型呢？</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015163607.png" alt=""></p><p>假设我们不止需要识别猫，而是想识别猫、狗和小鸡。把猫当作类1，把狗当作类2，把小鸡当作类3。如果不属于以上任何一类，就分到“其它”或“以上均不符合”这一类，我把它叫做类0。所以上面图中第一张图为3类，第二张图为1类，第三张图为2类，第四张图为0类，依次类推。我们使用C来表示输入会被分入的类别总个数。在这个例子中，我们有四种可能的类别。当有四个分类时，指示类别的数字0-C-1。就是0、1、2、3。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015171121.png" alt=""></p><p>最后一层的隐藏单元为4，为所分的类的数目。输出的值表示属于每个类的概率。它们加起来等于一。</p><p>Softmax的具体步骤如下：</p><p>在神经网络的最后一层，我们将会想往常一样计算各层的线性部分，z<sup>[ l ]</sup>是最后一层的z变量。z<sup>[ l ]</sup>=W<sup>[ l ]</sup> * a<sup>[l-1]</sup>+b<sup>[ l ]</sup>，算出了z后，我们需要应用Softmax激活函数。它的作用是这样的： 我们要计算一个临时变量，我们把它叫做t。它等于e<sup>z<sup>[ l ]</sup>&lt;/sup&gt;，这适用于每个元素。而这里的z<sup>[ l ]</sup>，在上面这个例子中，维度是4x1的。四维向量t=e<sup>z[ l ]</sup>，这是对所有元素求幂，t也是一个4x1维向量，然后输出a<sup>[ l ]</sup>，基本上就是向量t，但是会归一化，使和为1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015174838.png" alt=""></p><p>换句话说，a<sup>[ l ]</sup>也是一个4x1维向量。而这个思维向量的第 i 个元素：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175116.png" alt=""></p><p>讲个例子：假设我们算出了z<sup>[ l ]</sup>，这是一个四维向量。假设为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175309.png" alt=""></p><p>我们要做的就是用这个元素取幂方法来计算t，所以：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175348.png" alt=""></p><p>接着利用计算器计算得到以下值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175424.png" alt=""></p><p>如果把t的元素都加起来，把这四个数字加起来，得到176.3。最终：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015175556.png" alt=""></p><p>看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015172242.png" alt=""></p><p>例如第一个节点，会输出e<sup>5</sup>/176.3=0.842。这样来说，对于这张图片，它是0类的概率就是84.2%。下个节点输出0.042，也就是4.2%的几率是1类。其它类别以这种规律推出。</p><p>神经网络的输出a<sup>[ l ]</sup>，也就是y帽。是一个4x1维向量，如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015180202.png" alt=""></p><p>所以这种算法通过向量z计算出总和为1的四个概率。</p><p>之前，我们的激活函数都是接受单行数值输入，例如Sigmoid和ReLu激活函数，输入一个实数，输出一个实数。Softmax激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。</p><p>下面是分类的几个例子：</p><p>这是一个没有隐藏层的神经网络。他所做的就是z<sup>[1]</sup>=W<sup>[1]</sup> * x+b<sup>[1]</sup>，而输出的a<sup>[ l ]</sup>=g(z<sup>[1]</sup>)，就是Softmax激活函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015193021.png" alt=""></p><p>上面三张图的C=3，下面三张图从左到右的C等于4、5、6。</p><p>这显示了Softmax分类器在没有隐藏层的情况下能够做到的事情，当然更深的神经网络会有x，然后是一些隐藏单元，以及更多隐藏单元等等，你就可以学习更复杂的非线性决策边界，来区分多种不同分类。</p><h2 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h2><p>回忆我们之前举得例子，输出层计算的z<sup>[ l ]</sup>。我们有四个分类，z<sup>[ l ]</sup>可以是4x1维向量，我们计算了临时变量t。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015222626.png" alt=""></p><p>如果我们的激活函数是Softmax，那么输出是这样的:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015222734.png" alt=""></p><p>简单来说就是用临时变量t将它归一化，使总和为1。于是这就变成了a<sup>[ l ]</sup>。在这个向量z中，最大的元素是5。最大的概率是0.842。</p><p>Softmax这个是与所谓的<strong>hardmax</strong>对比，hardmax会把z变量变为[1 0  0  0]<sup>T</sup>，hardmax会观察z的元素，然后在z中最大元素的位置放上1，其它的输出都放0。</p><p>Softmax回归或Softmax激活函数将logistic激活函数推广到C类，而不仅仅是两类。而当C=2，那么的Softmax实际上变回了logistic回归，那么输出层将会输出两个数字。如果C=2的话，也许输出0.842和0.158，对吧？这两个数字加起来要等于1，因为它们的和必须为1，其实它们是冗余的，也许你不需要计算两个，而只需要计算其中一个，结果就是你最终计算那个数字的方式又回到了logistic回归计算单个输出的方式。</p><p>接下来我们来看怎样训练带有Softmax输出层的神经网络，具体而言，我们先定义训练神经网络使会用到的损失函数。举个例子，我们来看看训练集中某个样本的目标输出，真实标签是[0 1 0 0]<sup>T</sup>，用上一个视频中讲到过的例子，这表示这是一张猫的图片，因为它属于类1，现在我们假设你的神经网络输出的是 y帽，y帽是一个包括总和为1的概率的向量，y帽=[0.3 0.2 0.1 0.4]<sup>T</sup>，可以看到总和为1，这就是a<sup>[ l ]</sup>。对于这个样本神经网络的表现不佳，这实际上是一只猫，但却只分配到20%是猫的概率，所以在本例中表现不佳。</p><p>那么我们使用什么损失函数来训练这个神经网络？看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015224734.png" alt=""></p><p>注意在这个样本中，y<sub>1</sub>=y<sub>2</sub>=y<sub>3</sub>=0，因为这些都是0，只有y<sub>2</sub>=1，如果你看这个求和，所有含有值为0的y<sub>i</sub>的项都等于0，最后只剩下 -y<sub>2</sub>logy<sub>2</sub>帽，因为当你按照下标 j 全部加起来，所有的项都为0，除了j=2，因为y<sub>2</sub>=1，所以它就等于 -logy<sub>2</sub>帽。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191015225125.png" alt=""></p><p>这就意味着，如果你的学习算法试图将它变小，因为梯度下降法是用来减少训练集的损失的，要使它变小的唯一方式就是使 -logy<sub>2</sub>帽变小，要想做到这一点，就需要使 y<sub>2</sub>帽 尽可能大。也就是[0.3 0.2 0.1 0.4]<sup>T</sup>中的第二个元素。</p><p>接下来看看，我们在有Softmax的输出层如何实现梯度下降法。输出层会计算z<sup>[1]</sup>，它是C x 1维的。在上个例子中，是4 x1维的。然后用Softmax激活函数来得到a<sup>[ l ]</sup>或者y帽。然后计算出损失。反向传播的关键步骤是这个表达式dz<sup>[ l ]</sup>=y帽-y。我们用y帽这个4x1向量减去y这个4x1向量。这是对z<sup>[ l ]</sup>的损失函数的偏导数dz<sup>[ l ]</sup>=∂J/∂z<sup>[ l ]</sup>，然后开始反向传播的过程，计算整个神经网络中所需要的所有导数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;调试处理&quot;&gt;&lt;a href=&quot;#调试处理&quot; class=&quot;headerlink&quot; title=&quot;调试处理&quot;&gt;&lt;/a&gt;调试处理&lt;/h2&gt;&lt;p&gt;关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentu
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（2）</title>
    <link href="http://yoursite.com/2019/10/11/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(2)/"/>
    <id>http://yoursite.com/2019/10/11/优化算法(2)/</id>
    <published>2019-10-11T05:13:53.000Z</published>
    <updated>2019-10-21T09:03:52.658Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mini-batch-梯度下降"><a href="#Mini-batch-梯度下降" class="headerlink" title="Mini-batch 梯度下降"></a>Mini-batch 梯度下降</h2><p>我们之前学过，向量化能够让我们有效地对所有m个样本进行计算，所以我们要把训练样本放到巨大的矩阵X中。</p><p>X=[x<sup>(1)</sup> x<sup>(2)</sup>   x<sup>(3)</sup>   x<sup>(4)</sup>  ……  x<sup>(n)</sup> ]。Y也是如此，Y=[y<sup>(1)</sup> y<sup>(2)</sup>  y<sup>(3)</sup>   y<sup>(4)</sup>  ……  y<sup>(n)</sup> ]。所以X的维度是(n<sub>x</sub>,m)，Y的维度是(1,m)。向量化能够让我们相对较快地处理所有样本，如果m很大的话，处理速度仍然缓慢。</p><p>相比于mini-batch梯度下降法，我们大家更熟悉的应该是batch梯度下降法，即梯度下降法。那batch梯度下降法和mini-batch梯度下降法有什么区别吗？其实它俩的区别就存在于名字中，一个是batch，即进行梯度下降训练时，使用全部的训练集，而mini-batch，表示比batch小一些，就是指在进行梯度下降训练时，并不使用全部的训练集，只使用其中一部分数据集。</p><p>我们知道，不论是梯度下降法还是mini-batch梯度下降法，我们都可以通过向量化(vectorization)更加有效地计算所有样本。既然已经有了梯度下降法，我们为什么还要提出mini-batch梯度下降法呢？在实际计算中，我们可能会遇到特别大的数据集，这时再使用梯度下降法，每次迭代都要计算所有的数据集，计算量太大，效率低下，而mini-batch梯度下降法允许我们拿出一小部分数据集来运行梯度下降法，能够大大提高计算效率。</p><p>我们可以把训练集分割成小一点的子集训练，这些子集被取名为mini-batch，假设每一个子集中只要1000个样本，那么把x<sup>(1)</sup>到x<sup>(1000)</sup>取出来，将其称为第一个子训练集，也叫做mini-batch，然后我们再取出接下来的1000个样本，从x<sup>(1001)</sup>到x<sup>(2000)</sup>，然后再取1000个样本，以此类推。</p><p>接下来是吴恩达老师的一个新的符号，把x<sup>(1)</sup>到x<sup>(1000)</sup>称为X<sup>{1}</sup>，x<sup>(1001)</sup>到x<sup>(2000)</sup>称为X<sup>{2}</sup>，如果我们的训练样本一共有500万个，每个mini-batch都有1000个样本。也就是说，你有5000个mini-batch，因为5000乘以1000就是500万。最后得到的是X<sup>{5000}</sup>，对y进行相同的处理。</p><p>mini-batch的数量t组成X<sup>{t}</sup>和Y<sup>{t}</sup>，这就是10000个训练样本。包括相应的输入输出对。如果X<sup>{1}</sup>是一个有1000个样本的训练集，X<sup>{1}</sup>的维度应该是(n<sub>x</sub>,1000)，X<sup>[2]</sup>的应该也是(n<sub>x</sub>，1000)，以此类推，所有的子集维数都是(n<sub>x</sub>,1000)，对于Y也是一样的。</p><p>之前我们执行前向传播，就是执行z<sup>[1]</sup>=W<sup>[1]</sup>X+b<sup>[1]</sup>。变成mini-batch后呢，把X替换成X<sup>[t]</sup>，即z<sup>[1]</sup>=W<sup>[1]</sup>X<sup>{t}</sup>+b<sup>[1]</sup>，然后执行A<sup>[1]k</sup>=g<sup>[1]</sup>(Z<sup>[1]</sup>)，依次类推，直到A<sup>[L]</sup>=g<sup>[L]</sup>(Z<sup>[L]</sup>)，这就是我们的预测值。注意：这里一次性处理的是1000个样本不是500万个样本。接着计算成本函数，因为子集规模是1000，所以J=1/1000$\sum_{i=1}^{1000}$L(y<sup>(i)</sup> 帽,y<sup>(i)</sup>)。</p><p>如果我们使用了正则化，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011191909.png" alt=""></p><p>你也会注意到，我们做的一切似曾相识，其实跟之前我们执行梯度下降法如出一辙，除了现在的对象不是X，Y，而是X<sup>{t}</sup>和Y<sup>{t}</sup>。接下来，执行反向传播来计算J<sup>{t}</sup>的梯度，你只是使用X<sup>{t}</sup>和Y<sup>{t}</sup>，然后更新加权值，W实际上是W<sup>[l]</sup>，更新为W<sup>[l]</sup>=W<sup>[l]</sup>-adW<sup>[l]</sup>。对b做相同处理，b<sup>[l]</sup>=b<sup>[l]</sup>-adb<sup>[l]</sup>。这是使用mini-batch梯度下降法训练样本的一步。被称为进行“一代”（<strong>1 epoch</strong>）的训练。一代这个词意味着只是遍历了一次训练集。我们可以在外围加一个for循环，从1到5000，因为我们有5000个各有1000个样本的组。</p><p>使用batch梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用mini-batch梯度下降法，一次遍历训练集，能让你做5000个梯度下降。</p><h2 id="理解mini-batch"><a href="#理解mini-batch" class="headerlink" title="理解mini-batch"></a>理解mini-batch</h2><p>使用batch梯度下降法时，每次迭代都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许学习率太大。</p><p>使用mini-batch梯度下降法，如果你作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是X<sup>{t}</sup>和Y<sup>{t}</sup>，如果要作出成本函数J<sup>{t}</sup>的图，而J<sup>{t}</sup>只和X<sup>{t}</sup>，Y<sup>{t}</sup>有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的mini-batch。</p><p>我们需要决定的变量之一是mini-batch的大小，m就是训练集的大小。其中一个极端情况下，mini-batch梯度下降法就是batch梯度下降法。另一个极端情况，假设mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch，当你看第一个mini-batch，也就是X<sup>{1}</sup>和Y<sup>{1}</sup>，如果mini-batch大小为1，它就是你的第一个训练样本。接着再看第二个mini-batch，也就是第二个训练样本，采取梯度下降步骤，然后是第三个训练样本，以此类推，一次只处理一个。</p><p>实际上你选择的mini-batch大小在二者之间，大小在1和m之间，而1太小了，m太大了。如果取n，每个迭代需要处理大量训练样本，单次迭代耗时太长。如果训练样本不大，batch梯度下降法运行地很好。相反，如果使用随机梯度下降法，如果你只要处理一个样本，那这个方法很好，这样做没有问题，通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的mini-batch尺寸，实际上学习率达到最快。</p><p>首先，如果训练集较小，直接使用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，可以快速处理整个训练集，所以使用batch梯度下降法也很好，这里的少是说小于2000个样本，这样比较适合使用batch梯度下降法。不然，样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的n次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。所以我经常把mini-batch大小设成2的次方。</p><p>最后需要注意的是在你的mini-batch中，要确保X<sup>{t}</sup>和Y<sup>{t}</sup>要符合<strong>CPU</strong>/<strong>GPU</strong>内存，取决于你的应用方向以及训练集的大小。如果不符合内存，无论采取什么方法处理数据，结果变得惨不忍睹。</p><h2 id="指数加权平均数"><a href="#指数加权平均数" class="headerlink" title="指数加权平均数"></a>指数加权平均数</h2><p>指数加权平均也叫指数加权移动平均，是一种常用的序列数据处理方式。</p><p>它的计算公式是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222214.png" alt=""></p><p>其中，</p><ul><li>θ_t：为第 t 天的实际观察值，</li><li>V_t: 是要代替 θ_t 的估计值，也就是第 t 天的指数加权平均值，</li><li>β： 为 V_{t-1} 的权重，是可调节的超参。( 0 &lt; β &lt; 1 )</li></ul><p>下面是一组气温数据，图中横轴为一年中的第几天，纵轴为气温：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222352.png" alt=""></p><p>直接看上面的数据图会发现噪音很多，</p><p>这时，我们<strong>可以用 指数加权平均 来提取这组数据的趋势，</strong></p><p>按照前面的公式计算：</p><p>这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t，将计算后得到的V_t表示出来，得到下图红色线：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222623.png" alt=""></p><p>可以看出，红色的数据比蓝色的原数据更加平滑，少了很多噪音，并且刻画了原数据的趋势。</p><p>指数加权平均，作为原数据的估计值，不仅可以 <strong>1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来</strong>。</p><p>我们可以改变&beta;值，当&beta;=0.98时，得出下图的绿线。当&beta;=0.5，结果是下图的黄线。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011223152.png" alt=""></p><h2 id="理解指数加权平均数"><a href="#理解指数加权平均数" class="headerlink" title="理解指数加权平均数"></a>理解指数加权平均数</h2><p>上个小节，我们得出下面公式。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191011222214.png" alt=""></p><p>我们进一步分析，来理解如何计算出每日温度的平均值</p><p>使&beta;=0.9，得出下面公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012113654.png" alt=""></p><p>将第二个公式带到第一个，第三个公式带到第二个公式，一次类推，把这些展开：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012113917.png" alt=""></p><p>我们可以将v<sub>0</sub>，v<sub>1</sub>，v<sub>2</sub>等等写成明确的变量，不过在实际中执行的话，你要做的是，一开始将v<sub>0</sub>初始化为0，然后在第一天使v=&beta;v+(1-&beta;)&theta;<sub>1</sub>，然后第二天，更新v值，v=&beta;v+(1-&beta;)&theta;<sub>2</sub>，以此类推，有些人会把v加下标，来表示v是用来计算数据的指数加权平均数。</p><p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。</p><h2 id="指数加权平均的偏差修正"><a href="#指数加权平均的偏差修正" class="headerlink" title="指数加权平均的偏差修正"></a>指数加权平均的偏差修正</h2><p>偏差修正可以让平均数运算更加准确</p><p>在前几节中，如上图，红色曲线对应&beta;为0.9，绿色曲线对应的&beta;=0.98。吴恩达老师说执行下面这个公式：</p><blockquote><p>v<sub>t</sub>=&beta;v<sub>t-1</sub>+(1-&beta;)&theta;<sub>t</sub></p></blockquote><p>得到的就是紫色曲线，而不是绿色曲线。后面紫色和绿色有大部分重合。PS：为什么？同时0.98，紫色曲线的起点低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012160618.png" alt=""></p><p>计算移动平均数时，初始化v<sub>0</sub>，v<sub>1</sub>=0.98v<sub>0</sub>+0.02&theta;<sub>1</sub>，因为v<sub>0</sub>=0，所以0.98v<sub>0</sub>=0。所以v<sub>1</sub>=0.02&theta;<sub>1</sub>，如果第一天温度时40，v<sub>1</sub>=0.02 x 40=8。因此得到的值会小很多，所以第一天的温度估测不准。</p><p>v<sub>1</sub>=0.98v<sub>0</sub>+0.02&theta;<sub>1</sub>，如果带入v<sub>1</sub>，然后相乘，v<sub>2</sub>=0.98 x 0.02&theta;<sub>1</sub> + 0.02&theta;<sub>2</sub>，假如&theta;<sub>1</sub>和&theta;<sub>2</sub>都是正数，计算后的v<sub>2</sub>要远小于&theta;<sub>2</sub>和&theta;<sub>1</sub>，所以不能很好预估这一年前两天的温度。</p><p>有个办法可以修改这一估测，让估测变得更好，更准确，特别是在估测初期，也就是不用v<sub>t</sub>，而是用v<sub>t</sub>/1-&beta;<sup>t</sup>，t就是现在的天数。举个具体例子，当t=2时，1-&beta;<sup>t</sup>=1-0.98<sup>2</sup>=0.0396，因此对第二天温度的估测变成了</p><blockquote><p>v<sub>2</sub>/0.0396=(0.0196&theta;<sub>1</sub>+0.02&theta;<sub>2</sub>)/0.0396</p></blockquote><p>也就是和的加权平均数，并去除了偏差。你会发现随着t增加，&beta;<sup>t</sup>接近于0，所以当t很大的时候，偏差修正几乎没有作用.因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p><h2 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h2><p>还有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法。基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新的权重。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012173500.png" alt=""></p><p>如果优化成本函数，函数形状如图，红点代表最小值的位置，假设从这里（蓝色点）开始梯度下降法，如果进行梯度下降法的一次迭代，无论是batch或mini-batch下降法，也许会指向这里，现在在椭圆的另一边，计算下一步梯度下降，结果或许如此，然后再计算一步，再一步，计算下去。我们发现梯度下降法要很多计算步骤。</p><p>慢慢摆动到最小值，这种上下波动减慢了梯度下降法的速度，就无法使用更大的学习率，如果要用较大的学习率（紫色箭头），结果可能会偏离函数的范围，为了避免摆动过大，就要用一个较小的学习率。</p><p>另一个看待问题的角度是，在竖直方向上，我们希望学习慢一点，因为我们不想要这些摆动。但在水平方向，我们希望快速从左向右移动，移向最小值，移向红点。所以使用动力梯度下降法，在每次迭代中，都会计算dW，db。我们要做的是计算v<sub>dW</sub>=&beta;v<sub>dW</sub>+(1-&beta;)dW，接着同样计算v<sub>db</sub>=&beta;v<sub>db</sub>+(1-&beta;)db。然后重新赋值权重，W=W-av<sub>dW</sub>，同样b=b-av<sub>dW</sub>，从而减缓梯度下降的幅度。</p><p>举个例子，如果你站在一个地方不动，让你立刻向后转齐步走，你可以迅速向后转然后就向相反的方向走了起来，批梯度下降和随机梯度下降就是这样，某一时刻的梯度只与这一时刻有关，改变方向可以做到立刻就变。而如果你正在按照某个速度向前跑，再让你立刻向后转，可以想象得到吧，此时你无法立刻将速度降为0然后改变方向，你由于之前的速度的作用，有可能会慢慢减速然后转一个弯。</p><p>动量梯度下降是同理的，每一次梯度下降都会有一个之前的速度的作用，如果我这次的方向与之前相同，则会因为之前的速度继续加速；如果这次的方向与之前相反，则会由于之前存在速度的作用不会产生一个急转弯，而是尽量把路线向一条直线拉过去。</p><p>这就解决了文中第一个图的那个在普通梯度下降中存在的下降路线折来折去浪费时间的问题。</p><p>我们有两个超参数，学习率&alpha;以及&beta;参数。&beta;是指数加权平均数，常用值为0.9。</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>前面我们知道动力梯度下降法可以加快梯度下降，还有一个叫做<strong>RMSprop</strong>的算法，全称是<strong>root mean square prop</strong>算法，它也可以加速梯度下降，我们来看看它是如何运作的。</p><p>复习前面的内容，如果我们执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅动摆动。所以，我们想减缓纵轴方向的学习，同时加快横轴方向的学习(至少不减慢)。RMSprop算法可以实现这个目标。</p><p>在第t次迭代中，该算法会找出计算mini-batch的微分dW，db。这里我们不用v<sub>dW</sub>，而是用到新符号S<sub>dW</sub>。因此：</p><blockquote><p>S<sub>dW</sub>=&beta;S<sub>dW</sub>+(1-&beta;)(dW)<sup>2</sup>，这里的平方是对整个dW平方。</p><p>S<sub>db</sub>=&beta;S<sub>db</sub>+(1-&beta;)db<sup>2</sup>，这里的平方也是对整个db平方。</p></blockquote><p> 接着，RMSprop会更新参数值。如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012200635.png" alt=""></p><p>我们希望，S<sub>dW</sub>相对较少，S<sub>db</sub>较大。因为垂直方向的微分要比水平方向的大得多，所以斜率在垂直方向特别大。所以db较大，dW较小。db的平方较大，所以S<sub>db</sub>的平方较大。dW会小，S<sub>dW</sub>也会小。结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191012224237.png" alt=""></p><p><strong>RMSprop</strong>的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率a，然后加快学习，而无须在纵轴上垂直方向偏离。</p><h2 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h2><p><strong>Adam</strong>优化算法基本上就是将<strong>Momentum</strong>和<strong>RMSprop</strong>结合在一起，那么来看看如何使用Adam算法。</p><p>首先，初始化</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021165915.png" alt=""></p><p>然后用当前的 mini-batch 计算出 dW 和 db</p><p>接下来计算 Momentum 指数加权平均数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021165943.png" alt=""></p><blockquote><p>上图中用到的&beta;<sub>1</sub>是为了和下面的RMSprop公式中用到的&beta;<sub>2</sub>相互区分</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170221.png" alt=""></p><p>一般运用 Adam 算法的时候，我们还要对 v 和 S 的偏差进行修正：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170044.png" alt=""></p><p>然后就是权重的更新：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170019.png" alt=""></p><p>完整的如下所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013142056.png" alt=""></p><p>本算法有很多超参数，分别有a、&beta;<sub>1</sub>、&beta;<sub>2</sub>、ξ四个超参数。a需要调试，尝试一系列的值，然后看哪个有效。&beta;<sub>1</sub>常用的值为0.9。至于&beta;<sub>2</sub>，推荐使用0.999，ξ为10<sup>-8</sup>。</p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013143740.png" alt=""></p><p>假设使用mini-batch梯度下降法，mini-batch数量不大，大概64或者128个样本，在迭代过程中会有噪音（蓝色线），下降朝向这里的最小值，但是不会精确地收敛，所以你的算法最后在附近摆动，并不会真正收敛，因为你用的a是固定值，不同的mini-batch中有噪音。</p><p>但是如果要减少学习率的话，在初期的时候，a学习率还比较大，我们的学习还是相对较快，但随着a变小，我们的步伐也会变慢。而不是在训练过程中，大幅度在最小值附近摆动。</p><p>所以慢慢减少a的本质在于，在学习初期，承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p><p>我们应该拆分成不同的mini-batch。第一次遍历训练集叫做第一代，第二代就是第二代。以此类推，我们得出公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144516.png" alt=""></p><p>decay-rate称为衰减率，epoch-num为代数，&alpha;<sub>0</sub>为初始学习率），注意这个衰减率是另一个你需要调整的超参数。</p><p>当然还有其它衰减：</p><p>方法二：指数衰减</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144713.png" alt=""></p><p>方法三：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144738.png" alt=""></p><p>方法四：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013144804.png" alt=""></p><p>方法五：手动衰减</p><p>如果一次只训练一个模型，如果花上数小时或数天来训练，看看自己的模型训练，耗上数日，学习速率变慢了，然后把&alpha;调小一点。但这种方法只是在模型数量小的时候有用。</p><h2 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h2><p>也许我们想优化一些参数，我们把它们称之为W<sub>1</sub>和W<sub>2</sub>，平面的高度就是损失函数。在图中似乎各处都分布着局部最优。梯度下降法或者某个算法可能困在一个局部最优中，而不会抵达全局最优。如果要作图计算一个数字，比如说这两个维度，就容易出现有多个不同局部最优的图，而这些低维的图曾经影响了我们的理解，但是这些理解并不正确。事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191013150349.png" alt=""></p><p>在高维度空间，我们更可能碰到鞍点。</p><p>首先，你不太可能困在极差的局部最优中，条件是你在训练较大的神经网络，存在大量参数，并且成本函数J被定义在较高的维度空间。</p><p>第二点，平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或是RMSprop，Adam这样的算法，能够加速学习算法的地方。在这些情况下，更成熟的优化算法，如Adam算法，能够加快速度，让你尽早往下走出平稳段。</p><p><strong>名词解释</strong>：平稳段是一段区域，其中导数长时间接近于0。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Mini-batch-梯度下降&quot;&gt;&lt;a href=&quot;#Mini-batch-梯度下降&quot; class=&quot;headerlink&quot; title=&quot;Mini-batch 梯度下降&quot;&gt;&lt;/a&gt;Mini-batch 梯度下降&lt;/h2&gt;&lt;p&gt;我们之前学过，向量化能够让我们有效地
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>优化算法（1）</title>
    <link href="http://yoursite.com/2019/10/08/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(1)/"/>
    <id>http://yoursite.com/2019/10/08/优化算法(1)/</id>
    <published>2019-10-08T04:00:46.000Z</published>
    <updated>2019-10-21T12:02:20.923Z</updated>
    
    <content type="html"><![CDATA[<p>在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。</p><h2 id="训练、验证和测试集"><a href="#训练、验证和测试集" class="headerlink" title="训练、验证和测试集"></a>训练、验证和测试集</h2><p>我们通常会将这些数据划分成三部分，一部分作为训练集（<strong>train set</strong>），一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（<strong>dev set</strong>），其实都是同一个概念。最后一部分则作为测试集（<strong>test set</strong>）。</p><p>接下来，我们开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，我们选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况。</p><p>在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。</p><p>在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被人们称为训练集，而验证集则被称为测试集。</p><p>那么验证集和测试集有什么区别呢？为什么要划分训练集、验证集和测试集？</p><p>训练集用于训练模型参数，测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。</p><h2 id="偏差（Bias）、方差（Variance）"><a href="#偏差（Bias）、方差（Variance）" class="headerlink" title="偏差（Bias）、方差（Variance）"></a>偏差（Bias）、方差（Variance）</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123133.png" alt=""></p><p>假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（<strong>high bias</strong>）的情况，我们称为“欠拟合”（<strong>underfitting</strong>）。</p><p>相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（<strong>high variance</strong>），数据过度拟合（<strong>overfitting</strong>）。</p><p>在两者之间，可能还有复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（<strong>just right</strong>）是介于过度拟合和欠拟合中间的一类。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008123354.png" alt=""></p><p>下面举例子：</p><p>假定训练集误差是1%，为了方便论证，假定验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，<strong>验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。</strong></p><p>通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有高方差。<strong>也就是说衡量训练集和验证集误差就可以得出不同结论。</strong></p><p>假设训练集误差是15%，我们把训练集误差写在首行，验证集误差是16%，假设该案例中人的错误率几乎为0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，所以这种算法偏差高，因为它甚至不能拟合训练集，这与上一张图最左边的图片相似。</p><p>上面的分析都是基于假设预测，假设人眼辨别的错误率接近0%。最优误差也被称为贝叶斯误差。如果最优误差或贝叶斯误差非常高，比如15%。我们再看看上面这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt=""></p><h2 id="正则化（Rugularization）"><a href="#正则化（Rugularization）" class="headerlink" title="正则化（Rugularization）"></a>正则化（Rugularization）</h2><p>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差，下面我们就来讲讲正则化的作用原理。</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>我们用逻辑回归讲解原理。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008191606.png" alt=""></p><p>&lambda;/2m乘以&omega;范数的平方。&omega;是欧几里得范数，&omega;的平方等于&omega;<sub>j</sub> (j值从1到n<sub>x</sub>)，也可以表示为&omega;<sup>T</sup>&omega;。此方法称为L2正则化。这里的&lambda;就是正则化参数。因为这里使用了欧几里得法线，被称为向量参数&omega;的L2范式。</p><p>为什么只正则化参数&omega;呢？我们可以加上参数b吗？我们可以这么做，但是一般习惯省略不写。因为&omega;通常是一个高维参数矢量，已经可以表达高偏差问题，&omega;可能包含很多参数，我们不可能拟合所有参数，而b只是单个数字，所以&omega;几乎涵盖所有参数，而不是b。如果加了参数b，没什么影响。因为b也是众多参数的一个，想加就加，没有问题。</p><p>L2正则化是最常见的正则化类型，还有L1正则化。L1正则化加的不是L2范式，而是正则项 &lambda;/m乘以$\sum_{j=1}^n $ |&omega;|，这也被称为参数&omega;向量的L1范数。(这里的n就是项数，和L2范式的nx一样)</p><p>如果用的L1是正则化，最终会是稀疏的，也就是说&omega;向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是L1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用L2正则化。</p><h3 id="为什么正则化有利于预防过拟合？"><a href="#为什么正则化有利于预防过拟合？" class="headerlink" title="为什么正则化有利于预防过拟合？"></a>为什么正则化有利于预防过拟合？</h3><p>如果正则化&lambda;设置得足够大，权重矩阵W被设置为接近于0的值（我也没看懂）。实际上不会发生这种情况的。直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近高偏差状态。</p><p>假设我们用tanh作为我们的激活函数，用g(z)表示tanh(z)。那么我们发现，只要z非常小，z只涉及少量参数。这里我们利用双曲正切函数的线性状态，只要z可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。</p><p>总结一下，如果正则化参数变得很大，参数W很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上，z的取值范围很小，这个激活函数，也就是曲线函数tanh会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p><h3 id="dropout正则化"><a href="#dropout正则化" class="headerlink" title="dropout正则化"></a>dropout正则化</h3><p>除了L2正则化，还有一个非常使用的正则化方法——Dropout(随机失活)。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008212828.png" alt=""></p><p>假设你在训练上图这样的神经网络，它存在过拟合，这就是<strong>dropout</strong>所要处理的，我们复制这个神经网络，<strong>dropout</strong>会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191008213026.png" alt=""></p><p>这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。不过可想而知，我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。</p><h3 id="如何实施dropout？"><a href="#如何实施dropout？" class="headerlink" title="如何实施dropout？"></a>如何实施dropout？</h3><p>吴恩达老师讲了最常用的方法。就是inverted dropout(随机失活)。</p><p>首先定义变量d，d3表示一个三层的dropout向量:</p><blockquote><p>d3 = np.random.rand(a3.shape[0],a3.shape[1])</p></blockquote><p>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，keep-prob是一个具体数字，上个示例中它是0.5，而假设在本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2。keep-prob的作用是生成随机矩阵，如果对a3进行因子分解，效果也是一样的。d3是一个矩阵，其中d3中的值为1的概率都是0.8，对应为0的概率是0.2。</p><p>接下来要做的就是从第三层中获取激活函数，这里我们叫它a3，a3含有要计算的激活函数，等于上面的a3乘以d3</p><blockquote><p>a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为a3*=d3。</p></blockquote><p>它的作用就是让过滤d3中所有等于0的元素，而各个元素等于0的概率只有20%，乘法运算最终把d3中相应元素归零，即让d3中0元素与a3中相对元素归零。</p><p>最后，我们向外扩展a3。用它除以0.8，或者除以keep-prob参数。</p><blockquote><p>a3/= (keep-prob)</p></blockquote><p>解释一下最后一步，我们假设第三隐藏层上有50个神经元，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%）个。现在我们看下z<sup>[4]</sup>，z<sup>[4]</sup>=&omega;<sup>[4]</sup>*a<sup>[3]</sup>+b<sup>[4]</sup>，我们的预期是，a<sup>[3]</sup>减少20%，也就是说a<sup>[3]</sup>中有20%的元素被归零。为了不影响z<sup>[4]</sup>的期望值，我们需要用&omega;<sup>[4]</sup>a<sup>[3]</sup>/0.8，它将会修正或弥补我们所需的那20%，a<sup>[3]</sup>的期望值不会变。</p><p>它的功能是，不论keep-prop的值是多少0.8，0.9甚至是1，如果keep-prop设置为1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以keep-prob，确保a<sup>[3]</sup>的期望值不变。</p><p>代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Z2 = np.dot(W2, A1) + b2</span><br><span class="line">A2 = relu(Z2)</span><br><span class="line"><span class="comment"># 随机生成和 A2 相同维度的矩阵</span></span><br><span class="line">D2 = np.random.rand(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>])    </span><br><span class="line"><span class="comment"># 保留部分元素: 小于keep_prob的设为 True, 否则设为 0 </span></span><br><span class="line">D2 = D2 &lt; keep_prob                             </span><br><span class="line"><span class="comment"># 对应位置的元素相乘, 可以把 True 看做 1, False 看做 0. </span></span><br><span class="line">A2 = np.multiply(A2, D2)                        </span><br><span class="line"><span class="comment"># 如果没有下面这行就是普通的 Dropout</span></span><br><span class="line">A2 = A2/keep_prob</span><br></pre></td></tr></table></figure><p>要注意的是, 我们不希望在测试的时候, 得到的结果也是随机的, 因此Dropout <strong>在测试过程不使用</strong>。</p><h3 id="理解dropout"><a href="#理解dropout" class="headerlink" title="理解dropout"></a>理解dropout</h3><p><strong>Dropout</strong>可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？</p><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，所以不愿意给任何一个输入加上太多权重。因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果。和之前讲的L2正则化类似，实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009170758.png" alt=""></p><p>实施dropout的另一个细节是，这是一个拥有三个输入特征的网络，其中一个要选择的参数是keep-prob，它代表每一层上保留单元的概率。所以不同层的keep-prob也可以变化。第一层，矩阵W<sup>[1]</sup>是7×3，第二个权重矩阵W<sup>[2]</sup>是7×7，第三个权重矩阵W<sup>[3]</sup>是3×7，以此类推，W<sup>[2]</sup>是最大的权重矩阵，因为W<sup>[2]</sup>拥有最大参数集，即7×7，为了预防矩阵的过拟合，对于这一层，它的keep-prob值应该相对较低，假设是0.5。对于其它层，过拟合的程度可能没那么严重，它们的keep-prob值可能高一些，可能是0.7甚至更高，假设这里是0.7。如果在某一层，我们不必担心其过拟合的问题，那么keep-prob可以为1。</p><p><strong>总结：</strong>如果我们担心某些层比其他层更容易发生过拟合，可以把某些层的keep-prob值设置得比其他层更低，缺点是为了使用交叉验证，我们要搜索更多得超级参数。另一种方案是在一些层上应用dropout，而有些层不用dropout应用。dropout的层只含有keep-prob这一个超参数。</p><p>dropout一大缺点就是代价函数J不再被明确定义，因为每次迭代都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。所以吴恩达老师推荐通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。</p><h3 id="其它正则化方法"><a href="#其它正则化方法" class="headerlink" title="其它正则化方法"></a>其它正则化方法</h3><h4 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h4><p>假设我们正在拟合猫咪图片分类器，如果我们想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。</p><p>除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。</p><p>通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009193402.png" alt=""></p><p>对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字4看起来是波形的，其实不用对数字4做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个4看起来有点扭曲。所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。</p><h4 id="early-stopping"><a href="#early-stopping" class="headerlink" title="early stopping"></a>early stopping</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009203859.png" alt=""></p><p>因为在训练过程中，我们希望训练误差，代价函数J都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升。early stopping 代表提早停止训练神经网络。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009200928.png" alt=""></p><p>当我们还未在神经网络上运行太多迭代过程的时候，参数&omega;接近0，因为随机初始化&omega;值时，它的值可能都是较小的随机值，所以在我们长期训练神经网络之前&omega;依然很小，在迭代过程和训练过程中&omega;的值会变得越来越大，比如在这儿，神经网络中参数&omega;的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个&omega;值中等大小的弗罗贝尼乌斯范数，与L2正则化相似，选择参数&omega;范数较小的神经网络，但愿那时的神经网络过度拟合不严重。</p><p>early stopping 有一个缺点，接下来了解一下。</p><p>在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。在重点优化代价函数时，你只需要留意&omega;和b，J(&omega;,b)的值越小越好，你只需要想办法减小这个值，其它的不用关注。然后，预防过拟合还有其他任务，就是减少方差”。</p><p>缺点就是我们不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。</p><h2 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h2><p>假设我们有一个数据集，它有两个输入特征，所以数据是二维的。下图即为数据集散点图:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212210.png" alt=""></p><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：</p><ol><li>零均值化</li><li>归一化方差；</li></ol><p><strong>我们希望无论是训练集还是测试集都是通过相同的μ和&sigma;<sup>2</sup>定义的数据转换。</strong></p><p>第一步是零均值化。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$x<sup>(i)</sup></p></blockquote><p>μ是一个向量，x等于每个训练数据x减去μ，意思是移动数据集，直到完成零均值化。</p><p>输入数据经过零均值后，得出下面的散点图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009212709.png" alt=""></p><p>第二步是归一化方差，注意特征x<sub>1</sub>的方差比特征x<sub>2</sub>的方差要大得多。</p><blockquote><p>μ=1/m x $\sum_{i=1}^m$(x<sup>(i)</sup>)<sup>2</sup></p></blockquote><p> &sigma;<sup>2</sup>是一个向量，它的每个特征都有方差。经过归一化方差后，得出下面结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009214916.png" alt=""></p><p>如果你用它来调整训练数据，那么用相同的μ和σ<sup>2</sup>来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论的μ值是什么，也不论的σ<sup>2</sup>值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估μ和σ<sup>2</sup>。因为我们希望不论是训练数据还是测试数据，都是通过相同μ和σ<sup>2</sup>定义的相同数据转换，其中μ和σ<sup>2</sup>是由训练集数据计算得来的。</p><p>为什么我们需要归一化输入特征？回想一下代价函数</p><blockquote><p>J(&omega;,b)=1/m x $\sum_{i=1}^m$L(y<sup>(i)</sup> hat , y<sup>(i)</sup>)</p></blockquote><p>如果输入未归一化的输入特征，代价函数如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021170801.png" alt=""></p><p>然而如果归一化特征，代价函数平均起来看更对称，如果要在上图这样的代价函数上运行梯度下降法，必须使用一个非常小的学习率。因为如果是在这个位置（蓝色箭头所在位置），梯度下降法可能需要多次迭代过程，直到最后找到最小值。但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，我们可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行。</p><p>下图即为输入归一化输入特征的代价函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191009222132.png" alt=""></p><p>如果输入特征处于不同范围内，可能有些特征值从0到1，有些从1到1000，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。</p><h2 id="梯度消失-梯度爆炸"><a href="#梯度消失-梯度爆炸" class="headerlink" title="梯度消失/梯度爆炸"></a>梯度消失/梯度爆炸</h2><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p><p>吴恩达老师给我们的理解是：权重W只比1略大一点（或者说比单位矩阵略大一点），深度神经网络的激活函数将爆炸式增长，如果W比单位矩阵略小一点，激活函数将以指数级递减。</p><h2 id="神经网络的权重初始化"><a href="#神经网络的权重初始化" class="headerlink" title="神经网络的权重初始化"></a>神经网络的权重初始化</h2><p>为了避免上述的梯度爆炸，权重的初始化很重要。首先，我们来看看只有一个神经元的情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021163626.png" alt=""></p><p>假设b=0，可以得到：</p><p>Z=W<sub>1</sub>x<sub>1</sub>+W<sub>2</sub>x<sub>2</sub>+W<sub>3</sub>x<sub>3</sub>+……+W<sub>n</sub>x<sub>n</sub>，b=0。</p><p>为了不让Z那么大，我们尽量让W<sub>i</sub>小一些。实际做法如下：</p><p>​    使用Relu函数，W的初始化方法：</p><p>​    <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021163946.png" alt=""></p><p>​    使用tanh函数，W的初始化方法：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191021164016.png" alt=""></p><p>其中, n<sup>[l−1]</sup> 表示第l层的输入特征的个数, 也就是第 l - 1 层神经元的个数.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。&lt;/p&gt;
&lt;h2 id=&quot;训练、验证和测试集&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="改善深层神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>人脸验证和神经风格转换</title>
    <link href="http://yoursite.com/2019/10/05/%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2/"/>
    <id>http://yoursite.com/2019/10/05/人脸验证和神经风格转换/</id>
    <published>2019-10-05T01:39:06.000Z</published>
    <updated>2019-10-19T03:30:22.988Z</updated>
    
    <content type="html"><![CDATA[<h2 id="One—Shot学习"><a href="#One—Shot学习" class="headerlink" title="One—Shot学习"></a>One—Shot学习</h2><p>人脸识别所面临的一个挑战就是需要解决一次学习问题，这意味着在大多数人脸识别应用中，需要通过单单一张图片或者单单一个人脸样例就能去识别这个人。而在我们的学习过程中，发现深度学习只有一个训练样例时，它的表现并不好。接下来解决一下这个问题。</p><p>假设我们的数据库有以下四张照片：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005095438.png" alt=""></p><p>有一种办法是，将人的照片放进卷积神经网络中，使用softmax单元来输出4种，或者说5种标签，分别对应这4个人，或者4个都不是，所以softmax里我们会有5种输出。但实际上这样效果并不好，因为如此小的训练集不足以去训练一个稳健的神经网络。而且，假如有新人加入团队，我们现在将会有5个组员需要识别，所以输出就变成了6种，这时你要重新训练神经网络吗？这听起来实在不像一个好办法。</p><p>所以要让人脸识别能够做到一次学习，为了能有更好的效果，现在要做的应该是学习<strong>Similarity</strong>函数。详细地说，想要神经网络学习这样一个用d表示的函数：d(img1,img2)=degree of difference between images。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005100413.png" alt=""></p><p>它以两张图片作为输入，然后输出这两张图片的差异值。如果你放进同一个人的两张照片，你希望它能输出一个很小的值，如果放进两个长相差别很大的人的照片，它就输出一个很大的值。所以在识别过程中，如果这两张图片的差异值小于某个阈值τ，它是一个超参数，那么这时就能预测这两张图片是同一个人，如果差异值大于τ，就能预测这是不同的两个人，这就是解决人脸验证问题的一个可行办法。</p><h2 id="Siamese网络"><a href="#Siamese网络" class="headerlink" title="Siamese网络"></a>Siamese网络</h2><p>通过上一小节我们知道我们该怎么去做人脸识别，通过输入两张图片。它将让你解决一次学习问题。接下来我们学习如何训练我们的神经网络学会这个函数d。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005102040.png" alt=""></p><p>看上图，我们经常看到这样的卷积网络，输入图片，然后通过一些列卷积，池化和全连接层，最终得到编号1这样的特征向量。有时这个会被送进<strong>softmax</strong>单元来做分类。但是我们关注的重点是编号1，假如它有128个数，它是由网络深层的全连接层计算出来的，我们要给这128个数命个名字，把它叫做f(x<sup>(1)</sup>)。可以把f(x(<sup>(1)</sup>)看作是输入图像x<sup>(1)</sup>的编码，取这个输入图像（编号2），然后表示成128维的向量。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005102947.png" alt=""></p><p>建立一个人脸识别系统的方法就是，如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我们把第二张图片的编码叫做f(x<sup>(2)</sup>)。这里我用x<sup>(1)</sup>和x<sup>(2)</sup>仅仅代表两个输入图片，它们没必要非是第一个和第二个训练样本，可以是任意两个图片。最后如果相信这些编码很好地代表了这两个图片，我们要做的就是定义d，将x<sup>(1)</sup>和x<sup>(2)</sup>的距离定义为这两幅图片的编码之差的范数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005104620.png" alt=""></p><p>那么怎么训练这个Siamese神经网络呢？不要忘了这两个网络有相同的参数，所以实际要做的就是训练一个网络，它计算得到的编码可以用于函数d，它可以告诉我们两张图片是否是同一个人。更准确地说，神经网络的参数定义了一个编码函数，如果给定输入图像，这个网络会输出的128维的编码。你要做的就是学习参数，使得如果两个图片和是同一个人，那么你得到的两个编码的距离就小。相反，如果和是不同的人，那么我们会想让它们之间的编码距离大一点。</p><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>训练神经网络两种方法：Triplet 损失 和 人脸识别二分类</p><h3 id="Triplet损失"><a href="#Triplet损失" class="headerlink" title="Triplet损失"></a>Triplet损失</h3><p>要想学习神经网络参数来得到优质的人脸图片编码，方法之一就是定义三元组损失函数然后应用梯度下降。看到这个的时候，我也一脸懵逼。这是什么啊？接下来我们一起看下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005110336.png" alt=""></p><p>我们看下这是什么意思，为了应用三元组损失函数，你需要比较成对的图像，比如这个图片，为了学习网络的参数，你需要同时看几幅图片，比如这对图片（编号1和编号2），你想要它们的编码相似，因为这是同一个人。然而假如是这对图片（编号3和编号4），你会想要它们的编码差异大一些，因为这是不同的人。</p><p>用三元组损失的术语来说，我们要做的通常是看一个 Anchor 图片，想让Anchor图片和Positive图片（Positive意味着是同一个人）的距离很接近。然而，当Anchor图片与Negative图片（Negative意味着是非同一个人）对比时，我们会想让他们的距离离得更远一点。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005111313.png" alt=""></p><p>这就是为什么叫做三元组损失，它代表你通常会同时看三张图片，你需要看Anchor图片、Postive图片，还有Negative图片，我要把Anchor图片、Positive图片和Negative图片简写成A、P、N。</p><p>把以上内容写成公式的话，d= ||f(A)-f(P)||<sup>2</sup>。我们希望||f(A)-f(P)||<sup>2</sup> &lt;= ||f(A)-f(N)||<sup>2</sup>。||f(A)-f(P)||<sup>2</sup> 就是d(A,P)，||f(A)-f(N)||<sup>2</sup>时d(A,N)，我们可以把d看为距离函数。</p><p>对表达式修改一下，因为有一种情况满足这个表达式，但是没有用处，就是把所有的东西都学成0，如果总是输出0，即0-0≤0，这就是0减去0还等于0，如果所有图像的都是一个零向量，那么总能满足这个方程f。所以为了确保网络对于所有的编码不会总是输出0，也为了确保它不会把所有的编码都设成互相相等的。另一种方法能让网络得到这种没用的输出，就是如果每个图片的编码和其他图片一样，这种情况，还是得到0-0。</p><p>为了阻止网络出现上述状况，我们需要修改表达式。也就是||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>不能刚好小于等于0。应该是比0还要小，所以这个应该小于一个-a，也就是||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup> &lt;= -a。这里的a是一个超参数，是为了阻止网络输出无用的结果。</p><p>举个例子，假如间隔设置成0.2，如果在这个例子中，d(A,P)=0.5，如果 Anchor和 Negative图片的，即只大一点，比如说0.51，条件就不能满足。虽然0.51也是大于0.5的，但还是不够好，我们想要比大很多。你会想让这个值d(A,N)至少是0.7或者更高，这样间距至少达到0.2，你可以把这项调大或者这个调小。超参数a至少是0.2，在d(A,P)和d(A,N)之间至少相差0.2，这就是间隔参数的作用。</p><p>接下来定义损失函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005114357.png" alt=""></p><p>这个max函数的作用就是，只要这个||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a &lt;= 0，那么损失函数就是0。另一方面如果||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a&gt;=0，然后取最大值。最后得到||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a，这样就会得到一个正的损失值。通过最小化这个损失函数达到的效果就是使这部分||f(A)-f(P)||<sup>2</sup> - ||f(A)-f(N)||<sup>2</sup>+a小于等于0，只要这个损失函数小于等于0，网络不会关心它负值有多大。</p><p>这是一个三元组定义的损失，整个网络的代价函数应该是训练集中这些单个三元组损失的总和。假如你有一个10000个图片的训练集，里面是1000个不同的人的照片，你要做的就是取这10000个图片，然后生成这样的三元组，然后训练你的学习算法，对这种代价函数用梯度下降，这个代价函数就是定义在你数据集里的这样的三元组图片上。</p><p>注意，为了定义三元组的数据集你需要成对的A和P，即同一个人的成对的图片，为了训练你的系统你确实需要一个数据集，<strong>里面有同一个人的多个照片</strong>。这也是为什么在这个例子中，我说假设你有1000个不同的人的10000张照片，也许是这1000个人平均每个人10张照片，组成了你整个数据集。如果你只有每个人一张照片，那么根本没法训练这个系统。当然，训练完这个系统之后，你可以应用到你的一次学习问题上，对于你的人脸识别系统，可能你只有想要识别的某个人的一张照片。但对于训练集，你需要确保有同一个人的多个图片，至少是你训练集里的一部分人，这样就有成对的Anchor和Positive图片了。</p><p>那么我们该怎么选择这些三元组来形成训练集呢？一个问题是如果从训练集中，随机地选择A、P和N，遵守A和P是同一个人，而 A 和 N 是不同的人这一原则。有个问题就是，如果随机的选择它们，那么这个约束条件 d(A,P)+a&lt;=d(A,N) 很容易达到，因为随机选择的图片，A和N 比 A和P差别很大的概率很大。所以有很大的可能性|| f(A)-f(N) ||会比||(f(A) -f(P) )||大，而且差距远大于a，这样网络并不能从中学到什么。</p><p>所以为了构建一个数据集，要做的就是尽可能选择难训练的三元组A、P和N。具体而言，你想要所有的三元组都满足这个条件 d(A,P)+a&lt;=d(A,N)。难训练的三元组就是，你的A、P和N的选择使得很接近，即d(A,P)约等于d(A,N)。这样的学习算法会竭尽全力使d(A,N)式子变大，或者使d(A,P)变小，这样左右两边至少有一个的间隔。并且选择这样的三元组还可以增加你的学习算法的计算效率，如果随机的选择这些三元组，其中有太多会很简单，梯度算法不会有什么效果，因为网络总是很容易就得到了正确的结果，只有选择难的三元组梯度下降法才能发挥作用，使得这两边离得尽可能远。</p><h3 id="人脸验证与二分类"><a href="#人脸验证与二分类" class="headerlink" title="人脸验证与二分类"></a>人脸验证与二分类</h3><p>上述的Triplet loss是一个学习人脸识别卷积网络参数的好方法。另一个训练神经网络的方法是选取一对神经网络Siamese网络，使其同时计算这些嵌入，比如说128维的嵌入（编号1），或者更高维，然后将其输入到逻辑回归单元，然后进行预测，如果是相同的人，那么输出是1，若是不同的人，输出是0。这就把人脸识别问题转换为一个二分类问题，训练这种系统时可以替换Triplet loss的方法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005162142.png" alt=""></p><p>最后的逻辑回归单元输出 y帽：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005162852.png" alt=""></p><p>f(x<sup>(i)</sup>)<sub>k</sub> 代表图片x<sup>(i)</sup> 的编码，下标 k代表选择这个向量中的第k个元素。| f(x<sup>(i)</sup>)<sub>k</sub> -f(x<sup>(i)</sup>)<sub>k</sub> |是对着两个编码取元素差的绝对值。把这128个元素当作特征，然后把他们放入逻辑回归中，最后的逻辑回归可以增加参数&omega;<sub>i</sub> 和 b，就像普通的逻辑回归一样。你将在这128个单元上训练合适的权重，用来预测两张图片是否是一个人，这是一个很合理的方法来学习预测0或者1，即是否是同一个人。</p><p>   <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005165455.png" alt=""></p><p>但是在这个学习公式中，输入是一对图片，这是你的训练输入x（编号1、2），输出y是0或者1，取决于你的输入是相似图片还是非相似图片。与之前类似，你正在训练一个Siamese网络，意味着上面这个神经网络拥有的参数和下面神经网络的相同（编号3和4所示的网络），两组参数是绑定的，这样的系统效果很好。</p><p>如果这是一张新图片（编号1），当员工走进门时，希望门可以自动为他们打开，这个（编号2）是在数据库中的图片，不需要每次都计算这些特征（编号6），不需要每次都计算这个嵌入，你可以提前计算好，那么当一个新员工走近时，你可以使用上方的卷积网络来计算这些编码（编号5），然后使用它，和预先计算好的编码进行比较，然后输出预测值。因此不需要存储原始图像，如果你有一个很大的员工数据库，你不需要为每个员工每次都计算这些编码。这个预先计算的思想，可以节省大量的计算。</p><h2 id="神经风格迁移"><a href="#神经风格迁移" class="headerlink" title="神经风格迁移"></a>神经风格迁移</h2><p>这是卷积神经网络最有趣的应用。什么是神经风格迁移？</p><p>看例子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005170737.png" alt=""></p><p>为了描述如何实现神经网络迁移，我将使用来C表示内容图像，S表示风格图像，G表示生成的图像。这只是一个提出，在深入了解如何实现神经风格迁移之前，我们先看神经网络不同层之间的具体运算。</p><h2 id="CNN特征可视化"><a href="#CNN特征可视化" class="headerlink" title="CNN特征可视化"></a>CNN特征可视化</h2><p>其实我一直觉得神经网络的解释性真的蛮差的。让我们接下来看一下，深度卷积网络到底在学什么？</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191005193429.png" alt=""></p><p>看不懂，之后再补。。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;One—Shot学习&quot;&gt;&lt;a href=&quot;#One—Shot学习&quot; class=&quot;headerlink&quot; title=&quot;One—Shot学习&quot;&gt;&lt;/a&gt;One—Shot学习&lt;/h2&gt;&lt;p&gt;人脸识别所面临的一个挑战就是需要解决一次学习问题，这意味着在大多数人脸识别应
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="人脸验证" scheme="http://yoursite.com/tags/%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>目标检测组件</title>
    <link href="http://yoursite.com/2019/10/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%84%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/10/03/目标检测组件/</id>
    <published>2019-10-03T13:16:58.000Z</published>
    <updated>2019-10-29T01:34:31.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个问题，即定位分类问题。这意味着，我们不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来，这就是定位分类问题。其中“定位”的意思是判断汽车在图片中的具体位置。之后，我们再讲讲当图片中有多个对象时，应该如何检测它们，并确定出位置。比如，你正在做一个自动驾驶程序，程序不但要检测其它车辆，还要检测其它对象，如行人、摩托车等等，稍后我们再详细讲。</p><p>本节我们要研究的分类定位问题，通常只有一个较大的对象位于图片中间位置，我们要对它进行识别和定位。而在对象检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。因此，图片分类的思路可以帮助学习分类定位，而对象定位的思路又有助于学习对象检测，我们先从分类和定位开始讲起。</p><p>图片分类问题你已经并不陌生了，例如，输入一张图片到多层卷积神经网络。这就是卷积神经网络，它会输出一个特征向量，并反馈给<strong>softmax</strong>单元来预测图片类型。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004134535.png" alt=""></p><p>如果你正在构建汽车自动驾驶系统，那么对象可能包括以下几类：行人、汽车、摩托车和背景，这意味着图片中不含有前三种对象，也就是说图片中没有行人、汽车和摩托车，输出结果会是背景对象，这四个分类就是softmax函数可能输出的结果。</p><p>下面是如何为监督学习任务定义目标标签y，请注意，这有四个分类，神经网络输出的是这四个数字和一个分类标签，或分类标签出现的概率。目标标签y的定义如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004140243.png" alt=""></p><p>它是一个向量，第一个组件p<sub>c</sub>表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则p<sub>c</sub>=1，如果是背景，则图片中没有要检测的对象，则p<sub>c</sub>=0 。我们可以这样理解p<sub>c</sub>，它表示被检测对象属于某一分类的概率，背景分类除外。</p><p>如果检测到对象，就输出被检测对象的边界框参数b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>和b<sub>w</sub>。最后，如果存在某个对象，那么p<sub>c</sub>，同时输出c<sub>1</sub>、c<sub>2</sub> 和c<sub>3</sub>，表示该对象属于1-3类中的哪一类，是行人，汽车还是摩托车。鉴于我们所要处理的问题，我们假设图片中只含有一个对象，所以针对这个分类定位问题，图片最多只会出现其中一个对象。</p><p>我们再看几个例子，假如下图是一张训练集图片。在y当中，第一个元素p<sub>c</sub>，因为图中有一辆车，b<sub>x</sub>、b<sub>y</sub>、b<sub>h</sub>和b<sub>w</sub>会指明边界框的位置，所以标签训练集需要标签的边界框。图片中是一辆车，所以结果属于分类2，因为定位目标不是行人或摩托车，而是汽车，所以c<sub>1</sub>=0，c<sub>2</sub>=1，c<sub>3</sub>=0，c<sub>1</sub>、c<sub>2</sub>和c<sub>3</sub>中最多只有一个等于1。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143054.png" alt=""></p><p>上图是只有一个检测对象的情况，如果图片中没有检测对象呢？看下图</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143246.png" alt=""></p><p>这种情况下，p<sub>c</sub>=0，y的其它参数将变得毫无意义。这里我全部写成问号，表示”毫无意义“的参数，因为图片中不存在检测对象，所以不用考虑网络输出中边界框的大小。也不用考虑图片中的对象是属于c<sub>1</sub>、c<sub>2</sub>、c<sub>3</sub>中的哪一类。</p><p>最后，我们介绍一下神经网络的损失函数，其参数为类别y和网络输出y帽，如果采用平方误差策略，则</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004143820.png" alt=""></p><p>损失值等于每个元素相应差值的平方和。</p><p>如果图片中存在定位对象，那么y<sub>1</sub> = 1，所以y<sub>1 </sub>= p<sub>c</sub>，同样地，如果图片中存在定位对象，p<sub>c</sub> = 1，损失值就是不同元素的平方和。</p><p>另一种情况是，y<sub>1</sub>=0，也就是p<sub>c</sub>=0，损失值是(y<sub>1</sub>帽 - y<sub>1</sub>)^2，因为对于这种情况，我们不用考虑其它元素，只需要关注神经网络输出p<sub>c</sub>的准确度。</p><h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>假设我们正在构建一个人脸识别应用，如下图。出于某种原因，我们希望算法可以给出眼角的具体位置。眼角坐标为(x,y)，你可以让神经网络的最后一层多输出两个数字 l<sub>x</sub> 和 l<sub>y</sub>，作为眼角的坐标值。如果你想知道两只眼睛的四个眼角的具体位置，那么从左到右，依次用四个特征点来表示这四个眼角。对神经网络稍做些修改，输出第一个特征点（l<sub>1x</sub>，l<sub>1y</sub>），第二个特征点（l<sub>2x</sub>，l<sub>2y</sub>），依此类推，这四个脸部特征点的位置就可以通过神经网络输出了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004145802.png" alt=""></p><p>也许除了这四个特征点，你还想得到更多的特征点输出值，这些（图中眼眶上的红色特征点）都是眼睛的特征点，你还可以根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。如下图，为了便于说明，你可以设定特征点的个数，假设脸部有64个特征点，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004145926.png" alt=""></p><p>具体做法是，准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1或0，1表示有人脸，0表示没有人脸，然后输出（l<sub>1x</sub>，l<sub>1y</sub>）……直到（l<sub>64x</sub>，l<sub>64y</sub>）。这里我用 l 代表一个特征，这里有129个输出单元，其中1表示图片中有人脸，因为有64个特征，64×2=128，所以最终输出128+1=129个单元。</p><p>最后一个例子，看下图。如果你对人体姿态检测感兴趣，你还可以定义一些关键特征点，假设有32个，如胸部的中点，左肩，左肘，腰等等。然后通过神经网络标注人物姿态的关键特征点，再输出这些标注过的特征点，就相当于输出了人物的姿态动作。当然，要实现这个功能，你需要设定这些关键特征点，从胸部中心(l<sub>1x</sub>，l<sub>1y</sub>)一直向下，直到（l<sub>32x</sub>，l<sub>32y</sub>）。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004150615.png" alt=""></p><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。接下来，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151006.png" alt=""></p><p>假如你想构建一个汽车检测算法，步骤是，首先创建一个标签训练集，也就是x和y表示适当剪切的汽车图片样本，这张图片（编号1）x是一个正样本，因为它是一辆汽车图片，这几张图片（编号2、3）也有汽车，但这两张（编号4、5）没有汽车。出于我们对这个训练集的期望，你一开始可以使用适当剪切的图片，就是整张图片x几乎都被汽车占据，你可以照张照片，然后剪切，剪掉汽车以外的部分，使汽车居于中间位置，并基本占据整张图片。有了这个标签训练集，你就可以开始训练卷积网络了，输入这些适当剪切过的图片（编号6），卷积网络输出y，0或1表示图片中有汽车或没有汽车。训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004172940.png" alt=""></p><p>假设上图这是一张测试图片，首先选定一个特定大小的窗口，比如图片下方这个窗口，将这个红色小方块输入卷积神经网络，卷积网络开始进行预测，即判断红色方框内有没有汽车。</p><p>滑动窗口目标检测算法接下来会继续处理第二个图像，即红色方框稍向右滑动之后的区域，并输入给卷积网络，因此输入给卷积网络的只有红色方框内的区域，再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151258.png" alt=""></p><p>接着我们重复上述过程，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或1。如下图。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151527.png" alt=""></p><p>再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004151813.png" alt=""></p><p>这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。</p><p>滑动窗口目标检测算法也有很明显的缺点，就是计算成本，因为你在图片中剪切出太多小方块，卷积网络要一个个地处理。如果你选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本。</p><h2 id="滑动窗口的卷积实现"><a href="#滑动窗口的卷积实现" class="headerlink" title="滑动窗口的卷积实现"></a>滑动窗口的卷积实现</h2><p>为了构建滑动窗口的卷积应用，首先要知道如何把<strong>神经网络的全连接层转化为卷积层</strong>。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004152353.png" alt=""></p><p>假设对象检测算法输入一个14×14×3的图像，图像很小，不过演示起来方便。在这里过滤器大小为5×5，数量是16，14×14×3的图像在过滤器处理之后映射为10×10×16。然后通过参数为2×2的最大池化操作，图像减小到5×5×16。然后添加一个连接400个单元的全连接层，接着再添加一个全连接层，最后通过<strong>softmax</strong>单元输出。为了跟下图区分开，我先做一点改动，用4个数字来表示，它们分别对应softmax单元所输出的4个分类出现的概率。这4个分类可以是行人、汽车、摩托车和背景或其它对象。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004152537.png" alt=""></p><p>画一个这样的卷积网络，它的前几层和之前的一样，而对于下一层，也就是这个全连接层，我们可以用5×5的过滤器来实现，数量是400个（编号1所示），输入图像大小为5×5×16，用5×5的过滤器对它进行卷积操作，过滤器实际上是5×5×16，因为在卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致，输出结果为1×1。假设应用400个这样的5×5×16过滤器，输出维度就是1×1×400，我们不再把它看作一个含有400个节点的集合，而是一个1×1×400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5×5×16维度的过滤器，所以每个值都是上一层这些5×5×16激活值经过某个任意线性函数的输出结果。接着我们再添加另外一个卷积层（编号2所示），这里用的是1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，它其实就是上个网络中的这一全连接层。最后经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，我们最终得到这个1×1×4的输出层，而不是这4个数字（编号3所示）。以上就是用卷积层代替全连接层的过程，结果这几个单元集变成了1×1×400和1×1×4的维度。</p><p>掌握了卷积知识，我们再看看如何通过卷积实现滑动窗口对象检测算法。</p><p>假设向滑动窗口卷积网络输入14×14×3的图片，为了简化演示和计算过程，这里我们依然用14×14的小图片。和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4，我画得比较简单，严格来说，14×14×3应该是一个长方体，第二个10×10×16也是一个长方体。为了方便，立体部分随便画了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154252.png" alt=""></p><p>上图是输入给卷积网络的图片大小是14×14×3，测试集图片是16×16×3，如下图。现在给这个输入图片加上黄色条块，在最初的滑动窗口算法中，你会把这片蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。我们在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154529.png" alt=""></p><p>把这两个图放一起：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004154910.png" alt=""></p><p>结果发现，这4次卷积操作中很多计算都是重复的。所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在这一步操作中（编号1），卷积网络运行同样的参数，使得相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化（编号2），输出结果6×6×16。照旧应用400个5×5的过滤器（编号3），得到一个2×2×400的输出层，现在输出层为2×2×400，而不是1×1×400。应用1×1过滤器（编号4）得到另一个2×2×400的输出层。再做一次全连接的操作（编号5），最终得到2×2×4的输出层，而不是1×1×4。最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），也就是这个14×14区域经过卷积网络处理后的结果，同样，右下角这个方块是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果。</p><p>具体的计算步骤是以绿色方块为例，假设你剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6）</p><p>所以该卷积操作的原理是我们不需要把输入图像分割成四个子集，分别执行前向传播，而是把<strong>它们作为一张图片输入给卷积网络进行计算</strong>，其中的公共区域可以共享很多计算。</p><p>以上就是在卷积层上应用滑动窗口算法的内容，它提高了整个算法的效率。不过这种算法仍然存在一个缺点，就是边界框的位置可能不够准确。</p><h2 id="Bounding-Box预测"><a href="#Bounding-Box预测" class="headerlink" title="Bounding Box预测"></a>Bounding Box预测</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004161102.png" alt=""></p><p>看上面的预测结果，这些边界框没有一个能完美匹配汽车的位置。</p><p>其中一个能得到更精准边框的算法是YOLO算法。它是这么做的，比如你的输入图像是100×100的，然后在图像上放一个网格。为了介绍起来简单一些，我用3×3网格，实际实现时会用更精细的网格，可能是19×19，可能更精细。基本思路是：采用图像分类和定位算法，逐一应用到图像的九个格子中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004162024.png" alt=""></p><p>所以对于这里9个格子中任何一个，你都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。因为这里有3×3格子，然后对于每个格子，你都有一个8维向量，所以目标输出尺寸是3×3×8。</p><p>注意：把对象分配到一个格子的过程是，<strong>你观察对象的中点，然后将这个对象分配到其中点所在的格子</strong>。所以即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，就是3×3网络的其中一个格子，或者19×19网络的其中一个格子。如果我们在现实实践时，采用19x19的网格会更精细。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。</p><h2 id="交并比（IOU）"><a href="#交并比（IOU）" class="headerlink" title="交并比（IOU）"></a>交并比（IOU）</h2><p>在对象检测任务中，你希望能够同时定位对象，所以如果实际边界框是这样的，你的算法给出这个紫色的边界框，那么这个结果是好还是坏？所以交并比（lOU）函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域（绿色阴影表示区域），而交集就是这个比较小的区域（橙色阴影表示区域），那么交并比就是交集的大小，这个橙色阴影面积，然后除以绿色阴影的并集面积。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004162947.png" alt=""></p><p>一般约定，在计算机检测任务中，如果IOU&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，lOU就是1，因为交集就等于并集。但一般来说只要IOU&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将IOU定得更高，比如说大于0.6或者更大的数字，但IOU越高，边界框越精确。</p><h2 id="非极大值抑制（NMS）"><a href="#非极大值抑制（NMS）" class="headerlink" title="非极大值抑制（NMS）"></a>非极大值抑制（NMS）</h2><p>到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004165505.png" alt=""></p><p>假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上右边这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004165617.png" alt=""></p><p>图中的绿色点和黄色点分别为两辆车的中点。</p><p>实践中当你运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。对于左边的车子也一样，所以不仅仅是这个格子，如果这是你们以前见过的图像，不仅这个格（编号4）子会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。</p><p>我们分步介绍一下非极大抑制是怎么起效的，因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的p<sub>c</sub>，我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后可能会对同一个对象做出多次检测，所以非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。</p><p>所以具体上，这个算法做的是，首先看看每次报告每个检测结果相关的概率p<sub>c</sub>，首先看概率最大的那个，这个例子（右边车辆）中是0.9，然后就说这是最可靠的检测，所以我们就用高亮标记，就说我这里找到了一辆车。这么做之后，非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制。所以这两个矩形分别是0.6和0.7，这两个矩形和淡蓝色矩形重叠程度很高，所以会被抑制，变暗，表示它们被抑制了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004170554.png" alt=""></p><p>接下来，逐一审视剩下的矩形，找出概率最高p<sub>c</sub>= 0.8，我们就认为这里检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他lOU值很高的矩形。所以现在每个矩形都会被高亮显示或者变暗，如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些，这就是最后得到的两个预测结果。</p><p>所以这就是非极大值抑制，非最大值意味着你只输出概率最大的分类结果，但抑制很接近，但不是最大的其他预测结果，所以这方法叫做非极大值抑制。</p><p>在这里只介绍了算法检测单个对象的情况，如果你尝试同时检测三个对象，比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。</p><h2 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h2><p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，你可以这么做，就是使用<strong>anchor box</strong>这个概念。</p><p>假设有这样一张图片，对于这个例子，我们继续使用3x3网格。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004175438.png" alt=""></p><p>注意人的中点和汽车的中点几乎在同一个地方，两者都落入同一个格子。对于那个同一个格子，y输出</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004140243.png" alt=""></p><p>我们可以检测着三个类别：行人、汽车、摩托车。它将无法输出检测结果，所以我必须从两个检测结果中选一个。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191004185914.png" alt=""></p><p>而anchor box的思路是，这样子，预先定义两个不同形状的anchor box，或者anchor box形状，要做的是把预测结果和这两个anchor box关联起来。一般来说，你可能会用更多的anchor box，可能要5个甚至更多，但对于这个视频，我们就用两个anchor box，这样介绍起来简单一些。</p><p>我们要做的是定义类别标签。用的向量不再是上面那个y=[p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>]^T，而是重复两次，y=[p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>  p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub>]^T。前面的p<sub>c</sub>  b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  c<sub>1</sub>  c<sub>2</sub>  c<sub>3</sub> 是和anchor box1关联的8个参数，后面的8个参数适合anchor box2相关联。</p><p>因为行人的形状更类似于anchor box 1的形状，而不是anchor box 2的形状，所以可以用这8个数值（前8个参数），这么编码p<sub>c</sub>=1代表有个行人，用b<sub>x</sub>,b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>编码包住行人的边界框，然后用c<sub>1</sub>=1,c<sub>2</sub>=0,c<sub>3</sub>=0来说明这个对象是个行人。然后是车子，因为车子的边界框比起anchor box 1更像anchor box 2的形状。这样编码，p<sub>c</sub>=1,b<sub>x</sub>,b<sub>y</sub>,b<sub>h</sub>,b<sub>w</sub>, c<sub>1</sub>=0,c<sub>2</sub>=1,c<sub>3</sub>=0。</p><p>假设车子的边界框形状是这样，更像anchor box 2，如果这里只有一辆车，行人走开了，那么anchor box 2分量还是一样的，要记住这是向量对应anchor box 2的分量和anchor box 1对应的向量分量，我们要填的就是，里面没有任何对象，所以 ，然后剩下的就是？。编码：y=[0  ?  ?  ?  ?  ?  ?  ?  1 b<sub>x</sub>  b<sub>y</sub>  b<sub>h</sub>  b<sub>w</sub>  0  1  0]。</p><p>最后，应该怎么选择anchor box呢？人们一般手工指定anchor box形状，我们可以选择5到10个anchor box形状，覆盖到多种不同的形状，可以涵盖我们自己想要检测的对象的各种形状。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;目标定位&quot;&gt;&lt;a href=&quot;#目标定位&quot; class=&quot;headerlink&quot; title=&quot;目标定位&quot;&gt;&lt;/a&gt;目标定位&lt;/h2&gt;&lt;p&gt;图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度卷积网络</title>
    <link href="http://yoursite.com/2019/10/02/%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/10/02/深度卷积网络/</id>
    <published>2019-10-02T04:25:39.000Z</published>
    <updated>2019-10-03T12:56:52.847Z</updated>
    
    <content type="html"><![CDATA[<h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet - 5"></a>LeNet - 5</h3><p>LeNet-5是专门为灰度图训练的。所以他的图像样本的深度都是1。</p><p>LeNet-5的网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002154958.png" alt=""></p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002155614.png" alt=""></p><p>实际上论文的原文是使用224x224x3的，但经过试验发现227x227x3效果更好。</p><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p>网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002161015.png" alt=""></p><p>从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。</p><p>可以看到 VGG16 是13个卷积层+3个全连接层叠加而成。</p><h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>网络越深越难训练，因为存在梯度消失和梯度爆炸的问题。本小节学习跳远连接。跳远连接可从某一个网络层激活，然后迅速反馈给另外一层甚至是神经网络的更深层。我们可以利用跳远连接构建能够训练深度网络的ResNets。</p><p>ResNets是由残差块构建的。那什么是残差块？看下图</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002163255.png" alt=""></p><p>这是一个两层的神经网络，它在L层进行激活，得到a[ l +1]再次进行激活，两层之后得到a[ l +2]。下图中的黑色部分即为计算过程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/img_0007.png" alt=""></p><p>而在残差网络中，我们直接将a^[ l ]拷贝到蓝色箭头所指位置。在线性激活之后、Relu非线性激活之前加上a[ l ]，不在沿着原来的主路径传递。这就意味者我们主路径过程的第四个式子替换为蓝色式子，也正是这个蓝色式子中加上的a[ l ]产生了一个残差块。</p><h3 id="为什么残差网络有用？"><a href="#为什么残差网络有用？" class="headerlink" title="为什么残差网络有用？"></a>为什么残差网络有用？</h3><p>一个网络深度越深，它在训练集上训练网络的效率会有所减弱，但对于ResNets就不完全是这样了。</p><p>上一张图已经说过a^[ l +2]=g(z^[ l +2]+a^[ l ])，添加项a^[ l ]是刚添加的跳远连接的输入。解开这个式子，得：</p><p>a^[ l+2]=g(w^[ l+2] * a^[ l+1]+b^[ l+2]+a^[ l ])，这里w和b为关键值。如果w和b均为0，那a^[ l+2]=g(a^[ l ])=a^[ l ](假设这里得激活函数是Relu)。结果表明，残差块学习这个恒等式函数残差块并不难，跳远连接让我们很容易的得到a^[ l+2]=a^[ l ]</p><p>这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络。因为对它来说，学习恒等函数对它来说很简单，尽管它多了两层，也只是把a^[ l ]的值赋给a^[ l+2]。</p><p><strong>所以，残差网络有用的原因是这些函数残差块学习恒等函数非常容易。</strong></p><h2 id="1x1的卷积"><a href="#1x1的卷积" class="headerlink" title="1x1的卷积"></a>1x1的卷积</h2><p>看到这个标题，你也许会迷惑，1x1的卷积能做什么呢？不就是乘以数字吗？结果并非如此</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002205234.png" alt=""></p><p>看上图，你会觉得这个1x1的过滤器没什么用，只是对输入矩阵乘以某个数字。但这个1x1的过滤器仅仅是对于6x6x1的信道图片效果不好 。如果是一张6x6x32的图片就不一样了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002205913.png" alt=""></p><p>具体来说，1x1卷积所实现的功能是遍历这36个单元格。计算输入图中32个数字和过滤器中32个数字的乘积，然后应用Relu函数。我们以一个单元格为例，用着36个数字乘以这个输入层上1x1的切片，得到一个实数画在下面图中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002211057.png" alt=""></p><p>这个1x1x32的过滤器中的32可以这样理解，一个神经元的输入是32个数字，乘以相同高度和宽度上某个切片上的32个数字。这三十二个数字具有不同信道，乘以32个权重，然后应用Relu非线性函数。一般来说，如果过滤器不止一个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6x6x过滤器数量。所以1x1卷积可以从根本上理解为这32个单元都应用了一个全连接神经网络。全连接层的作用是输入32个数字和过滤器数量，标记为nc^[ l+1]。在36个单元上重复此过程，输出结果是6x6x过滤器数量。这种方法通常称为1x1卷积，也成为Network in Network。</p><p>下面是一个1x1卷积的应用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002213518.png" alt=""></p><p>假设这是一个28x28x192的输入层，我们可以利用池化层压缩它的高度和宽度。但是如果信道数量很大，我们该如何把它压缩为28x28x32维度的层呢？我们可以用32个大小为1x1的过滤器，每个过滤器的大小都是1x1x192维，因为过滤器中信道的数量必须与输入层中信道的数量一致。因此过滤器数量为32，输出层为28x28x32。这就是压缩nc的方法。</p><h2 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h2><p>构建卷积层是，我们需要决定过滤器的大小是3x3还是5x5或者其它大小？或者要不要添加池化层？而我们接下来要讲的Inception就是代替我们来做决定的。虽然网络结构会变得非常复杂，但网络表现得非常好。我们来看一下原理。</p><p>基本思想：不需要人为的决定使用哪个过滤器，或者是否需要池化，而是由网络自行确定这些参数。人们只需给出这些参数的所有可能值，然后把这些输出连起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合。</p><p>举个例子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003102559.png" alt=""></p><p>如果我们直接计算上图，我们的计算成本为28x28x32x5x5x192</p><p>但是我们用1x1卷积后：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003103150.png" alt=""></p><p>我们的计算成本变为28x28x16x192+28x28x32x5x5x16，使用1x1卷积后计算成本是没使用前的1/10。</p><p>下面再举一个例子：</p><p>例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=”SAME”)，输出数据的大小为100x100x256。其中，卷积层的参数为5x5x128x256。假如上一层输出先经过具有32个输出的1x1卷积层，这时得图片输出为100x100x32，接着再经过具有256个输出的5x5卷积层，那么最终的输出数据的大小仍为100x100x256，但卷积参数量已经减少为1x1x128x32 + 5x5x32x256，大约减少了4倍。</p><p>在inception结构中，大量采用了1x1的矩阵，主要是两点作用：1）对数据进行降维；2）引入更多的非线性，提高泛化能力，因为卷积后要经过ReLU激活函数。</p><h3 id="搭建inception网络："><a href="#搭建inception网络：" class="headerlink" title="搭建inception网络："></a>搭建inception网络：</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003111139.png" alt=""></p><p>inception就是将这些模块都组合到一起。</p><h3 id="Inception-network："><a href="#Inception-network：" class="headerlink" title="Inception network："></a>Inception network：</h3><p>下面这个图就是Inception的网络：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003111653.png" alt=""></p><p>看起来很复杂，但我们截取其中一个环节，如上图的红色框。就会发现这不正是我们搭建的inception吗。另外，再一些inception模块前网络会使用最大池化层来修改高和宽的维度。</p><p>其实上图的Inception并不完整，看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003112235.png" alt=""></p><p>多出了红色框住的部分。这些分支是做什么的呢?这些分支是通过隐藏层做出预测。</p><h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>我们在做计算机视觉的应用时，相对于从头训练权重，下载别人训练好的网络结构的权重作为我们预训练，然后转换到感兴趣的任务上。别人的训练过程可能是需要花费好几周，并且需要很多的GPU找最优的过程，这就意味着我们可以下载别人开源的权重参数并把它当做一个很好的初始化，用在我们的神经网络上。这就是迁移学习。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003130518.png" alt=""></p><p>假如我们在做一个识别任务，却没有很多的训练集。我们就可以把别人的网络下载，冻结所有层的参数。我们只需要训练和我们Softmax层有关的参数，然后把别人Softmax改成我们自己Softmax。通过别人的训练的权重，我们可能会得到一个好的结果，即使我们的训练集并不多。</p><p>由于前面的层都冻结了，相当于一个固定函数，不需要改变，因为我们不训练它。</p><p><strong>网络层数越多，需要冻结的层数越少，需要训练的层数就越多。</strong></p><h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><p> 数据扩充也叫数据增强。因为计算机视觉相对于机器学习，数据较少。所以数据增强为了增加数据的数量。下面我们讲一下数据增强的办法：</p><h3 id="垂直镜像对称"><a href="#垂直镜像对称" class="headerlink" title="垂直镜像对称"></a>垂直镜像对称</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003200138.png" alt=""></p><h3 id="随机裁剪"><a href="#随机裁剪" class="headerlink" title="随机裁剪"></a>随机裁剪</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003200328.png" alt=""></p><h3 id="色彩转换"><a href="#色彩转换" class="headerlink" title="色彩转换"></a>色彩转换</h3><p>给RGB三个通道加上不同的失真值</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191003201235.png" alt=""></p><p>这些可以轻易改变图像的颜色，但是对目标的识别还是保持不变的。所以使用这种数据增强方法使得模型对照片的颜色更改更具鲁棒性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;经典网络&quot;&gt;&lt;a href=&quot;#经典网络&quot; class=&quot;headerlink&quot; title=&quot;经典网络&quot;&gt;&lt;/a&gt;经典网络&lt;/h2&gt;&lt;h3 id=&quot;LeNet-5&quot;&gt;&lt;a href=&quot;#LeNet-5&quot; class=&quot;headerlink&quot; title=&quot;LeN
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>CNN详解</title>
    <link href="http://yoursite.com/2019/10/01/CNN%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2019/10/01/CNN详解/</id>
    <published>2019-10-01T09:00:13.000Z</published>
    <updated>2019-10-07T12:03:04.374Z</updated>
    
    <content type="html"><![CDATA[<h2 id="全连接神经网络的局限性"><a href="#全连接神经网络的局限性" class="headerlink" title="全连接神经网络的局限性"></a>全连接神经网络的局限性</h2><p>过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为每张图片都有 3 个颜色通道，它的数据量是 64×64×3=12288。</p><p>现代计算机中，64×64 真的是很小的图片，如果想处理大一点的图片，比如1000×1000，那么最终输入层的数据量是300万，假设我们有一个隐藏层，含有1000个神经元构成的全连接网络，那么数据量将达到 1000*300万，也就是30亿。在这样的情况下很难获得足够的数据防止过拟合，并且需要的内存大小很难得到满足。</p><p>本篇讲解的卷积神经网络（也称作 <strong>ConvNets</strong> 或 <strong>CNN</strong>）不仅可以达到减少参数的作用，而且在图像分类任务中还有其他优于全连接神经网络的特性。</p><h2 id="卷积神经网络概览"><a href="#卷积神经网络概览" class="headerlink" title="卷积神经网络概览"></a>卷积神经网络概览</h2><p>一个图像的相互靠近的局部具有联系性，卷积神经网络的思想就是用不同的神经元去学习图片的局部特征，<strong>每个神经元用来感受局部的图像区域</strong>，如下图不同颜色的圆圈所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001172911.png" alt=""></p><p>然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。</p><p>卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了由<strong>卷积层</strong>和<strong>子采样层</strong>构成的特征抽取器，并且在最后有几个<strong>全连接层</strong>用于对提取的特征进行分类。一个简单的卷积神经网络模型如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173024.png" alt=""></p><p>接下来我们讲解以下这几个层究竟在做的是什么。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层主要是做卷积运算。本文用 “*” 表示卷积操作。</p><h4 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h4><p>假如我们拥有一个6<em>6的灰度图，矩阵如下图表示，在矩阵右边还有一个3\</em>3的矩阵。我们称之为过滤器(filter)或核。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173735.png" alt=""></p><p>卷积的第一步就是将过滤器覆盖在灰度图的左上角的数字就是过滤器对应位置上的数字，如下图蓝色部分：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174317.png" alt=""></p><p>蓝色矩阵每个小个子的左上角的数字就是过滤器对应位置上的数字，将每个格子的两个数字相乘：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174505.png" alt=""></p><p>然后将得到的矩阵所有元素相加，即3+1+2+0+0+0+(-1)+(-8)+(-2)= -5，然后将这个值放到新的矩阵的左上角，最后的新的矩阵是一个4*4的矩阵。<strong>规律：n*n维的矩阵经过一个f*f过滤器后，得到一个（n-f+1）*（n-f+1）维度的新矩阵。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174907.png" alt=""></p><p>我们以1为步长(stride)向右移动一格，再进行类似的计算：</p><p>0 × 1 + 5 × 1 + 7 × 1 + 1 × 0 + 8 × 0 + 2 × 0 + 2 × (-1) + 9 × (-1) + 5 × (-1) = -4 写在刚刚得到的-5的右边。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001174952.png" alt=""></p><p>重复向右移动，直到第一行都计算完成：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175037.png" alt=""></p><p>然后将过滤器向下移动一格，从左边开始继续运算，得到-10，写在新的矩阵第二行第一个位置上：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175208.png" alt=""></p><p>不断的向右、向下移动，直到计算出所有的数据，这样就完成了卷积运算，得出下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001175359.png" alt=""></p><p>下面这张图也可以帮助我们理解卷积运算(黄色的矩阵式过滤器)：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/动态.gif" alt=""></p><p>最终经过卷积得到的图像，我们称之为<strong>特征图(Feature Map)</strong></p><p>对于上述过程，也许你有很多疑问，比如过滤器是什么，为什么是3×3的，作用是什么，滑动的步长stride为什么是1，为什么得到4×4的矩阵，卷积有什么作用等，我们一一解答。</p><h4 id="过滤器-filter"><a href="#过滤器-filter" class="headerlink" title="过滤器(filter)"></a>过滤器(filter)</h4><p>我们来看看下面的例子，本例子中我们用正数表示白色，0表示灰色，负数表示黑色。</p><p>左边 6×6 的矩阵是一个灰度图，左半部分是白色，右半部分是灰色。下面的3×3的过滤器是一种垂直边缘过滤器。</p><p>对二者进行卷积得到新的矩阵，这个矩阵的中间部分数值大于0，显示为白色，而两边为 0， 显示为灰色。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203230.png" alt=""></p><p>上图中三个红色箭头都是我们过滤得到的边缘。在这里，你可能会疑问为什么会有这么大的边缘，那是因为我们的原始图片太小，如果我们把原始灰度图放大一点：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203434.png" alt=""></p><p>再进行卷积，就可以得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203509.png" alt=""></p><p>这样就能大致看出过滤器确实过滤出了垂直边缘。如果想得到水平的边缘，将过滤器转置一下即可。而如果想得到其他角度的边缘，则需要在过滤器中设置不同的数值。下图是水平边缘过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001203557.png" alt=""></p><p>当然很难手动设置过滤器的值去过滤某个特殊角度（比如31°）的边缘，我们通常是<strong>将过滤器中的9个数字当成9个参数</strong>。随机初始化这些参数后，通过神经网络的学习来获得一组值，用于过滤某个角度的边缘。</p><p>目前主流网络的过滤器的大小一般是1x1、3x3、5x5等，数值一般不会太大，因为我们希望这些过滤器可以获得一些更细微的边缘特征。另外，一般将长和宽都取奇数，具体原因我们讲到Padding的时候再解释。</p><h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>Padding是填充。什么是填充呢？</p><p>在前面，我们的例子一个6*6的矩阵经过一个3*3的过滤器后，得到一个4*4的新矩阵。但是这样的话会有两个缺点，第一个缺点就是我们每次做卷积运算的时候，我们的图像都会缩小。可能我们在做完几次卷积之后，我们的图像就会变得很小。第二个缺点是角落边的像素点只会被使用一次，像下图的绿色。但是如果是中间的像素点(类似图中的红色框)，就会被使用很多次。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001210310.png" alt=""></p><p>所以，那些边缘或者角落的像素点在输出时使用较少。这意味者，我们将丢掉图片边缘区域。如果这里有重要信息，那我们就得不偿失了。</p><p>那我们该怎么解决这个问题呢？这时候就用到Padding了。我们可以在卷积操作之前填充这个图像。我们可以沿着图像的边缘再填充一层像素(通常为0，因为用0填充不影响原来的图片特征)。这样做的话，我们的原始图片像素就有6<em>6变成了8\</em>8。接着我们对这幅8*8的图像进行卷积，得到的新矩阵就不是4*4的，而是6*6的。<strong>规律：n*n维的矩阵填充了P个像素点后再经过一个f*f过滤器后，得到一个（n+2p-f+1）*（n+2p-f+1）维度的新矩阵。</strong></p><p>由上面这个规律：我们得出p=(f-1)/2，为了使p是对称填充，所以我们的f通常都是奇数。还有一个原因，是因为当我们有一个奇数的过滤器，我们会有一个中间像素点。这就是为什么前面我们说过滤器通常都是奇数维度。</p><p>那我们Padding填充的时候到底选择多少呢？1还是2还是更大的数字？</p><p>其实Padding有两个选择：一个Valid，一个是Same。</p><p>Valid：意味不填充。也就是之前的6<em>6矩阵经过一个3\</em>3的矩阵后，得出一个4*4的矩阵。</p><p>Same：意味我们的图片输出大小和输入大小是一样的。也就是上面的6<em>6矩阵在Padding=1之后经过一个3\</em>3的矩阵后，得出一个6*6的矩阵。图片输出大小和输入大小一样。</p><h4 id="卷积步长"><a href="#卷积步长" class="headerlink" title="卷积步长"></a>卷积步长</h4><p>我们先用 3x3 的过滤器以2为步长卷积 7x7 的图像，看看会得到什么。</p><p>第一步，对左上角的区域进行计算：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214116.png" alt=""></p><p>第二步，向右移动，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214201.png" alt=""></p><p>再继续向右向下移动过滤器，就可以得到下面的结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001214424.png" alt=""></p><p><strong>规律：我们用一个f<em>f的过滤器卷积一个n\</em>n的图像，Padding=p，步长为s，我们得到一个(n+2p-f)/s+1*(n+2p-f)/s+1</strong></p><p>最后一个问题：如果我们的商不是整数呢？这时我们采用向下求整的策略。</p><h4 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h4><p>上面的讲解你已经知道如何对灰度图进行卷积运算了，现在看看如何在有三个通道(RGB)的图像上进行卷积。</p><p>假设彩色图像尺寸 6×6×3，这里的 3 指的是三个颜色通道。<strong>我们约定图像的通道数必须和过滤器的通道数匹配</strong>，所以需要增加过滤器的个数到3个。将他们叠在一起，进行卷积操作。6*6*3的图像经过一个3<em>3\</em>3过滤器得出一个新的4*4*1图像。</p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223726.png" alt=""></p><p>具体过程就是，首先，把这个 3×3×3 的过滤器先放到最左上角的位置，依次取原图和过滤器对应位置上的各27 个数相乘后求和，得到的数值放到新矩阵的左上角。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223830.png" alt=""></p><p><strong>注意，由于我们是用三维的过滤器对三维的原图进行卷积操作，所以最终得到的矩阵只有一维。</strong></p><p>然后不断向右向下进行类似操作，直到“遍历”完整个原图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001223939.png" alt=""></p><p>至于效果是怎么样的。结合之前的知识，我们假设了一组垂直边界过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002081928.png" alt=""></p><p>它可以完成RGB三个通道上垂直边缘的检测工作。这也解释了为什么过滤器使用3个通道：也就是说如果你想获得RGB三个通道上的特征，那么你使用的过滤器也得是3个通道的。</p><p>如果你除了垂直边界，还想要检测更多的边界怎么办？可以使用更多 3维 甚至更多的过滤器，如下图框住的部分，我们增加了一组过滤器：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002082118.png" alt=""></p><p>这样，两组过滤器就得到了两组边界，我们一般会将卷积后得到的图叠加在一起，得到上图左边的 4x4x2 的图像。如果你还想得到更多的边界特征，使用更多的3维的过滤器即可。</p><p>现在我们对维度进行总结：</p><blockquote><p>首先我们假设我们的没有padding且步长为1，这时候使用 n × n × nc 原图和 nc’ 个 f × f × fc 的过滤器进行卷积，得到 (n-f+1) × (n-f+1) × nc’ 的图像</p></blockquote><p>nc 是 原图的通道数(3)，过滤器的通道数和原图的通道数必须一样，都是 nc。</p><p>nc’ 是最终得到的图像的通道数(在上面的例子为2)，由使用的过滤器个数决定(而 nc’ 又是下一层的输入图像的通道数)</p><h4 id="卷积层全貌"><a href="#卷积层全貌" class="headerlink" title="卷积层全貌"></a>卷积层全貌</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002083956.png" alt=""></p><p>对于卷积层，我们同样需要进行以下操作，这也是前向传播的操作：</p><blockquote><p>z^[1] = w^[1] * a^[0]+b^[1]</p><p>a^[1]=g(z^[1])</p></blockquote><p>上面所用到的变量解释：</p><blockquote><p>a^[0] 是我们原图的数据 X ，也就是上图 6x6x3 的RGB图。</p><p>w^[1] 是这一层的权重矩阵，由 2 个 3x3x3 的过滤器组成。</p><p>b^[1] 是第一层的偏置项，不同的过滤器对应不同的偏置值</p><p>g() 是激活函数，本例子中使用 ReLu</p></blockquote><p>卷积层的工作说明：</p><p>式子中 w^[1] <em> a^[0] 是代表进行<strong>卷积运算</strong>得到两个特征图(也就是2个4x4的矩阵)，如下图的绿色框所示，接着再对两个特征图<em>*加上不同的偏置</em></em>就得到了z^[1]，如下图红色框所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002084726.png" alt=""></p><p>进一步<strong>应用激活函数</strong>就得到了两个处理过的图像，将他们叠加在一起，就得到了这一层的输出 a^[1](上图右下角)。</p><p><strong>注意</strong>：上图中的加偏置(b1,b2)均使用了python的广播</p><p>简单来说，卷积层的工作就是：<strong>将输入的数据a^[l - 1]进行卷积操作，加上偏置，再经过激活函数非线性化处理得到a^[l]</strong>。到这里卷积层的任务就完成了。</p><h4 id="为什么要使用卷积？"><a href="#为什么要使用卷积？" class="headerlink" title="为什么要使用卷积？"></a>为什么要使用卷积？</h4><p>卷积层的主要优势在稀疏连接就和权值共享</p><h5 id="稀疏连接"><a href="#稀疏连接" class="headerlink" title="稀疏连接"></a>稀疏连接</h5><p>假设我们输入的是32×32×3的RGB图像。</p><p>如果是<strong>全连接网络</strong>，对于隐藏层的某个神经元，它不得不和前一层的所有输入（32×32×3）都保持连接。</p><p>但是，对<strong>卷积神经网络</strong>而言，神经元只和输入图像的局部相连接，如下图圆圈所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002115607.png" alt=""></p><p>这个局部连接的区域，我们称之为“感受野”，大小等同于过滤器的大小(3*3*3)。</p><p>看下面的图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002121345.png" alt=""></p><p>右边的绿色或者红色的输出单元仅仅与36个输入特征中的九个相连接，其它像素值对输出不会产生任何影响。</p><p>相比之下，卷积神经网络的神经元与图像的连接要稀疏的多，我们称之为<strong>稀疏连接</strong>。</p><h5 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h5><p>权值共享其实就是过滤器共享。特征检测如垂直边缘检测过滤器如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，如果我们用垂直过滤器扫描整张图片，就可以得到整张图片的所有垂直边缘，而换做水平过滤器就可以扫描出图片的水平边缘。</p><p>在CNN中，我们<strong>用不同的神经元(过滤器)去学习整张图片的不同特征</strong>，而不是利用不同的神经元 学习图片不同的局部特征。因为图像的不同部分也可能有相同的特征。每个特征检测过滤器都可以在输入图片的不同区域中使用同样的参数(即方格内的9个数字)，以便提取垂直边缘或其它特征。右边两张图每个位置是被<strong>同样</strong>的过滤器扫描而得的，所以<strong>权重</strong>是一样的，也就是<strong>共享</strong>。假设有100个神经元，全连接神经网络就需要32×32×3×100=307200个参数。</p><p>因为共享了权值，提取一个特征只需要 f×f 个参数，上图右边两张图的每个像素只与使用 3x3 的过滤器相关，顶多再加上一个偏置项。在本例子中，得到一个feature map 只用到了 3*3+1=10 个参数。如果想得到100个特征，也不过是用去1000个参数。这就解释了为什么利用卷积神经网络可以减少参数个数了。</p><p>当你对提取到的 水平垂直或者其他角度 的特征，再进行卷积操作，就可以得到更复杂的特征，比如一个曲线。如果对得到的曲线再进行卷积操作，又能得到更高维度的特征，比如一个车轮，如此往复，最终可以提取到整个图像全局的特征。</p><p>到这里卷积层的内容就结束了。</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>对于一个32x32像素的图像，假设我们使用400个3x3的过滤器提取特征，每一个过滤器和图像卷积都会得到含有 (32-3 + 1) × (32 - 3 + 1) = 900 个特征的feature map，由于有 400 个过滤器，所以每个样例 (example) 都会得到 900 × 400 = 360000 个特征。而如果是96x96的图像，使用400个8x8的过滤器提取特征，最终得到一个样本的特征个数为 3168400。对于如此大的输入，训练难度大，也容易发生过拟合。</p><p>为了减少下一个卷积层的输入，引入了采样层(也叫池化层)，采样操作分为最大值采样(Max pooling)，平均值采样(Mean pooling)。</p><p><strong>最大值采样</strong></p><p>举个例子，我们用3x3的过滤器以1为步长去扫描图像，每次将重叠的部分的最大值提取出来，作为这部分的特征，过程如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093125.png" alt=""></p><p>我们也可以使用2x2的过滤器。以2为步长进行特征值的提取。最终得到的图像长度和宽度都缩小一半：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093418.png" alt=""></p><p><strong>平均值采样</strong></p><p>与最大值采样不同的是 将覆盖部分特征的平均值作为特征，其他过程都一样。经过采样处理，可以提取更为抽象的特征，减少数据处理量的同时保留有效信息。</p><p>下图是过滤器为2x2，步长为2进行特征值的提取：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002093447.png" alt=""></p><p><strong>注意</strong>：池化层没有需要学习的参数。且padding大多数时候为0</p><h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>在经过几次卷积和池化的操作之后，卷积神经网络的最后一步是全连接层，全连接层的个数可能不止一个：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191001173024.png" alt=""></p><p><strong>最后一个采样层到全连接层的过渡是通过卷积来实现的</strong>，比如前层输出为 3x3x5 的feature map，那我们就用 3x3x5 的过滤器对前层的这些feature map进行卷积，于是得到了一个神经元的值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191002101140.png" alt=""></p><p>全连接层有1024个神经元，那我们就用1024个3x3x5 的过滤器进行卷积即可。</p><p><strong>第一个全连接层到第二个全连接层的过渡也是通过卷积实现的</strong>，若前层有1024个神经元，这层有512个，那可以看做前层有1024x1x1个feature map，然后用1x1的卷积核对这些feature map进行卷积，则得到100x1x1个feature map。</p><p>卷积神经网络要做的事情，大致分为两步：</p><ol><li>卷积层、采样层负责提取特征</li><li>全连接层用于对提取到的特性进行分类</li></ol><p>最后一步就和普通的神经网络没有什么区别，充当一个分类器的作用，输入是不同特征的特征值，输出是分类。在全连接层后，就可以根据实际情况用softmax对图片进行分类了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;全连接神经网络的局限性&quot;&gt;&lt;a href=&quot;#全连接神经网络的局限性&quot; class=&quot;headerlink&quot; title=&quot;全连接神经网络的局限性&quot;&gt;&lt;/a&gt;全连接神经网络的局限性&lt;/h2&gt;&lt;p&gt;过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="卷积神经网络" scheme="http://yoursite.com/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深层神经网络</title>
    <link href="http://yoursite.com/2019/09/29/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/29/深层神经网络/</id>
    <published>2019-09-29T13:11:55.000Z</published>
    <updated>2019-10-21T03:41:27.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="深层神经网络"><a href="#深层神经网络" class="headerlink" title="深层神经网络"></a>深层神经网络</h2><p>深层神经网络样子也许是下面这样的，也有可能是更多层的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190930110158.png" alt=""></p><p>上图是一个四层的神经网络(输入层一般记为零层或不记住)，我们用L表示神经网络的层数，n^[l] 表示第 l 层的神经元数量</p><p>由于神经网络具有许多层，在下图中用方框代表一层，每个层都要完成各自的任务，流程图大致如下所示：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward.png" alt=""></p><p>蓝框中的部分完成正向传播：</p><blockquote><p>该过程的输入是 X 也就是 A<sup>[0]</sup>，一层一层向后计算，最后得到A<sup>[L]</sup>。</p><p>并且，在各层 l 计算出 A^[l] 的同时，<strong>缓存</strong>各层的输出值 A^[l] 到变量 cache 中，因为反向传播将会用上。<strong>图中这里写错了，应该是A<sup>[1]</sup>、A<sup>[2]</sup>、A<sup>[3]</sup>的。</strong></p></blockquote><p>绿框中的部分完成反向传播：</p><blockquote><p>该过程的输入是dA<sup>[L]</sup>，并根据dA<sup>[L]</sup>一步步向前计算，得到各层对应的dZ<sup>[l]</sup>，dW<sup>[l]</sup>，db<sup>[l]</sup></p><p>此外，还要计算出 dA<sup>[l - 1]</sup> 作为前一层的输入</p></blockquote><p>具体过程请看<a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的正向传播/#more">浅层神经网络的正向传播</a>) 和 <a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的反向传播/#more">浅层神经网络的反向传播</a>)</p><h2 id="核对矩阵的维数"><a href="#核对矩阵的维数" class="headerlink" title="核对矩阵的维数"></a>核对矩阵的维数</h2><p>为了在写代码的过程中减少出现bug，尤其是在进行反向传播的时候，我们要注意核对矩阵的维数，这个知识点我们前面将 <a href="[https://brickexperts.github.io/2019/09/23/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#more](https://brickexperts.github.io/2019/09/23/浅层神经网络的反向传播/#more">浅层神经网络的反向传播</a>)的文章中也提到了，大致规律如下：</p><blockquote><p>W^[l] ： (n<sup>[ l ]</sup>, n<sup>[l - 1]</sup>)</p><p>Z^[l] ： (n<sup>[ l ]</sup>, m),</p><p>A^[l] ： (n<sup>[ l ]</sup>, m)</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/forward4.png" alt=""></p><p>另外，还有一个规律：</p><blockquote><p>对任意层而言：</p><p>dW 的维度和 W 的维度相同</p><p>dZ 的维度和 Z 的维度相同</p><p>dA 的维度和 A 的维度相同</p></blockquote><h2 id="为什么要使用深度神经网络？"><a href="#为什么要使用深度神经网络？" class="headerlink" title="为什么要使用深度神经网络？"></a>为什么要使用深度神经网络？</h2><p>我们都知道深度神经网络可以解决很多问题，网络并不一定要很大，但一定要有深度，需要比较多的隐藏层。这到底为什么呢？</p><p>如果我们在建立一个人脸识别或者检测系统，当我们输入一张脸部的图片时，我们可以把深度神经网络第一层当成一个特征探测器或者边缘检测器。在这个例子中，我们建立一个大概有20个隐藏单元的深度神经网络。看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190930164522.png" alt=""></p><p>隐藏单元就是这个图里这些小方块。每个小方块都是一个隐藏单元，它会去寻找一张图的边缘方向。它可能在水平方向找、也可能在竖直方向找。这个东西在卷积神经网络我们会细讲(别问，问就是当时不会)。这里就告诉你们我们可以先把神经网络的第一层当作看图，然后去找这张图片的各个边缘。接着我们把组成图片边缘的像素放在一起看。它可以把探测到的边缘组合成面部的不同部分。比如说，可能有一个神经元回去找眼睛的部分，另外还有找鼻子或其它的部分。然后把这许多边缘结合在一起，就可以开始检测人脸的不同部分，最后再把这些部分放在一起，就可以识别或检测不同的人脸。我们把前几层的神经网络当作简单的检测函数，例如：边缘检测等，之后把它们跟后几层结合在一起。注意：边缘探测器其实相对来说都是针对图片中非常小块的面积。</p><p>具体原因移步网易云课堂的吴恩达的深度学习。<a href="https://mooc.study.163.com/learn/2001281002?tid=2001392029&amp;_trace_c_p_k2_=613c87c1215241179940b8e203431a3d#/learn/content?type=detail&amp;id=2001701022&amp;cid=2001694285" target="_blank" rel="noopener">链接</a></p><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>在学习算法中还有其它参数，需要输入到学习算法中，比如学习率&alpha;、隐藏层的数量、使用的激活函数种类等都是超参数。因为它们影响着最终W和b的值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;深层神经网络&quot;&gt;&lt;a href=&quot;#深层神经网络&quot; class=&quot;headerlink&quot; title=&quot;深层神经网络&quot;&gt;&lt;/a&gt;深层神经网络&lt;/h2&gt;&lt;p&gt;深层神经网络样子也许是下面这样的，也有可能是更多层的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://r
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>上传项目到GitHub</title>
    <link href="http://yoursite.com/2019/09/28/%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E5%88%B0GitHub/"/>
    <id>http://yoursite.com/2019/09/28/上传项目到GitHub/</id>
    <published>2019-09-28T10:05:42.000Z</published>
    <updated>2019-10-21T08:14:30.290Z</updated>
    
    <content type="html"><![CDATA[<p>首先登录GitHub，没有账号的申请一个。很简单，跳过。</p><p>新建一个仓库：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193118.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193719.png" alt=""></p><p>记住这个网址，之后用到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928194701.png" alt=""></p><p>进入项目的目录，点击空白处，选择git Bash。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192144.png" alt=""></p><p>输入git init，会发现当前目录下多了一个.git文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928191933.png" alt=""></p><p>接着输入git add .   这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把.换成这个特定的文件名即可。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192510.png" alt=""></p><p>输入git commit -m “first commit”，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928192735.png" alt=""></p><p>输入git remote add origin 自己的仓库url地址（上面有说到） 将本地的仓库关联到github上，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928194818.png" alt=""></p><p>输入git push -u origin master，这是把代码上传到github仓库的意思</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928195326.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;首先登录GitHub，没有账号的申请一个。很简单，跳过。&lt;/p&gt;
&lt;p&gt;新建一个仓库：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928193
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Github的使用</title>
    <link href="http://yoursite.com/2019/09/28/Github%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/09/28/Github的使用/</id>
    <published>2019-09-28T08:10:43.000Z</published>
    <updated>2019-09-28T10:01:01.820Z</updated>
    
    <content type="html"><![CDATA[<p>github的账户创建和仓库创建比较简单，就不赘述了</p><h2 id="github添加ssh账户"><a href="#github添加ssh账户" class="headerlink" title="github添加ssh账户"></a>github添加ssh账户</h2><p>首先，点击账户，选择setting</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928164421.png" alt=""></p><p>直接添加就完事了。那我们怎么生成ssh公钥呢？</p><p>先回到用户的主目录下，编辑文件.gitconfig，修改某台机器的git配置。修改为注册github的邮箱，填写用户名</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928165020.png" alt=""></p><p>接着执行命令，ssh-keygen -t rsa -C “邮箱地址”，一路yes。最后生成三个文件：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928165739.png" alt=""></p><p>id_rsa是机器私钥，自己保留。id_rsa.pub就是我们的公钥。把公钥内容复制到第一张图New SSH Key后的位置。名字可以随便取。</p><h2 id="克隆项目"><a href="#克隆项目" class="headerlink" title="克隆项目"></a>克隆项目</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928170507.png" alt=""></p><p>接着git clone SSH</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928172714.png" alt=""></p><p>这个再cmd下执行同样有效。</p><p>如果出错了，执行以下代码：先执行eval “$(ssh-agent -s)”，再执行ssh-add</p><p>不只是可以使用SSH，也可以使用HTTPS。</p><p>git  clone  网址         </p><p>在cmd上同样是有效的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928172950.png" alt=""></p><h2 id="推送代码"><a href="#推送代码" class="headerlink" title="推送代码"></a>推送代码</h2><p>推送前：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928174030.png" alt=""></p><p>图中的红框是分支。</p><p>推送分支就是把给分支上的所有本地提交库推送到远程库，推送时要指定本地分支，这样，git就会把该分支推送到远程库对应的远程分支上。</p><p>git push origin 分支名称</p><h2 id="本地分支跟踪远程分支"><a href="#本地分支跟踪远程分支" class="headerlink" title="本地分支跟踪远程分支"></a>本地分支跟踪远程分支</h2><p>git branch —set-upstream-to=origin/远程分支名词  本地分支名称</p><p>跟踪后，如果本地分支和远程分支的进度不一样，使用命令 git status 会提醒。</p><h2 id="拉取代码"><a href="#拉取代码" class="headerlink" title="拉取代码"></a>拉取代码</h2><p>git pull orgin 分支名称</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;github的账户创建和仓库创建比较简单，就不赘述了&lt;/p&gt;
&lt;h2 id=&quot;github添加ssh账户&quot;&gt;&lt;a href=&quot;#github添加ssh账户&quot; class=&quot;headerlink&quot; title=&quot;github添加ssh账户&quot;&gt;&lt;/a&gt;github添加ssh账户
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="Github" scheme="http://yoursite.com/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>git的分支管理</title>
    <link href="http://yoursite.com/2019/09/27/git%E7%9A%84%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/"/>
    <id>http://yoursite.com/2019/09/27/git的分支管理/</id>
    <published>2019-09-27T10:01:21.000Z</published>
    <updated>2019-09-28T08:11:44.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分支原理"><a href="#分支原理" class="headerlink" title="分支原理"></a>分支原理</h2><p>git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。HEAD严格来说不是指向提交，而是指向master。master才是指向提交的版本，所以，HEAD指向的就是当前分支。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927191539.png" alt=""></p><p>每次提交，master分支都会向前移动一步。这样，随着不断提交，master分支的线也越来越长。</p><p>当我们创建新的分支dev，git创建了一个指针叫dev。指向master相同的提交。再把HEAD指向dev，就表示当前分支在dev上。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/20190927191917.png" alt=""></p><p>git创建一个分支很快，因为除了增加一个dev指针，改变HEAD的指向，工作区的文件没有任何变化。</p><p>假如我们在dev上的工作完成了，就可以把dev合并到master上。怎么合并的呢？git直接把master指向dev的当前提交，就完成了合并。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928102253.png" alt=""></p><p>合并分支也很快，就改改指针。工作区的内容不变。</p><p>合并完分支后，甚至可以删除dev分支，删除dev分支就是把dev指针给删掉。删掉后，我们只剩下一条master指针。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928102344.png" alt=""></p><h2 id="分支作用"><a href="#分支作用" class="headerlink" title="分支作用"></a>分支作用</h2><p>分支在实际中有很大用处。假设你准备开发一个新功能，但是需要两周才能完成，第一周只写了一半的代码。如果立刻提交，由于代码还没写完，不完整的代码库会导致别人无法干活，如果等代码全部写完，又存在丢失每天进度的巨大风险。有了分支，就不用怕这些事情。你创建自己的一个分支，别人看不到，也可以在原来的分支上工作。而你还在自己的分支上干活，想提交就提交。直到全部开发完，一次性合并到<strong>主分支</strong>。这样既安全，又不影响别人工作。</p><h2 id="分支基本操作"><a href="#分支基本操作" class="headerlink" title="分支基本操作"></a>分支基本操作</h2><h3 id="查看当前几个分支且能看到在哪个分支工作"><a href="#查看当前几个分支且能看到在哪个分支工作" class="headerlink" title="查看当前几个分支且能看到在哪个分支工作"></a>查看当前几个分支且能看到在哪个分支工作</h3><p>git branch</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928103803.png" alt=""></p><h3 id="创建分支"><a href="#创建分支" class="headerlink" title="创建分支"></a>创建分支</h3><p>git branch 分支名</p><h3 id="创建分支并切换到其上工作"><a href="#创建分支并切换到其上工作" class="headerlink" title="创建分支并切换到其上工作"></a>创建分支并切换到其上工作</h3><p>git checkout -b dev</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928104927.png" alt=""></p><h3 id="切换回master分支"><a href="#切换回master分支" class="headerlink" title="切换回master分支"></a>切换回master分支</h3><p>git checkoout master</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928110538.png" alt=""></p><h3 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h3><p>git merge 分支名字</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928114835.png" alt=""></p><p>注意上面的Fast-forward，也就是红色框。git告诉我们，这次合并是快速合并，也就是直接把master指向dev的当前提交，所以合并速度非常快</p><h4 id="解决冲突"><a href="#解决冲突" class="headerlink" title="解决冲突"></a>解决冲突</h4><p>合并冲突并不是一番风顺的。在两个分支上修改同一个文件并提交，就会起冲突。解决办法：手动解决冲突，再提交一次</p><p>看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928130118.png" alt=""></p><p>git告诉我们，git_test2.txt文件存在冲突，必须手动解决冲突再提交。</p><p>打开刚才修改的文件，发现文件修改了</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928131316.png" alt=""></p><p>将文件中新增加的&lt;、= 和 &gt;手动删掉，再提交一次</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928131626.png" alt=""></p><h3 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h3><p>git branch -d 分支名字</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928115530.png" alt=""></p><p>这个操作也是非常快的，直接把dev这个指针删了就好了。</p><h2 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h2><p>通常，合并分支时，如果可能，git会用fast forward模式。但是有些快速合并并不能合并但不会起冲突，合并之后并做一次新的提交。在这种模式下，删除分支后，会丢掉部分信息。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928141314.png" alt=""></p><p>如上图，merge并不会起冲突(因为不是同一个文件)。但是，会出来一个框：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928142350.png" alt=""></p><p>为什么会出现这个框？前面我们说了，合并分支无法合并但不会起冲突且做一次提交。在这次提交中，需要输入描述信息。就在弹窗输入描述信息</p><p><strong>PS：我太难了，我接下来一直退不出这个框。所以这个例子就这样吧。可以的跟我说一下，拜托了。</strong></p><p>禁用快速合并：</p><p>git merge —no-ff  -m  “描述”  分支名</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928152949.png" alt=""></p><h2 id="Bug分支"><a href="#Bug分支" class="headerlink" title="Bug分支"></a>Bug分支</h2><p>软件开发中，bug就像家常便饭，有了bug就要修复。在git中，由于分支是如此强大，所以，每个bug都可以通过一个新的临时分支来修复。修复后，合并分支，然后将临时分支删除。这个例子不难，就不贴图了。</p><p>假如你正在写代码，突然老大让你修改一个bug。你需要先保存一下工作现场，修复完bug后，再恢复工作现场。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20190928154851.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分支原理&quot;&gt;&lt;a href=&quot;#分支原理&quot; class=&quot;headerlink&quot; title=&quot;分支原理&quot;&gt;&lt;/a&gt;分支原理&lt;/h2&gt;&lt;p&gt;git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。H
      
    
    </summary>
    
      <category term="git和github" scheme="http://yoursite.com/categories/git%E5%92%8Cgithub/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
</feed>
