<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-16T10:54:44.743Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SVM解读(2)</title>
    <link href="http://yoursite.com/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/"/>
    <id>http://yoursite.com/2019/09/15/SVM解读(2)/</id>
    <published>2019-09-15T05:47:15.000Z</published>
    <updated>2019-09-16T10:54:44.743Z</updated>
    
    <content type="html"><![CDATA[<h2 id="非线性SVM与核函数"><a href="#非线性SVM与核函数" class="headerlink" title="非线性SVM与核函数"></a>非线性SVM与核函数</h2><h3 id="SVC在非线性数据上的推广"><a href="#SVC在非线性数据上的推广" class="headerlink" title="SVC在非线性数据上的推广"></a>SVC在非线性数据上的推广</h3><p>为了能够找出非线性数据的线性决策边界，我们需要将数据从原始的空间x投射到新空间Φ(x)中，Φ是一个映射函数，它代表了某种非线性的变换，如同我们之前所做过的使用r来升维一样，这种非线性变换看起来是一种非常有效的方式。使用这种变换，线性SVM的原理可以被很容易推广到非线性情况下，其推到过程核逻辑都与线性SVM一摸一样，只不过在第一决策边界之前，我们必须先对数据进行升维，即将原始的x转换成Φ。</p><h3 id="重要参数kernel"><a href="#重要参数kernel" class="headerlink" title="重要参数kernel"></a>重要参数kernel</h3><p>这种变换非常巧妙，但也带有一些实现问题。首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。其次，已知适当的映射函数，我们想要计算类似于Φ(x)<sub>i</sub>* Φ(x<sub>test</sub>)这样的点积，计算量无法巨大。要找出超平面所付出的代价是非常昂贵的。</p><p><strong>关键概念</strong>：核函数</p><p>而解决上面这些问题的数学方式，叫做”核技巧”。是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。具体体现为，K(u,v)= Φ(u)*Φ(v)。而这个原始空间中的点积函数K(u,v)，就被叫做“核函数”。</p><p>核函数能够帮助我们解决三个问题：</p><p>第一，有了核函数之后，我们无需去担心$\Phi$究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数(positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量的点积一定可以被低维空间中的这两个向量的某种计算来表示。</p><p>第二，使用核函数计算低维度中的向量关系比计算原来的Φ(x<sub>i</sub>) * Φ(x<sub>test</sub>)要简单太多了。</p><p>第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。 </p><p>选用不同的核函数，就可以解决不同数据分布下的寻找超平面问题。在SVC中，这个功能由参数“kernel”和一系列与核函数相关的参数来进行控制。之前的代码中我们一直使用这个参数并输入”linear”，但却没有给大家详细讲解，也是因为如果不先理解核函数本身，很难说明这个参数到底在做什么。参数“kernel”在sklearn中可选以下几种选项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt></p><p>可以看出，除了选项”linear”之外，其他核函数都可以处理非线性问题。多项式核函数有次数d，当d为1的时候它就是再处理线性问题，当d为更高次项的时候它就是在处理非线性问题。我们之前画图时使用的是选项“linear”，自然不能处理环形数据这样非线性的状况。而刚才我们使用的计算r的方法，其实是高斯径向基核函数所对应的功能，在参数”kernel“中输入”rbf“就可以使用这种核函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line">X,y = make_circles(<span class="number">100</span>, factor=<span class="number">0.1</span>, noise=<span class="number">.1</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(model,ax=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ax = plt.gca()</span><br><span class="line">    xlim = ax.get_xlim()</span><br><span class="line">    ylim = ax.get_ylim()</span><br><span class="line">    x = np.linspace(xlim[<span class="number">0</span>],xlim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    y = np.linspace(ylim[<span class="number">0</span>],ylim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    Y,X = np.meshgrid(y,x)</span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    ax.contour(X, Y, P,colors=<span class="string">"k"</span>,levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>],alpha=<span class="number">0.5</span>,linestyles=[<span class="string">"--"</span>,<span class="string">"-"</span>,<span class="string">"--"</span>])</span><br><span class="line">    ax.set_xlim(xlim)</span><br><span class="line">    ax.set_ylim(ylim)</span><br><span class="line">    plt.show()</span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">r = np.exp(-(X**<span class="number">2</span>).sum(<span class="number">1</span>))</span><br><span class="line">rlim = np.linspace(min(r),max(r),<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3D</span><span class="params">(elev=<span class="number">30</span>,azim=<span class="number">30</span>,X=X,y=y)</span>:</span></span><br><span class="line">    ax = plt.subplot(projection=<span class="string">"3d"</span>)</span><br><span class="line">    ax.scatter3D(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],r,c=y,s=<span class="number">50</span>,cmap=<span class="string">'rainbow'</span>)</span><br><span class="line">    ax.view_init(elev=elev,azim=azim)</span><br><span class="line">    ax.set_xlabel(<span class="string">"x"</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">"y"</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">"r"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_3D()</span><br><span class="line">clf = SVC(kernel = <span class="string">"rbf"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plot_svc_decision_function(clf)</span><br></pre></td></tr></table></figure><p>运行后，从效果图可以看到，决策边界被完美的找了出来。</p><h3 id="探索核函数在不同数据集上的表现"><a href="#探索核函数在不同数据集上的表现" class="headerlink" title="探索核函数在不同数据集上的表现"></a>探索核函数在不同数据集上的表现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles, make_moons, make_blobs,make_classification</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">datasets = [</span><br><span class="line">make_moons(n_samples=n_samples, noise=<span class="number">0.2</span>, random_state=<span class="number">0</span>),</span><br><span class="line">make_circles(n_samples=n_samples, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">1</span>),</span><br><span class="line">make_blobs(n_samples=n_samples, centers=<span class="number">2</span>, random_state=<span class="number">5</span>),</span><br><span class="line">make_classification(n_samples=n_samples,n_features =</span><br><span class="line"><span class="number">2</span>,n_informative=<span class="number">2</span>,n_redundant=<span class="number">0</span>, random_state=<span class="number">5</span>)</span><br><span class="line">]</span><br><span class="line">Kernel = [<span class="string">"linear"</span>,<span class="string">"poly"</span>,<span class="string">"rbf"</span>,<span class="string">"sigmoid"</span>]</span><br><span class="line"><span class="comment">#四个数据集分别是什么样子呢？</span></span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> datasets:</span><br><span class="line">    plt.figure(figsize=(<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">nrows=len(datasets)</span><br><span class="line">ncols=len(Kernel) + <span class="number">1</span></span><br><span class="line">fig, axes = plt.subplots(nrows, ncols,figsize=(<span class="number">20</span>,<span class="number">16</span>))</span><br><span class="line"><span class="keyword">for</span> ds_cnt, (X,Y) <span class="keyword">in</span> enumerate(datasets):</span><br><span class="line"><span class="comment">#在图像中的第一列，放置原数据的分布</span></span><br><span class="line">    ax = axes[ds_cnt, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">        ax.set_title(<span class="string">"Input data"</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line">    <span class="comment">#第二层循环：在不同的核函数中循环</span></span><br><span class="line">    <span class="comment">#从图像的第二列开始，一个个填充分类结果</span></span><br><span class="line">    <span class="keyword">for</span> est_idx, kernel <span class="keyword">in</span> enumerate(Kernel):</span><br><span class="line">    <span class="comment">#定义子图位置</span></span><br><span class="line">        ax = axes[ds_cnt, est_idx + <span class="number">1</span>]</span><br><span class="line">        <span class="comment">#建模</span></span><br><span class="line">        clf = svm.SVC(kernel=kernel, gamma=<span class="number">2</span>).fit(X, Y)</span><br><span class="line">        score = clf.score(X, Y)</span><br><span class="line">        <span class="comment">#绘制图像本身分布的散点图</span></span><br><span class="line">        ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y</span><br><span class="line">        ,zorder=<span class="number">10</span></span><br><span class="line">        ,cmap=plt.cm.Paired,edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制支持向量</span></span><br><span class="line">        ax.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>], s=<span class="number">50</span>,</span><br><span class="line">        facecolors=<span class="string">'none'</span>, zorder=<span class="number">10</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">        <span class="comment">#绘制决策边界</span></span><br><span class="line">        x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">        y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">        <span class="comment">#np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法</span></span><br><span class="line">        <span class="comment">#一次性使用最大值和最小值来生成网格</span></span><br><span class="line">        <span class="comment">#表示为[起始值：结束值：步长]</span></span><br><span class="line">        <span class="comment">#如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内</span></span><br><span class="line">        XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">        <span class="comment">#np.c_，类似于np.vstack的功能</span></span><br><span class="line">        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape)</span><br><span class="line">        <span class="comment">#填充等高线不同区域的颜色</span></span><br><span class="line">        ax.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">        <span class="comment">#绘制等高线</span></span><br><span class="line">        ax.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>], linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>],</span><br><span class="line">        levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment">#设定坐标轴为不显示</span></span><br><span class="line">        ax.set_xticks(())</span><br><span class="line">        ax.set_yticks(())</span><br><span class="line">        <span class="comment">#将标题放在第一行的顶上</span></span><br><span class="line">        <span class="keyword">if</span> ds_cnt == <span class="number">0</span>:</span><br><span class="line">            ax.set_title(kernel)</span><br><span class="line">        <span class="comment">#为每张图添加分类的分数</span></span><br><span class="line">        ax.text(<span class="number">0.95</span>, <span class="number">0.06</span>, (<span class="string">'%.2f'</span> % score).lstrip(<span class="string">'0'</span>)</span><br><span class="line">        , size=<span class="number">15</span></span><br><span class="line">        , bbox=dict(boxstyle=<span class="string">'round'</span>, alpha=<span class="number">0.8</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">        <span class="comment">#为分数添加一个白色的格子作为底色</span></span><br><span class="line">        , transform=ax.transAxes <span class="comment">#确定文字所对应的坐标轴，就是ax子图的坐标轴本身</span></span><br><span class="line">        , horizontalalignment=<span class="string">'right'</span> <span class="comment">#位于坐标轴的什么方向</span></span><br><span class="line">        )</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。 </p><h3 id="选取与核函数相关的参数"><a href="#选取与核函数相关的参数" class="headerlink" title="选取与核函数相关的参数"></a>选取与核函数相关的参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM52.png" alt></p><p>在知道如何选取核函数后，我们还要观察一下除了kernel之外的核函数相关的参数。对于线性核函数，”kernel”是唯一能够影响它的参数，但是对于其他三种非线性核函数，他们还受到参数gamma，degree以及coef0的影响。参数gamma就是表达式中的&gamma;，degree就是多项式核函数的次数d，参数coef0就是常数项&gamma;。其中，高斯径向基核函数受到gamma的影响，而多项式核函数受到全部三个参数的影响。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM54.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用交叉验证得出最好的参数和准确率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">gamma_range = np.logspace(<span class="number">-10</span>,<span class="number">1</span>,<span class="number">20</span>)</span><br><span class="line">coef0_range = np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">param_grid = dict(gamma = gamma_range</span><br><span class="line">,coef0 = coef0_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid = GridSearchCV(svm.SVC(kernel = <span class="string">"poly"</span>,degree=<span class="number">1</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid, cv=cv)</span><br><span class="line">grid.fit(X, y)</span><br><span class="line">print(<span class="string">"The best parameters are %s with a score of %0.5f"</span> % (grid.best_params_,</span><br><span class="line">grid.best_score_))</span><br></pre></td></tr></table></figure><h3 id="重要参数C"><a href="#重要参数C" class="headerlink" title="重要参数C"></a>重要参数C</h3><p><strong>关键概念</strong>：硬件隔与软件隔</p><p>当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在“硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM55.png" alt></p><p>看上图，原来的决策边界&omega;<em>x+b=0，原本的平行于决策边界的两个虚线超平面&omega;\</em>x+b=1和&omega;*x+b=-1都依然有效。我们的原始判别函数是:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM56.png" alt></p><p>不过，这些超平面现在无法让数据上的训练误差等于0了，因为此时存在了一个混杂在红色点中的紫色点。于是，我们需要放松我们原始判别函数中的不等条件，来让决策边界能够适用于我们的异常点。于是我们引入松弛系数Φ来帮助我们优化原始的判别函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM57.png" alt></p><p>其中ζ<sub>i</sub>&gt;0。可以看的出，这其实是将原来的虚线超平面向图像的上方和下方平移。松弛系数其实蛮好理解的，看上图，在红色点附近的蓝色点在原本的判别函数中必定会被分为红色，所有一定会判断错。我们现在做一条与决策边界平行，且过蓝色点的直线&omega;<em>x<sub>i</sub>+b=1- ζ<sub>i</sub>(图中的蓝色虚线)。这条直线是由&omega;*x<sub>i</sub>+b=1平移得到，所以两条直线在纵坐标上的差异就是ζ<sub>i</sub>。而点&omega;x<sub>i</sub>+b=1的距离就可以表示为 ζ<sub>i</sub>\</em>&omega;/||&omega;||，即ζ<sub>i</sub>在&omega;方向上的投影。由于单位向量是固定的，所以ζ<sub>i</sub>可以作为蓝色点在原始的决策边界上的分类错误的程度的表示。隔得越远，分的越错。<strong>注意：ζ<sub>i</sub>并不是点到决策超平面的距离本身。</strong></p><p>不难注意到，我们让&omega;*x<sub>i</sub>+b&gt;=1-ζ<sub>i</sub>作为我们的新决策超平面，是有一定的问题的。虽然我们把异常的蓝色点分类正确了，但我们同时也分错了一系列红色的点。所以我们必须在我们求解最大边际的损失函数中加上一个惩罚项，用来惩罚我们具有巨大松弛系数的决策超平面。我们的拉格朗日函数，拉格朗日对偶函数，也因此都被松弛系数改变。现在，我们的损失函数为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM58.png" alt></p><p>C是用来控制惩罚力度的系数</p><p>我们的拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM59.png" alt></p><p>需要满足的KKT条件为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM60.png" alt></p><p>拉格朗日对偶函数为</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM61.png" alt></p><p>sklearn类SVC背后使用的最终公式。公式中现在唯一的新变量，松弛系数的惩罚力度C，由我们的参数C来进行控制。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM62.png" alt></p><p>在实际使用中，C和核函数的相关参数（gamma，degree等等）们搭配，往往是SVM调参的重点。与gamma不同，C没有在对偶函数中出现，并且是明确了调参目标的，所以我们可以明确我们究竟是否需要训练集上的高精确度来调整C的方向。默认情况下C为1，通常来说这都是一个合理的参数。 如果我们的数据很嘈杂，那我们往往减小C。当然，我们也可以使用网格搜索或者学习曲线来调整C的值。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">C1_range = np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C2_range=np.linspace(<span class="number">0.01</span>,<span class="number">30</span>,<span class="number">50</span>)</span><br><span class="line">C3_range=np.linspace(<span class="number">5</span>,<span class="number">7</span>,<span class="number">50</span>)</span><br><span class="line">param_grid1 = dict(C=C1_range)</span><br><span class="line">cv = StratifiedShuffleSplit(n_splits=<span class="number">5</span>, test_size=<span class="number">0.3</span>, random_state=<span class="number">420</span>)</span><br><span class="line">grid1 = GridSearchCV(svm.SVC(kernel = <span class="string">"linear"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid1, cv=cv)</span><br><span class="line">grid1.fit(X, y)</span><br><span class="line">print(<span class="string">"linear  The best parameters are %s with a score of %0.5f"</span> % (grid1.best_params_,</span><br><span class="line">grid1.best_score_))</span><br><span class="line">param_grid2=dict(C=C2_range)</span><br><span class="line">grid2 = GridSearchCV(svm.SVC(kernel = <span class="string">"rbf"</span>,gamma=</span><br><span class="line">    <span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),</span><br><span class="line">param_grid=param_grid2, cv=cv)</span><br><span class="line">grid2.fit(X, y)</span><br><span class="line">print(<span class="string">"rbf(0.01,30,50)   The best parameters are %s with a score of %0.5f"</span> % (grid2.best_params_,</span><br><span class="line">grid2.best_score_))</span><br><span class="line">param_grid3=dict(C=C3_range)</span><br><span class="line">grid3=GridSearchCV(svm.SVC(kernel=<span class="string">"rbf"</span>,gamma=<span class="number">0.012742749857031322</span>,cache_size=<span class="number">5000</span>),param_grid=param_grid3)</span><br><span class="line">grid3.fit(X,y)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_params_)</span><br><span class="line">print(<span class="string">"rbf(5,7,50)  "</span>,grid3.best_score_)</span><br></pre></td></tr></table></figure><h2 id="SVC的参数、属性和接口"><a href="#SVC的参数、属性和接口" class="headerlink" title="SVC的参数、属性和接口"></a>SVC的参数、属性和接口</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM63.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM64.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM65.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM66.png" alt></p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM67.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;非线性SVM与核函数&quot;&gt;&lt;a href=&quot;#非线性SVM与核函数&quot; class=&quot;headerlink&quot; title=&quot;非线性SVM与核函数&quot;&gt;&lt;/a&gt;非线性SVM与核函数&lt;/h2&gt;&lt;h3 id=&quot;SVC在非线性数据上的推广&quot;&gt;&lt;a href=&quot;#SVC在非线性数
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost</title>
    <link href="http://yoursite.com/2019/09/12/XGBoost/"/>
    <id>http://yoursite.com/2019/09/12/XGBoost/</id>
    <published>2019-09-12T14:19:52.000Z</published>
    <updated>2019-09-15T06:13:57.540Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h2><p>在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中和MAC使用pip来安装xgboost的代码： </p><p>windows：</p><p>pip install xgboost #安装xgboost库<br>pip install –upgrade xgboost #更新xgboost库</p><p>我在这步遇到超时报错，查了以下，改成如下安装：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost7.png" alt></p><p>MAC：</p><p>brew install gcc@7<br>pip3 install xgboost </p><p>安装好XGBoost库后，我们有两种方式来使用我们的XGBoost库。第一种方式。是直接使用XGBoost库自己的建模流程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost1.png" alt></p><p>其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。params可能的取值以及xgboost.train的列表：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost2.png" alt></p><p>我们也可以选择第二种方法，使用xgboost库中的sklearn的API：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost3.png" alt></p><p>有人发现，这两种方法的参数是不同的。其实只是写法不同，功能是相同的。使用xgboost中设定的建模流程来建模，和使用sklearnAPI中的类来建模，模型效果是比较相似的，但是xgboost库本身的运算速度（尤其是交叉验证）以及调参手段比sklearn要简单。</p><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p>XGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的弱评估器，以及应用中的其他过程。</p><h3 id="梯度提升树"><a href="#梯度提升树" class="headerlink" title="梯度提升树"></a>梯度提升树</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost3.png" alt></p><h4 id="Boosting过程"><a href="#Boosting过程" class="headerlink" title="Boosting过程"></a>Boosting过程</h4><p>XGBoost的基础是梯度提升算法，因此我们必须先从了解梯度提升算法开始。梯度提升（Gradient boosting）是构建预测模型的最强大技术之一，它是集成算法中提升法（Boosting）的代表算法。集成算法通过在数据上构建多个弱评估器，汇总所有弱评估器的建模结果，以获取比单个模型更好的回归或分类表现。弱评估器被定义为是表现至少比随机猜测更好的模型，即预测准确率不低于50%的任意模型。 集成不同弱评估器的方法有很多种。有像我们曾经在随机森林的课中介绍的，一次性建立多个平行独立的弱评估器的装袋法。也有像我们今天要介绍的提升法这样，逐一构建弱评估器，经过多次迭代逐渐累积多个弱评估器的方法。提升法的中最著名的算法包括Adaboost和梯度提升树，XGBoost就是由梯度提升树发展而来的。梯度提升树中可以有回归树也可以有分类树，两者都以CART树算法作为主流，XGBoost背后也是CART树，<strong>这意味着XGBoost中所有的树都是二叉的</strong>。</p><p>接下来，我们来了解一些Boosting算法是上面工作：首先，梯度提升回归树是专注于回归的树模型的提升集成模型，其建模过程大致如下：最开始先建立一棵树，然后逐渐迭代，每次迭代过程中都增加一棵树，逐渐形成众多树模型集成的强评估器。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoosting6.png" alt></p><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost6.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor <span class="keyword">as</span> RFR</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LinearR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score <span class="keyword">as</span> CVS, train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">reg = XGBR(n_estimators=<span class="number">100</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"预测："</span>,reg.predict(Xtest)) <span class="comment">#传统接口predict</span></span><br><span class="line">print(<span class="string">"准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line">print(<span class="string">"均方误差："</span>,MSE(Ytest,reg.predict(Xtest)))</span><br><span class="line">print(<span class="string">"模型的重要性分数："</span>,reg.feature_importances_)</span><br></pre></td></tr></table></figure><p>使用参数学习曲线观察n_estimators对模型的影响</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">axisx = range(<span class="number">10</span>,<span class="number">1010</span>,<span class="number">50</span>)</span><br><span class="line">cv = KFold(n_splits=<span class="number">5</span>, shuffle = <span class="literal">True</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=i,random_state=<span class="number">420</span>)</span><br><span class="line">    rs.append(CVS(reg,Xtrain,Ytrain,cv=cv).mean())</span><br><span class="line">print(axisx[rs.index(min(rs))],max(rs))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"red"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="方差与泛化误差"><a href="#方差与泛化误差" class="headerlink" title="方差与泛化误差"></a>方差与泛化误差</h4><p>机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定。其中偏差就是训练集上的拟合程度决定，方差是模型的稳定性决定，噪音是不可控的。而泛化误差越小，模型就越理想。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost8.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">cv=KFold(n_splits=<span class="number">5</span>,shuffle=<span class="literal">True</span>,random_state=<span class="number">42</span>)</span><br><span class="line">axisx = range(<span class="number">100</span>,<span class="number">300</span>,<span class="number">10</span>)</span><br><span class="line">rs = []</span><br><span class="line">var = []</span><br><span class="line">ge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=i,random_state=<span class="number">420</span>)</span><br><span class="line">    cvresult = CVS(reg,Xtrain,Ytrain,cv=cv)</span><br><span class="line">    rs.append(cvresult.mean())</span><br><span class="line">    var.append(cvresult.var())</span><br><span class="line">    ge.append((<span class="number">1</span> - cvresult.mean())**<span class="number">2</span>+cvresult.var())</span><br><span class="line"><span class="comment">#得出最好的n_estimators</span></span><br><span class="line">print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])</span><br><span class="line">print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))</span><br><span class="line">print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))</span><br><span class="line">rs = np.array(rs)</span><br><span class="line">var = np.array(var)*<span class="number">0.01</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"black"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line"><span class="comment">#添加方差线</span></span><br><span class="line">plt.plot(axisx,rs+var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.plot(axisx,rs-var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">#泛化误差的可控部分</span></span><br><span class="line">plt.plot(axisx,ge,c=<span class="string">"gray"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从这个过程中观察n_estimators参数对模型的影响，我们可以得出以下结论： </p><p>首先，XGB中的树的数量决定了模型的学习能力，树的数量越多，模型的学习能力越强。只要XGB中树的数量足够 了，即便只有很少的数据， 模型也能够学到训练数据100%的信息，所以XGB也是天生过拟合的模型。但在这种情况 下，模型会变得非常不稳定。</p><p>第二，XGB中树的数量很少的时候，对模型的影响较大，当树的数量已经很多的时候，对模型的影响比较小，只能有 微弱的变化。当数据本身就处于过拟合的时候，再使用过多的树能达到的效果甚微，反而浪费计算资源。当唯一指标 或者准确率给出的n_estimators看起来不太可靠的时候，我们可以改造学习曲线来帮助我们。</p><p> 第三，树的数量提升对模型的影响有极限，开始，模型的表现会随着XGB的树的数量一起提升，但到达某个点之 后，树的数量越多，模型的效果会逐步下降，这也说明了暴力增加n_estimators不一定有效果。<br>这些都和随机森林中的参数n_estimators表现出一致的状态。在随机森林中我们总是先调整n_estimators，当 n_estimators的极限已达到，我们才考虑其他参数，但XGB中的状况明显更加复杂，当数据集不太寻常的时候会更加 复杂。这是我们要给出的第一个超参数，因此还是建议优先调整n_estimators，一般都不会建议一个太大的数目， 300以下为佳。</p><h4 id="subsample"><a href="#subsample" class="headerlink" title="subsample"></a>subsample</h4><p>我们训练模型之前，必然会有一个巨大的数据集。我们都知道树模型是天生过拟合的模型，并且如果数据量太过巨 大，树模型的计算会非常缓慢，因此，我们要对我们的原始数据集进行有放回抽样（bootstrap）。有放回的抽样每 次只能抽取一个样本，若我们需要总共N个样本，就需要抽取N次。每次抽取一个样本的过程是独立的，这一次被抽 到的样本会被放回数据集中，下一次还可能被抽到，因此抽出的数据集中，可能有一些重复的数据。在无论是装袋还是提升的集成算法中，有放回抽样都是我们防止过拟合。</p><p>在sklearn中，我们使用参数subsample来控制我们的随机抽样。在xgb和sklearn中，这个参数都默认为1且不能取到0，所以取值范围是(0,1]。这说明我们无法控制模型是否进行随机有放回抽样，只能控制抽样抽出来的样本量大概是多少。</p><h4 id="eta-or-learning-rate"><a href="#eta-or-learning-rate" class="headerlink" title="eta   or  learning_rate"></a>eta   or  learning_rate</h4><p>在逻辑回归中，我们自定义步长&alpha;来干涉我们的迭代速率，在XGB中看起来却没有这样的设置，但其实不然。在XGB<br>中，我们完整的迭代决策树的公式应该写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost11.png" alt></p><p>其中&eta;读作”eta”，是迭代决策树时的步长（shrinkage），又叫做学习率（learning rate）。和逻辑回归中的&alpha;类似，&eta;<br>越大，迭代的速度越快，算法的极限很快被达到，有可能无法收敛到真正的最佳。&eta;越小，越有可能找到更精确的最<br>佳值，更多的空间被留给了后面建立的树，但迭代速度会比较缓慢。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost12.png" alt></p><p>在sklearn中，我们使用参数learning_rate来干涉我们的学习速率：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost13.png" alt></p><p>梯度提升树是XGB的基础，本节中已经介绍了XGB中与梯度提升树的过程相关的四个参数：n_estimators，learning_rate ，silent，subsample。这四个参数的主要目的，其实并不是提升模型表现，更多是了解梯度提升树的原理。现在来看，我们的梯度提升树可是说是由三个重要的部分组成：</p><ol><li>一个能够衡量集成算法效果的，能够被最优化的损失函数</li><li>一个能够实现预测的弱评估器</li><li>一种能够让弱评估器集成的手段，包括我们讲解的迭代方法，抽样手段，样本加权等等过程 </li></ol><h4 id="booster"><a href="#booster" class="headerlink" title="booster"></a>booster</h4><p>梯度提升算法中不只有梯度提升树，XGB作为梯度提升算法的进化，自然也不只有树模型一种弱评估器。在XGB中，除了树模型，我们还可以选用线性模型，比如线性回归，来进行集成。虽然主流的XGB依然是树模型，但我们也可以使用其他的模型。基于XGB的这种性质，我们有参数“booster”来控制我们究竟使用怎样的弱评估器。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost14.png" alt></p><p>两个参数都默认为”gbtree”，如果不想使用树模型，则可以自行调整。当XGB使用线性模型的时候，它的许多数学过<br>程就与使用普通的Boosting集成非常相似。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> booster <span class="keyword">in</span> [<span class="string">"gbtree"</span>,<span class="string">"gblinear"</span>,<span class="string">"dart"</span>]:</span><br><span class="line">reg = XGBR(<span class="attribute">n_estimators</span>=180,learning_rate=0.1,random_state=420,booster=booster).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="builtin-name">print</span>(booster)</span><br><span class="line"><span class="builtin-name">print</span>(reg.score(Xtest,Ytest))</span><br></pre></td></tr></table></figure><h4 id="objective"><a href="#objective" class="headerlink" title="objective"></a>objective</h4><p>在众多机器学习算法中，损失函数的核心是衡量模型的泛化能力，即模型在未知数据上的预测的准确与否，我们训练模型的核心目标也是希望模型能够预测准确。在XGB中，预测准确自然是非常重要的因素，但我们之前提到过，XGB的是实现了模型表现和运算速度的平衡的算法。普通的损失函数，比如错误率，均方误差等，都只能够衡量模型的表现，无法衡量模型的运算速度。回忆一下，我们曾在许多模型中使用空间复杂度和时间复杂度来衡量模型的运算效率。XGB因此引入了模型复杂度来衡量算法的运算效率。因此XGB的目标函数被写作：传统损失函数 + 模型复杂度。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost15png" alt></p><p>其中i代表数据集中的第i个样本，m表示导入第K棵树的数据总量，K代表建立的所有树(n_estimators)。 第二项代表模型的复杂度，使用树模型的某种变换$\Omega$表示，这个变化代表了一个从树的结构来衡量树模型的复杂度的式子,可以有多种定义。我们在迭代每一颗的过程中，都最小化Obj来求最优的yi。</p><p>在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。一个集成模型(f)<br>在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定，而泛化误差越小，模型就越理想。从下面的图可以看出来，方差和偏差是此消彼长的，并且模型的复杂度越高，方差越大，偏差越小。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest6.png" alt></p><p>方差可以被简单地解释为模型在不同数据集上表现出来地稳定性，而偏差是模型预测的准确度。那方差-偏差困境就<br>可以对应到我们的Obj中了： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost17.png" alt></p><p>第一项是衡量我们的偏差，模型越不准确，第一项就会越大。第二项是衡量我们的方差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。所以我们求解Obj的最小值，其实是在求解方差与偏差的平衡点，以求模型的泛化误差最小，运行速度最快。 </p><p>在应用中，我们使用参数“objective”来确定我们目标函数的第一部分，也就是衡量损失的部分。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost18.png" alt></p><p>xgb自身的调用方式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost1.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#默认reg:linear</span></span><br><span class="line">reg = XGBR(n_estimators=<span class="number">180</span>,random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"sklearn中的XGboost准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line">print(<span class="string">"sklearn中的xgb均方误差："</span>,MSE(Ytest,reg.predict(Xtest)))</span><br><span class="line"><span class="comment">#xgb实现法</span></span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="comment">#使用类Dmatrix读取数据</span></span><br><span class="line">dtrain = xgb.DMatrix(Xtrain,Ytrain)</span><br><span class="line">dtest = xgb.DMatrix(Xtest,Ytest)</span><br><span class="line"><span class="comment">#写明参数，silent默认为False，通常需要手动将它关闭</span></span><br><span class="line">param = &#123;<span class="string">'silent'</span>:<span class="literal">False</span>,<span class="string">'objective'</span>:<span class="string">'reg:linear'</span>,<span class="string">"eta"</span>:<span class="number">0.1</span>&#125;</span><br><span class="line">num_round = <span class="number">180</span></span><br><span class="line"><span class="comment">#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment">#接口predict</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">print(<span class="string">"xgb中的准确率："</span>,r2_score(Ytest,bst.predict(dtest)))</span><br><span class="line">print(<span class="string">"xgb中的均方误差："</span>,MSE(Ytest,bst.predict(dtest)))</span><br></pre></td></tr></table></figure><p>看得出来，无论是从R<sup>2</sup>还是从MSE的角度来看，都是xgb库本身表现更优秀。</p><h4 id="alpha-or-reg-alpha-amp-lambda-or-reg-lambda"><a href="#alpha-or-reg-alpha-amp-lambda-or-reg-lambda" class="headerlink" title="(alpha or reg_alpha)  &amp; (lambda or  reg_lambda)"></a>(alpha or reg_alpha)  &amp; (lambda or  reg_lambda)</h4><p>对于XGB来说，每个叶子节点上会有一个预测分数（prediction score），也被称为叶子权重。这个叶子权重就是所有在这个叶子节点上的样本在这一棵树上的回归取值,用f<sub>k</sub>(x<sub>i</sub>)或者&omega;来表示。</p><p>当有多棵树的时候，集成模型的回归结果就是所有树的预测分数之和，假设这个集成模型中总共有K棵决策树，则整<br>个模型在这个样本i上给出的预测结果为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost21.png" alt></p><p>如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost22.png" alt></p><p>设一棵树上总共包含了T个叶子节点，其中每个叶子节点的索引为j，则这个叶子节点上的样本权重是w<sub>j</sub>。依据这个，我们定义模型的复杂度$\Omega$(f)为（注意这不是唯一可能的定义，我们当然还可以使用其他的定义，只要满足叶子越多/深度越大，复杂度越大的理论):</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost23.png" alt></p><p>使用L2正则项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost24.png" alt></p><p>使用L1正则项：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost25.png" alt></p><p>还可以两个一起用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost26.png" alt></p><p>这个结构中有两部分内容，一部分是控制树结构的&gamma;，另一部分则是我们的正则项。叶子数量<em>T</em>可以代表整个树结构，这是因为在XGBoost中所有的树都是CART树（二叉树），所以我们可以根据叶子的数量<em>T</em>判断出树的深度，而&gamma;是我们自定的控制叶子数量的参数。 至于第二部分正则项，类比一下我们岭回归和Lasso的结构，参数&alpha;和&lambda;的作用其实非常容易理解，他们都是控制正则化强度的参数，我们可以二选一使用，也可以一起使用加大正则化的力度。当 和 都为0的时候，目标函数就是普通的梯度提升树的目标函数。</p><p>来看正则化系数分别对应的参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost20.png" alt></p><h4 id="gamma"><a href="#gamma" class="headerlink" title="gamma"></a>gamma</h4><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost28.png" alt></p><p>回忆一下决策树中我们是如何进行计算：我们使用基尼系数或信息熵来衡量分枝之后叶子节点的不纯度，分枝前的信息熵与分治后的信息熵之差叫做信息增益，信息增益最大的特征上的分枝就被我们选中，当信息增益低于某个阈值时，就让树停止生长。在XGB中，我们使用的方式是类似的：我们首先使用目标函数来衡量树的结构的优劣，然后让树从深度0开始生长，每进行一次分枝，我们就计算目标函数减少了多少，当目标函数的降低低于我们设定的某个阈值时，就让树停止生长。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost27.png" alt></p><p>原理还不是很明白，先贴最后的Gain函数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost29.png" alt></p><p>从上面的Gain函数，从上面的目标函数和结构分数之差Gain的式子来看，&gamma;使我们每增加一片叶子就会被剪去的惩罚项。增加的叶子越多，结构分数之差Gain会被惩罚越重，所以&gamma;也被称为”复杂性控制“。所以&gamma;是我们用来防止过拟合的重要参数。&gamma;是对梯度提升树影响最大的参数之一，其效果不逊色与n_estimators和放过拟合神器max_depth。同时&gamma;还是我们让树停止生长的重要参数。</p><p>在XGB中，规定只要结构分数之差Gain大于0，即只要目标函数还能减小，我们就允许继续进行分枝。也就是说，我们对于目标函数减小量的要求是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost30.png" alt></p><p>因此，我们可以直接通过设定&gamma;的大小让XGB的树停止生长。&gamma;因此被定义为，在树的叶节点上进行进一步分枝所需的最小目标函数减少量，在决策树和随机森林中也有类似的参数(min_split_loss，min_samples_split)。 设定越大，算法就越保守，树的叶子数量就越少，模型的复杂度就越低。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> CVS</span><br><span class="line">data = load_boston()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">axisx = np.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">0.05</span>)</span><br><span class="line">rs = []</span><br><span class="line">var = []</span><br><span class="line">ge = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> axisx:</span><br><span class="line">    reg = XGBR(n_estimators=<span class="number">180</span>,random_state=<span class="number">420</span>,gamma=i)</span><br><span class="line">    result = CVS(reg,Xtrain,Ytrain,cv=<span class="number">20</span>)</span><br><span class="line">    rs.append(result.mean())</span><br><span class="line">    var.append(result.var())</span><br><span class="line">    ge.append((<span class="number">1</span> - result.mean())**<span class="number">2</span>+result.var())</span><br><span class="line">print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])</span><br><span class="line">print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))</span><br><span class="line">print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))</span><br><span class="line">rs = np.array(rs)</span><br><span class="line">var = np.array(var)*<span class="number">0.1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(axisx,rs,c=<span class="string">"black"</span>,label=<span class="string">"XGB"</span>)</span><br><span class="line">plt.plot(axisx,rs+var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.plot(axisx,rs-var,c=<span class="string">"red"</span>,linestyle=<span class="string">'-.'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>为了调整&gamma;，我们需要引入新的工具，xgboost库中的类xgboost.cv</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost31.png" alt></p><p>为了使用xgboost.cv，我们必须要熟悉xgboost自带的模型评估指标。xgboost在建库的时候本着大而全的目标，和sklearn类似，包括了大约20个模型评估指标，然而用于回归和分类的其实只有几个，大部分是用于一些更加高级的功能比如ranking。来看用于回归和分类的评估指标都有哪些：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost32.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">data2 = load_breast_cancer()</span><br><span class="line">x2 = data2.data</span><br><span class="line">y2 = data2.target</span><br><span class="line">dfull2 = xgb.DMatrix(x2,y2)</span><br><span class="line">param1 = &#123;<span class="string">'silent'</span>:<span class="literal">True</span>,<span class="string">'obj'</span>:<span class="string">'binary:logistic'</span>,<span class="string">"gamma"</span>:<span class="number">0</span>,<span class="string">"nfold"</span>:<span class="number">5</span>&#125;</span><br><span class="line">param2 = &#123;<span class="string">'silent'</span>:<span class="literal">True</span>,<span class="string">'obj'</span>:<span class="string">'binary:logistic'</span>,<span class="string">"gamma"</span>:<span class="number">2</span>,<span class="string">"nfold"</span>:<span class="number">5</span>&#125;</span><br><span class="line">num_round = <span class="number">100</span></span><br><span class="line">cvresult1 = xgb.cv(param1, dfull2, num_round,metrics=(<span class="string">"error"</span>))</span><br><span class="line">cvresult2 = xgb.cv(param2, dfull2, num_round,metrics=(<span class="string">"error"</span>))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult1.iloc[:,<span class="number">0</span>],c=<span class="string">"red"</span>,label=<span class="string">"train,gamma=0"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult1.iloc[:,<span class="number">2</span>],c=<span class="string">"orange"</span>,label=<span class="string">"test,gamma=0"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult2.iloc[:,<span class="number">0</span>],c=<span class="string">"green"</span>,label=<span class="string">"train,gamma=2"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>),cvresult2.iloc[:,<span class="number">2</span>],c=<span class="string">"blue"</span>,label=<span class="string">"test,gamma=2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="scale-pos-weight"><a href="#scale-pos-weight" class="headerlink" title="scale_pos_weight"></a>scale_pos_weight</h4><p>XGB中存在着调节样本不平衡的参数scale_pos_weight,这个参数非常类似于之前随机森林和支持向量机中我们都使用到过的class_weight参数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost34.png" alt></p><h4 id="其它参数"><a href="#其它参数" class="headerlink" title="其它参数"></a>其它参数</h4><p>XGBoost应用的核心之一就是减轻过拟合带来的影响。作为树模型，减轻过拟合的方式主要是靠对决策树剪枝来降低模型的复杂度，以求降低方差。在之前的讲解中，我们已经学习了好几个可以用来防止过拟合的参数，包括上一节提到的复杂度控制&lambda;，正则化的两个参数&lambda;和&alpha;，控制迭代速度的参数 以及管理每次迭代前进行的随机有放回抽样的参数subsample。所有的这些参数都可以用来减轻过拟合。但除此之外，我们还有几个影响重大的，专用于剪枝的参数： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/XGBoost33.png" alt></p><h2 id="使用Pickle保存和调用模型"><a href="#使用Pickle保存和调用模型" class="headerlink" title="使用Pickle保存和调用模型"></a>使用Pickle保存和调用模型</h2><p>pickle是python编程中比较标准的一个保存和调用模型的库，我们可以使用pickle和open函数的连用，来讲我们的模型保存到本地。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型的coding</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">data2 = load_breast_cancer()</span><br><span class="line">x2 = data2.data</span><br><span class="line">y2 = data2.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(x2,y2,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">dtrain = xgb.DMatrix(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#设定参数，对模型进行训练</span></span><br><span class="line">param = &#123;<span class="string">'silent'</span>:<span class="literal">True</span></span><br><span class="line">,<span class="string">'obj'</span>:<span class="string">'reg:linear'</span></span><br><span class="line">,<span class="string">"subsample"</span>:<span class="number">1</span></span><br><span class="line">,<span class="string">"eta"</span>:<span class="number">0.05</span></span><br><span class="line">,<span class="string">"gamma"</span>:<span class="number">20</span></span><br><span class="line">,<span class="string">"lambda"</span>:<span class="number">3.5</span></span><br><span class="line">,<span class="string">"alpha"</span>:<span class="number">0.2</span></span><br><span class="line">,<span class="string">"max_depth"</span>:<span class="number">4</span></span><br><span class="line">,<span class="string">"colsample_bytree"</span>:<span class="number">0.4</span></span><br><span class="line">,<span class="string">"colsample_bylevel"</span>:<span class="number">0.6</span></span><br><span class="line">,<span class="string">"colsample_bynode"</span>:<span class="number">1</span>&#125;</span><br><span class="line">num_round = <span class="number">180</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">pickle.dump(bst, open(<span class="string">"xgboostonboston.dat"</span>,<span class="string">"wb"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#调用模型的coding</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#注意，如果我们保存的模型是xgboost库中建立的模型，则导入的数据类型也必须是xgboost库中的数据类型</span></span><br><span class="line">dtest = xgb.DMatrix(Xtest,Ytest)</span><br><span class="line"><span class="comment">#导入模型</span></span><br><span class="line">loaded_model = pickle.load(open(<span class="string">"xgboostonboston.dat"</span>, <span class="string">"rb"</span>))</span><br><span class="line">print(<span class="string">"Loaded model from: xgboostonboston.dat"</span>)</span><br><span class="line"><span class="comment">#做预测</span></span><br><span class="line">ypreds = loaded_model.predict(dtest)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="keyword">as</span> MSE, r2_score</span><br><span class="line">print(<span class="string">"均方误差："</span>,MSE(Ytest,ypreds))</span><br><span class="line">print(r2_score(Ytest,ypreds))</span><br></pre></td></tr></table></figure><h2 id="使用Joblib保存和调用模型"><a href="#使用Joblib保存和调用模型" class="headerlink" title="使用Joblib保存和调用模型"></a>使用Joblib保存和调用模型</h2><p>Joblib是SciPy生态系统中的一部分，它为Python提供保存和调用管道和对象的功能，处理NumPy结构的数据尤其高<br>效，对于很大的数据集和巨大的模型非常有用。Joblib与pickle API非常相似 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor <span class="keyword">as</span> XGBR</span><br><span class="line">bst = XGBR(n_estimators=<span class="number">200</span>,eta=<span class="number">0.05</span>,gamma=<span class="number">20</span>,reg_lambda=<span class="number">3.5</span>,reg_alpha=<span class="number">0.2</span>,max_depth=<span class="number">4</span>,colsample_bytree=<span class="number">0.4</span>,colsample_bylevel=<span class="number">0.6</span>).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">joblib.dump(bst,<span class="string">"xgboost-boston.dat"</span>)</span><br><span class="line"><span class="comment">#调用模型</span></span><br><span class="line">loaded_model = joblib.load(<span class="string">"xgboost-boston.dat"</span>)</span><br><span class="line"><span class="comment">#这里可以直接导入Xtest</span></span><br><span class="line">ypreds = loaded_model.predict(Xtest)</span><br><span class="line"><span class="keyword">print</span>（MSE(Ytest, ypreds)）</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前沿&quot;&gt;&lt;a href=&quot;#前沿&quot; class=&quot;headerlink&quot; title=&quot;前沿&quot;&gt;&lt;/a&gt;前沿&lt;/h2&gt;&lt;p&gt;在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="http://yoursite.com/2019/09/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/09/12/神经网络/</id>
    <published>2019-09-12T14:19:10.000Z</published>
    <updated>2019-09-15T06:13:39.225Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h2><p>人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学习的基础。神经网络算法试图模拟生物神经系统的学习过程，以此实现强大的预测性能。不过由于是模仿人类大脑，所以神经网络的模型复杂度很高也是众所周知。在现实应用中，神经网络可以说是解释性最差的模型之一 。</p><p>神经网络原理最开始是基于感知机提出——感知机是最古老的机器学习分类算法之一 。和今天的大部分模型比较起来，感知机的泛化能力比较弱。但支持向量机和神经网络都基于它的原理来建立。感知机的原理在支持向量机中介绍过，使用一条线性决策边界z=&omega;*x+b来划分数据集，决策边界的上方是一类数据(z&gt;=0)，决策边界的下方是另一类数据(z&lt;0)的决策过程。使用神经元表示，如下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN1.png" alt></p><p>不同的特征数据被输入后，我们通过神经键将它输入我们的神经元。每条神经键上对应不同的参数&omega;，b。因此特征数据会经由神经键被匹配到一个参数向量&omega;，b。基于参数向量&omega;算法可以求解出决策边界z=&omega;*x+b，然后用决策函数sign(z)进行判断，最终预测出标签y并且将结果输出，其中函数sign(z)被称为”激活函数“。这是模拟人类的大脑激活神经元的过程所命名的，其实本质就是决定了预测标签的输出会是什么内容的预测函数。</p><p>注意：在这个过程中，有三个非常重要的点：</p><p>​    1、每个输入的特征都会匹配到一个参数&omega;，我们都知道参数向量&omega;中含有的参数数量与我们的特征数目是一致的，在感知机中也是如此。也就是说，任何基于感知机的算法，必须至少要有参数向量&omega;可求。</p><p>​    2、一个线性关系z，z是由参数和输入的数据共同决定的。这个线性关系，往往就是我们的决策边界，或者它也可以是多元线性回归，逻辑回归等算法的线性表达式</p><p>​    3、激活函数的结果，是基于激活函数的本身，参数向量&omega;和输入的数据一同计算出来的。也就是说，任何基于感知机的算法。必须要存在一个激活函数。</p><p>神经网络就相当于众多感知机的集成，因此，确定激活函数，并找出参数向量&omega;也是神经网络的计算核心。我们来看看神经网络的基本结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN2.png" alt></p><p>首先，神经网络有三层。第一层叫做输入层（Input layer），输入特征矩阵用，因此每个神经元上都是一个特征向量。极端情况下，如果一个神经网络只训练一个样本，则每个输入层的神经元上都是这个样本的一个特征取值。</p><p>最后一层叫做输出层（output layer），输出预测标签用。如果是回归类，一般输出层只有一个神经元，回归的是所有输入的样本的标签向量。如果是分类，可能会有多个神经元。二分类有两个神经元，多分类有多个神经元，分别输出所有输入的样本对应的每个标签分类下的概率。但无论如何，输出层只有一层，是用于输出预测结果用。</p><p>输入层和输出层中间的所有层，叫做隐藏层（Hidden layers），最少一层。<strong>也就是说整个神经网络是最少三层</strong>。隐藏层是我们用来让算法学习的网络层级，从更靠近输入层的地方开始叫做”上层”，相对而言，更靠近输出层的一层，叫做”下层”。在隐藏层上，每个神经元中都存在一个激活函数，我们的数据被逐层传递，每个下层的神经元都必须处理上层的神经元中的激活函数处理完毕的数据，本质是一个感知器嵌套的过程。隐藏层中上层的每个神经元，都与下层中的每个神经元相连，因此隐藏层的结构随着神经元的变多可以变得非常非常复杂。神经网络的两个要点：参数&omega;和激活函数，也都在这一层发挥作用，因此理解隐藏层是神经网络原理中最难的部分。</p><p><strong>神经网络的每一层的结果之间的关系是嵌套，不是迭代</strong>。由于我们执行的是嵌套，不是迭代。所以<strong>我们的每一个系数之间是相互独立的，每一层的系数之间也是相互独立的</strong>，我们不是在执行使用上一层或者上一个神经元的参数来求解下一层或者下一个神经元的参数的过程。我们不断求解，是激活函数的结果a，不是参数&omega;<strong>。在一次神经网络计算中，我们没有迭代参数&omega;</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN3.png" alt></p><p>上面这张图还不算真实数据中特别复杂的情况，但已经含有总共8*9*9*9*4=23328个&omega;。神经网络可能是我们遇到的最难调参的算法。接下来看看sklearn中的神经网络。</p><h2 id="sklearn中的神经网络"><a href="#sklearn中的神经网络" class="headerlink" title="sklearn中的神经网络"></a>sklearn中的神经网络</h2><p>sklearn是专注于机器学习的库，它在神经网络的模块中特地标注：sklearn不是用于深度学习的平台，因此这个神经网络不具备做深度学习的功能，也不具备处理大型数据的能力。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN4.png" alt></p><h3 id="neural-network-MLPClasifier"><a href="#neural-network-MLPClasifier" class="headerlink" title="neural_network.MLPClasifier"></a>neural_network.MLPClasifier</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN6.png" alt></p><h4 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h4><h5 id="hidden-layer-sizes"><a href="#hidden-layer-sizes" class="headerlink" title="hidden_layer_sizes"></a>hidden_layer_sizes</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ANN7.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier <span class="keyword">as</span> DNN</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier <span class="keyword">as</span> DTC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">times = time()</span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">100</span>,),random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"dnn的交叉验证："</span>,cv(dnn,X,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">print(<span class="string">"dnn所用的时间："</span>,time() - times)</span><br><span class="line"><span class="comment">#使用决策树进行一个对比</span></span><br><span class="line">times = time()</span><br><span class="line">clf = DTC(random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"决策树的交叉验证："</span>,cv(clf,X,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">print(<span class="string">"决策树所用的时间："</span>,time() - times)</span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">100</span>,),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dnn.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#使用重要参数n_layers_，得出层数</span></span><br><span class="line">print(<span class="string">"n_layers_："</span>,dnn.n_layers_)</span><br><span class="line"><span class="comment">#可见，默认层数是三层，由于必须要有输入和输出层，所以默认其实就只有一层隐藏层</span></span><br><span class="line"><span class="comment">#增加一个隐藏层上的神经元个数</span></span><br><span class="line">dnn = DNN(hidden_layer_sizes=(<span class="number">200</span>,),random_state=<span class="number">420</span>)</span><br><span class="line">dnn = dnn.fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"增加一个隐藏层的准确率是："</span>,dnn.score(Xtest,Ytest))</span><br><span class="line">s = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">2000</span>,<span class="number">100</span>):</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(int(i),),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">200</span>,<span class="number">2100</span>,<span class="number">100</span>),s)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#增加隐藏层，控制神经元个数</span></span><br><span class="line">s = []</span><br><span class="line">layers = [(<span class="number">100</span>,),(<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>),</span><br><span class="line">(<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>,<span class="number">100</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> layers:</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(i),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">3</span>,<span class="number">9</span>),s)</span><br><span class="line">plt.xticks([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Total number of layers"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#增加隐藏层，控制神经元个数</span></span><br><span class="line">s = []</span><br><span class="line">layers = [(<span class="number">100</span>,),(<span class="number">150</span>,<span class="number">150</span>),(<span class="number">200</span>,<span class="number">200</span>,<span class="number">200</span>),(<span class="number">300</span>,<span class="number">300</span>,<span class="number">300</span>,<span class="number">300</span>)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> layers:</span><br><span class="line">    dnn = DNN(hidden_layer_sizes=(i),random_state=<span class="number">420</span>).fit(Xtrain,Ytrain)</span><br><span class="line">    s.append(dnn.score(Xtest,Ytest))</span><br><span class="line">    print(i,max(s))</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(range(<span class="number">3</span>,<span class="number">7</span>),s)</span><br><span class="line">plt.xticks([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Total number of layers"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络概述&quot;&gt;&lt;a href=&quot;#神经网络概述&quot; class=&quot;headerlink&quot; title=&quot;神经网络概述&quot;&gt;&lt;/a&gt;神经网络概述&lt;/h2&gt;&lt;p&gt;人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>降维算法</title>
    <link href="http://yoursite.com/2019/09/12/%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/09/12/降维算法/</id>
    <published>2019-09-12T14:18:55.000Z</published>
    <updated>2019-09-15T06:14:16.309Z</updated>
    
    <content type="html"><![CDATA[<p>降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更快，效果更好。对图像来说，维度就是图像中特征向量的数量。</p><h2 id="sklearn中的降维算法"><a href="#sklearn中的降维算法" class="headerlink" title="sklearn中的降维算法"></a>sklearn中的降维算法</h2><p>sklearn中降维算法都被包括在模块decomposition中 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML131.png" alt></p><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>在降维过程中，我们会减少特征的数量，这意味着删除数据，数据量变少则表示模型可以获取的信息会变少，模型的表现可能会因此受影响。同时，在高维数据中，必然有一些特征是不带有有效的信息的（比如噪音），或者有一些特征带有的信息和其他一些特征是重复的（比如一些特征可能会线性相关）。我们希望能够找出一种办法来帮助我们衡量特征上所带的信息量，让我们在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵。 </p><p>在特征过程中，我们有说过一种特别的特征选择方法：方差过滤。如果一个特征的方差很小，则意味着这个特征上很可能有大量取值都相同（比如90%都是1，只有10%是0，甚至100%是1），那这一个特征的取值对样本而言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说明这个特征上带有大量的信息。因此，在降维中，PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML132.png" alt></p><p>Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML133.png" alt></p><p><strong>PCA和特征选择技术都是特征工程的一部分，他们有什么不同？</strong></p><p>特征工程中有三种方式：特征提取，特征创造和特征选择。仔细观察上面的降维例子和上周我们讲解过的特征<br>选择，你发现有什么不同了吗?<br>特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。而PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。以PCA为代表的降维算法因此是特征创造（feature creation，或feature construction）的一种。 </p><h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><p>n_compontents：n_components是我们降维后需要的维度，即降维后需要保留的特征数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">iris = load_iris()</span><br><span class="line">y = iris.target</span><br><span class="line">X = iris.data</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>) <span class="comment">#实例化</span></span><br><span class="line"><span class="comment">#也可以一步到位，x_dr=pca.fit_transform(X)</span></span><br><span class="line">pca = pca.fit(X) <span class="comment">#拟合模型</span></span><br><span class="line">X_dr = pca.transform(X) <span class="comment">#获取新矩阵</span></span><br><span class="line"><span class="string">"""可以写三行代码，也可以写成for循环</span></span><br><span class="line"><span class="string">plt.figure()</span></span><br><span class="line"><span class="string">plt.scatter(X_dr[y==0, 0], X_dr[y==0, 1], c="red", label=iris.target_names[0])</span></span><br><span class="line"><span class="string">plt.scatter(X_dr[y==1, 0], X_dr[y==1, 1], c="black", label=iris.target_names[1])</span></span><br><span class="line"><span class="string">plt.scatter(X_dr[y==2, 0], X_dr[y==2, 1], c="orange", label=iris.target_names[2])</span></span><br><span class="line"><span class="string">plt.legend()</span></span><br><span class="line"><span class="string">plt.title('PCA of IRIS dataset')</span></span><br><span class="line"><span class="string">plt.show()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">colors = [<span class="string">'red'</span>, <span class="string">'black'</span>, <span class="string">'orange'</span>]</span><br><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]:</span><br><span class="line">    plt.scatter(X_dr[y == i, <span class="number">0</span>],X_dr[y == i, <span class="number">1</span>],alpha=<span class="number">.7</span>,c=colors[i],label=iris.target_names[i])</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'PCA of IRIS dataset'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="选择最好的n-components"><a href="#选择最好的n-components" class="headerlink" title="选择最好的n_components"></a>选择最好的n_components</h4><h5 id="累积可解释方差贡献率曲线"><a href="#累积可解释方差贡献率曲线" class="headerlink" title="累积可解释方差贡献率曲线"></a>累积可解释方差贡献率曲线</h5><p>当参数n_components中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于转换了新特征空间，但没有减少特征的个数。一般来说，不会使用这种输入方式。但我们却可以使用这种输入方式来画出累计可解释方差贡献率曲线，以此选择最好的n_components的整数取值。累积可解释方差贡献率曲线是一条以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标的曲线，能够帮助我们决定n_components最好的取值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pca_line = PCA().fit(X)</span><br><span class="line"><span class="comment">#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比</span></span><br><span class="line"><span class="comment">#又叫做可解释方差贡献率</span></span><br><span class="line">plt.plot([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],np.cumsum(pca_line.explained_variance_ratio_))</span><br><span class="line">plt.xticks([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#这是为了限制坐标轴显示为整数</span></span><br><span class="line">plt.xlabel(<span class="string">"number of components after dimension reduction"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cumulative explained variance ratio"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h5 id="最大似然估计自选超参数"><a href="#最大似然估计自选超参数" class="headerlink" title="最大似然估计自选超参数"></a>最大似然估计自选超参数</h5><p>除了输入整数，n_components还有哪些选择呢？让PCA用最大似然估计(maximum likelihood estimation)自选超参数的方法，输入“mle”作为n_components的参数输入，就可以调用这种方法。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pca_mle = PCA(n_components=<span class="string">"mle"</span>)</span><br><span class="line">pca_mle = pca_mle.fit(X)</span><br><span class="line">X_mle = pca_mle.transform(X)</span><br><span class="line"><span class="comment">#从这里看出，mle自动为我们选了几个特征</span></span><br><span class="line">print(X_mle.shape)</span><br></pre></td></tr></table></figure><h5 id="按信息占比选超参数"><a href="#按信息占比选超参数" class="headerlink" title="按信息占比选超参数"></a>按信息占比选超参数</h5><p>输入[0,1]之间的浮点数，并且让参数svd_solver ==’full’，表示希望降维后的总解释性方差占比大于n_components指定的百分比，即是说，希望保留百分之多少的信息量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pca_f = PCA(n_components=<span class="number">0.97</span>,svd_solver=<span class="string">"full"</span>)</span><br><span class="line">pca_f = pca_f.fit(X)</span><br><span class="line">X_f = pca_f.transform(X)</span><br><span class="line"><span class="comment">#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比</span></span><br><span class="line">print(pca_f.explained_variance_ratio_)</span><br></pre></td></tr></table></figure><h3 id="PCA中的SVD"><a href="#PCA中的SVD" class="headerlink" title="PCA中的SVD"></a>PCA中的SVD</h3><p>其实上面的svd_solver是奇异值分解器。其实SVD可以跳过数学，不计算协方差矩阵，直接找出一个新特征向量组成的n维向量。也就是说，奇异值分解可以不计算协方差矩阵等等计算冗长的矩阵，就直接求出新特征空间和降维后的特征矩阵。简而言之，SVD在矩阵分解中的过程比PCA简单快速。</p><h4 id="重要参数-1"><a href="#重要参数-1" class="headerlink" title="重要参数"></a>重要参数</h4><h5 id="svd-solver"><a href="#svd-solver" class="headerlink" title="svd_solver"></a>svd_solver</h5><p>参数svd_solver是在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选：”auto”, “full”, “arpack”,”randomized”，默认”auto”。</p><p>“auto”：基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80％，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生</p><p>“full”：从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时间充足的情况。</p><p>“arpack”：从scipy.sparse.linalg.svds调用ARPACK分解器来运行截断奇异值分解(SVD truncated)，分解时就将特征数量降到n_components中输入的数值k，可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况，此过程包含一定的随机性。</p><p>“randomized”，通过Halko等人的随机方法进行随机SVD。在”full”方法中，分解器会根据原始数据和输入的n_components值去计算和寻找符合需求的新特征向量，但是在”randomized”方法中，分解器会先生成多个随机向量，然后一一去检测这些随机向量中是否有任何一个符合我们的分解需求，如果符合，就保留这个随机向量，并基于这个随机向量来构建后续的向量空间。这个方法已经被Halko等人证明，比”full”模式下计算快很多，并且还能够保证模型运行效果。适合特征矩阵巨大，计算量庞大的情况。</p><h5 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h5><p>参数random_state在参数svd_solver的值为”arpack” or “randomized”的时候生效，可以控制这两种SVD模式中的随机模式。通常我们就选用”auto“，不必对这个参数纠结太多。</p><h3 id="PCA参数、属性和接口"><a href="#PCA参数、属性和接口" class="headerlink" title="PCA参数、属性和接口"></a>PCA参数、属性和接口</h3><h5 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA1.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA2.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA3.png" alt></p><h5 id="属性列表"><a href="#属性列表" class="headerlink" title="属性列表"></a>属性列表</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA4.png" alt></p><h5 id="接口列表"><a href="#接口列表" class="headerlink" title="接口列表"></a>接口列表</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/PCA5.png" alt></p><h2 id="PCA对手写数字数据集的降维"><a href="#PCA对手写数字数据集的降维" class="headerlink" title="PCA对手写数字数据集的降维"></a>PCA对手写数字数据集的降维</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">r"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line">pca_line = PCA().fit(X)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(np.cumsum(pca_line.explained_variance_ratio_))</span><br><span class="line">plt.xlabel(<span class="string">"number of components after dimension reduction"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"cumulative explained variance ratio"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>接着降维后维度的学习曲线，继续缩小最佳维度的范围</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">101</span>,<span class="number">10</span>):</span><br><span class="line">    X_dr = PCA(i).fit_transform(X)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">    ,X_dr,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">101</span>,<span class="number">10</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>细化学习曲线，找出降维后的最佳维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>,<span class="number">25</span>):</span><br><span class="line">    X_dr = PCA(i).fit_transform(X)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_dr,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">10</span>,<span class="number">25</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>导入找出的最佳维度进行降维，查看模型效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_dr=PCA(<span class="number">23</span>).fit_transform(X)</span><br><span class="line">print(cross_val_score(RFC(n_estimators=<span class="number">100</span>,random_state=<span class="number">0</span>),x_dr,y,cv=<span class="number">5</span>).mean())</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更快，效果更好。对图像来说，维度就是图像中特征向量的数量。&lt;/p&gt;
&lt;h2 id=&quot;sklearn中的降维算法&quot;&gt;&lt;a href=&quot;#sklearn中的降维算法&quot; class=&quot;headerli
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>随机森林</title>
    <link href="http://yoursite.com/2019/09/08/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://yoursite.com/2019/09/08/随机森林/</id>
    <published>2019-09-08T08:10:06.000Z</published>
    <updated>2019-09-15T06:31:00.857Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成算法概述"><a href="#集成算法概述" class="headerlink" title="集成算法概述"></a>集成算法概述</h2><p>集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型建模结果。基本上所有的机器学习领域都可以看到集成学习的身影。</p><p>集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。</p><p>多个模型集成称为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器(base estimator)。通常来说，有三类集成算法：装袋法(Bagging)、提升法(Boosting)、stacking</p><p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结<br>果。装袋法的代表模型就是随机森林。<br>提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本<br>进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树 </p><h2 id="sklearn中的集成算法"><a href="#sklearn中的集成算法" class="headerlink" title="sklearn中的集成算法"></a>sklearn中的集成算法</h2><p>sklearn集成算法模块ensemble</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest1.png" alt></p><h2 id="分类树参数、属性和接口"><a href="#分类树参数、属性和接口" class="headerlink" title="分类树参数、属性和接口"></a>分类树参数、属性和接口</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest2.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h3 id="n-estimators"><a href="#n-estimators" class="headerlink" title="n_estimators"></a>n_estimators</h3><p>这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree5.png" alt></p><h3 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h3><p>随机森林的本质是一种装袋集成算法(bagging)，装袋集成算法是对基评估器的预测结果进行平均或用多数表决元则来决定集成评估器的结果。决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个功能由参数random_state控制。随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树。</p><h4 id="bootstrap-amp-oob-score"><a href="#bootstrap-amp-oob-score" class="headerlink" title="bootstrap&amp;oob_score"></a>bootstrap&amp;oob_score</h4><p>要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。<br>在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。<br><strong>bootstrap参数默认True，代表采用这种有放回的随机抽样技术</strong>。通常，这个参数不会被我们设置为False</p><p>如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果。</p><p>参数的详细解释和其它控制基评估器的参数请参考<a href="[https://brickexperts.github.io/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91/#more](https://brickexperts.github.io/2019/09/07/决策树/#more)">决策树</a></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>随机森林中有三个非常重要的属性：.estimators_，.oob_score_以及.feature_importances_。<br>.estimators_是用来查看随机森林中所有树的列表的。<br>.oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度。<br>而.feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性 </p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。</p><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest8.png" alt></p><p>所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致。 </p><h3 id="criterion："><a href="#criterion：" class="headerlink" title="criterion："></a>criterion：</h3><p>回归树衡量分枝质量的指标，支持的标准有三种：<br>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree1.png" alt></p><p>其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree2.png" alt></p><p>其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 </p><h2 id="随机森林coding："><a href="#随机森林coding：" class="headerlink" title="随机森林coding："></a>随机森林coding：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line">wine = load_wine()</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=<span class="number">0.3</span>)</span><br><span class="line">clf = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">rfc = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">clf = clf.fit(Xtrain,Ytrain)</span><br><span class="line">rfc = rfc.fit(Xtrain,Ytrain)</span><br><span class="line">score_c = clf.score(Xtest,Ytest)</span><br><span class="line">score_r = rfc.score(Xtest,Ytest)</span><br><span class="line">print(<span class="string">"Single Tree:&#123;&#125;"</span>.format(score_c),<span class="string">"Random Forest:&#123;&#125;"</span>.format(score_r))</span><br><span class="line"><span class="comment">#画出随机森林和决策树一组交叉验证的对比</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">25</span>)</span><br><span class="line">rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=<span class="number">10</span>)</span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf_s = cross_val_score(clf,wine.data,wine.target,cv=<span class="number">10</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),rfc_s,label = <span class="string">"RandomForest"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),clf_s,label = <span class="string">"Decision Tree"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#画出随机森林和决策树十组交叉验证的对比</span></span><br><span class="line">rfc_l = []</span><br><span class="line">clf_l = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=<span class="number">25</span>)</span><br><span class="line">    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    rfc_l.append(rfc_s)</span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line">    clf_s = cross_val_score(clf,wine.data,wine.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    clf_l.append(clf_s)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),rfc_l,label = <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),clf_l,label = <span class="string">"Decision Tree"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML38.png" alt></p><h2 id="机器学习中调参的基本思想"><a href="#机器学习中调参的基本思想" class="headerlink" title="机器学习中调参的基本思想"></a>机器学习中调参的基本思想</h2><h3 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h3><p>在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error） </p><p>当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果<br>不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，<br>当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力<br>就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest3.png" alt></p><p>1）模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点<br>2）模型太复杂就会过拟合，模型太简单就会欠拟合<br>3）对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂<br>4）树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest12.png" alt></p><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。<br>偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。<br>方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest4.png" alt></p><p>方差和偏差对模型的影响：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt></p><p>然而，方差和偏差是此消彼长的，不可能同时达到最小值 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest6.png" alt></p><p>从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。<br>相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。 </p><p>我们调参的目标是，达到方差和偏差的完美平衡 ！</p><h2 id="随机森林的调参"><a href="#随机森林的调参" class="headerlink" title="随机森林的调参"></a>随机森林的调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV,cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">rfc=RandomForestClassifier(n_estimators=<span class="number">100</span>,random_state=<span class="number">90</span>)</span><br><span class="line">score_pre=cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">scorel = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">200</span>,<span class="number">10</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i+<span class="number">1</span>,</span><br><span class="line">n_jobs=<span class="number">-1</span>,</span><br><span class="line">random_state=<span class="number">90</span>)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="调n-estimators"><a href="#调n-estimators" class="headerlink" title="调n_estimators"></a>调n_estimators</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从曲线看，n_estimators较平稳且准确率高的范围在35-45之间</span></span><br><span class="line">scorel = []</span><br><span class="line"><span class="comment">#在划分好的范围内，继续细化学习曲线</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">35</span>,<span class="number">45</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i,</span><br><span class="line">    n_jobs=<span class="number">-1</span>,</span><br><span class="line">    random_state=<span class="number">90</span>)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line"><span class="comment">#得出最高准确率的n_estimators，为39</span></span><br><span class="line">print(max(scorel),([*range(<span class="number">35</span>,<span class="number">45</span>)][scorel.index(max(scorel))]))</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">35</span>,<span class="number">45</span>),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="调整max-depth"><a href="#调整max-depth" class="headerlink" title="调整max_depth"></a>调整max_depth</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>)</span><br><span class="line">param_grid=&#123;<span class="string">"max_depth"</span>:np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)&#125;</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line"><span class="comment">#得出最好的深度</span></span><br><span class="line">print(<span class="string">"参数为："</span>,GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><h3 id="调整max-feature"><a href="#调整max-feature" class="headerlink" title="调整max_feature"></a>调整max_feature</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc=RandomForestClassifier(n_estimators=<span class="number">39</span>,max_depth=<span class="number">11</span>,random_state=<span class="number">90</span>)</span><br><span class="line">param_grid=&#123;<span class="string">"max_features"</span>:np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)&#125;</span><br><span class="line">GS=GridSearchCV(rfc,param_grid=param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line"><span class="comment">#得出最好的max_feature</span></span><br><span class="line">print(<span class="string">"参数为："</span>,GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：在这步的max_features升高之后，模型的准确率却没有变化。说明模型本身已经处于泛化误差最低点，已经达到了模型的预测上限，没有参数可以左右的部分了。剩下的那些误差，是噪声决定的，已经没有方差和偏差的舞台了。如果是现实案例，我们到这一步其实就可以停下了，因为复杂度和泛化误差的关系已经告诉我们，模型不能再进步了。调参和训练模型都需要很长的时间，明知道模型不能进步了还继续调整，不是一个有效率的做法。如果我们希望模型更进一步，我们会选择更换算法，或者更换做数据预处理的方式 。但我让我们的探究继续。ps：我不要你觉得，我要我觉得</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_grid=&#123;<span class="string">"min_samples_leaf"</span>:np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,max_depth=<span class="number">11</span>,random_state=<span class="number">90</span>)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>这步后的准确率还是没有变化，不要改变参数，让它默认就好</p><h3 id="调整min-samples-split"><a href="#调整min-samples-split" class="headerlink" title="调整min_samples_split"></a>调整min_samples_split</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">param_grid=&#123;<span class="string">'min_samples_split'</span>:np.arange(<span class="number">2</span>, <span class="number">2</span>+<span class="number">20</span>, <span class="number">1</span>)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>,max_depth=<span class="number">11</span></span><br><span class="line">)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>还是没有变化</p><h3 id="调整criterion"><a href="#调整criterion" class="headerlink" title="调整criterion"></a>调整criterion</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123;<span class="string">'criterion'</span>:[<span class="string">'gini'</span>, <span class="string">'entropy'</span>]&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>,max_depth=<span class="number">11</span>)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>在整个调参过程之中，我们首先调整了n_estimators（无论如何都请先走这一步），然后调整max_depth，通max_depth产生的结果，来判断模型位于复杂度-泛化误差图像的哪一边，从而选择我们应该调整的参数和调参的方向。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;集成算法概述&quot;&gt;&lt;a href=&quot;#集成算法概述&quot; class=&quot;headerlink&quot; title=&quot;集成算法概述&quot;&gt;&lt;/a&gt;集成算法概述&lt;/h2&gt;&lt;p&gt;集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据库的数据模型</title>
    <link href="http://yoursite.com/2019/09/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2019/09/07/数据库的数据模型/</id>
    <published>2019-09-07T11:17:44.000Z</published>
    <updated>2019-09-15T06:30:05.788Z</updated>
    
    <content type="html"><![CDATA[<p>建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。</p><p>由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求：</p><p>1、比较真实地描述现实世界</p><p>2、易为用户所理解</p><p>3、易于在计算机上实现</p><p><strong>为什么需要数据模型？</strong></p><p>由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。</p><p><strong>数据模型含有哪些内容？</strong></p><p>数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。</p><p>​    1、数据结构</p><p>​         用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面</p><p>​    2、数据操作</p><p>​        用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查</p><p>​    3、数据的约束条件</p><p>​        是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。</p><p><strong>实体联系数据模型的地位与作用：</strong></p><p>实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。</p><p>数据模型是用来描述数据的一组概念和定义，是描述数据的手段。</p><p>​    概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。</p><p>​    逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模</p><p>​    物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。</p><p>逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。</p><p>数据模式是对数据结构、联系和约束的描述。<strong>数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。</strong></p><p>信息世界中的基本概念：</p><p>​    (1)  实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。</p><p>​    (2)  属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画</p><p>​    (3)  键(Key)或称为码：唯一标识实体的属性集称为码</p><p>​    (4)  域(Domain)：属性的取值范围称为该属性的域</p><p>​    (5)  实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型</p><p>​    (6)  实体集(Entity Set)：同一类型实体的集合称为实体集</p><p>​    (7)  联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。</p><p>概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。</p><p>​          实体型：用矩形表示，矩形框内写明实体名</p><p>​           属性：用椭圆表示，并用无向边将其与相应的实体连接起来。</p><p>​           联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n）</p><p>​           键：用下划线表示。</p><p>最常用的数据模型</p><p>​    非关系模型</p><p>​        层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。</p><p>​    满足以下两个条件称为层次模型：</p><p>​        (1)  有且仅有一个结点无双亲。这个结点称为“根节点”</p><p>​        (2)  其它节点有且仅有一个双亲，但可以有多个后继</p><p>​        网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。</p><p>​            网状结构特点：</p><p>​                 (1) 允许一个以上的结点无双亲；</p><p>​                 (2)  一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。</p><p>​    关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。</p><p>​        关系数据模型的数据结构：</p><p>​            关系(Relation)：一个关系对应通常说的一张表</p><p>​            元祖(Tuple)：表中的一行即为一个元祖</p><p>​            属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。</p><p>​            主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。</p><p>​            域(Domain)：属性的取值范围</p><p>​            分量：元祖中的一个属性值</p><p>​            关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n)</p><p>​    面向对象模型：</p><p>​    对象关系模型</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。&lt;/p&gt;
&lt;p&gt;由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>模型的评判和调优</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2019/09/07/模型的评判和调优/</id>
    <published>2019-09-07T08:11:22.000Z</published>
    <updated>2019-09-15T06:30:48.827Z</updated>
    
    <content type="html"><![CDATA[<h2 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt></p><p>分类模型评估API：F1-score,反应了模型的稳健性</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML53.png" alt></p><h2 id="模型选择和调优"><a href="#模型选择和调优" class="headerlink" title="模型选择和调优"></a>模型选择和调优</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证为了让被评估的模型更加准确可信</p><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。</p><p>sklearn.model_Selection.GridSearchCV</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML54.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML55.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;精确率和召回率&quot;&gt;&lt;a href=&quot;#精确率和召回率&quot; class=&quot;headerlink&quot; title=&quot;精确率和召回率&quot;&gt;&lt;/a&gt;精确率和召回率&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bricke
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K-means</title>
    <link href="http://yoursite.com/2019/09/07/K-means/"/>
    <id>http://yoursite.com/2019/09/07/K-means/</id>
    <published>2019-09-07T08:09:47.000Z</published>
    <updated>2019-09-15T06:40:32.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习与聚类算法"><a href="#无监督学习与聚类算法" class="headerlink" title="无监督学习与聚类算法"></a>无监督学习与聚类算法</h2><p>有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。而聚类算法，就是无监督学习的代表算法。 聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。</p><p>聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要实例化，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML110.png" alt></p><h2 id="KMeans是如何工作的"><a href="#KMeans是如何工作的" class="headerlink" title="KMeans是如何工作的"></a>KMeans是如何工作的</h2><p><strong>关键概念</strong>：簇与质心</p><p>​    簇：KMeans算法将一组N个样本的特征矩阵x划分为k个无交集的簇，直观上来看是簇是一组组聚集在一起的数据，在一个簇中的数据被认为是同一类。簇就是聚类的结果表现。</p><p>​    质心：簇中所有数据的均值通常被称为这个簇的质心。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点纵坐标的均值。同理可推广到高维空间。</p><p>在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下 ：</p><p>​        1、随机抽取k个样本作为最初的质心</p><p>​        2、开始循环</p><p>​                2.1、将每个样本点分配到离他们最近的质心，生成k个簇</p><p>​                2.2、对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心</p><p>​        3、当质心的位置不在发生变化，迭代停止，聚类完成</p><p>那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。</p><p>对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距<br>离的衡量方法有多种，令x表示簇中的一个样本点，ui表示该簇中的质心，n表示每个样本点中的特征数目，i表示组<br>成点x的每个特征，则该样本点到质心的距离可以由以下距离来度量：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML111.png" alt></p><p>如我们采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML112.png" alt></p><p>其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum ofSquare），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。因此,KMeans追求的是，求解能够让Inertia最小化的质心。实际上，在质心不断变化不断迭代的过程中，总体平方和是越来越小的。我们可以使用数学来证明，当整体平方和最小的时候，质心就不再发生变化了。如此，K-Means的求解过程，就变成了一个最优化问题 </p><p>损失函数本质是用来衡量模型的拟合效果的，只有有着求解参数需求的算法，才会有损失函数。Kmeans不求解什么参数，它的模型本质也没有在拟合数据，而是在对数据进行一种探索。所以，<strong>K-Means不存在什么损失函数</strong>。Inertia更像是Kmeans的模型评估指标，而非损失函数。</p><h2 id="重要参数和属性"><a href="#重要参数和属性" class="headerlink" title="重要参数和属性"></a>重要参数和属性</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML113.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果。通常，在开始聚类之前，我们并不知道n_clusters究竟是多少，因此我们要对它进行探索。</p><p>在之前描述K-Means的基本流程时我们提到过，当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前， 我们也可以使用max_iter，大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下 来。有时候，当我们的n_clusters选择不符合数据的自然分布，或者我们为了业务需求，必须要填入与数据的自然 分布不合的n_clusters，提前让迭代停下来反而能够提升模型的表现。</p><p>max_iter：整数，默认300，单次运行的k-means算法的大迭代次数<br>tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML118.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML119.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>labels_：查看聚好的类别，每个样本所对应的类</p><p>cluster_centers_：查看质心</p><p>inertia_：查看总距离平方和</p><p>n_iter_：实际的迭代次数</p><p>coding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#这是自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数</span></span><br><span class="line"><span class="comment">#random_state代表随机性</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line">ax1.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]<span class="comment">#画点图</span></span><br><span class="line">,marker=<span class="string">'o'</span><span class="comment">#代表点的形状</span></span><br><span class="line">,s=<span class="number">8</span>)<span class="comment">#代表点的大小</span></span><br><span class="line"><span class="comment">#最开始数据集的形状</span></span><br><span class="line">plt.show()</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    ax1.scatter(X[y==i, <span class="number">0</span>], X[y==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="comment">#簇为3</span></span><br><span class="line">n_clusters = <span class="number">3</span></span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="comment">#查看聚好的列表</span></span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line">pre = cluster.fit_predict(X)</span><br><span class="line"><span class="comment">#查看质心</span></span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"><span class="comment">#查看总距离平方和</span></span><br><span class="line">inertia = cluster.inertia_</span><br><span class="line">print(inertia)</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ax1.scatter(X[y_pred==i, <span class="number">0</span>], X[y_pred==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line"><span class="comment">#画出质心</span></span><br><span class="line">ax1.scatter(centroid[:,<span class="number">0</span>],centroid[:,<span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">"x"</span></span><br><span class="line">,s=<span class="number">15</span></span><br><span class="line">,c=<span class="string">"black"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#簇为4</span></span><br><span class="line">n_clusters=<span class="number">4</span></span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="comment">#查看聚好的列表</span></span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line">pre = cluster.fit_predict(X)</span><br><span class="line"><span class="comment">#查看质心</span></span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"><span class="comment">#查看总距离平方和</span></span><br><span class="line">inertia = cluster.inertia_</span><br><span class="line">print(inertia)</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ax1.scatter(X[y_pred==i, <span class="number">0</span>], X[y_pred==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line"><span class="comment">#画出质心</span></span><br><span class="line">ax1.scatter(centroid[:,<span class="number">0</span>],centroid[:,<span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">"x"</span></span><br><span class="line">,s=<span class="number">15</span></span><br><span class="line">,c=<span class="string">"black"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="聚类算法的模型评估"><a href="#聚类算法的模型评估" class="headerlink" title="聚类算法的模型评估"></a>聚类算法的模型评估</h2><p>上面的算法的簇随着数字的增大，总的距离和在不断的变小。难道我们我们就这样实验得出簇的个数吗？难道簇越小越好吗？看看算法的评估。聚类模型的结果不是某种标签输出，并且聚类的结果是不确定的，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？ KMeans的目标是确保“簇内差异小，簇外差异大”，我们就可以通过衡量簇内差异来衡量聚类的效果。我们刚才说过，Inertia是用距离来衡量簇内差异的指标，因此，我们是否可以使用Inertia来作为聚类的衡量指标呢？Inertia越小模型越好嘛。可以，但是这个指标的缺点和极限太大。</p><h3 id="当真实标签未知的时候使用轮廓系数"><a href="#当真实标签未知的时候使用轮廓系数" class="headerlink" title="当真实标签未知的时候使用轮廓系数"></a>当真实标签未知的时候使用轮廓系数</h3><p>这样的聚类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中<strong>轮廓系数</strong>是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：<br>1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离<br>2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。单个样本的轮廓系数计算为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML114.png" alt></p><p>这个公式可以看作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML115.png" alt></p><p>很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。</p><p>在sklearn中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML51.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line">print(<span class="string">"4个簇的时候的轮廓系数："</span>,silhouette_score(X,y_pred))</span><br><span class="line">print(<span class="string">"四个簇的每个样本的轮廓系数："</span>,silhouette_samples(X,y_pred))</span><br></pre></td></tr></table></figure><h3 id="当真实标签未知的时候用CHI"><a href="#当真实标签未知的时候用CHI" class="headerlink" title="当真实标签未知的时候用CHI"></a>当真实标签未知的时候用CHI</h3><p>除了轮廓系数是常用的，我们还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML116.png" alt></p><p>在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言，Calinski-Harabaz指数s(k)写作如下公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML117.png" alt></p><p>其中N为数据集中的样本量，k为簇的个数(即类别的个数)，Bk是组间离散矩阵，即不同簇之间的协方差矩阵， Wk是簇内离散矩阵，即一个簇内数据的协方差矩阵，而tr表示矩阵的迹。在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A) 。<strong>数据之间的离散程度越高，协方差矩阵的迹就会越大</strong>。组内离散程度低，协方差的迹就会越小，Tr(Wk)也就越小，同时，组间离散程度大，协方差的的迹也会越大， Tr(Bk)就越大，这正是我们希望的，因此Calinski-harabaz指数越高越好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> calinski_harabaz_score</span><br><span class="line">print(calinski_harabaz_score(X, y_pred))</span><br></pre></td></tr></table></figure><h2 id="基于轮廓系数选择簇的个数"><a href="#基于轮廓系数选择簇的个数" class="headerlink" title="基于轮廓系数选择簇的个数"></a>基于轮廓系数选择簇的个数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数</span></span><br><span class="line"><span class="comment">#random_state代表随机性</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">n_clusters = <span class="number">4</span></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</span><br><span class="line">ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</span><br><span class="line">ax1.set_ylim([<span class="number">0</span>, X.shape[<span class="number">0</span>] + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</span><br><span class="line">clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>).fit(X)</span><br><span class="line">cluster_labels = clusterer.labels_</span><br><span class="line">silhouette_avg = silhouette_score(X, cluster_labels)</span><br><span class="line">print(<span class="string">"For n_clusters ="</span>, n_clusters,</span><br><span class="line"><span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</span><br><span class="line">sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">y_lower = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]</span><br><span class="line">    ith_cluster_silhouette_values.sort()</span><br><span class="line">    size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</span><br><span class="line">    y_upper = y_lower + size_cluster_i</span><br><span class="line">    color = cm.nipy_spectral(float(i)/n_clusters)</span><br><span class="line">    ax1.fill_betweenx(np.arange(y_lower, y_upper)</span><br><span class="line">    ,ith_cluster_silhouette_values</span><br><span class="line">    ,facecolor=color</span><br><span class="line">    ,alpha=<span class="number">0.7</span>)</span><br><span class="line">    ax1.text(<span class="number">-0.05</span></span><br><span class="line">    , y_lower + <span class="number">0.5</span> * size_cluster_i</span><br><span class="line">    , str(i))</span><br><span class="line">    y_lower = y_upper + <span class="number">10</span></span><br><span class="line">ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cluster label"</span>)</span><br><span class="line">ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">ax1.set_yticks([])</span><br><span class="line">ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)</span><br><span class="line">ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">'o'</span> <span class="comment">#点的形状</span></span><br><span class="line">,s=<span class="number">8</span> <span class="comment">#点的大小</span></span><br><span class="line">,c=colors)</span><br><span class="line">centers = clusterer.cluster_centers_</span><br><span class="line">ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], marker=<span class="string">'x'</span>,</span><br><span class="line">c=<span class="string">"red"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</span><br><span class="line">ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</span><br><span class="line">plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></span><br><span class="line"><span class="string">"with n_clusters = %d"</span> % n_clusters),</span><br><span class="line">fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]:</span><br><span class="line">    n_clusters = n_clusters</span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</span><br><span class="line">    ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</span><br><span class="line">    ax1.set_ylim([<span class="number">0</span>, X.shape[<span class="number">0</span>] + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</span><br><span class="line">    clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>).fit(X)</span><br><span class="line">    cluster_labels = clusterer.labels_</span><br><span class="line">    silhouette_avg = silhouette_score(X, cluster_labels)</span><br><span class="line">    print(<span class="string">"For n_clusters ="</span>, n_clusters,</span><br><span class="line">    <span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</span><br><span class="line">    sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">    y_lower = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]</span><br><span class="line">        ith_cluster_silhouette_values.sort()</span><br><span class="line">        size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</span><br><span class="line">        y_upper = y_lower + size_cluster_i</span><br><span class="line">        color = cm.nipy_spectral(float(i)/n_clusters)</span><br><span class="line">        ax1.fill_betweenx(np.arange(y_lower, y_upper)</span><br><span class="line">        ,ith_cluster_silhouette_values</span><br><span class="line">        ,facecolor=color</span><br><span class="line">        ,alpha=<span class="number">0.7</span></span><br><span class="line">        )</span><br><span class="line">        ax1.text(<span class="number">-0.05</span></span><br><span class="line">        , y_lower + <span class="number">0.5</span> * size_cluster_i</span><br><span class="line">        , str(i))</span><br><span class="line">        y_lower = y_upper + <span class="number">10</span></span><br><span class="line">ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cluster label"</span>)</span><br><span class="line">ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">ax1.set_yticks([])</span><br><span class="line">ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)</span><br><span class="line">ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">'o'</span> <span class="comment">#点的形状</span></span><br><span class="line">,s=<span class="number">8</span> <span class="comment">#点的大小</span></span><br><span class="line">,c=colors</span><br><span class="line">)</span><br><span class="line">centers = clusterer.cluster_centers_</span><br><span class="line">ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], marker=<span class="string">'x'</span>,</span><br><span class="line">        c=<span class="string">"red"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</span><br><span class="line">ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</span><br><span class="line">plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></span><br><span class="line"><span class="string">"with n_clusters = %d"</span> % n_clusters),</span><br><span class="line">fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无监督学习与聚类算法&quot;&gt;&lt;a href=&quot;#无监督学习与聚类算法&quot; class=&quot;headerlink&quot; title=&quot;无监督学习与聚类算法&quot;&gt;&lt;/a&gt;无监督学习与聚类算法&lt;/h2&gt;&lt;p&gt;有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/2019/09/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/逻辑回归/</id>
    <published>2019-09-07T08:09:32.000Z</published>
    <updated>2019-09-15T06:31:11.837Z</updated>
    
    <content type="html"><![CDATA[<h2 id="逻辑回归推导过程"><a href="#逻辑回归推导过程" class="headerlink" title="逻辑回归推导过程"></a>逻辑回归推导过程</h2><p>逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以下线性回归。线性回归写作一个人人熟悉的方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic1.png" alt></p><p>&theta;被统称为模型的参数，其中&theta;被称为截距。&theta;1——&theta;n被称为系数。我们可以用矩阵表示这个方程，其中x和&theta;都可以被看作是一个列矩阵：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic2.png" alt></p><p>线性回归的任务，就是构造一个预测函数z来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心<br>就是找出模型的参数：&theta;的转置矩阵和&theta;0，著名的最小二乘法就是用来求解线性回归中参数的数学方法。</p><p>通过函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务<br>（比如预测产品销量，预测股价等等）。那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型<br>变量，我们要怎么办呢？我们可以通过引入<strong>联系函数(link function)</strong>，将线性回归方程z变换为g(z)，并且令g(z)的值<br>分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分<br>类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic3.png" alt></p><p><strong>Sigmoid函数的公式和性质</strong>：Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1。</p><p>将z带入Sigmoid，得到二元逻辑回归的一般形式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic4.png" alt></p><p>而g(z)就是我们逻辑回归返回的标签值。此时,y(x)的取值都在[0,1]之间，因此 y(x)和1-y(x)相加必然为1。令y(x)除以1-y(x)可以得到形似几率得y(x)/(1-y(x))，在此基础上取对数，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic5.png" alt></p><p>不难发现，g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。</p><h2 id="逻辑回归的类"><a href="#逻辑回归的类" class="headerlink" title="逻辑回归的类"></a>逻辑回归的类</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic6.png" alt></p><p>逻辑回归有着基于训练数据求解参数&theta;的需求，并且希望训练出来的模型能够尽可能地拟合训练数据，即模型在训练集上的预测准确率越靠近100%越好。因此，我们使用”损失函数“这个评估指标，来衡量参数&theta;的优劣，即这一组参数能否使模型在训练集上表现优异。</p><p>​    <strong>关键概念</strong>：损失函数，衡量参数&theta;的优劣的评估指标，用来求解最优参数的工具损失函数小，模型在训练集上表现优异，拟合充分，参数优秀损失函数大，模型在训练集上表现差劲，拟合不足，参数糟糕我们追求，能够让损失函数最小化的参数组合。<strong>注意：没有”求解参数“需求的模型没有损失函数，比如KNN，决策树。</strong> </p><p>逻辑回归的损失函数是由最大似然法来推导出来的，具体结果可以写作 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic8.png" alt></p><p>&theta;表示求解出来的一组参数，m是样本的个数，yi是样本i上真实的标签。y&theta;(xi)是样本i上，基于参数&theta;计算出来的逻辑回归返回值，xi是样本i的取值。我们的目标，就是求解出使J(&theta;)最小的&theta;的取值。</p><p>由于我们追求损失函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题：如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。虽然逻辑回归和线性回归是天生欠拟合的模型，但我们还是需要控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。</p><h2 id="linear-model-LogisticRegression"><a href="#linear-model-LogisticRegression" class="headerlink" title="linear_model.LogisticRegression"></a>linear_model.LogisticRegression</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic7.png" alt></p><h3 id="正则化重要参数"><a href="#正则化重要参数" class="headerlink" title="正则化重要参数"></a>正则化重要参数</h3><p>正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic9.png" alt></p><p>其中J(&theta;)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数。j代表每个参数。在这里，J要大于等于1，因为我们的参数向量&theta;中，第一个参数是&theta;0，使我们的截距。它通常不参与正则化。上面的式子还有另一种写法，本质是一样的。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic15.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic10.png" alt></p><p>L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），参数 &theta;的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。</p><h3 id="multi-class"><a href="#multi-class" class="headerlink" title="multi_class"></a>multi_class</h3><p>输入”ovr”, “multinomial”, “auto”来告知模型，我们要处理的分类问题的类型。默认是”ovr”。</p><p>‘ovr’(one-vs-rest)：表示分类问题是二分类，或让模型使用”一对多”的形式来处理多分类问题。</p><p>‘multinomial’：表示处理多分类问题，这种输入在参数solver是’liblinear’时不可用。</p><p>“auto”：表示会根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型。比如说，如果数据是二分类，或者solver的取值为”liblinear”，”auto”会默认选择”ovr”。反之，则会选择”nultinomial”。<br><strong>注意：默认值将在0.22版本中从”ovr”更改为”auto”。</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="keyword">for</span> multi_class <span class="keyword">in</span> [<span class="string">'multinomial'</span>, <span class="string">'ovr'</span>]:</span><br><span class="line">    clf = LogisticRegression(solver=<span class="string">'sag'</span>, max_iter=<span class="number">100</span>, random_state=<span class="number">42</span>,</span><br><span class="line">    multi_class=multi_class).fit(iris.data, iris.target)</span><br><span class="line"><span class="comment">#打印两种multi_class模式下的训练分数</span></span><br><span class="line">    print(<span class="string">"training score : %.3f (%s)"</span> % (clf.score(iris.data, iris.target),multi_class))</span><br></pre></td></tr></table></figure><h3 id="solver"><a href="#solver" class="headerlink" title="solver"></a>solver</h3><p>对于小数据集，‘liblinear’是一个不错的选择，而’sag’和’saga’对于大数据集来说更快。对于多类问题，只有’newton-cg’，‘sag’，’saga’和’lbfgs’处理多项损失。‘newton-cg’，’lbfgs’和’sag’只处理L2 penalty，而’liblinear’和’saga’处理L1 penalty。</p><h3 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic19.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic20.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic21.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic22.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic23.png" alt></p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic24.png" alt></p><h2 id="逻辑回归的特征工程"><a href="#逻辑回归的特征工程" class="headerlink" title="逻辑回归的特征工程"></a>逻辑回归的特征工程</h2><h3 id="高效的embedded嵌入法"><a href="#高效的embedded嵌入法" class="headerlink" title="高效的embedded嵌入法"></a>高效的embedded嵌入法</h3><p>我们已经说明了，由于L1正则化会使得部分特征对应的参数为0，因此L1正则化可以用来做特征选择，结合嵌入法的模块SelectFromModel，我们可以很容易就筛选出让模型十分高效的特征。注意，此时我们的目的是，尽量保留原数据上的信息，让模型在降维后的数据上的拟合效果保持优秀，因此我们不考虑训练集测试集的问题，把所有的数据都放入模型进行降维。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">print(data.data.shape)</span><br><span class="line">LR_ = LR(solver=<span class="string">"liblinear"</span>,C=<span class="number">0.9</span>,random_state=<span class="number">420</span>)</span><br><span class="line">print(cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">print(X_embedded.shape)</span><br><span class="line">print(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br></pre></td></tr></table></figure><p>看看结果，特征数量被减小到个位数，并且模型的效果却没有下降太多，如果我们要求不高，在这里其实就可以停下了。但是，能否让模型的拟合效果更好呢？在这里，我们有两种调整方式：<br>1）调节SelectFromModel这个类中的参数threshold，这是嵌入法的阈值，表示删除所有参数的绝对值低于这个阈值的特征。现在threshold默认为None，所以SelectFromModel只根据L1正则化的结果来选择了特征，即选择了所有L1正则化后参数不为0的特征。我们此时，只要调整threshold的值（画出threshold的学习曲线），就可以观察不同的threshold下模型的效果如何变化。一旦调整threshold，就不是在使用L1正则化选择特征，而是使用模型的属性.coef_中生成的各个特征的系数来选择。coef_虽然返回的是特征的系数，但是系数的大小和决策树中的feature_ importances_以及降维算法中的可解释性方差explained_vairance_概念相似，其实都是衡量特征的重要程度和贡献度的，因此SelectFromModel中的参数threshold可以设置为coef_的阈值，即可以剔除系数小于threshold中输入的数字的所有特征。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">fullx = []</span><br><span class="line">fsx = []</span><br><span class="line">threshold = np.linspace(<span class="number">0</span>,abs((LR_.fit(data.data,data.target).coef_)).max(),<span class="number">20</span>)</span><br><span class="line">k=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> threshold:</span><br><span class="line">    X_embedded = SelectFromModel(LR_,threshold=i).fit_transform(data.data,data.target)</span><br><span class="line">    fullx.append(cross_val_score(LR_,data.data,data.target,cv=<span class="number">5</span>).mean())</span><br><span class="line">    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">5</span>).mean())</span><br><span class="line">    print((threshold[k],X_embedded.shape[<span class="number">1</span>]))</span><br><span class="line">    k+=<span class="number">1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(threshold,fullx,label=<span class="string">"full"</span>)</span><br><span class="line">plt.plot(threshold,fsx,label=<span class="string">"feature selection"</span>)</span><br><span class="line">plt.xticks(threshold)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>这种方法其实是比较无效的，大家可以用学习曲线来跑一跑：当threshold越来越大，被删除的特征越来越多，模型的效果也越来越差。<br>2）第二种调整方法，是调逻辑回归的类LR_，通过画C的学习曲线来实现： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">fullx = []</span><br><span class="line">fsx = []</span><br><span class="line">C=np.arange(<span class="number">0.01</span>,<span class="number">10.01</span>,<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> C:</span><br><span class="line">    LR_ = LR(solver=<span class="string">"liblinear"</span>,C=i,random_state=<span class="number">420</span>)</span><br><span class="line">    fullx.append(cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">    X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">print(<span class="string">"最好的准确率和对应的index："</span>,max(fsx),C[fsx.index(max(fsx))])</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(C,fullx,label=<span class="string">"full"</span>)</span><br><span class="line">plt.plot(C,fsx,label=<span class="string">"feature selection"</span>)</span><br><span class="line">plt.xticks(C)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#继续细化了学习曲线</span></span><br><span class="line">fullx = []</span><br><span class="line">fsx = []</span><br><span class="line">C=np.arange(<span class="number">6.05</span>,<span class="number">7.05</span>,<span class="number">0.005</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> C:</span><br><span class="line">    LR_ = LR(solver=<span class="string">"liblinear"</span>,C=i,random_state=<span class="number">420</span>)</span><br><span class="line">    fullx.append(cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">    X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">    fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">print(max(fsx),C[fsx.index(max(fsx))])</span><br><span class="line">plt.figure(figsize=(<span class="number">40</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(C,fullx,label=<span class="string">"full"</span>)</span><br><span class="line">plt.plot(C,fsx,label=<span class="string">"feature selection"</span>)</span><br><span class="line">plt.xticks(C)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#验证模型效果：降维之前</span></span><br><span class="line">LR_ = LR(solver=<span class="string">"liblinear"</span>,C=<span class="number">6.069999999999999</span>,random_state=<span class="number">420</span>)</span><br><span class="line">print(<span class="string">"降维之前的模型效果："</span>,cross_val_score(LR_,data.data,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line"><span class="comment">#验证模型效果：降维之后</span></span><br><span class="line">LR_ = LR(solver=<span class="string">"liblinear"</span>,C=<span class="number">6.069999999999999</span>,random_state=<span class="number">420</span>)</span><br><span class="line">X_embedded = SelectFromModel(LR_,norm_order=<span class="number">1</span>).fit_transform(data.data,data.target)</span><br><span class="line">print(<span class="string">"降维之后的模型效果："</span>,cross_val_score(LR_,X_embedded,data.target,cv=<span class="number">10</span>).mean())</span><br><span class="line">print(X_embedded.shape)</span><br></pre></td></tr></table></figure><h3 id="系数累加法"><a href="#系数累加法" class="headerlink" title="系数累加法"></a>系数累加法</h3><p>系数累加法的原理非常简单。在PCA中，我们通过绘制累积可解释方差贡献率曲线来选择超参数，在逻辑回归中我们可以使用系数coef_来这样做，并且我们选择特征个数的逻辑也是类似的：找出曲线由锐利变平滑的转折点，转折点之前被累加的特征都是我们需要的，转折点之后的我们都不需要。不过这种方法相对比较麻烦，因为我们要先对特征系数进行从大到小的排序，还要确保我们知道排序后的每个系数对应的原始特征的位置，才能够正确找出那些重要的特征。如果要使用这样的方法，不如直接使用嵌入法来得方便。</p><h3 id="包装法"><a href="#包装法" class="headerlink" title="包装法"></a>包装法</h3><p>相对的，包装法可以直接设定我们需要的特征个数。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>之前提到过，逻辑回归的数学目的是求解能够让模型最优化的参数&theta;的值，即求解能够让损失函数最小化的 的值，而这个求解过程，对于二元逻辑回归来说，有多种方法可以选择，最常见的有梯度下降法(Gradient Descent)，坐标轴下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic8.png" alt></p><p>以梯度下降法为例，我们来看看求解过程是如何完成的。下面这个华丽的平面就是我们的损失函数在输入了一组特征矩阵和标签之后在三维立体坐标系中的图像。现在，我们寻求的是损失函数的最小值，也就是图像的最低点（看起来像是深蓝色区域的某处），一旦我们获取了图像在最低点的取值J(&theta;)0，在我们的损失函数公式中，唯一未知的就是我们的参数向量&theta;。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic11.png" alt></p><p><strong>步长的概念与解惑</strong>：</p><p>许多博客和教材在描述步长的时候，声称它是”梯度下降中每一步沿梯度的反方向前进的长度“，”沿着最陡峭最易下山的位置走的那一步的长度“或者”梯度下降中每一步损失函数减小的量“，甚至有说，步长是二维平面著名的求导三角形中的”斜边“或者“对边”的。<strong>这些说法都是错误的</strong></p><p>请看下图，A(&theta;a,J(&theta;a))就是小球最初的位置，B(&theta;b,J(&theta;b))就是一次滚动后小球移动到的位置。从A到B的方向就是梯度向量的反方向，指向损失函数在A点下降最快的方向。而梯度向量的大小是点A在图像上对&theta;求导后的结果，也是点A切线方向的斜率，橙色角的tan结果，记作d。 梯度下降每走一步，损失函数减小的量，是损失函数在&theta;变化之后的取值的变化，写作J(&theta;b)-J(&theta;a)。梯度下降每走一步，参数变量的变化，写作&theta;a-&theta;b，根据我们参数向量的迭代公式，就是我们的步长<em>梯度向量的大小。记作&alpha;\</em>d，这是三角形的邻边。梯度下降中每走一步，也就是三角形中的斜边。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic16.png" alt></p><p><strong>所以，步长不是任何物理距离，它甚至不是梯度下降过程中任何距离的直接变化，它是梯度向量的大小d上的一个比例，影响着参数向量&theta;每次迭代后改变的部分。</strong> </p><p>不难发现，既然参数迭代是靠梯度向量的大小d * 步长&alpha;来实现的，而J(&theta;)的降低又是靠调节&theta;来实现的，所以步长可以调节损失函数下降的速率。在损失函数降低的方向上，步长越长,&theta;的变动就越大。相对的，步长如果很短，&theta;的每次变动就很小。具体地说，如果步长太大，损失函数下降得就非常快，需要的迭代次数就很少，但梯度下降过程可能跳过损失函数的最低点，无法获取最优值。而步长太小，虽然函数会逐渐逼近我们需要的最低点，但迭代的速度却很缓慢，迭代次数就需要很多。 </p><p>在彩色图中，小球在进入深蓝色区域后，并没有直接找到某个点，而是在深蓝色区域中来回震荡了数次才停下，这种”震荡“其实就是因为我们设置的步长太大的缘故。但是在我们开始梯度下降之前，我们并不知道什么样的步长才合适，但梯度下降一定要在某个时候停止才可以，否则模型可能会无限地迭代下去。因此，在sklearn当中，我们设置参数max_iter最大迭代次数来代替步长，帮助我们控制模型的迭代速度并适时地让模型停下。max_iter越大，代表步长越小，模型迭代时间越长，反之，则代表步长设置很大，模型迭代时间很短。 迭代结束，获取到J(&theta;)的最小值后，我们就可以找出这个最小值&theta;对应的参数向量 ，逻辑回归的预测函数也就可以根据这个参数向量&theta;来建立了。 </p><p>接下来的是max_iter的学习曲线coding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">l2 = []</span><br><span class="line">l2test = []</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>):</span><br><span class="line">    lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.9</span>,max_iter=i)</span><br><span class="line">    lrl2 = lrl2.fit(Xtrain,Ytrain)</span><br><span class="line">    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))</span><br><span class="line">    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))</span><br><span class="line">graph = [l2,l2test]</span><br><span class="line">color = [<span class="string">"black"</span>,<span class="string">"gray"</span>]</span><br><span class="line">label = [<span class="string">"L2"</span>,<span class="string">"L2test"</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(graph)):</span><br><span class="line">    plt.plot(np.arange(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>),graph[i],color[i],label=label[i])</span><br><span class="line">plt.legend(loc=<span class="number">4</span>)</span><br><span class="line">plt.xticks(np.arange(<span class="number">1</span>, <span class="number">201</span>, <span class="number">10</span>))</span><br><span class="line">plt.show()</span><br><span class="line">lr = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.9</span>,max_iter=<span class="number">300</span>).fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#.n_iter_来调用本次调用本次求解中真正实现的迭代次数</span></span><br><span class="line">print(lr.n_iter_)</span><br></pre></td></tr></table></figure><p>运行上面的代码。会弹出红色警告。因为max_iter中限制的步数已经走完了，逻辑回归却还没找到损失函数的最小值。参数&theta;还没有收敛，sklearn就会弹出警告：</p><p>当参数solver=”liblinear”：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic17.png" alt></p><p>当参数solver=“sag”：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic18.png" alt></p><p>虽然写法看起来略有不同，但其实都是一个含义，这是在提醒我们：参数没有收敛，请增大max_iter中输入的数字。但我们不一定要听sklearn的。max_iter很大，意味着步长小，模型运行得会更加缓慢。虽然我们在梯度下降中追求的是损失函数的最小值，但这也可能意味着我们的模型会过拟合（在训练集上表现得太好，在测试集上却不一定），因此，如果在max_iter报红条的情况下，模型的训练和预测效果都已经不错了，那我们就不需要再增大max_iter中的数目了，毕竟一切都以模型的预测效果为基准——只要最终的预测效果好，运行又快，那就一切都好，无所谓是否报红色警告了 </p><h2 id="coding"><a href="#coding" class="headerlink" title="coding"></a>coding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立两个逻辑回归，L1正则化和L2正则化。效果一目了然</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">data.data.shape</span><br><span class="line">lrl1 = LR(penalty=<span class="string">"l1"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.5</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line">lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.5</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line"><span class="comment">#逻辑回归的重要属性coef_，查看每个特征所对应的参数</span></span><br><span class="line">lrl1 = lrl1.fit(X,y)</span><br><span class="line">lrl1.coef_</span><br><span class="line">(lrl1.coef_ != <span class="number">0</span>).sum(axis=<span class="number">1</span>)</span><br><span class="line">lrl2 = lrl2.fit(X,y)</span><br><span class="line">lrl2.coef_</span><br><span class="line">l1 = []</span><br><span class="line">l2 = []</span><br><span class="line">l1test = []</span><br><span class="line">l2test = []</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0.05</span>,<span class="number">1</span>,<span class="number">19</span>):</span><br><span class="line">    lrl1 = LR(penalty=<span class="string">"l1"</span>,solver=<span class="string">"liblinear"</span>,C=i,max_iter=<span class="number">1000</span>)</span><br><span class="line">    lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=i,max_iter=<span class="number">1000</span>)</span><br><span class="line">    lrl1 = lrl1.fit(Xtrain,Ytrain)</span><br><span class="line">    l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain))</span><br><span class="line">    l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest))</span><br><span class="line">    lrl2 = lrl2.fit(Xtrain,Ytrain)</span><br><span class="line">    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))</span><br><span class="line">    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))</span><br><span class="line">graph = [l1,l2,l1test,l2test]</span><br><span class="line">color = [<span class="string">"green"</span>,<span class="string">"black"</span>,<span class="string">"lightgreen"</span>,<span class="string">"gray"</span>]</span><br><span class="line">label = [<span class="string">"L1"</span>,<span class="string">"L2"</span>,<span class="string">"L1test"</span>,<span class="string">"L2test"</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(graph)):</span><br><span class="line">    plt.plot(np.linspace(<span class="number">0.05</span>,<span class="number">1</span>,<span class="number">19</span>),graph[i],color[i],label=label[i])</span><br><span class="line">plt.legend(loc=<span class="number">4</span>) <span class="comment">#图例的位置在哪里?        4表示，右下角</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML49.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;逻辑回归推导过程&quot;&gt;&lt;a href=&quot;#逻辑回归推导过程&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归推导过程&quot;&gt;&lt;/a&gt;逻辑回归推导过程&lt;/h2&gt;&lt;p&gt;逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归</title>
    <link href="http://yoursite.com/2019/09/07/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/线性回归/</id>
    <published>2019-09-07T08:09:19.000Z</published>
    <updated>2019-09-15T06:31:22.569Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h2><p>回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向<br>量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目<br>标根本不是求解出标签，注意加以区别。</p><p>线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这<br>些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决<br>于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型<br>变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归<br>可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。</p><p>线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个<br>有 个特征的样本 而言，它的回归结果可以写作一个几乎人人熟悉的方程 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png" alt></p><p>W被统称为模型的参数。W0被称为截距W1-Wn被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 Xi1-Xin是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML82.png" alt></p><p>我们可以使用矩阵来表示这个方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML83.png" alt></p><p>y=Xw，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中w可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量w 。</p><p>在多元线性回归中，我们在损失函数如下定义：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML84.png" alt></p><p>因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML85.png" alt></p><p><strong>第一次看不明白上面红字，这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数</strong>。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。</p><p>现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对w求导。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML88.png" alt></p><p>我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML89.png" alt></p><h2 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h2><p>sklearn中的线性模型模块是linear_model。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML90.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML75.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML76.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML77.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML78.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML79.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML80.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML81.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML74.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML91.png" alt></p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML42.png" alt></p><h2 id="模型评估指标"><a href="#模型评估指标" class="headerlink" title="模型评估指标"></a>模型评估指标</h2><p>回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等<br>评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算<br>法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不<br>同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。<br>第一，我们是否预测到了正确的数值。<br>第二，我们是否拟合到了足够的信息。 </p><p>sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML43.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML45.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML44.png" alt></p><p>​            <strong>注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue=fch()</span><br><span class="line">xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">std_x=StandardScaler()</span><br><span class="line">xtrain=std_x.fit_transform(xtrain)</span><br><span class="line">xtest=std_x.transform(xtest)</span><br><span class="line">std_y=StandardScaler()</span><br><span class="line">ytrain=std_y.fit_transform(ytrain.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">ytest=std_y.transform(ytest.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">reg=LR().fit(xtrain,ytrain)</span><br><span class="line">ypredict=reg.predict(xtest)</span><br><span class="line">print(std_y.inverse_transform(ypredict))</span><br><span class="line">print(<span class="string">"均方误差："</span>,mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))</span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>这里如果我们运行上面的代码，会在第十九行报错：</p><p>我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评<br>判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，<br>会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，<br>所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是<br>neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助:</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML93.png" alt></p><p>在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。</p><p>R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种<br>是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两<br>种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"R2"</span>,r2_score(ypredict,ytest))</span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML94.png" alt></p><p>????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是<strong>预测值在分子，真实值在分母</strong>。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用r2_score</span></span><br><span class="line">print(<span class="string">"R2"</span>,r2_score(y_true=ytest,y_pred=ypredict))</span><br><span class="line"><span class="comment">#利用接口score</span></span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#EVS的两种调用方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> explained_variance_score <span class="keyword">as</span> evs</span><br><span class="line"><span class="comment">#第一种</span></span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"explained_variance"</span>))</span><br><span class="line"><span class="comment">#第二种</span></span><br><span class="line">print(<span class="string">"evs"</span>,evs(ytest,ypredict))</span><br></pre></td></tr></table></figure><h2 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line1.png" alt></p><p>矩阵A中第一行和第三行的关系，被称为“精确相关关系”，即完全相关，一行可使另一行为0。在这种精确相关关系下，矩阵A的行列式为0，则矩阵A的逆不可能存在。在我们的最小二乘法中，如果矩阵中存在这种精确相关关系，则逆不存在，最小二乘法完全无法使用，线性回归会无法求出结果。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line2.png" alt></p><p>矩阵B中第一行和第三行的关系不太一样，他们之间非常接近于”精确相关关系“，但又不是完全相关，一行不能使另一行为0，这种关系被称为”高度相关关系“。在这种高度相关关系下，矩阵的行列式不为0，但是一个非常接近0数，矩阵A的逆存在，不过接近于无限大。在这种情况下，最小二乘法可以使用，不过得到的逆会很大，直接影响我们对参数向量w的求解：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line3.png" alt></p><p>这样求解出来的参数向量w会很大，因此会影响建模的结果，造成模型有偏差或者不可用。精确相关关系和高度相关关系并称为”多重共线性”。在多重共线性下，模型无法建立，或者模型不可用。</p><p>相对的，矩阵C的行之间结果相互独立，梯形矩阵看起来非常正常，它的对角线上没有任何元素特别接近于0，因此其行列式也就不会接近0或者为0，因此矩阵C得出的参数向量w就不会有太大偏差，对于我们拟合而言是比较理想的。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line4.png" alt></p><p>从上面的所有过程我们可以看得出来，<strong>一个矩阵如果要满秩，则要求矩阵中每个向量之间不能存在多重共线性</strong>。这也构成了线性回归算法对于特征矩阵的要求。</p><h3 id="多重共线性与相关性"><a href="#多重共线性与相关性" class="headerlink" title="多重共线性与相关性"></a>多重共线性与相关性</h3><p>多重共线性如果存在，则线性回归就无法使用最小二乘法来进行求解，或者求解就会出现偏差。幸运的是，不能存在<br>多重共线性，不代表不能存在相关性——机器学习不要求特征之间必须独立，必须不相关，只要不是高度相关或者精<br>确相关就好。</p><p><strong>关键概念：</strong>多重共线性与相关性</p><p>多重共线性是一种统计现象，是指线性模型中的特征（解释变量）之间由于存在精确相关关系或高度相关关系，多重共线性的存在会使模型无法建立，或者估计失真。多重共线性使用指标方差膨胀因子（variance inflation factor，VIF）来进行衡量（from statsmodels.stats.outliers_influence import variance_inflation_factor），通常当我们提到“共线性”，都特指多重共线性。</p><p>相关性是衡量两个或多个变量一起波动的程度的指标，它可以是正的，负的或者0。当我们说变量之间具有相关性，通常是指线性相关性，线性相关一般由皮尔逊相关系数进行衡量，非线性相关可以使用斯皮尔曼相关系数或者互信息法进行衡量。 </p><h3 id="处理多重共线性的方法"><a href="#处理多重共线性的方法" class="headerlink" title="处理多重共线性的方法"></a>处理多重共线性的方法</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line5.png" alt></p><p>这三种手段中，第一种相对耗时耗力，需要较多的人工操作，并且会需要混合各种统计学中的知识和检验来进行使<br>用。第二种手段在现实中应用较多，不过由于理论复杂，效果也不是非常高效。我们的核心是使用第三种方法：改进线性回归来处理多重共线性。为此，岭回归、Lasso、弹性网就被研究出来了。</p><h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>在线性模型之中，除了线性回归之外，最知名的就是岭回归与Lasso了。这两个算法非常神秘，他们的原理和应用都不像其他算法那样高调，学习资料也很少。这可能是因为<strong>这两个算法不是为了提升模型表现，而是为了修复漏洞而设计的。</strong></p><p>岭回归在多元线性回归的损失函数上加上了正则项，表达为系数w的L2范式（即系数w的平方项）乘以正则化系数&alpha;。岭回归的损失函数的完整表达式写作：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line6.png" alt></p><h3 id="linear-model-Ridge"><a href="#linear-model-Ridge" class="headerlink" title="linear_model.Ridge"></a>linear_model.Ridge</h3><p>在sklearn中，岭回归由线性模型库中的Ridge类来调用 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line7.png" alt></p><p>和线性回归相比，岭回归的参数稍微多了那么一点点，但是真正核心的参数就是我们的<strong>正则项的系数&alpha;</strong> ，其他的参数是当我们希望使用最小二乘法之外的求解方法求解岭回归的时候才需要的，通常我们完全不会去触碰这些参数。所以,大家只需要了解&alpha;的用法就可以了。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, LinearRegression, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line"><span class="comment">#print(housevalue.data)</span></span><br><span class="line">print(X.head())</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#数据集索引恢复</span></span><br><span class="line">print(Xtest.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [Xtrain,Xtest]:</span><br><span class="line">    i.index = range(i.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#使用岭回归来进行建模</span></span><br><span class="line">reg = Ridge(alpha=<span class="number">1</span>).fit(Xtrain,Ytrain)</span><br><span class="line">reg.score(Xtest,Ytest)</span><br><span class="line">alpharange = np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>)</span><br><span class="line">ridge, lr = [], []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpharange:</span><br><span class="line">    reg = Ridge(alpha=alpha)</span><br><span class="line">    linear = LinearRegression()</span><br><span class="line">    regs = cross_val_score(reg,X,y,cv=<span class="number">5</span>,scoring = <span class="string">"r2"</span>).mean()</span><br><span class="line">    linears = cross_val_score(linear,X,y,cv=<span class="number">5</span>,scoring = <span class="string">"r2"</span>).mean()</span><br><span class="line">    ridge.append(regs)</span><br><span class="line">    lr.append(linears)</span><br><span class="line">plt.plot(alpharange,ridge,color=<span class="string">"red"</span>,label=<span class="string">"Ridge"</span>)</span><br><span class="line">plt.plot(alpharange,lr,color=<span class="string">"orange"</span>,label=<span class="string">"LR"</span>)</span><br><span class="line">plt.title(<span class="string">"Mean"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行上面代码，可以看出，加利佛尼亚数据集上，岭回归的结果轻微上升，随后骤降。可以说，加利佛尼亚房屋价值数据集带有很轻微的一部分共线性，这种共线性被正则化参数 消除后，模型的效果提升了一点点，但是对于整个模型而言是杯水车薪。在过了控制多重共线性的点后，模型的效果飞速下降，显然是正则化的程度太重，挤占了参数 本来的估计空<br>间。从这个结果可以看出，加利佛尼亚数据集的核心问题不在于多重共线性，岭回归不能够提升模型表现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#观察模型方差</span></span><br><span class="line">alpharange = np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>)</span><br><span class="line">ridge, lr = [], []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpharange:</span><br><span class="line">    reg = Ridge(alpha=alpha)</span><br><span class="line">    linear = LinearRegression()</span><br><span class="line">    varR = cross_val_score(reg,X,y,cv=<span class="number">5</span>,scoring=<span class="string">"r2"</span>).var()</span><br><span class="line">    varLR = cross_val_score(linear,X,y,cv=<span class="number">5</span>,scoring=<span class="string">"r2"</span>).var()</span><br><span class="line">    ridge.append(varR)</span><br><span class="line">    lr.append(varLR)</span><br><span class="line">plt.plot(alpharange,ridge,color=<span class="string">"red"</span>,label=<span class="string">"Ridge"</span>)</span><br><span class="line">plt.plot(alpharange,lr,color=<span class="string">"orange"</span>,label=<span class="string">"LR"</span>)</span><br><span class="line">plt.title(<span class="string">"Variance"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以发现，模型的方差上升快速。虽然岭回归和Lasso不是设计来提升模型表现，而是专注于解决多重共线性问题的，但当&alpha;在一定范围内变动的时候，消除多重共线性也许能够一定程度上提高模型的泛化能力。 <strong>不是很明白这句话，后面补</strong></p><h3 id="选取最佳的正则化参数"><a href="#选取最佳的正则化参数" class="headerlink" title="选取最佳的正则化参数"></a>选取最佳的正则化参数</h3><p>既然要选择&alpha;的范围，我们就不可避免的进行最优参数的选择。在各种机器学习教材中，总是教导使用岭迹图来判断正则项参数的最佳取值。传统的岭迹图长成下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line8.png" alt></p><p>这一个以正则化参数为横坐标，线性模型求解的系数&alpha;为纵坐标的图像，其中每一条彩色的线都是一个系数&alpha;。其目标是建立正则化参数与系数&alpha;之间的直接关系，以此来观察正则化参数的变化如何影响了系数w的拟合。岭迹图认为，线条交叉越多，则说明特征之间的多重共线性越高。我们应该选择系数较为平稳的喇叭口所对应的&alpha;取值作为最佳的正则化参数的取值。绘制岭迹图的方法非常简单，代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="comment">#创造10*10的希尔伯特矩阵</span></span><br><span class="line">X = <span class="number">1.</span> / (np.arange(<span class="number">1</span>, <span class="number">11</span>) + np.arange(<span class="number">0</span>, <span class="number">10</span>)[:, np.newaxis])</span><br><span class="line">y = np.ones(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#计算横坐标</span></span><br><span class="line">n_alphas = <span class="number">200</span></span><br><span class="line">alphas = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, n_alphas)</span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alphas:</span><br><span class="line">    ridge = linear_model.Ridge(alpha=a, fit_intercept=<span class="literal">False</span>)</span><br><span class="line">    ridge.fit(X, y)</span><br><span class="line">    coefs.append(ridge.coef_)</span><br><span class="line"><span class="comment">#绘图展示结果</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.plot(alphas, coefs)</span><br><span class="line">ax.set_xscale(<span class="string">'log'</span>)</span><br><span class="line">ax.set_xlim(ax.get_xlim()[::<span class="number">-1</span>]) <span class="comment">#将横坐标逆转</span></span><br><span class="line">plt.xlabel(<span class="string">'正则化参数alpha'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'系数w'</span>)</span><br><span class="line">plt.title(<span class="string">'岭回归下的岭迹图'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><strong>我非常不建议大家使用岭迹图来作为寻找最佳参数的标准。</strong><br>有这样的两个理由：</p><p>1、岭迹图的很多细节，很难以解释。比如为什么多重共线性存在会使得线与线之间有很多交点？当&alpha;很大了之后看上去所有的系数都很接近于0，难道不是那时候线之间的交点最多吗？</p><p>2、岭迹图的评判标准，非常模糊。哪里才是最佳的喇叭口？哪里才是所谓的系数开始变得”平稳“的时候？一千个读者一千个哈姆雷特的画像？未免也太不严谨了</p><p>我们应该使用交叉验证来选择最佳的正则化系数。在sklearn中，我们有带交叉验证的岭回归可以使用：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line9.png" alt></p><p>RidgeCV的重要参数、属性和接口：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line10.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV, LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Ridge_ = RidgeCV(alphas=np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>),store_cv_values=<span class="literal">True</span>).fit(X, y)</span><br><span class="line"><span class="comment">#无关交叉验证的岭回归结果</span></span><br><span class="line">print(<span class="string">"没有交叉验证的岭回归："</span>,Ridge_.score(X,y))</span><br><span class="line"><span class="comment">#调用所有交叉验证的结果</span></span><br><span class="line">print(<span class="string">"调用所有交叉验证："</span>,Ridge_.cv_values_.shape)</span><br><span class="line"><span class="comment">#进行平均后可以查看每个正则化系数取值下的交叉验证结果</span></span><br><span class="line">print(<span class="string">"平均后的每个正则化系数交叉验证结果："</span>,max(Ridge_.cv_values_.mean(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="comment">#查看被选择出来的最佳正则化系数</span></span><br><span class="line">print(<span class="string">"最佳正则化系数："</span>,Ridge_.alpha_)</span><br></pre></td></tr></table></figure><h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><p>除了岭回归之外，最常被人们提到还有模型Lasso。Lasso全称最小绝对收缩和选择算子（least absolute shrinkage and selection operator），由于这个名字过于复杂所以简称为Lasso。和岭回归一样，Lasso是被创造来作用于多重共线性问题的算法，不过Lasso使用的是系数w的L1范式（L1范式则是系数w的绝对值）乘以正则化系数&alpha; ，所以,Lasso的损失函数表达式为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line11.png" alt></p><p><strong>岭回归VSLasso</strong>：岭回归可以解决特征间的精确相关关系导致的最小二乘法无法使用的问题，而Lasso不行。</p><p><strong>Lasso不是从根本上解决多重共线性问题，而是限制多重共线性带来的影响。</strong></p><h3 id="Lasso的核心作用：特征选择"><a href="#Lasso的核心作用：特征选择" class="headerlink" title="Lasso的核心作用：特征选择"></a>Lasso的核心作用：特征选择</h3><p>sklearn中我们使用类Lasso来调用lasso回归，众多参数中我们需要比较在意的就是参数&alpha; ，正则化系数。另外需要注意的就是参数positive。当这个参数为”True”的时候，是我们要求Lasso回归出的系数必须为正数，以此来保证我们的&alpha;一定以增大来控制正则化的程度。 需要注意的是，在sklearn中我们的Lasso使用的损失函数是：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line14.png" alt></p><p>红色框框住的只是作为系数存在。用来消除我们对损失函数求导后多出来的那个2的（求解w时所带的1/2），然后对整体的RSS求了一个平均而已，无论时从损失函数的意义来看还是从Lasso的性质和功能来看，这个变化没有造成任何影响，只不过计算上会更加简便一些。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, LinearRegression, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#恢复索引</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [Xtrain,Xtest]:</span><br><span class="line">    i.index = range(i.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#线性回归进行拟合</span></span><br><span class="line">reg = LinearRegression().fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"线性回归："</span>,(reg.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#岭回归进行拟合</span></span><br><span class="line">Ridge_ = Ridge(alpha=<span class="number">0</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"岭回归："</span>,(Ridge_.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#Lasso进行拟合</span></span><br><span class="line">lasso_ = Lasso(alpha=<span class="number">0</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"Lasso"</span>,(lasso_.coef_*<span class="number">100</span>).tolist())</span><br></pre></td></tr></table></figure><p>可以看到，岭回归没有报出错误，但Lasso就不一样了，虽然依然对系数进行了计算，但是报出了整整三个警告：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line15.png" alt></p><p>这三条分别是这样的内容：</p><p>​    1、正则化系数为0，这样算法不可收敛！如果你想让正则化系数为0，请使用线性回归吧</p><p>​    2、没有正则项的坐标下降法可能会导致意外的结果，不鼓励这样做！</p><p>​    3、目标函数没有收敛，你也许想要增加迭代次数，使用一个非常小的alpha来拟合模型可能会造成精确度问题！ </p><p>sklearn不推荐我们使用0这样的正则化系数。如果我们的确希望取到0，那我们可以使用一个比较很小的数，比如0.01这样的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#岭回归拟合</span></span><br><span class="line">Ridge_ = Ridge(alpha=<span class="number">0.01</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print((Ridge_.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#Lasso进行拟合</span></span><br><span class="line">lasso_ = Lasso(alpha=<span class="number">0.01</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print((lasso_.coef_*<span class="number">100</span>).tolist())</span><br></pre></td></tr></table></figure><p>这样就不会报任何警告了。</p><h3 id="选取最佳的正则化"><a href="#选取最佳的正则化" class="headerlink" title="选取最佳的正则化"></a>选取最佳的正则化</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line16.png" alt></p><p><img src="C:%5CUsers%5C%E6%B9%9B%E8%93%9D%E6%98%9F%E7%A9%BA%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1568183504767.png" alt="1568183504767"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">housevalue=fch()</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#自己建立Lasso进行alpha选择的范围</span></span><br><span class="line">alpharange = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, <span class="number">200</span>,base=<span class="number">10</span>)</span><br><span class="line">lasso_ = LassoCV(alphas=alpharange <span class="comment">#自行输入的alpha的取值范围</span></span><br><span class="line">,cv=<span class="number">5</span> <span class="comment">#交叉验证的折数</span></span><br><span class="line">).fit(Xtrain, Ytrain)</span><br><span class="line"><span class="comment">#查看被选择出来的最佳正则化系数</span></span><br><span class="line">print(<span class="string">"最佳的正则化系数："</span>,lasso_.alpha_)</span><br><span class="line"><span class="comment">#调用所有交叉验证的结果</span></span><br><span class="line">print(<span class="string">"所有交叉验证的结果："</span>,lasso_.mse_path_)</span><br><span class="line">print(lasso_.mse_path_.shape) <span class="comment">#返回每个alpha下的五折交叉验证结果</span></span><br><span class="line">print(lasso_.mse_path_.mean(axis=<span class="number">1</span>)) <span class="comment">#有注意到在岭回归中我们的轴向是axis=0吗？</span></span><br><span class="line"><span class="comment">#在岭回归当中我们的交叉验证结果返回的是，每一个样本在每个alpha下的交叉验证结果</span></span><br><span class="line"><span class="comment">#因此我们要求每个alpha下的交叉验证均值，就是axis=0，跨行求均值</span></span><br><span class="line"><span class="comment">#而在这里，我们返回的是，每一个alpha取值下，每一折交叉验证的结果</span></span><br><span class="line"><span class="comment">#因此我们要求每个alpha下的交叉验证均值，就是axis=1，跨列求均值</span></span><br><span class="line"><span class="comment">#最佳正则化系数下获得的模型的系数结果</span></span><br><span class="line">print(<span class="string">"最佳正则化系数获得的模型结果："</span>,lasso_.coef_)</span><br><span class="line">print(<span class="string">"最佳正则化系数的准确率："</span>,lasso_.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#与线性回归相比如何？</span></span><br><span class="line">reg = LinearRegression().fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"线性回归的准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#使用lassoCV自带的正则化路径长度和路径中的alpha个数来自动建立alpha选择的范围</span></span><br><span class="line">ls_ = LassoCV(eps=<span class="number">0.00001</span></span><br><span class="line">,n_alphas=<span class="number">300</span></span><br><span class="line">,cv=<span class="number">5</span></span><br><span class="line">).fit(Xtrain, Ytrain)</span><br><span class="line">print(ls_.alpha_)</span><br><span class="line">print(ls_.alphas_) </span><br><span class="line">print(ls_.alphas_.shape)</span><br><span class="line">print(ls_.score(Xtest,Ytest))</span><br><span class="line">print(ls_.coef_)</span><br></pre></td></tr></table></figure><p>模型效果上表现和普通的Lasso没有太大的区别，不过他们都在各个方面对原有的Lasso做了一些相应的改进（比如说提升了本来就已经很快的计算速度，增加了模型选择的维度，因为均方误差作为损失函数只考虑了偏差，不考虑方差的在。除了解决多重共线性这个核心问题之外，<strong>线性模型还有更重要的事情要做：提升模型表现</strong>。这才是机器学习最核心的需求，而Lasso和岭回归不是为此而设计的。<strong>为了提升模型表现而做出的改进：多项式回归</strong>。 </p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>首先，“线性”这个词用于描述不同事物时有着不同的含义，我们最常使用的线性是指“变量之间的线性关系(linear relationship)”。它表示两个变量之间的关系可以展示为一条直线，即可以使用方程y=ax+b来进行拟合。要探索两个变量之间的关系是否线性，最简单的方式就是绘制散点图，如果散点图能够相对均匀地分布在一条直线的两端，则说明这两个变量之间的关系是线性的。从线性关系这个概念出发，我们有了一种说法叫做“线性数据”。通常来说，一组数据由多个特征和标签组成。当这些特征分别与标签存在线性关系的时候，我们就说这一组数据是线性数据。</p><p>但当我们在进行分类的时候，我们的数据分布往往是这样的：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line19.png" alt></p><p>这些数据都不能由一条直线来进行拟合，他们也没有均匀分布在某一条线的周围，那我们怎么判断，这些数据是线性数据还是非线性数据呢？在这里就要注意了，当我们在回归中绘制图像时，绘制的是特征与标签的关系图，横坐标是特征，纵坐标是标签，我们的标签是连续型的，所以我们可以通过是否能够使用一条直线来拟合图像判断数据究竟属于线性还是非线性。然而在分类中，我们绘制的是数据分布图，横坐标是其中一个特征，纵坐标是另一个特征，标签则是数据点的颜色。因此在分类数据中，我们使用“是否线性可分”（linearly separable）这个概念来划分分类数据集。<strong>当分类数据的分布上可以使用一条直线来将两类数据分开时，我们就说数据是线性可分的。反之，数据不是线性可分的。</strong> ps：上面那张图我也不知道是不是线性可分的，大家知道以下这个概念就好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">rnd = np.random.RandomState(<span class="number">42</span>) <span class="comment">#设置随机数种子</span></span><br><span class="line">X = rnd.uniform(<span class="number">-3</span>, <span class="number">3</span>, size=<span class="number">100</span>) <span class="comment">#random.uniform，从输入的任意两个整数中取出size个随机数</span></span><br><span class="line"><span class="comment">#生成y的思路：先使用NumPy中的函数生成一个sin函数图像，然后再人为添加噪音</span></span><br><span class="line">y = np.sin(X) + rnd.normal(size=len(X)) / <span class="number">3</span> <span class="comment">#random.normal，生成size个服从正态分布的随机数</span></span><br><span class="line"><span class="comment">#使用散点图观察建立的数据集是什么样子</span></span><br><span class="line">plt.scatter(X, y,marker=<span class="string">'o'</span>,c=<span class="string">'k'</span>,s=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#为后续建模做准备：sklearn只接受二维以上数组作为特征矩阵的输入</span></span><br><span class="line">X = X.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#使用原始数据进行建模</span></span><br><span class="line">LinearR = LinearRegression().fit(X, y)</span><br><span class="line">TreeR = DecisionTreeRegressor(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line"><span class="comment">#放置画布</span></span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#创建测试数据：一系列分布在横坐标上的点</span></span><br><span class="line">line = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="literal">False</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#将测试数据带入predict接口，获得模型的拟合效果并进行绘制</span></span><br><span class="line">ax1.plot(line, LinearR.predict(line), linewidth=<span class="number">2</span>, color=<span class="string">'green'</span>,</span><br><span class="line">label=<span class="string">"linear regression"</span>)</span><br><span class="line">ax1.plot(line, TreeR.predict(line), linewidth=<span class="number">2</span>, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">"decision tree"</span>)</span><br><span class="line"><span class="comment">#将原数据上的拟合绘制在图像上</span></span><br><span class="line">ax1.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line"><span class="comment">#其他图形选项</span></span><br><span class="line">ax1.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Regression output"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"Input feature"</span>)</span><br><span class="line">ax1.set_title(<span class="string">"Result before discretization"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从图像上可以看出，线性回归无法拟合出这条带噪音的正弦曲线的真实面貌，只能够模拟出大概的趋势，而决策树却<br>通过建立复杂的模型将几乎每个点都拟合出来了。可见，使用线性回归模型来拟合非线性数据的效果并不好，而决策<br>树这样的模型却拟合得太细致，相比之下，还是决策树的拟合效果更好一些。<strong>线性模型可以用来拟合非线性数据，而非线性模型也可以用来拟合线性数据，更神奇的是，有的算法没有模型也可以处理各类数据，而有的模型可以既可以是线性，也可以是非线性模型。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line20.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归原理&quot;&gt;&lt;a href=&quot;#线性回归原理&quot; class=&quot;headerlink&quot; title=&quot;线性回归原理&quot;&gt;&lt;/a&gt;线性回归原理&lt;/h2&gt;&lt;p&gt;回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向&lt;br&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>模型的保存和加载</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/"/>
    <id>http://yoursite.com/2019/09/07/模型的保存和加载/</id>
    <published>2019-09-07T08:09:05.000Z</published>
    <updated>2019-09-15T06:29:51.490Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML40.png" alt></p><p>joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。</p><p>joblib.load()：读取模型。参数是模型的目录</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML56.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;模型的保存和加载&quot;&gt;&lt;a href=&quot;#模型的保存和加载&quot; class=&quot;headerlink&quot; title=&quot;模型的保存和加载&quot;&gt;&lt;/a&gt;模型的保存和加载&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Br
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2019/09/07/决策树/</id>
    <published>2019-09-07T08:05:29.000Z</published>
    <updated>2019-09-15T06:40:08.859Z</updated>
    
    <content type="html"><![CDATA[<h2 id="决策树推导"><a href="#决策树推导" class="headerlink" title="决策树推导"></a>决策树推导</h2><p>首先看看下面这组数据集：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML96.png" alt></p><p>得出下面这颗决策树：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML97.png" alt></p><p><strong>关键概念</strong>：</p><p>​    信息熵公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML34.png" alt></p><p>​    信息增益公式：就是熵和特征条件熵的差</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML35.png" alt></p><p>​    随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好</p><p>决策树算法的需要解决的核心问题：</p><p>​    1、如何从数据表中找出最佳节点和最佳分支？</p><p>​    2、如何让决策树停止生长，防止过拟合？</p><p>决策树的基本过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML101.png" alt></p><p>直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。</p><h2 id="决策树五大模块"><a href="#决策树五大模块" class="headerlink" title="决策树五大模块"></a>决策树五大模块</h2><p>sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML98.png" alt></p><h2 id="分类树参数、属性和接口"><a href="#分类树参数、属性和接口" class="headerlink" title="分类树参数、属性和接口"></a>分类树参数、属性和接口</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML102.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h4><p>为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标<br>叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心<br>大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 </p><p>criterion这个参数正是用来决定不纯度的计算方法的：</p><p>​    1、输入”entropy“，使用信息熵</p><p>​    2、输入”gini“，使用基尼系数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML100.png" alt></p><p>其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵<br>时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。</p><h4 id="random-state-amp-splitter"><a href="#random-state-amp-splitter" class="headerlink" title="random_state&amp;splitter"></a>random_state&amp;splitter</h4><p>random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据<br>（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。<br>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会<br>优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在<br>分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这<br>也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能<br>性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 </p><h4 id="max-depth"><a href="#max-depth" class="headerlink" title="max_depth"></a>max_depth</h4><p>限制树的最大深度，超过设定深度的树枝全部剪掉<br>这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所<br>以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效<br>果再决定是否增加设定深度。</p><h4 id="min-samples-leaf-amp-min-samples-split"><a href="#min-samples-leaf-amp-min-samples-split" class="headerlink" title="min_samples_leaf&amp;min_samples_split"></a>min_samples_leaf&amp;min_samples_split</h4><p>min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。<br>min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则<br>分枝就不会发生。 </p><h4 id="max-features-amp-min-impurity-decrease"><a href="#max-features-amp-min-impurity-decrease" class="headerlink" title="max_features&amp;min_impurity_decrease"></a>max_features&amp;min_impurity_decrease</h4><p>max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。<br>min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的<br>功能，在0.19版本之前时使用min_impurity_split。 </p><h4 id="class-weight-amp-min-weight-fraction-leaf"><a href="#class-weight-amp-min-weight-fraction-leaf" class="headerlink" title="class_weight&amp;min_weight_fraction_leaf"></a>class_weight&amp;min_weight_fraction_leaf</h4><p>完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 </p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>分类树的七个参数，一个属性，四个接口，以及绘图所用的代码。<br>七个参数：Criterion，两个随机性相关的参数（random_state，splitter），四个剪枝参数（max_depth， min_sample_leaf，max_feature，min_impurity_decrease）<br>一个属性：feature_importances_ ，属性是在模型训练之后，能够调用查看的模型的各种性质。</p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>四个接口：ﬁt，score，apply，predicd</p><p>apply：返回每个测试样本所在叶子节点的索引</p><p>predict：返回每个测试样本的分类/回归结果</p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML37.png" alt></p><h2 id="回归树参数解读"><a href="#回归树参数解读" class="headerlink" title="回归树参数解读"></a>回归树参数解读</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/judge1.png" alt></p><p>几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。</p><h4 id="criterion："><a href="#criterion：" class="headerlink" title="criterion："></a>criterion：</h4><p>回归树衡量分枝质量的指标，支持的标准有三种：<br>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree1.png" alt></p><p>其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree2.png" alt></p><p>其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 </p><h2 id="决策树的本地保存：Graphviz"><a href="#决策树的本地保存：Graphviz" class="headerlink" title="决策树的本地保存：Graphviz"></a>决策树的本地保存：Graphviz</h2><p>windows版本下载地址：<a href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html" target="_blank" rel="noopener">https://graphviz.gitlab.io/_pages/Download/Download_windows.html</a></p><p>双击msi文件，一直next就完事了。</p><p>找到bin文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML103.png" alt></p><p>在下面这张图片的位置加入环境变量</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML106.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML105.png" alt></p><p>用dot -version检查是否安装成功</p><p>将dot文件转为png文件的命令：dot -Tpng *.dot -o *.png</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML107.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类树</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">titan=pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line">x=titan[[<span class="string">"pclass"</span>,<span class="string">"age"</span>,<span class="string">"sex"</span>]]</span><br><span class="line">y=titan[<span class="string">"survived"</span>]</span><br><span class="line">x[<span class="string">"age"</span>].fillna(x[<span class="string">"age"</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.25</span>)</span><br><span class="line">dict=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">x_train=dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">x_test=dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">dec=DecisionTreeClassifier()</span><br><span class="line">dec.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line"><span class="comment">#图形化</span></span><br><span class="line">export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">"age"</span>,<span class="string">"pclass=1st"</span>,<span class="string">"pclass=2nd"</span>,<span class="string">"pclass=3rd"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#回归树</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">X = np.sort(<span class="number">5</span> * rng.rand(<span class="number">80</span>,<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::<span class="number">5</span>] += <span class="number">3</span> * (<span class="number">0.5</span> - rng.rand(<span class="number">16</span>))</span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=<span class="number">5</span>)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line">X_test = np.arange(<span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">0.01</span>)[:, np.newaxis]</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=<span class="number">20</span>, edgecolor=<span class="string">"black"</span>,c=<span class="string">"darkorange"</span>, label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(X_test, y_1, color=<span class="string">"cornflowerblue"</span>,label=<span class="string">"max_depth=2"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_2, color=<span class="string">"yellowgreen"</span>, label=<span class="string">"max_depth=5"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">"data"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"target"</span>)</span><br><span class="line">plt.title(<span class="string">"Decision Tree Regression"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;决策树推导&quot;&gt;&lt;a href=&quot;#决策树推导&quot; class=&quot;headerlink&quot; title=&quot;决策树推导&quot;&gt;&lt;/a&gt;决策树推导&lt;/h2&gt;&lt;p&gt;首先看看下面这组数据集：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubuserconten
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="http://yoursite.com/2019/09/07/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://yoursite.com/2019/09/07/朴素贝叶斯/</id>
    <published>2019-09-07T08:05:14.000Z</published>
    <updated>2019-09-15T06:14:44.504Z</updated>
    
    <content type="html"><![CDATA[<h2 id="贝叶斯的性质"><a href="#贝叶斯的性质" class="headerlink" title="贝叶斯的性质"></a>贝叶斯的性质</h2><p><strong>关键概念</strong></p><p>​    联合概率：X取值为x和Y取值为y两个事件同时发生的概率，表示为P(X=x,Y=y)</p><p>​    条件概率：在X取值为x的前提下，Y取值为y的概率。表示为P(Y=y|X=x)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML39.png" alt></p><p>这里的C代表类别，W代表特征。</p><p>我们学习的其它分类算法总是有一个特点：这些算法先从训练集中学习，获取某种信息来建立模型，然后用模型去对测试集进行预测。比如逻辑回归，我们要先从训练集中获取让损失函数最小的参数，然后用参数建立模型，再对测试集进行预测。在比如支持向量机，我们要先从训练集中获取让边际最大的决策边界，然后 用决策边界对测试集进行预测。相同的流程在决策树，随机森林中也出现，我们在ﬁt的时候必然已经构造好了能够让对测试集进行判断的模型。而朴素贝叶斯，似乎没有这个过程。这说明，朴素贝叶斯是一个不建模的算法。以往我们学的不建模算法，比如KMeans，比如PCA，都是无监督学习，而朴素贝叶斯是第一个有监督的，不建模的分类算法。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类。</p><h2 id="sklearn中的朴素贝叶斯"><a href="#sklearn中的朴素贝叶斯" class="headerlink" title="sklearn中的朴素贝叶斯"></a>sklearn中的朴素贝叶斯</h2><p>sklearn中，基于高斯分布、伯努利分布、多项式分布为我们提供了四个朴素贝叶斯的分类器</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML71.png" alt></p><h2 id="高斯朴素贝叶斯"><a href="#高斯朴素贝叶斯" class="headerlink" title="高斯朴素贝叶斯"></a>高斯朴素贝叶斯</h2><p>sklearnnaive_bayes.GaussianNB(priors=None,var_smoothing=1e-09)</p><p>高斯朴素贝叶斯，通过假设P(Xi|Y)是服从高斯分布(正态分布)，估计每个特征下每个类别上的条件概率。对于每个特征下的取值，有以下公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML72..png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML73.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">gnb = GaussianNB().fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#查看分数</span></span><br><span class="line">acc_score = gnb.score(Xtest,Ytest)</span><br><span class="line"><span class="comment">#查看预测结果</span></span><br><span class="line">Y_pred = gnb.predict(Xtest)</span><br><span class="line"><span class="comment">#查看预测的概率结果</span></span><br><span class="line">prob = gnb.predict_proba(Xtest)</span><br><span class="line">print(prob)</span><br></pre></td></tr></table></figure><h2 id="概率类模型的评估指标"><a href="#概率类模型的评估指标" class="headerlink" title="概率类模型的评估指标"></a>概率类模型的评估指标</h2><h3 id="布里尔分数Brier-Score"><a href="#布里尔分数Brier-Score" class="headerlink" title="布里尔分数Brier Score　"></a>布里尔分数Brier Score　</h3><p>概率预测的准确程度被称为“校准程度”，是衡量算法预测出的概率和真实结果的差异的一种方式。一种比较常用的指标叫做布里尔分数，它被计算为是概率预测相对于测试样本的均方误差，表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes1.png" alt></p><p>其中N是样本数量，Pi为朴素贝叶斯预测出的概率，Oi是样本所对应的真实结果，只能取到0或者1，如果事件发生则为1，如果不发生则为0。这个指标衡量了我们的概率距离真实标签结果的差异，其实看起来非常像是均方误差。布里尔分数的范围是从0到1，分数越高则预测结果越差劲，校准程度越差，因此布里尔分数越接近0越好。由于它的本质也是在衡量一种损失，所以在sklearn当中，布里尔得分被命名为brier_score_loss。我们可以从模块metrics中导入这个分数来衡量我们的模型评估结果： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss</span><br><span class="line"><span class="comment">#注意，第一个参数是真实标签，第二个参数是预测出的概率值</span></span><br><span class="line"><span class="comment">#我们的pos_label与prob中的索引一致，就可以查看这个类别下的布里尔分数是多少</span></span><br><span class="line">print(brier_score_loss(Ytest, prob[:,<span class="number">1</span>], pos_label=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h3><p>另一种常用的概率损失衡量是对数损失（log_loss），又叫做对数似然，逻辑损失或者交叉熵损失。由于是损失，因此对数似然函数的取值越小，则证明概率估计越准确，模型越理想。值得注意得是，<strong>对数损失只能用于评估分类型模型</strong> </p><p>在sklearn，我们可以从metrics模块中导入我们的对数似然函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss</span><br><span class="line">print(log_loss(Ytest,prob))</span><br></pre></td></tr></table></figure><p>第一个参数是真实标签，第二个参数是我们预测的概率。真正的概率必须要以接口predict_proba来调用，千万避免混淆。</p><p><strong>那什么时候使用对数似然，什么时候使用布里尔分数？</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes2.png" alt></p><h3 id="可靠性曲线"><a href="#可靠性曲线" class="headerlink" title="可靠性曲线"></a>可靠性曲线</h3><p>可靠性曲线（reliability curve），又叫做概率校准曲线（probability calibration curve），可靠性图（reliability<br>diagrams），这是一条以预测概率为横坐标，真实标签为纵坐标的曲线。我们希望预测概率和真实值越接近越好，<br>最好两者相等，因此<strong>一个模型/算法的概率校准曲线越靠近对角线越好</strong>。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification <span class="keyword">as</span> mc</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X, y = mc(n_samples=<span class="number">100000</span>,n_features=<span class="number">20</span> <span class="comment">#总共20个特征</span></span><br><span class="line">,n_classes=<span class="number">2</span> <span class="comment">#标签为2分类</span></span><br><span class="line">,n_informative=<span class="number">2</span> <span class="comment">#其中两个代表较多信息</span></span><br><span class="line">,n_redundant=<span class="number">10</span> <span class="comment">#10个都是冗余特征</span></span><br><span class="line">,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment">#样本量足够大，因此使用1%的样本作为训练集</span></span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,test_size=<span class="number">0.99</span>,random_state=<span class="number">42</span>)</span><br><span class="line">gnb = GaussianNB()</span><br><span class="line">gnb.fit(Xtrain,Ytrain)</span><br><span class="line">y_pred = gnb.predict(Xtest)</span><br><span class="line">prob_pos = gnb.predict_proba(Xtest)[:,<span class="number">1</span>] <span class="comment">#我们的预测概率 - 横坐标</span></span><br><span class="line">clf_score=brier_score_loss(Ytest,prob_pos,pos_label=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#Ytest - 我们的真实标签 - 横坐标</span></span><br><span class="line"><span class="comment">#在我们的横纵表坐标上，概率是由顺序的（由小到大），为了让图形规整一些，我们要先对预测概率和真实标签按照预测</span></span><br><span class="line"><span class="comment">#概率进行一个排序，这一点我们通过DataFrame来实现</span></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">"ytrue"</span>:Ytest[:<span class="number">500</span>],<span class="string">"probability"</span>:prob_pos[:<span class="number">500</span>]&#125;)</span><br><span class="line">df = df.sort_values(by=<span class="string">"probability"</span>)</span><br><span class="line">df.index = range(df.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#紧接着我们就可以画图了</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = plt.subplot()</span><br><span class="line">ax1.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">"k:"</span>, label=<span class="string">"Perfectly calibrated"</span>) <span class="comment">#得做一条对角线来对比呀</span></span><br><span class="line">ax1.plot(df[<span class="string">"probability"</span>],df[<span class="string">"ytrue"</span>],<span class="string">"s-"</span>,label=<span class="string">"%s (%1.3f)"</span> % (<span class="string">"Bayes"</span>, clf_score))</span><br><span class="line">ax1.set_ylabel(<span class="string">"True label"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"predcited probability"</span>)</span><br><span class="line">ax1.set_ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes3.png" alt></p><p>为什么存在这么多上下穿梭的直线？因为我们是按照预测概率的顺序进行排序的，而预测概率从0开始到1的过程中，真实取值不断在0和1之间变化，而我们是绘制折线图，因此无数个纵坐标分布在0和1的被链接起来了，所以看起来如此混乱。那我们换成散点图来试试看呢？ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = plt.subplot()</span><br><span class="line">ax1.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">"k:"</span>, label=<span class="string">"Perfectly calibrated"</span>)</span><br><span class="line">ax1.scatter(df[<span class="string">"probability"</span>],df[<span class="string">"ytrue"</span>],s=<span class="number">10</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"True label"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"predcited probability"</span>)</span><br><span class="line">ax1.set_ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可以看到，由于真实标签是0和1，所以所有的点都在y=1和y=0这两条直线上分布，这完全不是我们希望看到的图像。回想一下我们的可靠性曲线的横纵坐标：横坐标是预测概率，而纵坐标是真实值，我们希望预测概率很靠近真实值，那我们的真实取值必然也需要是一个概率才可以，如果使用真实标签，那我们绘制出来的图像完全是没有意义的。但是，我们去哪里寻找真实值的概率呢？这是不可能找到的——如果我们能够找到真实的概率，那我们何必还用算法来估计概率呢，直接去获取真实的概率不就好了么？所以真实概率在现实中是不可获得的。但是，我们可以获得类概率的指标来帮助我们进行校准。一个简单的做法是，<strong>将数据进行分箱，然后规定每个箱子中真实的少数类所占的比例为这个箱上的真实概率trueproba，这个箱子中预测概率的均值为这个箱子的预测概率predproba，然后以trueproba为纵坐标，predproba为横坐标，来绘制我们的可靠性曲线</strong>。</p><p>在sklearn中，这样的做法可以通过绘制可靠性曲线的类<strong>calibration_curve</strong>来实现。和ROC曲线类似，类calibration_curve可以帮助我们获取我们的横纵坐标，然后使用matplotlib来绘制图像。该类有如下参数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes5.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> calibration_curve</span><br><span class="line"><span class="comment">#从类calibiration_curve中获取横坐标和纵坐标</span></span><br><span class="line">trueproba, predproba = calibration_curve(Ytest, prob_pos</span><br><span class="line">,n_bins=<span class="number">10</span> <span class="comment">#输入希望分箱的个数</span></span><br><span class="line">)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = plt.subplot()</span><br><span class="line">ax1.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">"k:"</span>, label=<span class="string">"Perfectly calibrated"</span>)</span><br><span class="line">ax1.plot(predproba, trueproba,<span class="string">"s-"</span>,label=<span class="string">"%s (%1.3f)"</span> % (<span class="string">"Bayes"</span>, clf_score))</span><br><span class="line">ax1.set_ylabel(<span class="string">"True probability for class 1"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"Mean predcited probability"</span>)</span><br><span class="line">ax1.set_ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>根据不同的n_bins取值得出不同的曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,figsize=(<span class="number">18</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> ind,i <span class="keyword">in</span> enumerate([<span class="number">3</span>,<span class="number">10</span>,<span class="number">100</span>]):</span><br><span class="line">    ax = axes[ind]</span><br><span class="line">    ax.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">"k:"</span>, label=<span class="string">"Perfectly calibrated"</span>)</span><br><span class="line">    trueproba, predproba = calibration_curve(Ytest, prob_pos,n_bins=i)</span><br><span class="line">    ax.plot(predproba, trueproba,<span class="string">"s-"</span>,label=<span class="string">"n_bins = &#123;&#125;"</span>.format(i))</span><br><span class="line">    ax.set_ylabel(<span class="string">"True probability for class 1"</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">"Mean predcited probability"</span>)</span><br><span class="line">    ax.set_ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">    ax.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>很明显可以看出，n_bins越大，箱子越多，概率校准曲线就越精确，但是太过精确的曲线不够平滑，无法和我们希望的完美概率密度曲线相比较。n_bins越小，箱子越少，概率校准曲线就越粗糙，虽然靠近完美概率密度曲线，但是无法真实地展现模型概率预测地结果。因此我们需要取一个既不是太大，也不是太小的箱子个数，让概率校准曲线既不是太精确，也不是太粗糙，而是一条相对平滑，又可以反应出模型对概率预测的趋势的曲线。通常来说，建议先试试看箱子数等于10的情况。箱子的数目越大，所需要的样本量也越多，否则曲线就会太过精确。</p><h4 id="校准可靠性曲线"><a href="#校准可靠性曲线" class="headerlink" title="校准可靠性曲线"></a>校准可靠性曲线</h4><p>sklearn中的概率校正类CalibratedClassifierCV来对二分类情况下的数据集进行概率校正</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes6.png" alt></p><h5 id="base-estimator"><a href="#base-estimator" class="headerlink" title="base_estimator"></a>base_estimator</h5><p>需要校准其输出决策功能的分类器，必须存在predict_proba或decision_function接口。 如果参数cv = prefit，分类<br>器必须已经拟合数据完毕。</p><h5 id="cv"><a href="#cv" class="headerlink" title="cv"></a>cv</h5><p>整数，确定交叉验证的策略。可能输入是：<br>None，表示使用默认的3折交叉验证。在版本<em>0.20</em>中更改：在<em>0.22</em>版本中输入<em>“None”</em>，将由使用<em>3</em>折交叉验证改为<em>5</em>折交叉验证 </p><p>任意整数，指定折数对于输入整数和None的情况下来说，如果时二分类，则自动使用类sklearn.model_selection.StratifiedKFold进行折数分割。如果y是连续型变量，则使用sklearn.model_selection.KFold进行分割。</p><p>已经使用其他类建好的交叉验证模式或生成器cv。</p><p>可迭代的，已经分割完毕的测试集和训练集索引数组。</p><p>输入”prefit”，则假设已经在分类器上拟合完毕数据。在这种模式下，使用者必须手动确定用来拟合分类器的数<br>据与即将倍校准的数据没有交集</p><h5 id="method"><a href="#method" class="headerlink" title="method"></a>method</h5><p>进行概率校准的方法，可输入”sigmoid”或者”isotonic”<br>    输入’sigmoid’，使用基于Platt的Sigmoid模型来进行校准<br>    输入’isotonic’，使用等渗回归来进行校准<br>当校准的样本量太少（比如，小于等于1000个测试样本）的时候，不建议使用等渗回归，因为它倾向于过拟合。样<br>本量过少时请使用sigmoids，即Platt校准。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification <span class="keyword">as</span> mc</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss</span><br><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> calibration_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X, y = mc(n_samples=<span class="number">100000</span>,n_features=<span class="number">20</span> <span class="comment">#总共20个特征</span></span><br><span class="line">,n_classes=<span class="number">2</span> <span class="comment">#标签为2分类</span></span><br><span class="line">,n_informative=<span class="number">2</span> <span class="comment">#其中两个代表较多信息</span></span><br><span class="line">,n_redundant=<span class="number">10</span> <span class="comment">#10个都是冗余特征</span></span><br><span class="line">,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment">#样本量足够大，因此使用1%的样本作为训练集</span></span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,test_size=<span class="number">0.99</span>,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_calib</span><span class="params">(models,name,Xtrain,Xtest,Ytrain,Ytest,n_bins=<span class="number">10</span>)</span>:</span></span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">20</span>, <span class="number">6</span>))</span><br><span class="line">    ax1.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">"k:"</span>, label=<span class="string">"Perfectly calibrated"</span>)</span><br><span class="line">    <span class="keyword">for</span> clf, name_ <span class="keyword">in</span> zip(models, name):</span><br><span class="line">        clf.fit(Xtrain, Ytrain)</span><br><span class="line">        y_pred = clf.predict(Xtest)</span><br><span class="line">    <span class="comment"># hasattr(obj,name)：查看一个类obj中是否存在名字为name的接口，存在则返回True</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(clf, <span class="string">"predict_proba"</span>):</span><br><span class="line">            prob_pos = clf.predict_proba(Xtest)[:, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># use decision function</span></span><br><span class="line">            prob_pos = clf.decision_function(Xtest)</span><br><span class="line">            prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())</span><br><span class="line">    <span class="comment"># 返回布里尔分数</span></span><br><span class="line">        clf_score = brier_score_loss(Ytest, prob_pos, pos_label=y.max())</span><br><span class="line">        trueproba, predproba = calibration_curve(Ytest, prob_pos, n_bins=n_bins)</span><br><span class="line">        ax1.plot(predproba, trueproba, <span class="string">"s-"</span>, label=<span class="string">"%s (%1.3f)"</span> % (name_, clf_score))</span><br><span class="line">        ax2.hist(prob_pos, range=(<span class="number">0</span>, <span class="number">1</span>), bins=n_bins, label=name_, histtype=<span class="string">"step"</span>, lw=<span class="number">2</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="string">"Distribution of probability"</span>)</span><br><span class="line">    ax2.set_xlabel(<span class="string">"Mean predicted probability"</span>)</span><br><span class="line">    ax2.set_xlim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">    ax2.legend(loc=<span class="number">9</span>)</span><br><span class="line">    ax2.set_title(<span class="string">"Distribution of probablity"</span>)</span><br><span class="line">    ax1.set_ylabel(<span class="string">"True probability for class 1"</span>)</span><br><span class="line">    ax1.set_xlabel(<span class="string">"Mean predcited probability"</span>)</span><br><span class="line">    ax1.set_ylim([<span class="number">-0.05</span>, <span class="number">1.05</span>])</span><br><span class="line">    ax1.legend()</span><br><span class="line">    ax1.set_title(<span class="string">'Calibration plots(reliability curve)'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> CalibratedClassifierCV</span><br><span class="line">name = [<span class="string">"GaussianBayes"</span>,<span class="string">"Logistic"</span>,<span class="string">"Bayes+isotonic"</span>,<span class="string">"Bayes+sigmoid"</span>]</span><br><span class="line">gnb = GaussianNB()</span><br><span class="line">models = [gnb</span><br><span class="line"> ,LR(C=<span class="number">1.</span>, solver=<span class="string">'lbfgs'</span>,max_iter=<span class="number">3000</span>,multi_class=<span class="string">"auto"</span>)</span><br><span class="line"><span class="comment">#定义两种校准方式</span></span><br><span class="line">,CalibratedClassifierCV(gnb, cv=<span class="number">2</span>, method=<span class="string">'isotonic'</span>)</span><br><span class="line">,CalibratedClassifierCV(gnb, cv=<span class="number">2</span>, method=<span class="string">'sigmoid'</span>)]</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">plot_calib(models,name,Xtrain,Xtest,Ytrain,Ytest)</span><br></pre></td></tr></table></figure><h2 id="多项式朴素贝叶斯"><a href="#多项式朴素贝叶斯" class="headerlink" title="多项式朴素贝叶斯"></a>多项式朴素贝叶斯</h2><p>在sklearn中，用来执行多项式朴素贝叶斯的类MultinomialNB包含如下的参数和属性：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes7.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss</span><br><span class="line">class_1 = <span class="number">500</span></span><br><span class="line">class_2 = <span class="number">500</span> <span class="comment">#两个类别分别设定500个样本</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">0.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y</span><br><span class="line">,test_size=<span class="number">0.3</span></span><br><span class="line">,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数</span></span><br><span class="line">mms = MinMaxScaler().fit(Xtrain)</span><br><span class="line">Xtrain_ = mms.transform(Xtrain)</span><br><span class="line">Xtest_ = mms.transform(Xtest)</span><br><span class="line"><span class="comment">#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数</span></span><br><span class="line">mms = MinMaxScaler().fit(Xtrain)</span><br><span class="line">Xtrain_ = mms.transform(Xtrain)</span><br><span class="line">Xtest_ = mms.transform(Xtest)</span><br><span class="line">mnb = MultinomialNB().fit(Xtrain_, Ytrain)</span><br><span class="line">mnb.predict(Xtest_)</span><br><span class="line">mnb.predict_proba(Xtest_)</span><br><span class="line">mnb.score(Xtest_,Ytest)</span><br><span class="line">print(brier_score_loss(Ytest,mnb.predict_proba(Xtest_)[:,<span class="number">1</span>],pos_label=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>其实效果不是很理想，来试试看把Xtrain转换成分类型数据吧。注意我们的Xtrain没有经过归一化，因为做哑变量之后自然所有的数据就不会又负数了。看下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line">kbs = KBinsDiscretizer(n_bins=<span class="number">10</span>, encode=<span class="string">'onehot'</span>).fit(Xtrain)</span><br><span class="line">Xtrain_ = kbs.transform(Xtrain)</span><br><span class="line">Xtest_ = kbs.transform(Xtest)</span><br><span class="line">mnb = MultinomialNB().fit(Xtrain_, Ytrain)</span><br><span class="line">mnb.score(Xtest_,Ytest)</span><br><span class="line">print(brier_score_loss(Ytest,mnb.predict_proba(Xtest_)[:,<span class="number">1</span>],pos_label=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="伯努利朴素贝叶斯"><a href="#伯努利朴素贝叶斯" class="headerlink" title="伯努利朴素贝叶斯"></a>伯努利朴素贝叶斯</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes8.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> brier_score_loss</span><br><span class="line">class_1 = <span class="number">500</span></span><br><span class="line">class_2 = <span class="number">500</span> <span class="comment">#两个类别分别设定500个样本</span></span><br><span class="line">centers = [[<span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">2.0</span>, <span class="number">2.0</span>]] <span class="comment">#设定两个类别的中心</span></span><br><span class="line">clusters_std = [<span class="number">0.5</span>, <span class="number">0.5</span>] <span class="comment">#设定两个类别的方差</span></span><br><span class="line">X, y = make_blobs(n_samples=[class_1, class_2],</span><br><span class="line">centers=centers,</span><br><span class="line">cluster_std=clusters_std,</span><br><span class="line">random_state=<span class="number">0</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y</span><br><span class="line">,test_size=<span class="number">0.3</span></span><br><span class="line">,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数</span></span><br><span class="line">mms = MinMaxScaler().fit(Xtrain)</span><br><span class="line">Xtrain_ = mms.transform(Xtrain)</span><br><span class="line">Xtest_ = mms.transform(Xtest)</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> BernoulliNB</span><br><span class="line"><span class="comment">#普通来说我们应该使用二值化的类sklearn.preprocessing.Binarizer来将特征一个个二值化</span></span><br><span class="line"><span class="comment">#然而这样效率过低，因此我们选择归一化之后直接设置一个阈值</span></span><br><span class="line">mms = MinMaxScaler().fit(Xtrain)</span><br><span class="line">Xtrain_ = mms.transform(Xtrain)</span><br><span class="line">Xtest_ = mms.transform(Xtest)</span><br><span class="line"><span class="comment">#不设置二值化</span></span><br><span class="line">bnl_ = BernoulliNB().fit(Xtrain_, Ytrain)</span><br><span class="line">bnl_.score(Xtest_,Ytest)</span><br><span class="line">brier_score_loss(Ytest,bnl_.predict_proba(Xtest_)[:,<span class="number">1</span>],pos_label=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#设置二值化阈值为0.5</span></span><br><span class="line">bnl = BernoulliNB(binarize=<span class="number">0.5</span>).fit(Xtrain_, Ytrain)</span><br><span class="line">bnl.score(Xtest_,Ytest)</span><br><span class="line">print(brier_score_loss(Ytest,bnl.predict_proba(Xtest_)[:,<span class="number">1</span>],pos_label=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="补集朴素贝叶斯"><a href="#补集朴素贝叶斯" class="headerlink" title="补集朴素贝叶斯"></a>补集朴素贝叶斯</h2><p>在sklearn中，补集朴素贝叶斯由类ComplementNB完成，它包含的参数和多项式贝叶斯也非常相似：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/bayes9.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;贝叶斯的性质&quot;&gt;&lt;a href=&quot;#贝叶斯的性质&quot; class=&quot;headerlink&quot; title=&quot;贝叶斯的性质&quot;&gt;&lt;/a&gt;贝叶斯的性质&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;关键概念&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    联合概率：X取值为x和Y取值为y两个事件
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>k-近邻算法</title>
    <link href="http://yoursite.com/2019/09/07/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/09/07/k-近邻算法/</id>
    <published>2019-09-07T08:03:32.000Z</published>
    <updated>2019-09-15T06:29:29.612Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类算法-k近邻算法-KNN-："><a href="#分类算法-k近邻算法-KNN-：" class="headerlink" title="分类算法-k近邻算法(KNN)："></a>分类算法-k近邻算法(KNN)：</h2><p>如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><p>如何求距离：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML29.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML30.png" alt></p><p>k值取很小，容易受异常点影响</p><p>k值取很大,容易受k值数量的波动</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML31.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类算法-k近邻算法-KNN-：&quot;&gt;&lt;a href=&quot;#分类算法-k近邻算法-KNN-：&quot; class=&quot;headerlink&quot; title=&quot;分类算法-k近邻算法(KNN)：&quot;&gt;&lt;/a&gt;分类算法-k近邻算法(KNN)：&lt;/h2&gt;&lt;p&gt;如果一个样本在特征空间中的k
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读(1)</title>
    <link href="http://yoursite.com/2019/09/05/SVM%E8%A7%A3%E8%AF%BB(1)/"/>
    <id>http://yoursite.com/2019/09/05/SVM解读(1)/</id>
    <published>2019-09-05T14:16:50.000Z</published>
    <updated>2019-09-16T08:12:35.675Z</updated>
    
    <content type="html"><![CDATA[<p>​    支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。</p><h2 id="sklearn中的支持向量机"><a href="#sklearn中的支持向量机" class="headerlink" title="sklearn中的支持向量机"></a>sklearn中的支持向量机</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM30.png" alt></p><p>注意，除了特别表明是线性的两个类LinearSVC和LinearSVR之外，其他的所有类都是同时支持线性和非线性的。NuSVC和NuSVC可以手动调节支持向量的数目，其他参数都与最常用的SVC和SVR一致。注意OneClassSVM是无监督的类。<br>除了本身所带的类之外，sklearn还提供了直接调用libsvm库的几个函数。libsvm是一个简单、易于使用和快速有效的英文的SVM库。</p><h2 id="SVM原理解读"><a href="#SVM原理解读" class="headerlink" title="SVM原理解读"></a>SVM原理解读</h2><p>支持向量机原理的三层理解：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM21.png" alt></p><h3 id="第一层理解"><a href="#第一层理解" class="headerlink" title="第一层理解"></a>第一层理解</h3><p>支持向量机所做的事情其实非常容易理解，看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM1.png" alt></p><p>上图是一组两种标签的数据，两种标签分别用圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在位置数据集上的分类误差(泛化误差)尽量小。</p><p><strong>关键概念</strong>：</p><p>​    超平面：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。 </p><p>​    决策边界：在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就 说这个超平面是数据的“决策边界”。</p><p>决策边界一侧的所有点在分类为属于一个类，而另一个类的所有店分类属于另一个类。比如上面的数据集，我们很容易的在方块和圆的中间画一条线并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。 所以，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM2.png" alt></p><p>但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有B1和B2两条可能的决策边界。我们可以把决策边界B1向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到b11和b12的距离相等。<strong>在b11和b12中间的距离，叫做 这条决策边界的边际(margin)，通常记作d</strong>。对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM3.png" alt></p><p>接着我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误认成了圆，有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，<strong>拥有更大边际的决策边界在分类中的泛化误差更小</strong>，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM4.png" alt></p><p><strong>结论：支持向量机，就是通过找出边际最多的决策边界，来对数据进行分类的分类器。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM5.png" alt></p><p>我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决 策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM6.png" alt></p><p>我们将此表达式变换一下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM7.png" alt></p><p>其中[a, -1]就是我们的参数向量 ， 就是我们的特征向量， 是我们的截距。</p><p>我们要求解参数向量w和截距b 。如果在决策边界上任意取两个点Xa，Xb，并带入决策边界的表达式，则有：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM.png" alt></p><p>将两式相减，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM8.png" alt></p><p>Xa和Xb是一条直线上的两个点，相减后的得到的向量方向是由Xb指向Xa，所以Xa-Xb的方向是平行于他们所在的直线——我们的决策边界的。而w与Xa-Xb相互垂直，所以参数向量w方向必然是垂直于我们的决策边界。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM9.png" alt></p><p>此时，我们有了我们得决策边界，任意一个紫色得点Xp就可以表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM10.png" alt></p><p>由于紫色的点所代表的标签y是1，所以我们规定，p&gt;0。</p><p>同样的，对于任意一个红色的点Xr而言，我们可以将它表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM11.png" alt></p><p>由于红色点所表示的标签y是-1，所以我们规定，r&lt;0</p><p>由上面得知：如果我们有新的测试数据Xt，则Xt的标签就可以根据以下式子来判定：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM12.png" alt></p><p><strong>注意：p和r的符号是我们人为规定的</strong></p><p>两类数据中距离我们的决策边界 最近的点，这些点就被称为支持向量。</p><p>紫色类的点为Xp ，红色类的点为Xr，则我们可以得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM13.png" alt></p><p>两个式子相减，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM14.png" alt></p><p>如下图所示： (Xp-Xr)可表示为两点之间的连线，而我们的边际d是平行于w的，所以我们现在，相当于是得到 了三角型中的斜边，并且知道一条直角边的方向。在线性代数中，向量有这样的性质：向量a除以向量b的模长||b|| ，可以得到向量a在向量b的方向上的投影的长度。所以，我们另上述式子两边同时除以||w||，则可以得到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM16.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM15.png" alt></p><p>最大边界所对应的决策边界，那问题就简单了，要最大化d，就求解w的最小值。极值问题可以相互转化，我们可以把求解w的最小值转化为，求解以下函数的最小值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM19.png" alt></p><p>之所以要在模长上加上平方，是因为模长的本质是一个距离，所以它是一个带根号的存在，我们对它取平方，是为了消除根号。</p><p>我们得到我们SVM的损失函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM20.png" alt></p><p>至此，SVM的第一层理解就完成了。</p><p><strong>关键概念</strong>：函数间隔与几何间隔</p><p>对于给定的数据集T和超平面(&omega;,b),定义超平面(&omega;,b)关于样本点(x<sub>i</sub>,y<sub>i</sub>)的函数间隔为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM31.png" alt></p><p>这其实是我们的虚线超平面的表达式整理过后得到的式子。函数间隔可以表示分类预测的正确性以及确信度。再在这个函数间隔的基础上除以&omega;的模长||&omega;||来得到几何间隔：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM32.png" alt></p><p>几何间隔的本质其实是点x<sub>i</sub>到超平面(&omega;,b)，即到我们的决策边界的带符号的距离(signed distance)。</p><h3 id="第二层理解"><a href="#第二层理解" class="headerlink" title="第二层理解"></a>第二层理解</h3><h4 id="用拉格朗日对偶函数求解线性SVM"><a href="#用拉格朗日对偶函数求解线性SVM" class="headerlink" title="用拉格朗日对偶函数求解线性SVM"></a>用拉格朗日对偶函数求解线性SVM</h4><p>有了我们的损失函数以后，我们就需要对损失函数进行求解。我们之前得到了线性SVM损失函数的最初形态。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM20.png" alt></p><p>这个损失函数分为两部分：需要最小化的函数，以及参数求解后必须满足的约束条件。这是一个最优化问题。</p><h5 id="为什么要进行转换？"><a href="#为什么要进行转换？" class="headerlink" title="为什么要进行转换？"></a>为什么要进行转换？</h5><p>我们的目标是求解让损失函数最小化的&omega;,但其实很容易看的出来，如果||&omega;||为0，f(&omega;)必然最小了。但是，||&omega;||=0其实是一个无效的值，原因很简单：首先，我们的决策边界是&omega;<em>x+b=0，如果&omega;为0，则这个向量的所有元素都为0，那就有b=0这个唯一值。如果b和&omega;都为0，决策边界就不在是一条直线了，函数间隔y<sub>i</sub>(&omega;\</em>x<sub>i</sub>+b)就会为0，条件中的y(&omega;*x<sub>i</sub>+b)&gt;=1就不可能实现，所有&omega;不可以是一个0向量。可见，单纯让f(&omega;)为0，是不能求解出合理的&omega;的。我们希望能够能够找出一种方式，能够让我们的条件y<sub>i</sub>(&omega;*x<sub>i</sub>+b)&gt;=1在计算中也被纳入考虑，其中一种方法就是使用拉格朗日乘数法。</p><h5 id="为什么可以进行转换？"><a href="#为什么可以进行转换？" class="headerlink" title="为什么可以进行转换？"></a>为什么可以进行转换？</h5><p>我们的损失函数是二次的(quadratic)，并且我们损失函数中的约束条件在参数&omega;和b下是线性的，求解这样的损失函数被称为“凸优化问题”(convex optimization problem)。拉格朗日乘数法正好可以用来解决凸优化问题，这种方法也是业界常用的，用来解决带约束条件，尤其是带有不等式的约束条件的函数的数学方法。首先第一步，我们需要使用拉格朗日乘数来将损失函数改写为考虑了约束条件的形式： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM34.png" alt></p><p>上面被称为拉格朗日函数。其中&alpha;<sub>i</sub>就叫做拉格朗日乘数。此时此刻，我们要求解的就不只有参数向量&omega;和截距b了，我们也要求拉格朗日乘数&alpha;，而我们的x<sub>i</sub>和y<sub>i</sub>都是我们已知的特征矩阵和标签。</p><h5 id="怎么进行转换？"><a href="#怎么进行转换？" class="headerlink" title="怎么进行转换？"></a>怎么进行转换？</h5><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM34.png" alt></p><p>拉格朗日函数也分为两部分。第一部分和我们原始的损失函数一样，第二部分呈现了我们带有不等式的约束条件。我们希望，<em>L</em>(&omega;,b,&alpha;)不仅能够代表我们原有的损失函数f(&omega;)和约束条件。还能够表示我们想要最小化损失函数来求解&omega;和b的意图，所以我们要先以&alpha;为参数，<em>L</em>(&omega;,b,&alpha;)的最大值，再以&omega;和b为参数，求解<em>L</em>(&omega;,b,&alpha;)的最小值。因此，我们的目标可以写作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM35.png" alt></p><p>首先，我们先执行max，即最大化<em>L</em>(&omega;,b,&alpha;)，那就有两种情况：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM36.png" alt></p><p>若把函数第二部分当作一个惩罚项来看待，则y<sub>i</sub>(&omega;*x<sub>i</sub>+b)大于1时函数没有收到惩罚。而y<sub>i</sub>(&omega;*x<sub>i</sub>+b)小于1时函数受到极致的惩罚，即加上一个正无穷项，函数整体永远不可能渠道最小值。所以第二部，我们执行min的命令，求解函数整体的最小值，我们就永远不能让&alpha;必须取到正无穷的状况出现，即是说永远不让y<sub>i</sub>(&omega;*x<sub>i</sub>+b)&lt;1的状况出现。从而实现了求解最小值的同时让约束条件满足。现在，<em>L</em>(&omega;,b,&alpha;)就是我们新的损失函数。我们的目标时通过先最大化，再最小化它来求解参数向量&omega;和截距b的值。</p><h4 id="拉格朗日函数转换为拉格朗日对偶函数"><a href="#拉格朗日函数转换为拉格朗日对偶函数" class="headerlink" title="拉格朗日函数转换为拉格朗日对偶函数"></a>拉格朗日函数转换为拉格朗日对偶函数</h4><h5 id="为什么要进行转换？-1"><a href="#为什么要进行转换？-1" class="headerlink" title="为什么要进行转换？"></a>为什么要进行转换？</h5><p>要求极值，最简单的方法还是对参数求导后让一阶导数等于0。我们先来试试看对拉格朗日函数求极值，在这里我<br>们对参数向量&omega;和截距b分别求偏导并且让他们等于0。 求导过程如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM37.png" alt></p><p>由于两个求偏导结果中都带有未知的拉格朗日乘数&alpha;<sub>i</sub>，因此我们还是无法求解出&omega;和b，我们必须想出一种方法来求解拉格朗日乘数&alpha;<sub>i</sub>。幸运地是，拉格朗日函数可以被转换成一种只带有&alpha;<sub>i</sub>，而不带有&omega;和b的形式，这种形式被称为拉格朗日对偶函数。在对偶函数下，我们就可以求解出拉格朗日乘数&alpha;<sub>i</sub>，然后带入到上面推导出的(1)和(2)式中来求解&omega;和b。 </p><h5 id="为什么能够进行转换？"><a href="#为什么能够进行转换？" class="headerlink" title="为什么能够进行转换？"></a>为什么能够进行转换？</h5><p>对于任何一个拉格朗日函数，都存在一个与它对于的对偶函数<em>g</em>(&alpha;)，只带有拉格朗日乘数&alpha;作为唯一的参数。如果<em>L</em>(x,a)的最优解存在并可以表示为min <em>L</em>(x,a)，并且对偶函数的最优解也存在并可以表示为max <em>g</em>(&alpha;)，则可以定义对偶差异。即拉格朗日函数的最优解与其对偶函数的最优解之间的差值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM38.png" alt></p><p>如果$\Delta$=0，则称<em>L</em>(x,&alpha;)与其对偶函数之间存在<strong>强对偶关系(strong duality property)，此时我们就可以通过求解其对偶函数的最优解来替代求解原始函数的最优解</strong>。 那强对偶关系上面时候存在呢？那就是这个拉格朗日函数必须满足KTT(Karush-Kuhn-Tucker)条件：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM39.png" alt></p><p>这里的条件其实都比较好理解。首先是所有参数的一阶导数必须为0，然后约束条件中的函数本身需要小于等于0，拉格朗日乘数需要大于等于0，以及约束条件乘以拉格朗日乘数必须等于0，即不同i的取值下，两者之中至少有一个为0。当所有限制都被满足，则拉格朗日函数<em>L</em>(x,&alpha;)的最优解与其对偶函数的最优解相等，我们就可以将原始的最优化问题转换成为对偶函数的最优化问题。而不难注意到，对于我们的损失函数<em>L</em>(&alpha;,b,&alpha;)而言，KTT条件都是可以操作的。如果我们能够人为让KTT条件全部成立，我们就可以求解出<em>L</em>(&omega;,b,&alpha;)的对偶函数来解出&alpha;。</p><p>之前我们已经让拉格朗日函数上对参数&omega;和b的求导为0，得到了式子：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM40.png" alt></p><p>并且在我们的函数中，我们通过先求解最大值再求解最小值的方法使得函数天然满足：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM41.png" alt></p><p>接下来，我们只需要再满足一个条件：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM42.png" alt></p><p>这个条件其实很容易满足，能够让y<sub>i</sub>(&omega;*x<sub>i</sub>+b)-1=0的就是落在虚线的超平面上的样本点，即我们的支持向量。所有不是支持向量的样本点则必须满足&alpha;<sub>i</sub>=0。满足这个式子说明了，我们求解的参数&omega;和b以及求解的超平面的存在，只与支持向量相关，与其他样本点无关。五个条件满足后，就可以使用<em>L</em>(&omega;,b,&alpha;)的对偶函数来求解&alpha;了。</p><h5 id="怎么进行转换？-1"><a href="#怎么进行转换？-1" class="headerlink" title="怎么进行转换？"></a>怎么进行转换？</h5><p>首先让拉格朗日函数对参数&omega;和b求导后的结果为0，本质时再探索拉格朗日函数的最小值。然后：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM43.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM44.png" alt></p><p>这个<em>L</em><sub>d</sub>就是我们的对偶函数。对所有存在对偶函数的拉格朗日函数我们有对偶差异如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM45.png" alt></p><p>则对于我们的<em>L</em>(&omega;,b,&alpha;)和<em>L</em><sub>d</sub>，我们则有：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM46.png" alt></p><p>我们推到<em>L</em><sub>d</sub>的第一步是对<em>L</em>(&omega;,b,&alpha;)求偏导并让偏导数都为0。所有我们求解对偶函数的过程其实是在求解<em>L</em>(&omega;,b,&alpha;)的最小值。所以我们又可以把公式写成：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM47.png" alt></p><p>如此，我们只需要求解对偶函数的最大值，就可以求出&alpha;了。最终，我们的目标函数变化为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM48.png" alt></p><h4 id="求解拉格朗日对偶函数"><a href="#求解拉格朗日对偶函数" class="headerlink" title="求解拉格朗日对偶函数"></a>求解拉格朗日对偶函数</h4><p>到了这一步，我们就需要使用梯度下降，smo或者二次规划(QP，quadratic programming)来求解&alpha;。由于数学太难且我的数学也是太差，我不看了也就不讲了，头都要秃了。大家只需要知道，一但我们求得了&alpha;值，我们就可以使用求导后得到的(1)式求解&omega;，并可以使用(1)式子和决策边界的表达式结合，得到下面的式子来求解b：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM49.png" alt></p><p>当求得特征向量&omega;和b，我们就得到了我们的决策边界的表达式。也就可以利用决策边界和其有关的超平面来进行分类，我们的决策函数就可以写作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM50.png" alt></p><p>其中x<sub>test</sub>是任意测试样本，sign(h)是h&gt;0时返回-1的符号函数。到这里，我们也就完成了对SVM的第二层理解的大部分内容，我们了解了线性SVM的四种相关函数：损失函数的初始形态、拉格朗日函数、拉格朗日对偶函数以及最后的决策函数。</p><h3 id="第三层理解"><a href="#第三层理解" class="headerlink" title="第三层理解"></a>第三层理解</h3><p>熟练以上的推导过程，就是我们的第三层理解。</p><h2 id="线性SVM决策过程的可视化"><a href="#线性SVM决策过程的可视化" class="headerlink" title="线性SVM决策过程的可视化"></a>线性SVM决策过程的可视化</h2><p>我们可以使用sklearn的式子来可视化我们的决策边界、支持向量、以及决策边界平行的两个超平面。</p><p>画决策边界的函数：contour</p><p>contour是专门用来画等高线的函数。等高线，本质上是在二维图像表现三维图像的一种形式，其中两维X和Y是两条坐标轴上的取值，而Z表示高度。Contour就是将由X和Y构成平面上的所有点中，高度一致的点连接成线段的函数，在同一条等高线上的点一定具有相同的Z值。我们可以利用这个性质来绘制我们的决策边界。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/50.png" alt></p><p>回忆一下，我们的决策边界是&omega;*x+b=0，并在决策边界的两边找出两个超平面，使得超平面到决策边界的相对距离为1。那其实，我们只需要在我们的样本构成的平面上，把所有到决策边界的距离为0的点相连，就是我们的决策边界，而把所有到决策边界的相对距离为1的点相连，就是我们的两个平行于决策边界的超平面了。此时，我们的Z就是平面上的任意点到达超平面的距离。<br>那首先，我们需要获取样本构成的平面，作为一个对象。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X,y = make_blobs(n_samples=<span class="number">50</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>,cluster_std=<span class="number">0.6</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line"><span class="comment">#制作数据的散点图</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(model,ax=None)</span>:</span></span><br><span class="line"><span class="comment">#获取当前的子图，如果没有，就创建    </span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ax = plt.gca()</span><br><span class="line"><span class="comment">#获取x轴的最小最大值</span></span><br><span class="line">    xlim = ax.get_xlim()</span><br><span class="line"><span class="comment">#获取y轴的最小最大值</span></span><br><span class="line">    ylim = ax.get_ylim()</span><br><span class="line"><span class="comment">#在两个最小最大值间生成30个值</span></span><br><span class="line">    x = np.linspace(xlim[<span class="number">0</span>],xlim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line">    y = np.linspace(ylim[<span class="number">0</span>],ylim[<span class="number">1</span>],<span class="number">30</span>)</span><br><span class="line"><span class="comment">#使用meshgrid函数将两个一维向量转换为特征矩阵</span></span><br><span class="line">    Y,X = np.meshgrid(y,x)</span><br><span class="line"><span class="comment">#列合并numpy </span></span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line"><span class="comment">#decision_function是一个重要接口，返回每个输入的样本所对应到决策边界的距离，</span></span><br><span class="line"><span class="comment">#P的本质是输入的样本到决策边界的距离，而contour函数中的level其实是输入了这个距离</span></span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    ax.contour(X, Y, P,colors=<span class="string">"k"</span>,levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>],alpha=<span class="number">0.5</span>,linestyles=[<span class="string">"--"</span>,<span class="string">"-"</span>,<span class="string">"--"</span>])</span><br><span class="line">    ax.set_xlim(xlim)</span><br><span class="line">    ax.set_ylim(ylim)</span><br><span class="line">    plt.show()</span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plot_svc_decision_function(clf)</span><br><span class="line">print(<span class="string">"预测值："</span>,clf.predict(X))</span><br><span class="line">print(<span class="string">"准确度："</span>,clf.score(X,y))</span><br><span class="line">print(<span class="string">"返回支持向量："</span>,clf.support_vectors_)</span><br><span class="line">print(<span class="string">"返回每个类中支持向量的个数："</span>,clf.n_support_)</span><br></pre></td></tr></table></figure><p>那如果推广到非线性数据上呢，比如环形数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</span><br><span class="line">X,y = make_circles(<span class="number">100</span>, factor=<span class="number">0.1</span>, noise=<span class="number">.1</span>)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>).fit(X,y)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=y,s=<span class="number">50</span>,cmap=<span class="string">"rainbow"</span>)</span><br><span class="line">plot_svc_decision_function(clf)</span><br></pre></td></tr></table></figure><p>运行代码后看效果图，很明显，现在线性SVM已经不适合于我们的状况了，我们无法找出一条直线来划分我们的数据集，让直线的两边分别是两种类别。这个时候，如果我们能够在原本的X和y的基础上，添加一个维度r，变成三维，我们可视化这个数据，来看看添加维度让我们的数据如何变化。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#即使下面的代码没有看出来用了这个模块，但是必须要引入</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="comment">#定义一个绘制三维图像的函数</span></span><br><span class="line"><span class="comment">#elev表示上下旋转的角度</span></span><br><span class="line"><span class="comment">#azim表示平行旋转的角度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3D</span><span class="params">(elev=<span class="number">30</span>,azim=<span class="number">30</span>,X=X,y=y)</span>:</span></span><br><span class="line">    ax = plt.subplot(projection=<span class="string">"3d"</span>)</span><br><span class="line">    ax.scatter3D(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],r,c=y,s=<span class="number">50</span>,cmap=<span class="string">'rainbow'</span>)</span><br><span class="line">    ax.view_init(elev=elev,azim=azim)</span><br><span class="line">    ax.set_xlabel(<span class="string">"x"</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">"y"</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">"r"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_3D()</span><br></pre></td></tr></table></figure><p>可以看见，此时此刻我们的数据明显是线性可分的了：我们可以使用一个平面来将数据完全分开，并使平面的上方的所有数据点为一类，平面下方的所有数据点为另一类。此时我们的数据在三维空间中，我们的超平面就是一个二维平面。明显我们可以用一个平面将两类数据隔开，这个平面就是我们的决策边界了。我们刚才做的，计算r，并将r作为数据的第三维度来将数据升维的过程，被称为“核变换”，即是将数据投影到高维空间中，以寻找能够将数据完美分割的超平面，即是说寻找能够让数据线性可分的高维空间。为了详细解释这个过程，我们需要引入SVM中的核心概念：<strong>核函数</strong> </p><p>后续内容请看<a href="https://brickexperts.github.io/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/#more](https://brickexperts.github.io/2019/09/15/SVM解读(2)/#more" target="_blank" rel="noopener">SVM解读(2)</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。&lt;/p&gt;
&lt;h2 id=&quot;sklearn中的支持向量机&quot;&gt;&lt;a href=&quot;#sklearn中的支持向量机&quot; 
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>数据库概述</title>
    <link href="http://yoursite.com/2019/09/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    <id>http://yoursite.com/2019/09/04/数据库概述/</id>
    <published>2019-09-04T12:02:01.000Z</published>
    <updated>2019-09-15T06:28:55.338Z</updated>
    
    <content type="html"><![CDATA[<p>​    大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。</p><p>数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。</p><p>数据库数据具有永久存储、有组织、可共享三个基本特点。</p><p>数据库系统(DBS)由计算机硬件、数据库、数据库管理系统、应用软件和数据库管理系统组成。</p><p>数据库设计时面向数据模型对象、数据库系统的数据冗余度小、数据共享度高、数据库系统的数据和程序之间具有较高的独立性、数据库中数据的最小存取单位是数据项、数据库系统通过DBMS进行数据安全性、完整性、并发控制和数据恢复控制。</p><p>数据库数据物理独立性高是指当数据的物理结构（存储结构）发生变化时，应用程序不需要修改也可以正常工作</p><p>数据库数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行</p><p>DBMS的功能结构</p><p>​    数据定义功能：能够提供数据定义语言(DDL)和相应的建库机制。用户利用DDL可以方便建立数据库。</p><p>​    数据操纵功能：实现数据的插入、修改、删除、查询、统计等数据存取操作的功能称为数据操纵功能。数据库管理系统通过提供数据操纵语言(DML)实现其数据操纵功能。</p><p>​    运行管理功能：包括并发控制、数据的存取控制、数据完整性条件的检查和执行、数据库内部的维护等</p><p>​    建立和维护功能：指数据的载入、转储、重组织功能及数据库的恢复功能，指数据库结构的修改、变更及扩充功能。</p><p>从数据库管理系统的角度查看，数据库系统通常采用外模式、模式、内模式三级模式结构。从数据库最终用户的角度看，数据库系统的就够分为单用户结构、主从结构、分布式结构、客户/服务器结构。</p><p>外模式：也称为子模式或用户模式。它是数据库用户看见和使用的、对局部数据的逻辑结构和特征的描述，是与某一应用有关的数据的逻辑表示。</p><p>模式：模式是数据库中数据的逻辑结构和特征的描述。它仅仅涉及到<strong>型</strong>的描述，不涉及到具体的值。模式的具体值被称为模式的一个实例。同一模式可以有很多实例。模式是相对稳定的，实例是相对变动的。模式反映的是数据的结构及其关系，而实例反映的是数据某一时刻的状态。</p><p>内模式：也称存储模式，它是对数据的物理结构和存储结构的描述，是数据在数据库内部的表示方式。</p><p>为了能够在内部实现这三个抽象层次的联系和转换，数据库系统在这三级模式之间提供了两层映像：外模式/概念模式映像、概念模式/内模式映像。这两层映像保证了数据库系统中的数据能够具有较高的逻辑独立性和物理独立性。</p><p>外模式/模式映像保证了数据与程序间的逻辑独立性，模式/内模式映像保证了数据的物理独立性。</p><p>逻辑独立性：当数据库的整体逻辑结构发生变化时，通过调整外模式和模式之间的映像，使得外模式中的局部数据及其结构不变，程序不在修改。</p><p>物理独立性：当数据库的存储结构发生变化时，通过调整模式和内模式之间的映像，使得整体模式不变，当然外模式及应用程序不用改变。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。&lt;/p&gt;
&lt;p&gt;数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。&lt;/p&gt;
&lt;p&gt;数据库数据具有永久存储、有组织、可共享三个基本特点。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>激活函数以及作用</title>
    <link href="http://yoursite.com/2019/09/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/09/03/激活函数/</id>
    <published>2019-09-03T13:37:07.000Z</published>
    <updated>2019-09-15T06:28:15.886Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客介绍以下激活函数以及激活函数的作用。</p><p>首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL50.png" alt></p><p>激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。</p><p>问题一：为什么我们不能在不激活输入信号的情况下完成此操作呢？</p><p>如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。</p><p>问题二：那么为什么我们需要非线性函数？</p><p>非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。</p><p>接下来介绍一下常用的激活函数：sigmoid、tanh、Relu</p><p>sigmoid：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL45.png" alt></p><p>缺点：当 zz 值<strong>非常大</strong>或者<strong>非常小</strong>时，通过上图我们可以看到，sigmoid函数的导数 g′(z)将接近 00 。这会导致权重 W 的梯度将接近 00 ，使得梯度更新十分缓慢，即<strong>梯度消失</strong></p><p>tanh：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1) 之间，其公式与图形为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL42.png" alt></p><p>tanh函数的缺点同sigmoid函数的第一个缺点一样，当 zz <strong>很大或很小</strong>时，g′(z)接近于 0 ，会导致梯度很小，权重更新非常缓慢，即<strong>梯度消失问题</strong>。</p><p>Relu函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL41.png" alt></p><p>Leaky Relu：这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL43.png" alt></p><p>  其中a取值在（0，1）之间</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客介绍以下激活函数以及激活函数的作用。&lt;/p&gt;
&lt;p&gt;首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="激活函数" scheme="http://yoursite.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>SSD实战-tensorflow实现目标检测</title>
    <link href="http://yoursite.com/2019/09/03/SSD%E5%AE%9E%E6%88%98-tensorflow%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/09/03/SSD实战-tensorflow实现目标检测/</id>
    <published>2019-09-03T11:35:07.000Z</published>
    <updated>2019-09-15T06:37:03.682Z</updated>
    
    <content type="html"><![CDATA[<p>​    因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow&lt;/p&gt;

      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/SSD/"/>
    
    
      <category term="SSD实战" scheme="http://yoursite.com/tags/SSD%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>人脸检测数据集构造</title>
    <link href="http://yoursite.com/2019/08/28/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E9%80%A0/"/>
    <id>http://yoursite.com/2019/08/28/人脸检测数据集构造/</id>
    <published>2019-08-28T07:08:15.000Z</published>
    <updated>2019-09-15T06:46:23.677Z</updated>
    
    <content type="html"><![CDATA[<p>在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。</p><p>Caffe-SSD数据集构造流程：</p><p>1、生成VOC个数数据集（图片、XML标注信息文件）</p><p>2、修改Caffe-SSD数据打包脚本相关路径配置</p><p>3、运行Caffe-SSD数据打包脚本</p><p>VOC格式数据集的目录下有三个文件夹：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD6.png" alt></p><p>Annotation中保存的是xml格式的label信息</p><p>ImageSet中Main目录中存放不同照片列表文件</p><p>​    train.txt：训练图片文件名列表</p><p>​    val.txt：验证图片文件名列表</p><p>​    trianval.txt：训练和验证的图片文件名列表</p><p>​    test.txt：测试图片文件名列表</p><p>JPEGImages目录存放所有的图片集。Annotation和JPEGImages一一对应。</p><p>WIDERFace数据集：打开<a href="http://mmlab.ie.cuhk.edu.hk/project/WIDERFace" target="_blank" rel="noopener">人脸检测数据集地址</a>下载数据集。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD7.png" alt></p><p>下载数据的训练集、验证集、测试集和标注信息。</p><p>打开标注信息，可以看到文件夹中有很多txt文件，打开看一下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD9.png" alt>****</p><p>对于VOC格式的数据集，最主要的是生成两个文件。一个是图片的标注数据，另一个是图片数据。对于图片数据，我们只需要将wider_face的图片放到对应目录下，而对于标注数据，我们需要将上面图片的文件进行解析来重新生成针对每张图片的标注信息，这个信息用XML格式存储。</p><p>对于wider_face转为voc，首先创建Annotation、ImageSets、JPEGImages三个文件夹，其中ImageSets中再创建一个Main文件夹。</p><p>将wider_face转为voc的coding：</p><p><code>import os,cv2,sys,shutilfrom xml.dom.minidom import Documentdef writexml(filename,saveing,bboxes,xmlpath):  doc=Document()  annotation=doc.createElement(&quot;annotation&quot;)  doc.appendChild(annotation)  folder=doc.createElement(&quot;folder&quot;)  folder_name=doc.createTextNode(&quot;widerface&quot;)  folder.appendChild(folder_name)  annotation.appendChild(folder)  filenamenode=doc.createElement(&quot;filename&quot;)  filename_name=doc.createTextNode(filename)  filenamenode.appendChild(filename_name)  annotation.appendChild(filenamenode)  source=doc.createElement(&quot;source&quot;)  annotation.appendChild(source)  database=doc.createElement(&quot;database&quot;)  database.appendChild(doc.createTextNode(&quot;wider face database&quot;))  source.appendChild(database)  annotation_s=doc.createElement(&quot;annotation&quot;)  annotation_s.appendChild(doc.createTextNode(&quot;PASCAL VOC2007&quot;))  source.appendChild(annotation_s)  image=doc.createElement(&quot;image&quot;)  image.appendChild(doc.createTextNode(&quot;flickr&quot;))  source.appendChild(image)  flickrid=doc.createElement(&quot;flickrid&quot;)  source.appendChild(doc.createTextNode(&quot;-1&quot;))  source.appendChild(flickrid)  owner=doc.createElement(&quot;owner&quot;)  annotation.appendChild(owner)  flickrid_o=doc.createElement(&quot;flickrid&quot;)  flickrid_o.appendChild(doc.createTextNode(&quot;yanyu&quot;))  owner.appendChild(flickrid_o)  name_o=doc.createElement(&quot;name&quot;)  name_o.appendChild(doc.createTextNode(&quot;yanyu&quot;))  owner.appendChild(name_o)  size=doc.createElement(&quot;size&quot;)  annotation.appendChild(size)  width=doc.createElement(&quot;width&quot;)  width.appendChild(doc.createTextNode(str(saveing.shape[1])))  height=doc.createElement(&quot;height&quot;)  height.appendChild(doc.createTextNode(str(saveing.shape[0])))  depth=doc.createElement(&quot;depth&quot;)  depth.appendChild(doc.createTextNode(str(saveing.shape[2])))  size.appendChild(width)  size.appendChild(height)  size.appendChild(depth)  segmented=doc.createElement(&quot;segmented&quot;)  segmented.appendChild(doc.createTextNode(&quot;0&quot;))  annotation.appendChild(segmented)  for i in range(len(bboxes)):    bbox=bboxes[i]    objects=doc.createElement(&quot;object&quot;)    annotation.appendChild(objects)    object_name=doc.createElement(&quot;name&quot;)    object_name.appendChild(doc.createTextNode(&quot;face&quot;))    objects.appendChild(object_name)    pose=doc.createElement(&quot;pose&quot;)    pose.appendChild(doc.createTextNode(&quot;Unspecified&quot;))    objects.appendChild(pose)    truncated=doc.createElement(&quot;truncated&quot;)    truncated.appendChild(doc.createTextNode(&quot;1&quot;))    objects.appendChild(truncated)    difficult=doc.createElement(&quot;difficult&quot;)    difficult.appendChild(difficult)    difficult.appendChild(doc.createTextNode(&quot;0&quot;))    objects.appendChild(difficult)    bndbox=doc.createElement(&quot;bndbox&quot;)    objects.appendChild(bndbox)    xmin=doc.createElement(&quot;xmin&quot;)    xmin.appendChild(doc.createTextNode(str(bbox[0])))    bndbox.appendChild(xmin)    ymin=doc.createElement(&quot;ymin&quot;)    ymin.appendChild(doc.createTextNode(str(bbox[1])))    bndbox.appendChild(ymin)    xmax=doc.createElement(&quot;xmax&quot;)    xmax.appendChild(doc.createTextNode(str(bbox[0]+bbox[2])))    bndbox.appendChild(xmax)    ymax=doc.createElement(&quot;ymax&quot;)    ymax.appendChild(doc.createTextNode(str(bbox[1]+bbox[3])))    bndbox.appendChild(ymax)  f=open(xmlpath,&quot;w&quot;)  f.write(doc.toprettyxml(indent=&quot; &quot;))  f.close()rootdir=&quot;./wider_face&quot;def convertimgset(img_set):  imgdir=rootdir+&quot;/WIDER_&quot;+img_set+&quot;/images&quot;  gtfilepath=rootdir+&quot;/wider_face_split/wider_face_&quot;+img_set+&quot;_bbx_gt.txt&quot;  fwrite=open(rootdir+&quot;/ImageSets/Main&quot;+img_set+&quot;.txt&quot;,&quot;w&quot;)  index=0  with open(gtfilepath,&quot;r&quot;) as gtfiles:    while(index&lt;1000):      filename=gtfiles.readline()[:-1]      if (filename==&quot;&quot;):        continue      imgpath=imgdir+&quot;/&quot;+filename      img=cv2.imread(imgpath)      if not img.data:        break      numbbox=int(gtfiles.readline())      bboxes=[]      for i in range(numbbox):        line=gtfiles.readline()        lines=line.split()        lines=lines[0:4]        bbox=(int(lines[0]),int(lines[1]),int(lines[2]),int(lines[3]))        bboxes.append(bbox)      filename=filename.replace(&quot;/&quot;,&quot; &quot;)      if len(bboxes)==0:        print(&quot;no face&quot;)      cv2.imwrite(&quot;{}/JPEGImages/{}&quot;.format(rootdir,filename),img)      fwrite.write(filename.split(&quot;.&quot;)[0]+&quot;\n&quot;)      xmlpath=&quot;{}/Annotations/{}&quot;.format(rootdir,filename.split(&quot;.&quot;)[0])      writexml(filename,img,bboxes,xmlpath)      print(&quot;第%d张成功的图片&quot; % index)      index+=1    fwrite.close()if __name__==&quot;__main__&quot;:  img_sets=[&quot;train&quot;,&quot;val&quot;]  for img_set in img_sets:    convertimgset(img_set)  shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;train.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;trainval.txt&quot;)  shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;val.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;test.txt&quot;)</code></p><p>这篇博客废了。。。。caffe的环境太难搭了。。。。</p><p>我将框架换成tensorflow，进行另一个实战</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。&lt;/p&gt;
&lt;p&gt;C
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/SSD/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>人脸检测综述</title>
    <link href="http://yoursite.com/2019/08/28/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2019/08/28/人脸检测综述/</id>
    <published>2019-08-28T01:51:26.000Z</published>
    <updated>2019-09-15T06:41:29.931Z</updated>
    
    <content type="html"><![CDATA[<p>人脸标注方法：矩形标注和椭圆形标注</p><p>矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、椭圆圆心x坐标、椭圆圆心y坐标。</p><p>判断算法性能好坏：</p><p>每一个标记只允许有一个检测与之相对应，也就是说，我们检测出来的每一个人脸图像只能同我们在标注中人脸图像中数据中的一个相对应。如果有多个，就会视为重复检测。重复检测会被视为错误检测。接着我们就可以得出正确率和错误率，可以画出ROC曲线和PR曲线。可以根据曲线看出算法的好坏。</p><p>人脸采集常用方法：</p><p><strong>活体检测</strong>  判断用户是否为正常操作，通过指定用户做随机动作，一搬有张嘴、摇头、点头、凝视、眨眼等等，防止照片攻击。判断用户是否真实在操作，指定用户上下移动手机，防止视频攻击和非正常动作的攻击。</p><p><strong>3D检测</strong>  验证采集到的是否为立体人像，能够防止平面照片、不同弯曲程度的照片等。</p><p><strong>连续检测</strong>  通过连续的检测，验证运动轨迹是否正常，防止跳过活体检测直接替换采集的照片，也能防止中途换人。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;人脸标注方法：矩形标注和椭圆形标注&lt;/p&gt;
&lt;p&gt;矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/SSD/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
