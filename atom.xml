<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-10T07:14:27.135Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>随机森林</title>
    <link href="http://yoursite.com/2019/09/08/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://yoursite.com/2019/09/08/随机森林/</id>
    <published>2019-09-08T08:10:06.000Z</published>
    <updated>2019-09-10T07:14:27.135Z</updated>
    
    <content type="html"><![CDATA[<h2 id="集成算法概述"><a href="#集成算法概述" class="headerlink" title="集成算法概述"></a>集成算法概述</h2><p>集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型建模结果。基本上所有的机器学习领域都可以看到集成学习的身影。</p><p>集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。</p><p>多个模型集成称为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器(base estimator)。通常来说，有三类集成算法：装袋法(Bagging)、提升法(Boosting)、stacking</p><p>装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结<br>果。装袋法的代表模型就是随机森林。<br>提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本<br>进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树 </p><h2 id="sklearn中的集成算法"><a href="#sklearn中的集成算法" class="headerlink" title="sklearn中的集成算法"></a>sklearn中的集成算法</h2><p>sklearn集成算法模块ensemble</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest1.png" alt></p><h2 id="分类树参数、属性和接口"><a href="#分类树参数、属性和接口" class="headerlink" title="分类树参数、属性和接口"></a>分类树参数、属性和接口</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest2.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h3 id="n-estimators"><a href="#n-estimators" class="headerlink" title="n_estimators"></a>n_estimators</h3><p>这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree5.png" alt></p><h3 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a>random_state</h3><p>随机森林的本质是一种装袋集成算法(bagging)，装袋集成算法是对基评估器的预测结果进行平均或用多数表决元则来决定集成评估器的结果。决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个功能由参数random_state控制。随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树。</p><h4 id="bootstrap-amp-oob-score"><a href="#bootstrap-amp-oob-score" class="headerlink" title="bootstrap&amp;oob_score"></a>bootstrap&amp;oob_score</h4><p>要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。<br>在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。<br><strong>bootstrap参数默认True，代表采用这种有放回的随机抽样技术</strong>。通常，这个参数不会被我们设置为False</p><p>如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果。</p><p>参数的详细解释和其它控制基评估器的参数请参考<a href="[https://brickexperts.github.io/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91/#more](https://brickexperts.github.io/2019/09/07/决策树/#more)">决策树</a></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>随机森林中有三个非常重要的属性：.estimators_，.oob_score_以及.feature_importances_。<br>.estimators_是用来查看随机森林中所有树的列表的。<br>.oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度。<br>而.feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性 </p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。</p><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest8.png" alt></p><p>所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致。 </p><h3 id="criterion："><a href="#criterion：" class="headerlink" title="criterion："></a>criterion：</h3><p>回归树衡量分枝质量的指标，支持的标准有三种：<br>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree1.png" alt></p><p>其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree2.png" alt></p><p>其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 </p><h2 id="随机森林coding："><a href="#随机森林coding：" class="headerlink" title="随机森林coding："></a>随机森林coding：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line">wine = load_wine()</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=<span class="number">0.3</span>)</span><br><span class="line">clf = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">rfc = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">clf = clf.fit(Xtrain,Ytrain)</span><br><span class="line">rfc = rfc.fit(Xtrain,Ytrain)</span><br><span class="line">score_c = clf.score(Xtest,Ytest)</span><br><span class="line">score_r = rfc.score(Xtest,Ytest)</span><br><span class="line">print(<span class="string">"Single Tree:&#123;&#125;"</span>.format(score_c),<span class="string">"Random Forest:&#123;&#125;"</span>.format(score_r))</span><br><span class="line"><span class="comment">#画出随机森林和决策树一组交叉验证的对比</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">25</span>)</span><br><span class="line">rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=<span class="number">10</span>)</span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf_s = cross_val_score(clf,wine.data,wine.target,cv=<span class="number">10</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),rfc_s,label = <span class="string">"RandomForest"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),clf_s,label = <span class="string">"Decision Tree"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#画出随机森林和决策树十组交叉验证的对比</span></span><br><span class="line">rfc_l = []</span><br><span class="line">clf_l = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=<span class="number">25</span>)</span><br><span class="line">    rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    rfc_l.append(rfc_s)</span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line">    clf_s = cross_val_score(clf,wine.data,wine.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    clf_l.append(clf_s)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),rfc_l,label = <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">11</span>),clf_l,label = <span class="string">"Decision Tree"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML38.png" alt></p><h2 id="机器学习中调参的基本思想"><a href="#机器学习中调参的基本思想" class="headerlink" title="机器学习中调参的基本思想"></a>机器学习中调参的基本思想</h2><h3 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h3><p>在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error） </p><p>当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果<br>不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，<br>当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力<br>就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest3.png" alt></p><p>1）模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点<br>2）模型太复杂就会过拟合，模型太简单就会欠拟合<br>3）对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂<br>4）树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest12.png" alt></p><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。<br>偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。<br>方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest4.png" alt></p><p>方差和偏差对模型的影响：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest5.png" alt></p><p>然而，方差和偏差是此消彼长的，不可能同时达到最小值 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/forest6.png" alt></p><p>从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。<br>相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。 </p><p>我们调参的目标是，达到方差和偏差的完美平衡 ！</p><h2 id="随机森林的调参"><a href="#随机森林的调参" class="headerlink" title="随机森林的调参"></a>随机森林的调参</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV,cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data=load_breast_cancer()</span><br><span class="line">rfc=RandomForestClassifier(n_estimators=<span class="number">100</span>,random_state=<span class="number">90</span>)</span><br><span class="line">score_pre=cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">scorel = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">200</span>,<span class="number">10</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i+<span class="number">1</span>,</span><br><span class="line">n_jobs=<span class="number">-1</span>,</span><br><span class="line">random_state=<span class="number">90</span>)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">201</span>,<span class="number">10</span>),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="调n-estimators"><a href="#调n-estimators" class="headerlink" title="调n_estimators"></a>调n_estimators</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从曲线看，n_estimators较平稳且准确率高的范围在35-45之间</span></span><br><span class="line">scorel = []</span><br><span class="line"><span class="comment">#在划分好的范围内，继续细化学习曲线</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">35</span>,<span class="number">45</span>):</span><br><span class="line">    rfc = RandomForestClassifier(n_estimators=i,</span><br><span class="line">    n_jobs=<span class="number">-1</span>,</span><br><span class="line">    random_state=<span class="number">90</span>)</span><br><span class="line">    score = cross_val_score(rfc,data.data,data.target,cv=<span class="number">10</span>).mean()</span><br><span class="line">    scorel.append(score)</span><br><span class="line"><span class="comment">#得出最高准确率的n_estimators，为39</span></span><br><span class="line">print(max(scorel),([*range(<span class="number">35</span>,<span class="number">45</span>)][scorel.index(max(scorel))]))</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">35</span>,<span class="number">45</span>),scorel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="调整max-depth"><a href="#调整max-depth" class="headerlink" title="调整max_depth"></a>调整max_depth</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>)</span><br><span class="line">param_grid=&#123;<span class="string">"max_depth"</span>:np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)&#125;</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line"><span class="comment">#得出最好的深度</span></span><br><span class="line">print(<span class="string">"参数为："</span>,GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><h3 id="调整max-feature"><a href="#调整max-feature" class="headerlink" title="调整max_feature"></a>调整max_feature</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rfc=RandomForestClassifier(n_estimators=<span class="number">39</span>,max_depth=<span class="number">11</span>,random_state=<span class="number">90</span>)</span><br><span class="line">param_grid=&#123;<span class="string">"max_features"</span>:np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)&#125;</span><br><span class="line">GS=GridSearchCV(rfc,param_grid=param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line"><span class="comment">#得出最好的max_feature</span></span><br><span class="line">print(<span class="string">"参数为："</span>,GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：在这步的max_features升高之后，模型的准确率却没有变化。说明模型本身已经处于泛化误差最低点，已经达到了模型的预测上限，没有参数可以左右的部分了。剩下的那些误差，是噪声决定的，已经没有方差和偏差的舞台了。如果是现实案例，我们到这一步其实就可以停下了，因为复杂度和泛化误差的关系已经告诉我们，模型不能再进步了。调参和训练模型都需要很长的时间，明知道模型不能进步了还继续调整，不是一个有效率的做法。如果我们希望模型更进一步，我们会选择更换算法，或者更换做数据预处理的方式 。但我让我们的探究继续。ps：我不要你觉得，我要我觉得</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_grid=&#123;<span class="string">"min_samples_leaf"</span>:np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,max_depth=<span class="number">11</span>,random_state=<span class="number">90</span>)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>这步后的准确率还是没有变化，不要改变参数，让它默认就好</p><h3 id="调整min-samples-split"><a href="#调整min-samples-split" class="headerlink" title="调整min_samples_split"></a>调整min_samples_split</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">param_grid=&#123;<span class="string">'min_samples_split'</span>:np.arange(<span class="number">2</span>, <span class="number">2</span>+<span class="number">20</span>, <span class="number">1</span>)&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>,max_depth=<span class="number">11</span></span><br><span class="line">)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>还是没有变化</p><h3 id="调整criterion"><a href="#调整criterion" class="headerlink" title="调整criterion"></a>调整criterion</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">param_grid = &#123;<span class="string">'criterion'</span>:[<span class="string">'gini'</span>, <span class="string">'entropy'</span>]&#125;</span><br><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">39</span></span><br><span class="line">,random_state=<span class="number">90</span>,max_depth=<span class="number">11</span>)</span><br><span class="line">GS = GridSearchCV(rfc,param_grid,cv=<span class="number">10</span>)</span><br><span class="line">GS.fit(data.data,data.target)</span><br><span class="line">print(GS.best_params_)</span><br><span class="line">print(GS.best_score_)</span><br></pre></td></tr></table></figure><p>在整个调参过程之中，我们首先调整了n_estimators（无论如何都请先走这一步），然后调整max_depth，通max_depth产生的结果，来判断模型位于复杂度-泛化误差图像的哪一边，从而选择我们应该调整的参数和调参的方向。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;集成算法概述&quot;&gt;&lt;a href=&quot;#集成算法概述&quot; class=&quot;headerlink&quot; title=&quot;集成算法概述&quot;&gt;&lt;/a&gt;集成算法概述&lt;/h2&gt;&lt;p&gt;集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="随机森林" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>数据库的数据模型</title>
    <link href="http://yoursite.com/2019/09/07/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2019/09/07/数据库的数据模型/</id>
    <published>2019-09-07T11:17:44.000Z</published>
    <updated>2019-09-07T14:10:30.951Z</updated>
    
    <content type="html"><![CDATA[<p>建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。</p><p>由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求：</p><p>1、比较真实地描述现实世界</p><p>2、易为用户所理解</p><p>3、易于在计算机上实现</p><p><strong>为什么需要数据模型？</strong></p><p>由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。</p><p><strong>数据模型含有哪些内容？</strong></p><p>数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。</p><p>​    1、数据结构</p><p>​         用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面</p><p>​    2、数据操作</p><p>​        用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查</p><p>​    3、数据的约束条件</p><p>​        是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。</p><p><strong>实体联系数据模型的地位与作用：</strong></p><p>实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。</p><p>数据模型是用来描述数据的一组概念和定义，是描述数据的手段。</p><p>​    概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。</p><p>​    逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模</p><p>​    物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。</p><p>逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。</p><p>数据模式是对数据结构、联系和约束的描述。<strong>数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。</strong></p><p>信息世界中的基本概念：</p><p>​    (1)  实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。</p><p>​    (2)  属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画</p><p>​    (3)  键(Key)或称为码：唯一标识实体的属性集称为码</p><p>​    (4)  域(Domain)：属性的取值范围称为该属性的域</p><p>​    (5)  实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型</p><p>​    (6)  实体集(Entity Set)：同一类型实体的集合称为实体集</p><p>​    (7)  联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。</p><p>概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。</p><p>​          实体型：用矩形表示，矩形框内写明实体名</p><p>​           属性：用椭圆表示，并用无向边将其与相应的实体连接起来。</p><p>​           联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n）</p><p>​           键：用下划线表示。</p><p>最常用的数据模型</p><p>​    非关系模型</p><p>​        层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。</p><p>​    满足以下两个条件称为层次模型：</p><p>​        (1)  有且仅有一个结点无双亲。这个结点称为“根节点”</p><p>​        (2)  其它节点有且仅有一个双亲，但可以有多个后继</p><p>​        网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。</p><p>​            网状结构特点：</p><p>​                 (1) 允许一个以上的结点无双亲；</p><p>​                 (2)  一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。</p><p>​    关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。</p><p>​        关系数据模型的数据结构：</p><p>​            关系(Relation)：一个关系对应通常说的一张表</p><p>​            元祖(Tuple)：表中的一行即为一个元祖</p><p>​            属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。</p><p>​            主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。</p><p>​            域(Domain)：属性的取值范围</p><p>​            分量：元祖中的一个属性值</p><p>​            关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n)</p><p>​    面向对象模型：</p><p>​    对象关系模型</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。&lt;/p&gt;
&lt;p&gt;由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="数据模型" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>模型的评判和调优</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98/"/>
    <id>http://yoursite.com/2019/09/07/模型的评判和调优/</id>
    <published>2019-09-07T08:11:22.000Z</published>
    <updated>2019-09-10T03:42:12.618Z</updated>
    
    <content type="html"><![CDATA[<h2 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML52.png" alt></p><p>分类模型评估API：F1-score,反应了模型的稳健性</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML53.png" alt></p><h2 id="模型选择和调优"><a href="#模型选择和调优" class="headerlink" title="模型选择和调优"></a>模型选择和调优</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证为了让被评估的模型更加准确可信</p><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。</p><p>sklearn.model_Selection.GridSearchCV</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML54.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML55.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;精确率和召回率&quot;&gt;&lt;a href=&quot;#精确率和召回率&quot; class=&quot;headerlink&quot; title=&quot;精确率和召回率&quot;&gt;&lt;/a&gt;精确率和召回率&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Bricke
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型的评判和调优" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>K-means</title>
    <link href="http://yoursite.com/2019/09/07/K-means/"/>
    <id>http://yoursite.com/2019/09/07/K-means/</id>
    <published>2019-09-07T08:09:47.000Z</published>
    <updated>2019-09-09T14:24:11.123Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无监督学习与聚类算法"><a href="#无监督学习与聚类算法" class="headerlink" title="无监督学习与聚类算法"></a>无监督学习与聚类算法</h2><p>有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。而聚类算法，就是无监督学习的代表算法。 聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。</p><p>聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要实例化，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML110.png" alt></p><h2 id="KMeans是如何工作的"><a href="#KMeans是如何工作的" class="headerlink" title="KMeans是如何工作的"></a>KMeans是如何工作的</h2><p><strong>关键概念</strong>：簇与质心</p><p>​    簇：KMeans算法将一组N个样本的特征矩阵x划分为k个无交集的簇，直观上来看是簇是一组组聚集在一起的数据，在一个簇中的数据被认为是同一类。簇就是聚类的结果表现。</p><p>​    质心：簇中所有数据的均值通常被称为这个簇的质心。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点纵坐标的均值。同理可推广到高维空间。</p><p>在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下 ：</p><p>​        1、随机抽取k个样本作为最初的质心</p><p>​        2、开始循环</p><p>​                2.1、将每个样本点分配到离他们最近的质心，生成k个簇</p><p>​                2.2、对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心</p><p>​        3、当质心的位置不在发生变化，迭代停止，聚类完成</p><p>那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。</p><p>对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距<br>离的衡量方法有多种，令x表示簇中的一个样本点，ui表示该簇中的质心，n表示每个样本点中的特征数目，i表示组<br>成点x的每个特征，则该样本点到质心的距离可以由以下距离来度量：</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML111.png" alt></p><p>如我们采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为:</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML112.png" alt></p><p>其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum ofSquare），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。因此,KMeans追求的是，求解能够让Inertia最小化的质心。实际上，在质心不断变化不断迭代的过程中，总体平方和是越来越小的。我们可以使用数学来证明，当整体平方和最小的时候，质心就不再发生变化了。如此，K-Means的求解过程，就变成了一个最优化问题 </p><p>损失函数本质是用来衡量模型的拟合效果的，只有有着求解参数需求的算法，才会有损失函数。Kmeans不求解什么参数，它的模型本质也没有在拟合数据，而是在对数据进行一种探索。所以，<strong>K-Means不存在什么损失函数</strong>。Inertia更像是Kmeans的模型评估指标，而非损失函数。</p><h2 id="重要参数和属性"><a href="#重要参数和属性" class="headerlink" title="重要参数和属性"></a>重要参数和属性</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML113.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果。通常，在开始聚类之前，我们并不知道n_clusters究竟是多少，因此我们要对它进行探索。</p><p>在之前描述K-Means的基本流程时我们提到过，当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前， 我们也可以使用max_iter，大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下 来。有时候，当我们的n_clusters选择不符合数据的自然分布，或者我们为了业务需求，必须要填入与数据的自然 分布不合的n_clusters，提前让迭代停下来反而能够提升模型的表现。</p><p>max_iter：整数，默认300，单次运行的k-means算法的大迭代次数<br>tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML118.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML119.png" alt></p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>labels_：查看聚好的类别，每个样本所对应的类</p><p>cluster_centers_：查看质心</p><p>inertia_：查看总距离平方和</p><p>n_iter_：实际的迭代次数</p><p>coding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#这是自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数</span></span><br><span class="line"><span class="comment">#random_state代表随机性</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line">ax1.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]<span class="comment">#画点图</span></span><br><span class="line">,marker=<span class="string">'o'</span><span class="comment">#代表点的形状</span></span><br><span class="line">,s=<span class="number">8</span>)<span class="comment">#代表点的大小</span></span><br><span class="line"><span class="comment">#最开始数据集的形状</span></span><br><span class="line">plt.show()</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    ax1.scatter(X[y==i, <span class="number">0</span>], X[y==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="comment">#簇为3</span></span><br><span class="line">n_clusters = <span class="number">3</span></span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="comment">#查看聚好的列表</span></span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line">pre = cluster.fit_predict(X)</span><br><span class="line"><span class="comment">#查看质心</span></span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"><span class="comment">#查看总距离平方和</span></span><br><span class="line">inertia = cluster.inertia_</span><br><span class="line">print(inertia)</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ax1.scatter(X[y_pred==i, <span class="number">0</span>], X[y_pred==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line"><span class="comment">#画出质心</span></span><br><span class="line">ax1.scatter(centroid[:,<span class="number">0</span>],centroid[:,<span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">"x"</span></span><br><span class="line">,s=<span class="number">15</span></span><br><span class="line">,c=<span class="string">"black"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#簇为4</span></span><br><span class="line">n_clusters=<span class="number">4</span></span><br><span class="line">cluster = KMeans(n_clusters=n_clusters, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line"><span class="comment">#查看聚好的列表</span></span><br><span class="line">y_pred = cluster.labels_</span><br><span class="line">pre = cluster.fit_predict(X)</span><br><span class="line"><span class="comment">#查看质心</span></span><br><span class="line">centroid = cluster.cluster_centers_</span><br><span class="line"><span class="comment">#查看总距离平方和</span></span><br><span class="line">inertia = cluster.inertia_</span><br><span class="line">print(inertia)</span><br><span class="line">color = [<span class="string">"red"</span>,<span class="string">"pink"</span>,<span class="string">"orange"</span>,<span class="string">"gray"</span>]</span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ax1.scatter(X[y_pred==i, <span class="number">0</span>], X[y_pred==i, <span class="number">1</span>]</span><br><span class="line">    ,marker=<span class="string">'o'</span></span><br><span class="line">    ,s=<span class="number">8</span></span><br><span class="line">    ,c=color[i])</span><br><span class="line"><span class="comment">#画出质心</span></span><br><span class="line">ax1.scatter(centroid[:,<span class="number">0</span>],centroid[:,<span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">"x"</span></span><br><span class="line">,s=<span class="number">15</span></span><br><span class="line">,c=<span class="string">"black"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="聚类算法的模型评估"><a href="#聚类算法的模型评估" class="headerlink" title="聚类算法的模型评估"></a>聚类算法的模型评估</h2><p>上面的算法的簇随着数字的增大，总的距离和在不断的变小。难道我们我们就这样实验得出簇的个数吗？难道簇越小越好吗？看看算法的评估。聚类模型的结果不是某种标签输出，并且聚类的结果是不确定的，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？ KMeans的目标是确保“簇内差异小，簇外差异大”，我们就可以通过衡量簇内差异来衡量聚类的效果。我们刚才说过，Inertia是用距离来衡量簇内差异的指标，因此，我们是否可以使用Inertia来作为聚类的衡量指标呢？Inertia越小模型越好嘛。可以，但是这个指标的缺点和极限太大。</p><h3 id="当真实标签未知的时候使用轮廓系数"><a href="#当真实标签未知的时候使用轮廓系数" class="headerlink" title="当真实标签未知的时候使用轮廓系数"></a>当真实标签未知的时候使用轮廓系数</h3><p>这样的聚类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中<strong>轮廓系数</strong>是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：<br>1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离<br>2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。单个样本的轮廓系数计算为 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML114.png" alt></p><p>这个公式可以看作：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML115.png" alt></p><p>很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。</p><p>在sklearn中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML51.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line">print(<span class="string">"4个簇的时候的轮廓系数："</span>,silhouette_score(X,y_pred))</span><br><span class="line">print(<span class="string">"四个簇的每个样本的轮廓系数："</span>,silhouette_samples(X,y_pred))</span><br></pre></td></tr></table></figure><h3 id="当真实标签未知的时候用CHI"><a href="#当真实标签未知的时候用CHI" class="headerlink" title="当真实标签未知的时候用CHI"></a>当真实标签未知的时候用CHI</h3><p>除了轮廓系数是常用的，我们还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML116.png" alt></p><p>在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言，Calinski-Harabaz指数s(k)写作如下公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML117.png" alt></p><p>其中N为数据集中的样本量，k为簇的个数(即类别的个数)，Bk是组间离散矩阵，即不同簇之间的协方差矩阵， Wk是簇内离散矩阵，即一个簇内数据的协方差矩阵，而tr表示矩阵的迹。在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A) 。<strong>数据之间的离散程度越高，协方差矩阵的迹就会越大</strong>。组内离散程度低，协方差的迹就会越小，Tr(Wk)也就越小，同时，组间离散程度大，协方差的的迹也会越大， Tr(Bk)就越大，这正是我们希望的，因此Calinski-harabaz指数越高越好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> calinski_harabaz_score</span><br><span class="line">print(calinski_harabaz_score(X, y_pred))</span><br></pre></td></tr></table></figure><h2 id="基于轮廓系数选择簇的个数"><a href="#基于轮廓系数选择簇的个数" class="headerlink" title="基于轮廓系数选择簇的个数"></a>基于轮廓系数选择簇的个数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_samples</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数</span></span><br><span class="line"><span class="comment">#random_state代表随机性</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">500</span>,n_features=<span class="number">2</span>,centers=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">n_clusters = <span class="number">4</span></span><br><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</span><br><span class="line">ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</span><br><span class="line">ax1.set_ylim([<span class="number">0</span>, X.shape[<span class="number">0</span>] + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</span><br><span class="line">clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>).fit(X)</span><br><span class="line">cluster_labels = clusterer.labels_</span><br><span class="line">silhouette_avg = silhouette_score(X, cluster_labels)</span><br><span class="line">print(<span class="string">"For n_clusters ="</span>, n_clusters,</span><br><span class="line"><span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</span><br><span class="line">sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">y_lower = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]</span><br><span class="line">    ith_cluster_silhouette_values.sort()</span><br><span class="line">    size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</span><br><span class="line">    y_upper = y_lower + size_cluster_i</span><br><span class="line">    color = cm.nipy_spectral(float(i)/n_clusters)</span><br><span class="line">    ax1.fill_betweenx(np.arange(y_lower, y_upper)</span><br><span class="line">    ,ith_cluster_silhouette_values</span><br><span class="line">    ,facecolor=color</span><br><span class="line">    ,alpha=<span class="number">0.7</span>)</span><br><span class="line">    ax1.text(<span class="number">-0.05</span></span><br><span class="line">    , y_lower + <span class="number">0.5</span> * size_cluster_i</span><br><span class="line">    , str(i))</span><br><span class="line">    y_lower = y_upper + <span class="number">10</span></span><br><span class="line">ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cluster label"</span>)</span><br><span class="line">ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">ax1.set_yticks([])</span><br><span class="line">ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)</span><br><span class="line">ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">'o'</span> <span class="comment">#点的形状</span></span><br><span class="line">,s=<span class="number">8</span> <span class="comment">#点的大小</span></span><br><span class="line">,c=colors)</span><br><span class="line">centers = clusterer.cluster_centers_</span><br><span class="line">ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], marker=<span class="string">'x'</span>,</span><br><span class="line">c=<span class="string">"red"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</span><br><span class="line">ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</span><br><span class="line">plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></span><br><span class="line"><span class="string">"with n_clusters = %d"</span> % n_clusters),</span><br><span class="line">fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">for</span> n_clusters <span class="keyword">in</span> [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]:</span><br><span class="line">    n_clusters = n_clusters</span><br><span class="line">    fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    fig.set_size_inches(<span class="number">18</span>, <span class="number">7</span>)</span><br><span class="line">    ax1.set_xlim([<span class="number">-0.1</span>, <span class="number">1</span>])</span><br><span class="line">    ax1.set_ylim([<span class="number">0</span>, X.shape[<span class="number">0</span>] + (n_clusters + <span class="number">1</span>) * <span class="number">10</span>])</span><br><span class="line">    clusterer = KMeans(n_clusters=n_clusters, random_state=<span class="number">10</span>).fit(X)</span><br><span class="line">    cluster_labels = clusterer.labels_</span><br><span class="line">    silhouette_avg = silhouette_score(X, cluster_labels)</span><br><span class="line">    print(<span class="string">"For n_clusters ="</span>, n_clusters,</span><br><span class="line">    <span class="string">"The average silhouette_score is :"</span>, silhouette_avg)</span><br><span class="line">    sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">    y_lower = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]</span><br><span class="line">        ith_cluster_silhouette_values.sort()</span><br><span class="line">        size_cluster_i = ith_cluster_silhouette_values.shape[<span class="number">0</span>]</span><br><span class="line">        y_upper = y_lower + size_cluster_i</span><br><span class="line">        color = cm.nipy_spectral(float(i)/n_clusters)</span><br><span class="line">        ax1.fill_betweenx(np.arange(y_lower, y_upper)</span><br><span class="line">        ,ith_cluster_silhouette_values</span><br><span class="line">        ,facecolor=color</span><br><span class="line">        ,alpha=<span class="number">0.7</span></span><br><span class="line">        )</span><br><span class="line">        ax1.text(<span class="number">-0.05</span></span><br><span class="line">        , y_lower + <span class="number">0.5</span> * size_cluster_i</span><br><span class="line">        , str(i))</span><br><span class="line">        y_lower = y_upper + <span class="number">10</span></span><br><span class="line">ax1.set_title(<span class="string">"The silhouette plot for the various clusters."</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"The silhouette coefficient values"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Cluster label"</span>)</span><br><span class="line">ax1.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">ax1.set_yticks([])</span><br><span class="line">ax1.set_xticks([<span class="number">-0.1</span>, <span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>, <span class="number">0.8</span>, <span class="number">1</span>])</span><br><span class="line">colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)</span><br><span class="line">ax2.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>]</span><br><span class="line">,marker=<span class="string">'o'</span> <span class="comment">#点的形状</span></span><br><span class="line">,s=<span class="number">8</span> <span class="comment">#点的大小</span></span><br><span class="line">,c=colors</span><br><span class="line">)</span><br><span class="line">centers = clusterer.cluster_centers_</span><br><span class="line">ax2.scatter(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], marker=<span class="string">'x'</span>,</span><br><span class="line">        c=<span class="string">"red"</span>, alpha=<span class="number">1</span>, s=<span class="number">200</span>)</span><br><span class="line">ax2.set_title(<span class="string">"The visualization of the clustered data."</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"Feature space for the 1st feature"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"Feature space for the 2nd feature"</span>)</span><br><span class="line">plt.suptitle((<span class="string">"Silhouette analysis for KMeans clustering on sample data "</span></span><br><span class="line"><span class="string">"with n_clusters = %d"</span> % n_clusters),</span><br><span class="line">fontsize=<span class="number">14</span>, fontweight=<span class="string">'bold'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无监督学习与聚类算法&quot;&gt;&lt;a href=&quot;#无监督学习与聚类算法&quot; class=&quot;headerlink&quot; title=&quot;无监督学习与聚类算法&quot;&gt;&lt;/a&gt;无监督学习与聚类算法&lt;/h2&gt;&lt;p&gt;有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="K-means" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/K-means/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归</title>
    <link href="http://yoursite.com/2019/09/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/逻辑回归/</id>
    <published>2019-09-07T08:09:32.000Z</published>
    <updated>2019-09-09T07:02:57.185Z</updated>
    
    <content type="html"><![CDATA[<h2 id="逻辑回归推导过程"><a href="#逻辑回归推导过程" class="headerlink" title="逻辑回归推导过程"></a>逻辑回归推导过程</h2><p>逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以下线性回归。线性回归写作一个人人熟悉的方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic1.png" alt></p><p>&theta;被统称为模型的参数，其中&theta;被称为截距。&theta;1——&theta;n被称为系数。我们可以用矩阵表示这个方程，其中x和&theta;都可以被看作是一个列矩阵：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic2.png" alt></p><p>线性回归的任务，就是构造一个预测函数z来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心<br>就是找出模型的参数：&theta;的转置矩阵和&theta;0，著名的最小二乘法就是用来求解线性回归中参数的数学方法。</p><p>通过函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务<br>（比如预测产品销量，预测股价等等）。那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型<br>变量，我们要怎么办呢？我们可以通过引入<strong>联系函数(link function)</strong>，将线性回归方程z变换为g(z)，并且令g(z)的值<br>分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分<br>类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic3.png" alt></p><p><strong>Sigmoid函数的公式和性质</strong>：Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1。</p><p>将z带入Sigmoid，得到二元逻辑回归的一般形式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic4.png" alt></p><p>而g(z)就是我们逻辑回归返回的标签值。此时,y(x)的取值都在[0,1]之间，因此 y(x)和1-y(x)相加必然为1。令y(x)除以1-y(x)可以得到形似几率得y(x)/(1-y(x))，在此基础上取对数，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic5.png" alt></p><p>不难发现，g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。</p><h2 id="逻辑回归的类"><a href="#逻辑回归的类" class="headerlink" title="逻辑回归的类"></a>逻辑回归的类</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic6.png" alt></p><p>逻辑回归有着基于训练数据求解参数&theta;的需求，并且希望训练出来的模型能够尽可能地拟合训练数据，即模型在训练集上的预测准确率越靠近100%越好。因此，我们使用”损失函数“这个评估指标，来衡量参数&theta;的优劣，即这一组参数能否使模型在训练集上表现优异。</p><p>​    <strong>关键概念</strong>：损失函数，衡量参数&theta;的优劣的评估指标，用来求解最优参数的工具损失函数小，模型在训练集上表现优异，拟合充分，参数优秀损失函数大，模型在训练集上表现差劲，拟合不足，参数糟糕我们追求，能够让损失函数最小化的参数组合。<strong>注意：没有”求解参数“需求的模型没有损失函数，比如KNN，决策树。</strong> </p><p>逻辑回归的损失函数是由最大似然法来推导出来的，具体结果可以写作 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic8.png" alt></p><p>&theta;表示求解出来的一组参数，m是样本的个数，yi是样本i上真实的标签。y&theta;(xi)是样本i上，基于参数&theta;计算出来的逻辑回归返回值，xi是样本i的取值。我们的目标，就是求解出使J(&theta;)最小的&theta;的取值。</p><p>由于我们追求损失函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题：如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。虽然逻辑回归和线性回归是天生欠拟合的模型，但我们还是需要控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。</p><h2 id="正则化-重要参数"><a href="#正则化-重要参数" class="headerlink" title="正则化 重要参数"></a>正则化 重要参数</h2><p>正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基<br>于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中<br>的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic9.png" alt></p><p>其中J(&theta;)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数。j代表每个参数。在这里，J要大于等于1，因为我们的参数向量&theta;中，第一个参数是&theta;0，使我们的截距。它通常不参与正则化。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic7.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic10.png" alt></p><p>L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），参数 &theta;的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化 。</p><h2 id="梯度下降重要参数"><a href="#梯度下降重要参数" class="headerlink" title="梯度下降重要参数"></a>梯度下降重要参数</h2><p>之前提到过，逻辑回归的数学目的是求解能够让模型最优化的参数&theta;的值，即求解能够让损失函数最小化的 的值，而这个求解过程，对于二元逻辑回归来说，有多种方法可以选择，最常见的有梯度下降法(Gradient Descent)，坐标轴下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic8.png" alt></p><p>以梯度下降法为例，我们来看看求解过程是如何完成的。下面这个华丽的平面就是我们的损失函数在输入了一组特征矩阵和标签之后在三维立体坐标系中的图像。现在，我们寻求的是损失函数的最小值，也就是图像的最低点（看起来像是深蓝色区域的某处），一旦我们获取了图像在最低点的取值J(&theta;)0，在我们的损失函数公式中，唯一未知的就是我们的参数向量&theta;。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/logistic11.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line">data.data.shape</span><br><span class="line">lrl1 = LR(penalty=<span class="string">"l1"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.5</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line">lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=<span class="number">0.5</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line"><span class="comment">#逻辑回归的重要属性coef_，查看每个特征所对应的参数</span></span><br><span class="line">lrl1 = lrl1.fit(X,y)</span><br><span class="line">lrl1.coef_</span><br><span class="line">(lrl1.coef_ != <span class="number">0</span>).sum(axis=<span class="number">1</span>)</span><br><span class="line">lrl2 = lrl2.fit(X,y)</span><br><span class="line">lrl2.coef_</span><br><span class="line">l1 = []</span><br><span class="line">l2 = []</span><br><span class="line">l1test = []</span><br><span class="line">l2test = []</span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0.05</span>,<span class="number">1</span>,<span class="number">19</span>):</span><br><span class="line">    lrl1 = LR(penalty=<span class="string">"l1"</span>,solver=<span class="string">"liblinear"</span>,C=i,max_iter=<span class="number">1000</span>)</span><br><span class="line">    lrl2 = LR(penalty=<span class="string">"l2"</span>,solver=<span class="string">"liblinear"</span>,C=i,max_iter=<span class="number">1000</span>)</span><br><span class="line">    lrl1 = lrl1.fit(Xtrain,Ytrain)</span><br><span class="line">    l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain))</span><br><span class="line">    l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest))</span><br><span class="line">    lrl2 = lrl2.fit(Xtrain,Ytrain)</span><br><span class="line">    l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain))</span><br><span class="line">    l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))</span><br><span class="line">graph = [l1,l2,l1test,l2test]</span><br><span class="line">color = [<span class="string">"green"</span>,<span class="string">"black"</span>,<span class="string">"lightgreen"</span>,<span class="string">"gray"</span>]</span><br><span class="line">label = [<span class="string">"L1"</span>,<span class="string">"L2"</span>,<span class="string">"L1test"</span>,<span class="string">"L2test"</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(graph)):</span><br><span class="line">    plt.plot(np.linspace(<span class="number">0.05</span>,<span class="number">1</span>,<span class="number">19</span>),graph[i],color[i],label=label[i])</span><br><span class="line">plt.legend(loc=<span class="number">4</span>) <span class="comment">#图例的位置在哪里?        4表示，右下角</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML49.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;逻辑回归推导过程&quot;&gt;&lt;a href=&quot;#逻辑回归推导过程&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归推导过程&quot;&gt;&lt;/a&gt;逻辑回归推导过程&lt;/h2&gt;&lt;p&gt;逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归</title>
    <link href="http://yoursite.com/2019/09/07/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://yoursite.com/2019/09/07/线性回归/</id>
    <published>2019-09-07T08:09:19.000Z</published>
    <updated>2019-09-09T14:32:11.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h2><p>回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向<br>量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目<br>标根本不是求解出标签，注意加以区别。</p><p>线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这<br>些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决<br>于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型<br>变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归<br>可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png" alt></p><p>线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个<br>有 个特征的样本 而言，它的回归结果可以写作一个几乎人人熟悉的方程 ：</p><p>W被统称为模型的参数。W0被称为截距W1-Wn被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 Xi1-Xin是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML82.png" alt></p><p>我们可以使用矩阵来表示这个方程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML83.png" alt></p><p>y=Xw，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中w可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量w 。</p><p>在多元线性回归中，我们在损失函数如下定义：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML84.png" alt></p><p>因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML85.png" alt></p><p><strong>第一次看不明白上面红字，这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数</strong>。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。</p><p>现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对w求导。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML88.png" alt></p><p>我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML89.png" alt></p><h2 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h2><p>sklearn中的线性模型模块是linear_model。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML74.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML90.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML75.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML76.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML77.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML78.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML79.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML80.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML81.png" alt></p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML42.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML91.png" alt></p><h2 id="模型评估指标"><a href="#模型评估指标" class="headerlink" title="模型评估指标"></a>模型评估指标</h2><p>回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等<br>评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算<br>法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不<br>同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。<br>第一，我们是否预测到了正确的数值。<br>第二，我们是否拟合到了足够的信息。 </p><p>sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML43.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML45.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML44.png" alt></p><p>​            <strong>注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue=fch()</span><br><span class="line">xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">std_x=StandardScaler()</span><br><span class="line">xtrain=std_x.fit_transform(xtrain)</span><br><span class="line">xtest=std_x.transform(xtest)</span><br><span class="line">std_y=StandardScaler()</span><br><span class="line">ytrain=std_y.fit_transform(ytrain.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">ytest=std_y.transform(ytest.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">reg=LR().fit(xtrain,ytrain)</span><br><span class="line">ypredict=reg.predict(xtest)</span><br><span class="line">print(std_y.inverse_transform(ypredict))</span><br><span class="line">print(<span class="string">"均方误差："</span>,mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))</span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>这里如果我们运行上面的代码，会在第十九行报错：</p><p>我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评<br>判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，<br>会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，<br>所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是<br>neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br></pre></td></tr></table></figure><p>为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助:</p><p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML93.png" alt></p><p>在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。</p><p>R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种<br>是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两<br>种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"R2"</span>,r2_score(ypredict,ytest))</span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML94.png" alt></p><p>????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是<strong>预测值在分子，真实值在分母</strong>。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用r2_score</span></span><br><span class="line">print(<span class="string">"R2"</span>,r2_score(y_true=ytest,y_pred=ypredict))</span><br><span class="line"><span class="comment">#利用接口score</span></span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#EVS的两种调用方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> explained_variance_score <span class="keyword">as</span> evs</span><br><span class="line"><span class="comment">#第一种</span></span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"explained_variance"</span>))</span><br><span class="line"><span class="comment">#第二种</span></span><br><span class="line">print(<span class="string">"evs"</span>,evs(ytest,ypredict))</span><br></pre></td></tr></table></figure><p>过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML46.png" alt></p><p>解决方法：正则化。用岭回归实现</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML47.png" alt></p><p>欠拟合：一个假设在训练数据上不能获得更好的拟合，但是在训练数据外的数据集上也不能很好地拟合数据。此时认为这个假设出现了欠拟合的现象。(模型过于简单) 解决办法是添加数据的特征数量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性回归原理&quot;&gt;&lt;a href=&quot;#线性回归原理&quot; class=&quot;headerlink&quot; title=&quot;线性回归原理&quot;&gt;&lt;/a&gt;线性回归原理&lt;/h2&gt;&lt;p&gt;回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向&lt;br&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>模型的保存和加载</title>
    <link href="http://yoursite.com/2019/09/07/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/"/>
    <id>http://yoursite.com/2019/09/07/模型的保存和加载/</id>
    <published>2019-09-07T08:09:05.000Z</published>
    <updated>2019-09-07T08:13:11.828Z</updated>
    
    <content type="html"><![CDATA[<h2 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML40.png" alt></p><p>joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。</p><p>joblib.load()：读取模型。参数是模型的目录</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML56.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;模型的保存和加载&quot;&gt;&lt;a href=&quot;#模型的保存和加载&quot; class=&quot;headerlink&quot; title=&quot;模型的保存和加载&quot;&gt;&lt;/a&gt;模型的保存和加载&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Br
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型的保存和加载" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://yoursite.com/2019/09/07/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2019/09/07/决策树/</id>
    <published>2019-09-07T08:05:29.000Z</published>
    <updated>2019-09-09T15:23:04.343Z</updated>
    
    <content type="html"><![CDATA[<h2 id="决策树推导"><a href="#决策树推导" class="headerlink" title="决策树推导"></a>决策树推导</h2><p>首先看看下面这组数据集：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML96.png" alt></p><p>得出下面这颗决策树：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML97.png" alt></p><p><strong>关键概念</strong>：</p><p>​    信息熵公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML34.png" alt></p><p>​    信息增益公式：就是熵和特征条件熵的差</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML35.png" alt></p><p>​    随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好</p><p>决策树算法的需要解决的核心问题：</p><p>​    1、如何从数据表中找出最佳节点和最佳分支？</p><p>​    2、如何让决策树停止生长，防止过拟合？</p><p>决策树的基本过程：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML101.png" alt></p><p>直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。</p><h2 id="决策树五大模块"><a href="#决策树五大模块" class="headerlink" title="决策树五大模块"></a>决策树五大模块</h2><p>sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML98.png" alt></p><h2 id="分类树参数、属性和接口"><a href="#分类树参数、属性和接口" class="headerlink" title="分类树参数、属性和接口"></a>分类树参数、属性和接口</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML102.png" alt></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h4><p>为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标<br>叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心<br>大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 </p><p>criterion这个参数正是用来决定不纯度的计算方法的：</p><p>​    1、输入”entropy“，使用信息熵</p><p>​    2、输入”gini“，使用基尼系数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML100.png" alt></p><p>其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵<br>时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。</p><h4 id="random-state-amp-splitter"><a href="#random-state-amp-splitter" class="headerlink" title="random_state&amp;splitter"></a>random_state&amp;splitter</h4><p>random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据<br>（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。<br>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会<br>优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在<br>分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这<br>也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能<br>性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 </p><h4 id="max-depth"><a href="#max-depth" class="headerlink" title="max_depth"></a>max_depth</h4><p>限制树的最大深度，超过设定深度的树枝全部剪掉<br>这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所<br>以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效<br>果再决定是否增加设定深度。</p><h4 id="min-samples-leaf-amp-min-samples-split"><a href="#min-samples-leaf-amp-min-samples-split" class="headerlink" title="min_samples_leaf&amp;min_samples_split"></a>min_samples_leaf&amp;min_samples_split</h4><p>min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。<br>min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则<br>分枝就不会发生。 </p><h4 id="max-features-amp-min-impurity-decrease"><a href="#max-features-amp-min-impurity-decrease" class="headerlink" title="max_features&amp;min_impurity_decrease"></a>max_features&amp;min_impurity_decrease</h4><p>max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。<br>min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的<br>功能，在0.19版本之前时使用min_impurity_split。 </p><h4 id="class-weight-amp-min-weight-fraction-leaf"><a href="#class-weight-amp-min-weight-fraction-leaf" class="headerlink" title="class_weight&amp;min_weight_fraction_leaf"></a>class_weight&amp;min_weight_fraction_leaf</h4><p>完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 </p><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>分类树的七个参数，一个属性，四个接口，以及绘图所用的代码。<br>七个参数：Criterion，两个随机性相关的参数（random_state，splitter），四个剪枝参数（max_depth， min_sample_leaf，max_feature，min_impurity_decrease）<br>一个属性：feature_importances_ ，属性是在模型训练之后，能够调用查看的模型的各种性质。</p><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p>四个接口：ﬁt，score，apply，predicd</p><p>apply：返回每个测试样本所在叶子节点的索引</p><p>predict：返回每个测试样本的分类/回归结果</p><p>coding：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML37.png" alt></p><h2 id="回归树参数解读"><a href="#回归树参数解读" class="headerlink" title="回归树参数解读"></a>回归树参数解读</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/judge1.png" alt></p><p>几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。</p><h4 id="criterion："><a href="#criterion：" class="headerlink" title="criterion："></a>criterion：</h4><p>回归树衡量分枝质量的指标，支持的标准有三种：<br>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree1.png" alt></p><p>其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/Tree2.png" alt></p><p>其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 </p><h2 id="决策树的本地保存：Graphviz"><a href="#决策树的本地保存：Graphviz" class="headerlink" title="决策树的本地保存：Graphviz"></a>决策树的本地保存：Graphviz</h2><p>windows版本下载地址：<a href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html" target="_blank" rel="noopener">https://graphviz.gitlab.io/_pages/Download/Download_windows.html</a></p><p>双击msi文件，一直next就完事了。</p><p>找到bin文件夹</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML103.png" alt></p><p>在下面这张图片的位置加入环境变量</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML106.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML105.png" alt></p><p>用dot -version检查是否安装成功</p><p>将dot文件转为png文件的命令：dot -Tpng *.dot -o *.png</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML107.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类树</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">titan=pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line">x=titan[[<span class="string">"pclass"</span>,<span class="string">"age"</span>,<span class="string">"sex"</span>]]</span><br><span class="line">y=titan[<span class="string">"survived"</span>]</span><br><span class="line">x[<span class="string">"age"</span>].fillna(x[<span class="string">"age"</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=<span class="number">0.25</span>)</span><br><span class="line">dict=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">x_train=dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">x_test=dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">dec=DecisionTreeClassifier()</span><br><span class="line">dec.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">"准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line"><span class="comment">#图形化</span></span><br><span class="line">export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">"age"</span>,<span class="string">"pclass=1st"</span>,<span class="string">"pclass=2nd"</span>,<span class="string">"pclass=3rd"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#回归树</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rng = np.random.RandomState(<span class="number">1</span>)</span><br><span class="line">X = np.sort(<span class="number">5</span> * rng.rand(<span class="number">80</span>,<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::<span class="number">5</span>] += <span class="number">3</span> * (<span class="number">0.5</span> - rng.rand(<span class="number">16</span>))</span><br><span class="line">regr_1 = DecisionTreeRegressor(max_depth=<span class="number">2</span>)</span><br><span class="line">regr_2 = DecisionTreeRegressor(max_depth=<span class="number">5</span>)</span><br><span class="line">regr_1.fit(X, y)</span><br><span class="line">regr_2.fit(X, y)</span><br><span class="line">X_test = np.arange(<span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">0.01</span>)[:, np.newaxis]</span><br><span class="line">y_1 = regr_1.predict(X_test)</span><br><span class="line">y_2 = regr_2.predict(X_test)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=<span class="number">20</span>, edgecolor=<span class="string">"black"</span>,c=<span class="string">"darkorange"</span>, label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(X_test, y_1, color=<span class="string">"cornflowerblue"</span>,label=<span class="string">"max_depth=2"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_test, y_2, color=<span class="string">"yellowgreen"</span>, label=<span class="string">"max_depth=5"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">"data"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"target"</span>)</span><br><span class="line">plt.title(<span class="string">"Decision Tree Regression"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;决策树推导&quot;&gt;&lt;a href=&quot;#决策树推导&quot; class=&quot;headerlink&quot; title=&quot;决策树推导&quot;&gt;&lt;/a&gt;决策树推导&lt;/h2&gt;&lt;p&gt;首先看看下面这组数据集：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubuserconten
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="http://yoursite.com/2019/09/07/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://yoursite.com/2019/09/07/朴素贝叶斯/</id>
    <published>2019-09-07T08:05:14.000Z</published>
    <updated>2019-09-07T15:14:14.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类算法—朴素贝叶斯"><a href="#分类算法—朴素贝叶斯" class="headerlink" title="分类算法—朴素贝叶斯"></a>分类算法—朴素贝叶斯</h2><p><strong>关键概念</strong></p><p>​    联合概率：X取值为x和Y取值为y两个事件同时发生的概率，表示为P(X=x,Y=y)</p><p>​    条件概率：在X取值为x的前提下，Y取值为y的概率。表示为P(Y=y|X=x)</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML39.png" alt></p><p>这里的C代表类别，W代表特征。</p><p>我们学习的其它分类算法总是有一个特点：这些算法先从训练集中学习，获取某种信息来建立模型，然后用模型去对测试集进行预测。比如逻辑回归，我们要先从训练集中获取让损失函数最小的参数，然后用参数建立模型，再对测试集进行预测。在比如支持向量机，我们要先从训练集中获取让边际最大的决策边界，然后 用决策边界对测试集进行预测。相同的流程在决策树，随机森林中也出现，我们在ﬁt的时候必然已经构造好了能够让对测试集进行判断的模型。而朴素贝叶斯，似乎没有这个过程。这说明，朴素贝叶斯是一个不建模的算法。以往我们学的不建模算法，比如KMeans，比如PCA，都是无监督学习，而朴素贝叶斯是第一个有监督的，不建模的分类算法。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类。</p><p>sklearn中，基于高斯分布、伯努利分布、多项式分布为我们提供了四个朴素贝叶斯的分类器</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML71.png" alt></p><p>sklearnnaive_bayes.GaussianNB(priors=None,var_smoothing=1e-09):高斯朴素贝叶斯，通过假设P(Xi|Y)是服从高斯分布(正态分布)，估计每个特征下每个类别上的条件概率。对于每个特征下的取值，有以下公式：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML72..png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML73.png" alt></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">digits = load_digits()</span><br><span class="line">X, y = digits.data, digits.target</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">gnb = GaussianNB().fit(Xtrain,Ytrain)</span><br><span class="line"><span class="comment">#查看分数</span></span><br><span class="line">acc_score = gnb.score(Xtest,Ytest)</span><br><span class="line"><span class="comment">#查看预测结果</span></span><br><span class="line">Y_pred = gnb.predict(Xtest)</span><br><span class="line"><span class="comment">#查看预测的概率结果</span></span><br><span class="line">prob = gnb.predict_proba(Xtest)</span><br><span class="line">print(prob)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML33.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML32.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类算法—朴素贝叶斯&quot;&gt;&lt;a href=&quot;#分类算法—朴素贝叶斯&quot; class=&quot;headerlink&quot; title=&quot;分类算法—朴素贝叶斯&quot;&gt;&lt;/a&gt;分类算法—朴素贝叶斯&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;关键概念&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;​    联合概率
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="朴素贝叶斯" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>k-近邻算法</title>
    <link href="http://yoursite.com/2019/09/07/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/09/07/k-近邻算法/</id>
    <published>2019-09-07T08:03:32.000Z</published>
    <updated>2019-09-07T08:05:39.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类算法-k近邻算法-KNN-："><a href="#分类算法-k近邻算法-KNN-：" class="headerlink" title="分类算法-k近邻算法(KNN)："></a>分类算法-k近邻算法(KNN)：</h2><p>如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><p>如何求距离：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML29.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML30.png" alt></p><p>k值取很小，容易受异常点影响</p><p>k值取很大,容易受k值数量的波动</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML31.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类算法-k近邻算法-KNN-：&quot;&gt;&lt;a href=&quot;#分类算法-k近邻算法-KNN-：&quot; class=&quot;headerlink&quot; title=&quot;分类算法-k近邻算法(KNN)：&quot;&gt;&lt;/a&gt;分类算法-k近邻算法(KNN)：&lt;/h2&gt;&lt;p&gt;如果一个样本在特征空间中的k
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="k-近邻算法" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SVM解读</title>
    <link href="http://yoursite.com/2019/09/05/SVM%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2019/09/05/SVM解读/</id>
    <published>2019-09-05T14:16:50.000Z</published>
    <updated>2019-09-06T14:13:05.314Z</updated>
    
    <content type="html"><![CDATA[<p>​    支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。</p><p>支持向量机原理的三层理解：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM21.png" alt></p><p>支持向量机所做的事情其实非常容易理解，看下图：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM1.png" alt></p><p>上图是一组两种标签的数据，两种标签分别用圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在位置数据集上的分类误差(泛化误差)尽量小。</p><p><strong>关键概念</strong>：</p><p>​    超平面：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。 </p><p>​    决策边界：在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就 说这个超平面是数据的“决策边界”。</p><p>决策边界一侧的所有点在分类为属于一个类，而另一个类的所有店分类属于另一个类。比如上面的数据集，我们很容易的在方块和圆的中间画一条线并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。 所以，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM2.png" alt></p><p>但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有B1和B2两条可能的决策边界。我们可以把决策边界B1向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到b11和b12的距离相等。<strong>在b11和b12中间的距离，叫做 这条决策边界的边际(margin)，通常记作d</strong>。对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM3.png" alt></p><p>接着我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误认成了圆，有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，<strong>拥有更大边际的决策边界在分类中的泛化误差更小</strong>，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM4.png" alt></p><p><strong>结论：支持向量机，就是通过找出边际最多的决策边界，来对数据进行分类的分类器。</strong></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM5.png" alt></p><p>我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决 策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM6.png" alt></p><p>我们将此表达式变换一下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM7.png" alt></p><p>其中[a, -1]就是我们的参数向量 ， 就是我们的特征向量， 是我们的截距。</p><p>我们要求解参数向量w和截距b 。如果在决策边界上任意取两个点Xa，Xb，并带入决策边界的表达式，则有：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM.png" alt></p><p>将两式相减，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM8.png" alt></p><p>Xa和Xb是一条直线上的两个点，相减后的得到的向量方向是由Xb指向Xa，所以Xa-Xb的方向是平行于他们所在的直线——我们的决策边界的。而w与Xa-Xb相互垂直，所以参数向量w方向必然是垂直于我们的决策边界。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM9.png" alt></p><p>此时，我们有了我们得决策边界，任意一个紫色得点Xp就可以表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM10.png" alt></p><p>由于紫色的点所代表的标签y是1，所以我们规定，p&gt;0。</p><p>同样的，对于任意一个红色的点Xr而言，我们可以将它表示为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM11.png" alt></p><p>由于红色点所表示的标签y是-1，所以我们规定，r&lt;0</p><p>由上面得知：如果我们有新的测试数据Xt，则Xt的标签就可以根据以下式子来判定：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM12.png" alt></p><p><strong>注意：p和r的符号是我们人为规定的</strong></p><p>两类数据中距离我们的决策边界 最近的点，这些点就被称为支持向量。</p><p>紫色类的点为Xp ，红色类的点为Xr，则我们可以得到：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM13.png" alt></p><p>两个式子相减，得：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM14.png" alt></p><p>如下图所示： (Xp-Xr)可表示为两点之间的连线，而我们的边际d是平行于w的，所以我们现在，相当于是得到 了三角型中的斜边，并且知道一条直角边的方向。在线性代数中，向量有这样的性质：向量a除以向量b的模长||b|| ，可以得到向量a在向量b的方向上的投影的长度。所以，我们另上述式子两边同时除以||w||，则可以得到</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM16.png" alt></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM15.png" alt></p><p>最大边界所对应的决策边界，那问题就简单了，要最大化d，就求解w的最小值。极值问题可以相互转化，我们可以把求解w的最小值转化为，求解以下函数的最小值：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM19.png" alt></p><p>之所以要在模长上加上平方，是因为模长的本质是一个距离，所以它是一个带根号的存在，我们对它取平方，是为了消除根号。</p><p>我们得到我们SVM的损失函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM20.png" alt></p><p>至此，SVM的第一层理解就完成了。</p><p>用拉格朗日对偶函数求解线性SVM</p><p>当然，不是所有数据都是线性可分的，不是所有数据我们都能够一眼看出，有一条直线，或一个平面，甚至一个超平面可以将数据完全分开。比如下面的环形数据。对于这样的数据，我们需要对它进行一个升维变化，将数据从原始的空间x投射到新空间F(x)中。升维之后，我们明显可以找出一个平面，能够将数据切分开来。 F是一个映射函数，它代表了某种能够将数据升维的非线性的变换，我们对数据进行这样的变换，确保数据在自己的空间中一定能够线性可分。 </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SVM22.png" alt></p><p>这种变换非常巧妙，但也带有一些实现问题。 首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。而解决这些问题的是<strong>核函数</strong>。</p><p><strong>关键概念：</strong>核函数是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。&lt;/p&gt;
&lt;p&gt;支持向量机原理的三层理解：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.gith
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="SVM" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/"/>
    
      <category term="SVM解读" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/SVM/SVM%E8%A7%A3%E8%AF%BB/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>数据库概述</title>
    <link href="http://yoursite.com/2019/09/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    <id>http://yoursite.com/2019/09/04/数据库概述/</id>
    <published>2019-09-04T12:02:01.000Z</published>
    <updated>2019-09-04T14:30:34.203Z</updated>
    
    <content type="html"><![CDATA[<p>​    大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。</p><p>数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。</p><p>数据库数据具有永久存储、有组织、可共享三个基本特点。</p><p>数据库系统(DBS)由计算机硬件、数据库、数据库管理系统、应用软件和数据库管理系统组成。</p><p>数据库设计时面向数据模型对象、数据库系统的数据冗余度小、数据共享度高、数据库系统的数据和程序之间具有较高的独立性、数据库中数据的最小存取单位是数据项、数据库系统通过DBMS进行数据安全性、完整性、并发控制和数据恢复控制。</p><p>数据库数据物理独立性高是指当数据的物理结构（存储结构）发生变化时，应用程序不需要修改也可以正常工作</p><p>数据库数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行</p><p>DBMS的功能结构</p><p>​    数据定义功能：能够提供数据定义语言(DDL)和相应的建库机制。用户利用DDL可以方便建立数据库。</p><p>​    数据操纵功能：实现数据的插入、修改、删除、查询、统计等数据存取操作的功能称为数据操纵功能。数据库管理系统通过提供数据操纵语言(DML)实现其数据操纵功能。</p><p>​    运行管理功能：包括并发控制、数据的存取控制、数据完整性条件的检查和执行、数据库内部的维护等</p><p>​    建立和维护功能：指数据的载入、转储、重组织功能及数据库的恢复功能，指数据库结构的修改、变更及扩充功能。</p><p>从数据库管理系统的角度查看，数据库系统通常采用外模式、模式、内模式三级模式结构。从数据库最终用户的角度看，数据库系统的就够分为单用户结构、主从结构、分布式结构、客户/服务器结构。</p><p>外模式：也称为子模式或用户模式。它是数据库用户看见和使用的、对局部数据的逻辑结构和特征的描述，是与某一应用有关的数据的逻辑表示。</p><p>模式：模式是数据库中数据的逻辑结构和特征的描述。它仅仅涉及到<strong>型</strong>的描述，不涉及到具体的值。模式的具体值被称为模式的一个实例。同一模式可以有很多实例。模式是相对稳定的，实例是相对变动的。模式反映的是数据的结构及其关系，而实例反映的是数据某一时刻的状态。</p><p>内模式：也称存储模式，它是对数据的物理结构和存储结构的描述，是数据在数据库内部的表示方式。</p><p>为了能够在内部实现这三个抽象层次的联系和转换，数据库系统在这三级模式之间提供了两层映像：外模式/概念模式映像、概念模式/内模式映像。这两层映像保证了数据库系统中的数据能够具有较高的逻辑独立性和物理独立性。</p><p>外模式/模式映像保证了数据与程序间的逻辑独立性，模式/内模式映像保证了数据的物理独立性。</p><p>逻辑独立性：当数据库的整体逻辑结构发生变化时，通过调整外模式和模式之间的映像，使得外模式中的局部数据及其结构不变，程序不在修改。</p><p>物理独立性：当数据库的存储结构发生变化时，通过调整模式和内模式之间的映像，使得整体模式不变，当然外模式及应用程序不用改变。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。&lt;/p&gt;
&lt;p&gt;数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。&lt;/p&gt;
&lt;p&gt;数据库数据具有永久存储、有组织、可共享三个基本特点。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="数据库概述" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>激活函数以及作用</title>
    <link href="http://yoursite.com/2019/09/03/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/09/03/激活函数/</id>
    <published>2019-09-03T13:37:07.000Z</published>
    <updated>2019-09-03T14:01:05.002Z</updated>
    
    <content type="html"><![CDATA[<p>本片博客介绍以下激活函数以及激活函数的作用。</p><p>首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL50.png" alt></p><p>激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。</p><p>问题一：为什么我们不能在不激活输入信号的情况下完成此操作呢？</p><p>如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。</p><p>问题二：那么为什么我们需要非线性函数？</p><p>非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。</p><p>接下来介绍一下常用的激活函数：sigmoid、tanh、Relu</p><p>sigmoid：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL45.png" alt></p><p>缺点：当 zz 值<strong>非常大</strong>或者<strong>非常小</strong>时，通过上图我们可以看到，sigmoid函数的导数 g′(z)将接近 00 。这会导致权重 W 的梯度将接近 00 ，使得梯度更新十分缓慢，即<strong>梯度消失</strong></p><p>tanh：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1) 之间，其公式与图形为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL42.png" alt></p><p>tanh函数的缺点同sigmoid函数的第一个缺点一样，当 zz <strong>很大或很小</strong>时，g′(z)接近于 0 ，会导致梯度很小，权重更新非常缓慢，即<strong>梯度消失问题</strong>。</p><p>Relu函数：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL41.png" alt></p><p>Leaky Relu：这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL43.png" alt></p><p>  其中a取值在（0，1）之间</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本片博客介绍以下激活函数以及激活函数的作用。&lt;/p&gt;
&lt;p&gt;首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="激活函数" scheme="http://yoursite.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>SSD实战-tensorflow实现目标检测</title>
    <link href="http://yoursite.com/2019/09/03/SSD%E5%AE%9E%E6%88%98-tensorflow%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/09/03/SSD实战-tensorflow实现目标检测/</id>
    <published>2019-09-03T11:35:07.000Z</published>
    <updated>2019-09-03T14:05:37.345Z</updated>
    
    <content type="html"><![CDATA[<p>​    因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow&lt;/p&gt;

      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/"/>
    
      <category term="SSD实战" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%AE%9E%E6%88%98/"/>
    
    
      <category term="SSD实战" scheme="http://yoursite.com/tags/SSD%E5%AE%9E%E6%88%98/"/>
    
  </entry>
  
  <entry>
    <title>人脸检测数据集构造</title>
    <link href="http://yoursite.com/2019/08/28/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E9%80%A0/"/>
    <id>http://yoursite.com/2019/08/28/人脸检测数据集构造/</id>
    <published>2019-08-28T07:08:15.000Z</published>
    <updated>2019-09-03T11:33:26.161Z</updated>
    
    <content type="html"><![CDATA[<p>在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。</p><p>Caffe-SSD数据集构造流程：</p><p>1、生成VOC个数数据集（图片、XML标注信息文件）</p><p>2、修改Caffe-SSD数据打包脚本相关路径配置</p><p>3、运行Caffe-SSD数据打包脚本</p><p>VOC格式数据集的目录下有三个文件夹：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD6.png" alt></p><p>Annotation中保存的是xml格式的label信息</p><p>ImageSet中Main目录中存放不同照片列表文件</p><p>​    train.txt：训练图片文件名列表</p><p>​    val.txt：验证图片文件名列表</p><p>​    trianval.txt：训练和验证的图片文件名列表</p><p>​    test.txt：测试图片文件名列表</p><p>JPEGImages目录存放所有的图片集。Annotation和JPEGImages一一对应。</p><p>WIDERFace数据集：打开<a href="http://mmlab.ie.cuhk.edu.hk/project/WIDERFace" target="_blank" rel="noopener">人脸检测数据集地址</a>下载数据集。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD7.png" alt></p><p>下载数据的训练集、验证集、测试集和标注信息。</p><p>打开标注信息，可以看到文件夹中有很多txt文件，打开看一下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD9.png" alt>****</p><p>对于VOC格式的数据集，最主要的是生成两个文件。一个是图片的标注数据，另一个是图片数据。对于图片数据，我们只需要将wider_face的图片放到对应目录下，而对于标注数据，我们需要将上面图片的文件进行解析来重新生成针对每张图片的标注信息，这个信息用XML格式存储。</p><p>对于wider_face转为voc，首先创建Annotation、ImageSets、JPEGImages三个文件夹，其中ImageSets中再创建一个Main文件夹。</p><p>将wider_face转为voc的coding：</p><p><code>import os,cv2,sys,shutilfrom xml.dom.minidom import Documentdef writexml(filename,saveing,bboxes,xmlpath):  doc=Document()  annotation=doc.createElement(&quot;annotation&quot;)  doc.appendChild(annotation)  folder=doc.createElement(&quot;folder&quot;)  folder_name=doc.createTextNode(&quot;widerface&quot;)  folder.appendChild(folder_name)  annotation.appendChild(folder)  filenamenode=doc.createElement(&quot;filename&quot;)  filename_name=doc.createTextNode(filename)  filenamenode.appendChild(filename_name)  annotation.appendChild(filenamenode)  source=doc.createElement(&quot;source&quot;)  annotation.appendChild(source)  database=doc.createElement(&quot;database&quot;)  database.appendChild(doc.createTextNode(&quot;wider face database&quot;))  source.appendChild(database)  annotation_s=doc.createElement(&quot;annotation&quot;)  annotation_s.appendChild(doc.createTextNode(&quot;PASCAL VOC2007&quot;))  source.appendChild(annotation_s)  image=doc.createElement(&quot;image&quot;)  image.appendChild(doc.createTextNode(&quot;flickr&quot;))  source.appendChild(image)  flickrid=doc.createElement(&quot;flickrid&quot;)  source.appendChild(doc.createTextNode(&quot;-1&quot;))  source.appendChild(flickrid)  owner=doc.createElement(&quot;owner&quot;)  annotation.appendChild(owner)  flickrid_o=doc.createElement(&quot;flickrid&quot;)  flickrid_o.appendChild(doc.createTextNode(&quot;yanyu&quot;))  owner.appendChild(flickrid_o)  name_o=doc.createElement(&quot;name&quot;)  name_o.appendChild(doc.createTextNode(&quot;yanyu&quot;))  owner.appendChild(name_o)  size=doc.createElement(&quot;size&quot;)  annotation.appendChild(size)  width=doc.createElement(&quot;width&quot;)  width.appendChild(doc.createTextNode(str(saveing.shape[1])))  height=doc.createElement(&quot;height&quot;)  height.appendChild(doc.createTextNode(str(saveing.shape[0])))  depth=doc.createElement(&quot;depth&quot;)  depth.appendChild(doc.createTextNode(str(saveing.shape[2])))  size.appendChild(width)  size.appendChild(height)  size.appendChild(depth)  segmented=doc.createElement(&quot;segmented&quot;)  segmented.appendChild(doc.createTextNode(&quot;0&quot;))  annotation.appendChild(segmented)  for i in range(len(bboxes)):    bbox=bboxes[i]    objects=doc.createElement(&quot;object&quot;)    annotation.appendChild(objects)    object_name=doc.createElement(&quot;name&quot;)    object_name.appendChild(doc.createTextNode(&quot;face&quot;))    objects.appendChild(object_name)    pose=doc.createElement(&quot;pose&quot;)    pose.appendChild(doc.createTextNode(&quot;Unspecified&quot;))    objects.appendChild(pose)    truncated=doc.createElement(&quot;truncated&quot;)    truncated.appendChild(doc.createTextNode(&quot;1&quot;))    objects.appendChild(truncated)    difficult=doc.createElement(&quot;difficult&quot;)    difficult.appendChild(difficult)    difficult.appendChild(doc.createTextNode(&quot;0&quot;))    objects.appendChild(difficult)    bndbox=doc.createElement(&quot;bndbox&quot;)    objects.appendChild(bndbox)    xmin=doc.createElement(&quot;xmin&quot;)    xmin.appendChild(doc.createTextNode(str(bbox[0])))    bndbox.appendChild(xmin)    ymin=doc.createElement(&quot;ymin&quot;)    ymin.appendChild(doc.createTextNode(str(bbox[1])))    bndbox.appendChild(ymin)    xmax=doc.createElement(&quot;xmax&quot;)    xmax.appendChild(doc.createTextNode(str(bbox[0]+bbox[2])))    bndbox.appendChild(xmax)    ymax=doc.createElement(&quot;ymax&quot;)    ymax.appendChild(doc.createTextNode(str(bbox[1]+bbox[3])))    bndbox.appendChild(ymax)  f=open(xmlpath,&quot;w&quot;)  f.write(doc.toprettyxml(indent=&quot; &quot;))  f.close()rootdir=&quot;./wider_face&quot;def convertimgset(img_set):  imgdir=rootdir+&quot;/WIDER_&quot;+img_set+&quot;/images&quot;  gtfilepath=rootdir+&quot;/wider_face_split/wider_face_&quot;+img_set+&quot;_bbx_gt.txt&quot;  fwrite=open(rootdir+&quot;/ImageSets/Main&quot;+img_set+&quot;.txt&quot;,&quot;w&quot;)  index=0  with open(gtfilepath,&quot;r&quot;) as gtfiles:    while(index&lt;1000):      filename=gtfiles.readline()[:-1]      if (filename==&quot;&quot;):        continue      imgpath=imgdir+&quot;/&quot;+filename      img=cv2.imread(imgpath)      if not img.data:        break      numbbox=int(gtfiles.readline())      bboxes=[]      for i in range(numbbox):        line=gtfiles.readline()        lines=line.split()        lines=lines[0:4]        bbox=(int(lines[0]),int(lines[1]),int(lines[2]),int(lines[3]))        bboxes.append(bbox)      filename=filename.replace(&quot;/&quot;,&quot; &quot;)      if len(bboxes)==0:        print(&quot;no face&quot;)      cv2.imwrite(&quot;{}/JPEGImages/{}&quot;.format(rootdir,filename),img)      fwrite.write(filename.split(&quot;.&quot;)[0]+&quot;\n&quot;)      xmlpath=&quot;{}/Annotations/{}&quot;.format(rootdir,filename.split(&quot;.&quot;)[0])      writexml(filename,img,bboxes,xmlpath)      print(&quot;第%d张成功的图片&quot; % index)      index+=1    fwrite.close()if __name__==&quot;__main__&quot;:  img_sets=[&quot;train&quot;,&quot;val&quot;]  for img_set in img_sets:    convertimgset(img_set)  shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;train.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;trainval.txt&quot;)  shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;val.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;test.txt&quot;)</code></p><p>这篇博客废了。。。。caffe的环境太难搭了。。。。</p><p>我将框架换成tensorflow，进行另一个实战</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。&lt;/p&gt;
&lt;p&gt;C
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one—stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/SSD/"/>
    
      <category term="SSD实战" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/SSD/SSD%E5%AE%9E%E6%88%98/"/>
    
      <category term="构造数据集" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one%E2%80%94stage/SSD/SSD%E5%AE%9E%E6%88%98/%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>SSD实战-人脸检测</title>
    <link href="http://yoursite.com/2019/08/28/SSD%E5%AE%9E%E6%88%98-%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/08/28/SSD实战-人脸检测/</id>
    <published>2019-08-28T01:51:26.000Z</published>
    <updated>2019-09-01T11:56:18.670Z</updated>
    
    <content type="html"><![CDATA[<p>人脸标注方法：矩形标注和椭圆形标注</p><p>矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、椭圆圆心x坐标、椭圆圆心y坐标。</p><p>判断算法性能好坏：</p><p>每一个标记只允许有一个检测与之相对应，也就是说，我们检测出来的每一个人脸图像只能同我们在标注中人脸图像中数据中的一个相对应。如果有多个，就会视为重复检测。重复检测会被视为错误检测。接着我们就可以得出正确率和错误率，可以画出ROC曲线和PR曲线。可以根据曲线看出算法的好坏。</p><p>人脸采集常用方法：</p><p><strong>活体检测</strong>  判断用户是否为正常操作，通过指定用户做随机动作，一搬有张嘴、摇头、点头、凝视、眨眼等等，防止照片攻击。判断用户是否真实在操作，指定用户上下移动手机，防止视频攻击和非正常动作的攻击。</p><p><strong>3D检测</strong>  验证采集到的是否为立体人像，能够防止平面照片、不同弯曲程度的照片等。</p><p><strong>连续检测</strong>  通过连续的检测，验证运动轨迹是否正常，防止跳过活体检测直接替换采集的照片，也能防止中途换人。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;人脸标注方法：矩形标注和椭圆形标注&lt;/p&gt;
&lt;p&gt;矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/"/>
    
      <category term="SSD实战" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%AE%9E%E6%88%98/"/>
    
      <category term="人脸检测综述" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%AE%9E%E6%88%98/%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>目标检测常用名词</title>
    <link href="http://yoursite.com/2019/08/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D/"/>
    <id>http://yoursite.com/2019/08/22/目标检测常用名词/</id>
    <published>2019-08-22T01:49:17.000Z</published>
    <updated>2019-09-03T12:17:18.677Z</updated>
    
    <content type="html"><![CDATA[<p>图像分类：一张图像中是否包含某种物体</p><p>物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。</p><p>语义分割：按对象得内容进行图像得分割，分割的依据是内容，即对象类别。</p><p>实例分割：按对象个体进行分割，分割的依据是单个目标。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8717.png" alt></p><p><strong>滑动窗口</strong>——为什么要有候选区域？既然目标是在图像中的某一区域，那么最直接的方法就是滑窗法（sliding window approach），就是遍历图像的所有区域，用不同大小的窗口在整个图像上滑动，那么就会产生所有的矩形区域，然后再后续排查，思路很简单，但是开销巨大。</p><p>region proposal（RP）：候选区域</p><p>IOU：region proposal与Ground Truth的窗口的交集比并集的比值，相当于准确率。‘</p><p>SPP：Spatial Pyramid Pooling  空间金字塔采样  在pooing的过程中计算pooling后的结果对应的两个像素点映射到feature map上所占的范围，然后在那个范围中进行max或者average。</p><p>ROI Pooling：就是将一个个大小不同的box矩形框，都映射到大小为w*h的矩形框。</p><p>Anchor：请看<a href="[https://brickexperts.github.io/2019/08/18/Two-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/#more](https://brickexperts.github.io/2019/08/18/Two-stage基本介绍/#more)">Two-stage基本介绍</a></p><p>GT box:Ground Truth box</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8722.png" alt></p><p>如上图所示，绿色的框为飞机的Ground Truth，红色的框是提取的Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU&lt;0.5)，那么这张图相当于没有正确的检测出飞机。如果我们能对红色的框进行微调，使得经过微调后的窗口跟Ground Truth更接近，这样岂不是定位会更准确。</p><p>带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。</p><p>如下图所示，(a)是普通的3×33×3卷积，其视野就是3×33×3，(b)是扩张率为2，此时视野变成7×77×7，(c)扩张率为4时，视野扩大为15×1515×15，但是视野的特征更稀疏了。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD10.png" alt></p><p>后面遇见会继续完善。。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;图像分类：一张图像中是否包含某种物体&lt;/p&gt;
&lt;p&gt;物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。&lt;/
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="常用名词解释" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/"/>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SSD系列算法</title>
    <link href="http://yoursite.com/2019/08/21/SSD%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/08/21/SSD系列算法/</id>
    <published>2019-08-21T01:05:48.000Z</published>
    <updated>2019-09-04T11:54:49.116Z</updated>
    
    <content type="html"><![CDATA[<h2 id="主干网络介绍"><a href="#主干网络介绍" class="headerlink" title="主干网络介绍"></a>主干网络介绍</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8720.png" alt></p><p>主干网络原始作者采用的VGG16，我们也可以将其他神经网络作为主干网络。例如：ResNet、MobileNets等</p><p>输入300<em>300的image，将VGG16网络FC6、FC7换成conv6和conv7，同时将池化层变为stride=1，pool_size=3\</em>3，这样做的目的是为了不减少feature map  size，为了配合这种变化，conv6会使用扩张率为6的带孔卷积。</p><p>​    <strong>带孔卷积</strong>：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。具体看<a href="[https://brickexperts.github.io/2019/08/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D/#more](https://brickexperts.github.io/2019/08/22/目标检测常用名词/#more)">目标检测常用名词</a></p><p>接着然后移除dropout层和fc8层，并新增conv7，conv8，conv9，conv10，conv11，在检测数据集上做finetuing。其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是38×38，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet），以保证和后面的检测层差异不是很大，这个和Batch Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma。</p><h2 id="多尺度feature-map预测"><a href="#多尺度feature-map预测" class="headerlink" title="多尺度feature map预测"></a>多尺度feature map预测</h2><p>多尺度feature map预测，也就是在预测的时候，在接下来预测的时候，针对接下来六个不同的尺寸进行预测。如下图的六条连线，分别是38*38、19*19、10*10、5*5、3*3、1*1。将这六个不同尺度的feature map分别作为检测、预测层的输入，最后通过NMS进行筛选和合并。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8720.png" alt></p><p>对于六种不同尺度的网络，我们通常使用pooling来降采样。对于每一层的feature map，我们输入到相应的预测网络中。而预测网络中，我们会包括Prior box的提取过程。Prior box对应Fast R-CNN中Anchor的概念，也就是说，在Prior box中，feature map上的每一个点都作为一个cell（相当于Anchor）。以这个cell为中心，按照等比的放缩，找到它在原始图片的位置。接着以这个点为中心，提取不同尺度bounding box。而这些不同尺度的bounding box就是Prior box。然后对于每一个Prior box，我们通过和真值比较，就能够拿到它的label。对于每一个Prior box，我们都会分别预测它的类别概率和坐标（x，y，w，h）。也就是说，对于每一个cell，我们会将它对应到不同的Prior box，分别来预测当前这个Prior box所对应当前这个类别的概率分布和坐标。</p><p><strong>对Prior Box的具体定义：</strong></p><p>这里我们假设Prior Box的输入是m*n维的feature map。</p><p>如果每一个点都作为cell，那就会有m<em>n个cell。接着每个cell上生成固定尺寸和不同长宽比例的box。每个cell对应k个bounging box，每个bounding box预测c个类别分数和4个偏移坐标。其中c个类别分数实际上是当前bounding box所对应的不同类别的概率分布。如果输入大小为m\</em>n，那就会输出(c+4)*k*m*n。其中尺寸(scale)和比例(ratio)是超参数。</p><p>接下来我们看看Prior box是怎么生成的：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8721.png" alt></p><p>每个feature map上的点定义了六种长宽比的default box。也就是说，最后对于每一个anchor都会获得六个不同尺寸和长宽比的default box。对于38<em>38层，每个feature map上的点，我们都会提取4个default box作为prior box。对于19\</em>19层、10*10层、5*5，提取6个default box也就是全部都是prior box。而3*3、1*1提取4个default作为prior box。所以最后得到8732个prior box(38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4)。<strong>prior box就是选择的default box</strong>。尺寸和比例都是可以通过SSD的配置文件进行配置，后面实战详解。</p><p>对default box进行筛选成为prior box：每一个feature map cell不是k个default box都取，prior box与GT box(<a href="https://brickexperts.github.io/" target="_blank" rel="noopener">Ground Truth box</a>)做匹配，IOU&gt;阈值为正样本。IOU&lt;阈值 为负样本。介于正样本和负样本中间阈值的default box去掉。</p><h2 id="SSD系列算法优化及扩展"><a href="#SSD系列算法优化及扩展" class="headerlink" title="SSD系列算法优化及扩展"></a>SSD系列算法优化及扩展</h2><p>SSD算法对小目标不够鲁棒，原因最主要是浅层feature map的表征能力不够强。</p><p>DSSD：</p><p>DSSD相当原来的SSD模型主要作了两大更新。一是替换掉VGG，而改用了Resnet-101作为特征提取网络并在对不同尺度feature maps特征进行default boxes检测时使用了更新的检测单元；二则在网络的后端使用了多个deconvolution layers以有效地扩展低维度信息的contextual information，从而有效地提高了小尺度目标的检测。</p><p>下图为DSSD模型与SSD模型的整体网络结构对比：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD4.png" alt></p><p>DSOD：</p><p>SSD+DenseNet=DSOD</p><p>DSOD可以从0开始训练数据，不需要预训练模型。</p><p>FSSD：</p><p>借鉴了FPN的思想，重构了一组pyramid feature map（金字塔特征），使得算法的精度有了明显特征，速度也没有下降很多。具体是把网络中某些feature调整为同一size再contact（连接），得到一个像素层，以此层为base layer来生成pyramid feature map，作者称之为Feature Fusion Module。</p><p>Feature Fusion</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD2.png" alt></p><p>对上面图的解读：</p><p>​    (a) image pyramid </p><p>​    (b) rcnn系列，只在最后一层feature预测 </p><p>​    (c) FPN，语义信息一层传递回去，而且有很多相加的计算 </p><p>​    (d) SSD，在各个level的feature上直接预测，每个level之间没联系 </p><p>​    (e) FSSD的做法，把各个level的feature concat，然后从fusion feature上生成feature pyramid</p><p>FSSD网络结构：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD3.png" alt></p><p>RSSD：</p><p>rainbow concatenation方式(pooling加deconvolution)融合不同层的特征，再增加不同层之间feature map关系的同时也增加了不同层的feature map个数。这种融合方式不仅解决了传统SSD算法存在的重复框问题，同时一定程度上解决了small object的检测问题。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/SSD1.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;主干网络介绍&quot;&gt;&lt;a href=&quot;#主干网络介绍&quot; class=&quot;headerlink&quot; title=&quot;主干网络介绍&quot;&gt;&lt;/a&gt;主干网络介绍&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Brickexper
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="one-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/"/>
    
      <category term="SSD" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/"/>
    
      <category term="SSD原理" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/one-stage/SSD/SSD%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="SSD算法" scheme="http://yoursite.com/tags/SSD%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>One-stage基本介绍</title>
    <link href="http://yoursite.com/2019/08/20/One-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/08/20/One-stage基本介绍/</id>
    <published>2019-08-20T01:44:40.000Z</published>
    <updated>2019-08-20T09:25:33.086Z</updated>
    
    <content type="html"><![CDATA[<p>​    One-stage也是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。</p><p>One-stage常见算法：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8715.png" alt></p><p>One-stage核心组件：</p><p>CNN网络</p><p>CNN网络设计原则：从简到繁到简的卷积神经网    多尺度特征融合的网络    更轻量级的CNN网络</p><p>回归网络    </p><p><strong>One-stage和Two-stage的区别在于是否存在RPN网络。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    One-stage也是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。&lt;/p&gt;
&lt;p&gt;One-stage常见算法：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.gi
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="One-stage" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/"/>
    
      <category term="One-stage基本介绍" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/One-stage/One-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="One-stage" scheme="http://yoursite.com/tags/One-stage/"/>
    
  </entry>
  
  <entry>
    <title>Two-stage基本介绍</title>
    <link href="http://yoursite.com/2019/08/18/Two-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yoursite.com/2019/08/18/Two-stage基本介绍/</id>
    <published>2019-08-18T09:09:22.000Z</published>
    <updated>2019-08-22T02:23:46.976Z</updated>
    
    <content type="html"><![CDATA[<p>​    Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。</p><p>Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8714.png" alt></p><p>Two-stage核心组件：</p><p><strong>CNN网络</strong></p><p> CNN网络设计原则：</p><p>从简到繁再到简的卷积神经网</p><p>多尺度特征融合的网络</p><p>更轻量级的CNN网络</p><p><strong>RPN网络</strong></p><p>区域推荐（Anchor机制）：首先我们需要知道anchor的本质是什么，本质是SPP(spatial pyramid pooling)思想的逆向。而SPP本身是做什么的呢，就是将不同尺寸的输入resize成为相同尺寸的输出。所以SPP的逆向就是，将相同尺寸的输出，倒推得到不同尺寸的输入。当前的feature map的大小是n*c*w*h（n是张数，c是层数，w是宽，h是高），对于当前这个feature map上，也就是w<em>h上，选择其中的每一个点作为锚点（也就是候选区域的中心点），我们以每一个点作为中心点去提取候选区域。这样的每一个点都是Anchor。而这个候选区域通常都有比例：对于Fast  R-CNN三个面积尺寸（128^2，256^2，512^2），然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）。一个feature map可以提取w\</em>h*9个候选区域。我们示意图如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/%E7%9B%AE%E6%A0%8716.png" alt></p><p>ROI Pooling</p><p>分类和回归</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。&lt;/p&gt;
&lt;p&gt;Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://
      
    
    </summary>
    
      <category term="目标检测" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Two-stage算法" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/Two-stage%E7%AE%97%E6%B3%95/"/>
    
      <category term="Two-stage基本介绍" scheme="http://yoursite.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/Two-stage%E7%AE%97%E6%B3%95/Two-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="Two-stage" scheme="http://yoursite.com/tags/Two-stage/"/>
    
  </entry>
  
</feed>
