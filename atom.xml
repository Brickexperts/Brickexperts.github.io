<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DY的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-24T08:13:29.577Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>湛蓝星空</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>检查点</title>
    <link href="http://yoursite.com/2020/02/23/%E6%A3%80%E6%9F%A5%E7%82%B9/"/>
    <id>http://yoursite.com/2020/02/23/检查点/</id>
    <published>2020-02-23T11:26:06.000Z</published>
    <updated>2020-02-24T08:13:29.577Z</updated>
    
    <content type="html"><![CDATA[<p>保存模型并不限于训练模型后，在训练模型之中也需要保存，因为Tensorflow训练模型时难免会出现中断的情况，我们自然希望能够将训练得到的参数保存下来，否则下次又要重新训练。这种在训练中保存模型，习惯上称之为保存检查点。</p><p>tf.train.get_checkpoint_state(checkpoint_dir,latest_filename=None)：该函数表示如果断点文件夹中包含有效断点状态文件，则返回该文件。参数说明：</p><p>​                checkpoint_dir：表示存储断点文件的目录</p><p>​                latest_filename=None：断点文件的可选名称，默认为”checkpoint”</p><p>通过添加检查点，可以生成载入检查点文件，并能够指定生成检查文件的个数。saver中的max_to_keep=1，表面最多只保存一个检查点文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver=tf.train.Saver(max_to_keep=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>对checkpoint文件进行加载的第一种方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cpkt = tf.train.get_checkpoint_state(savedir)</span><br><span class="line"><span class="keyword">if</span> cpkt <span class="keyword">and</span> cpkt.model_checkpoint_path:</span><br><span class="line">　　saver.restore(sess2, cpkt.model_checkpoint_path)</span><br></pre></td></tr></table></figure><p>上面代码位置：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/image-bed3/master/20200224160752.png" alt=""></p><p>第二种方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kpt = tf.train.latest_checkpoint(savedir)</span><br><span class="line">saver.restore(sess2, kpt)</span><br></pre></td></tr></table></figure><p>我们还可以用更加简便的方法进行检查点的保存，tf.train.MonitoredTrainingSession()函数，该函数可以直接实现保存载入检查点模型的文件，与前面的方法不同的是，它是按照训练时间来保存检查点的，可以通过指定save_checkpoint_secs参数的具体秒数，设置多久保存一次检查点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#每5秒后，保存一次检查点。默认的保存时间间隔是10分钟</span></span><br><span class="line"><span class="keyword">with</span> tf.train.MonitoredTrainingSession(checkpoint_dir=savedir+<span class="string">"linear.cpkt"</span>,save_checkpoint_secs=<span class="number">5</span>) <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>这种按照时间保存的模式更适合用于使用大型数据集来训练复杂模型的情况。</p><p><strong>使用该方法前，必须要定义global_step变量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">global_step=tf.train.get_or_create_global_step()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">train_x = np.linspace(<span class="number">-5</span>, <span class="number">3</span>, <span class="number">50</span>)</span><br><span class="line">train_y = train_x * <span class="number">5</span> + <span class="number">10</span> + np.random.random(<span class="number">50</span>) * <span class="number">10</span> - <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.plot(train_x, train_y, 'r.')</span></span><br><span class="line"><span class="comment"># plt.grid(True)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(dtype=tf.float32)</span><br><span class="line">Y = tf.placeholder(dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.random.truncated_normal([<span class="number">1</span>]), name=<span class="string">'Weight'</span>)</span><br><span class="line">b = tf.Variable(tf.random.truncated_normal([<span class="number">1</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line">z = tf.multiply(X, w) + b</span><br><span class="line"></span><br><span class="line">cost = tf.reduce_mean(tf.square(Y - z))</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">training_epochs = <span class="number">30</span></span><br><span class="line">display_step = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">global_step = tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">step = tf.assign_add(global_step, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">savedir = <span class="string">"check-point/"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">with</span> tf.train.MonitoredTrainingSession(checkpoint_dir=savedir + <span class="string">'linear.cpkt'</span>, save_checkpoint_secs=<span class="number">5</span>) <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        loss_list = []</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">            sess.run(global_step)</span><br><span class="line">            <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_x, train_y):</span><br><span class="line">                sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">                loss = sess.run(cost, feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">                loss_list.append(loss)</span><br><span class="line">                print(<span class="string">'Iter: '</span>, epoch, <span class="string">' Loss: '</span>, loss)</span><br><span class="line"></span><br><span class="line">            w_, b_ = sess.run([w, b], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">            sess.run(step)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">" Finished "</span>)</span><br><span class="line">        print(<span class="string">"W: "</span>, w_, <span class="string">" b: "</span>, b_, <span class="string">" loss: "</span>, loss)</span><br><span class="line">        plt.plot(train_x, train_x * w_ + b_, <span class="string">'g-'</span>, train_x, train_y, <span class="string">'r.'</span>)</span><br><span class="line">        plt.grid(<span class="literal">True</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    load_epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess2:</span><br><span class="line">        sess2.run(tf.global_variables_initializer())</span><br><span class="line">        kpt = tf.train.latest_checkpoint(savedir + <span class="string">'linear.cpkt'</span>)</span><br><span class="line">        saver.restore(sess2, kpt)</span><br><span class="line">        print(sess2.run([w, b], feed_dict=&#123;X: train_x, Y: train_y&#125;))</span><br></pre></td></tr></table></figure><p>第二个保存检查点的方法参考： <a href="https://www.cnblogs.com/baby-lily/p/10930591.html" target="_blank" rel="noopener">https://www.cnblogs.com/baby-lily/p/10930591.html</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;保存模型并不限于训练模型后，在训练模型之中也需要保存，因为Tensorflow训练模型时难免会出现中断的情况，我们自然希望能够将训练得到的参数保存下来，否则下次又要重新训练。这种在训练中保存模型，习惯上称之为保存检查点。&lt;/p&gt;
&lt;p&gt;tf.train.get_checkp
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>识别手写数字实战</title>
    <link href="http://yoursite.com/2020/02/19/%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2020/02/19/识别手写数字实战/</id>
    <published>2020-02-19T10:59:02.000Z</published>
    <updated>2020-02-23T11:21:42.620Z</updated>
    
    <content type="html"><![CDATA[<p>本次实战使用的数据集是mnist。tensorflow提供了一个库，可以直接用在下载MNIST，见下面代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"./mnist_dataset/"</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>运行上面的代码，会自动下载数据集并将文件解压到当前代码所在的同级目录下。one_hot=True表示将样本标签转化为one-hot编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回各子集样本数</span></span><br><span class="line">print(<span class="string">"train data size"</span>,mnist.train.num_examples)</span><br><span class="line"><span class="comment">#train data size  55000</span></span><br><span class="line">print(<span class="string">"validation data size"</span>,mnist.validation.num_examples)</span><br><span class="line"><span class="comment">#validation data size 5000</span></span><br><span class="line">print(<span class="string">"test data size"</span>,mnist.test.num_example)</span><br><span class="line"><span class="comment">#test data size 10000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回标签，第1张图片的one-hot编码</span></span><br><span class="line">mnist.train.labels[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#返回数据，第1张图片的784个像素点</span></span><br><span class="line">mnist.train.images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variables</span><span class="params">(shape)</span>:</span></span><br><span class="line">    w=tf.Variable(tf.random_normal(shape=shape,mean=<span class="number">1.0</span>,stddev=<span class="number">1.0</span>))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variables</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b=tf.Variable(tf.constant(<span class="number">0.0</span>,shape=shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">()</span>:</span></span><br><span class="line">    x=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">    y_true=tf.placeholder(tf.int32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line">    <span class="comment">#随机初始化权重,第一层卷积：5*5*1，32个，strides=1</span></span><br><span class="line">    w_conv1=weight_variables([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])</span><br><span class="line">    b_conv1=bias_variables([<span class="number">32</span>])</span><br><span class="line">    <span class="comment">#将图片大小转为对应成4D输入 [None,784]-----&gt;[None,28,28,1]</span></span><br><span class="line">    x_reshape=tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#先卷积后激活函数，卷积：[None,28,28,1]-----&gt;[None,28,28,32]</span></span><br><span class="line">    x_relu1=tf.nn.relu(tf.nn.conv2d(x_reshape,w_conv1,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)+b_conv1)</span><br><span class="line">    <span class="comment">#池化 2*2 strides=2  [None,28,28,32]-----&gt;[None,14,14,32]</span></span><br><span class="line">    x_pool1=tf.nn.max_pool(x_relu1,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    <span class="comment">#第二个卷积层：[5,5,32,64] 偏置 64</span></span><br><span class="line">    w_conv2=weight_variables([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])</span><br><span class="line">    b_conv2=bias_variables([<span class="number">64</span>])</span><br><span class="line">    <span class="comment">#卷积 [None,14,14,32]-----&gt;[None,14,14,64]</span></span><br><span class="line">    x_relu2 = tf.nn.relu(tf.nn.conv2d(x_pool1, w_conv2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>) + b_conv2)</span><br><span class="line">    <span class="comment">#池化 2*2 stride2 [None,14,14,64]------&gt;[None,7,7,64]</span></span><br><span class="line">    x_pool2=tf.nn.max_pool(x_relu2,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    w_fc=weight_variables([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>,<span class="number">10</span>])</span><br><span class="line">    b_fc=bias_variables([<span class="number">10</span>])</span><br><span class="line">    x_fc_reshape=tf.reshape(x_pool2,[<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">    <span class="comment">#矩阵运算</span></span><br><span class="line">    y_predict=tf.matmul(x_fc_reshape,w_fc)+b_fc</span><br><span class="line">    <span class="keyword">return</span> x,y_true,y_predict</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_fc</span><span class="params">()</span>:</span></span><br><span class="line">    mnist=input_data.read_data_sets(<span class="string">"./mnist_dataset/"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line">    x,y_true,y_predict=model()</span><br><span class="line">    <span class="comment"># 求平均交叉熵损失</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))</span><br><span class="line">    <span class="comment"># 梯度下降求出损失</span></span><br><span class="line">    train_op = tf.train.GradientDescentOptimizer(<span class="number">0.0001</span>).minimize(loss)</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    equal_list = tf.equal(tf.argmax(y_true, <span class="number">1</span>), tf.argmax(y_predict, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">#cast转换数据类型</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            mnist_x,mnist_y=mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">            sess.run(train_op,feed_dict=&#123;x:mnist_x,y_true:mnist_y&#125;)</span><br><span class="line">            print(<span class="string">"训练第%d步，准确率为：%f"</span> % (i,sess.run(accuracy,feed_dict=&#123;x:mnist_x,y_true:mnist_y&#125;)))</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    conv_fc()</span><br></pre></td></tr></table></figure><p>上面的代码训练效果实在惨不忍睹，训练了半天还是连20%都没过。所以，又去抄了大佬的代码学习，他分成三个文件：mnist_forward、mnist_backward、mnist_test。</p><p>mnist_forward</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#输入节点</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line"><span class="comment">#输出十个数，每个数代表每个数字的概率</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line"><span class="comment">#隐藏节点个数</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    <span class="comment">#随机生成w</span></span><br><span class="line">    w = tf.Variable(tf.truncated_normal(shape,stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="comment">#正则化</span></span><br><span class="line">    <span class="keyword">if</span> regularizer != <span class="literal">None</span>: tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span>  </span><br><span class="line">    b = tf.Variable(tf.zeros(shape))  </span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">    b1 = get_bias([LAYER1_NODE])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">    b2 = get_bias([OUTPUT_NODE])</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>mnist_backward</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#每轮训练200张图片</span></span><br><span class="line">BATCH_SIZE = <span class="number">200</span></span><br><span class="line"><span class="comment">#初始学习率</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span></span><br><span class="line"><span class="comment">#学习率衰减率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line"><span class="comment">#正则化系数</span></span><br><span class="line">REGULARIZER = <span class="number">0.0001</span></span><br><span class="line"><span class="comment">#共训练50000</span></span><br><span class="line">STEPS = <span class="number">50000</span></span><br><span class="line"><span class="comment">#滑动平均衰减率</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line"><span class="comment">#模型存放的路径</span></span><br><span class="line">MODEL_SAVE_PATH=<span class="string">"./model/"</span></span><br><span class="line"><span class="comment">#模型保存文件名</span></span><br><span class="line">MODEL_NAME=<span class="string">"mnist_model"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(mnist)</span>:</span></span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">    y = mnist_forward.forward(x, REGULARIZER)</span><br><span class="line">    <span class="comment">#设定global_step初值为0，设定为不可训练</span></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    <span class="comment">#加上正则化</span></span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line">    <span class="comment">#学习率衰减</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, </span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"><span class="comment">#滑动平均</span></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"After %d training step(s), loss on training batch is %g."</span> % (step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>mnist_test</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> mnist_backward</span><br><span class="line">TEST_INTERVAL_SECS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">        x = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">        y = mnist_forward.forward(x, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">                <span class="comment">#保存节点状态</span></span><br><span class="line">                <span class="comment">#26、27行代码用于访问到最新保存的节点文件</span></span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br><span class="line">                    print(<span class="string">"After %s training step(s), test accuracy = %g"</span> % (global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">'No checkpoint file found'</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">            time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本次实战使用的数据集是mnist。tensorflow提供了一个库，可以直接用在下载MNIST，见下面代码。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span clas
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>房价预测实战</title>
    <link href="http://yoursite.com/2020/02/19/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2020/02/19/房价预测实战/</id>
    <published>2020-02-19T08:38:48.000Z</published>
    <updated>2020-02-19T10:57:42.860Z</updated>
    
    <content type="html"><![CDATA[<p>用到的模块也是pandas、numpy、tensorflow。</p><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#归一化处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(df)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> df.apply(<span class="keyword">lambda</span> column:(column-column.mean())/column.std())</span><br><span class="line">df=pd.read_csv(<span class="string">"data1.csv"</span>,names=[<span class="string">"square"</span>,<span class="string">"bedrooms"</span>,<span class="string">"price"</span>])</span><br><span class="line">df=normalize_feature(df)</span><br><span class="line">print(df.head())</span><br><span class="line">X_data=np.array(df[df.columns[<span class="number">0</span>:<span class="number">2</span>]])</span><br><span class="line">y_data=np.array(df[df.columns[<span class="number">-1</span>]]).reshape(len(df),<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>为什么归一化？</p><p> 最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。 </p><h2 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">alpha = <span class="number">0.01</span> <span class="comment"># 学习率 alpha</span></span><br><span class="line">epoch = <span class="number">500</span> <span class="comment"># 轮数</span></span><br><span class="line">X = tf.placeholder(tf.float32, X_data.shape, name=<span class="string">'X'</span>)</span><br><span class="line">    <span class="comment"># 输出 y，形状[47, 1]</span></span><br><span class="line">y = tf.placeholder(tf.float32, y_data.shape, name=<span class="string">'y'</span>)</span><br><span class="line"><span class="comment">#W = tf.get_variable("weights",(X_data.shape[1], 1),initializer=tf.constant_initializer())</span></span><br><span class="line">W=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>]))</span><br><span class="line">b=tf.Variable(tf.random_normal([<span class="number">1</span>]))</span><br><span class="line">    <span class="comment">#  y_pred  形状[47,1]</span></span><br><span class="line">y_pred = tf.matmul(X, W, name=<span class="string">'y_pred'</span>)+b</span><br><span class="line">  <span class="comment"># tf.matmul(a,b,transpose_a=True) 表示：矩阵a的转置乘矩阵b，即 [1,47] X [47,1]</span></span><br><span class="line">loss_op = <span class="number">1</span> / (<span class="number">2</span> * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=<span class="literal">True</span>)</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(learning_rate=alpha).minimize(loss_op)</span><br></pre></td></tr></table></figure><h2 id="全部代码"><a href="#全部代码" class="headerlink" title="全部代码"></a>全部代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#归一化处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(df)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> df.apply(<span class="keyword">lambda</span> column:(column-column.mean())/column.std())</span><br><span class="line">df=pd.read_csv(<span class="string">"data1.csv"</span>,names=[<span class="string">"square"</span>,<span class="string">"bedrooms"</span>,<span class="string">"price"</span>])</span><br><span class="line">df=normalize_feature(df)</span><br><span class="line">print(df.head())</span><br><span class="line">X_data=np.array(df[df.columns[<span class="number">0</span>:<span class="number">2</span>]])</span><br><span class="line">y_data=np.array(df[df.columns[<span class="number">-1</span>]]).reshape(len(df),<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">alpha = <span class="number">0.01</span> <span class="comment"># 学习率 alpha</span></span><br><span class="line">epoch = <span class="number">500</span> <span class="comment"># 轮数</span></span><br><span class="line">X = tf.placeholder(tf.float32, X_data.shape, name=<span class="string">'X'</span>)</span><br><span class="line">    <span class="comment"># 输出 y，形状[47, 1]</span></span><br><span class="line">y = tf.placeholder(tf.float32, y_data.shape, name=<span class="string">'y'</span>)</span><br><span class="line"><span class="comment">#W = tf.get_variable("weights",(X_data.shape[1], 1),initializer=tf.constant_initializer())</span></span><br><span class="line">W=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>]))</span><br><span class="line">b=tf.Variable(tf.random_normal([<span class="number">1</span>]))</span><br><span class="line">    <span class="comment">#  y_pred  形状[47,1]</span></span><br><span class="line">y_pred = tf.matmul(X, W, name=<span class="string">'y_pred'</span>)+b</span><br><span class="line">  <span class="comment"># tf.matmul(a,b,transpose_a=True) 表示：矩阵a的转置乘矩阵b，即 [1,47] X [47,1]</span></span><br><span class="line">loss_op = <span class="number">1</span> / (<span class="number">2</span> * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=<span class="literal">True</span>)</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(learning_rate=alpha).minimize(loss_op)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化全局变量</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 记录所有损失值</span></span><br><span class="line">    loss_data = []</span><br><span class="line">    <span class="comment"># 开始训练模型</span></span><br><span class="line">    <span class="comment"># 因为训练集较小，所以采用批梯度下降优化算法，每次都使用全量数据训练</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">1</span>, epoch + <span class="number">1</span>):</span><br><span class="line">        _, loss, w = sess.run([train_op, loss_op, W], feed_dict=&#123;X: X_data, y: y_data&#125;)</span><br><span class="line">        <span class="comment"># 记录每一轮损失值变化情况</span></span><br><span class="line">        loss_data.append(float(loss))</span><br><span class="line">        <span class="keyword">if</span> e % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            log_str = <span class="string">"Epoch %d \t Loss=%.4g \t"</span></span><br><span class="line">            print(log_str % (e, loss))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用到的模块也是pandas、numpy、tensorflow。&lt;/p&gt;
&lt;h2 id=&quot;数据处理&quot;&gt;&lt;a href=&quot;#数据处理&quot; class=&quot;headerlink&quot; title=&quot;数据处理&quot;&gt;&lt;/a&gt;数据处理&lt;/h2&gt;&lt;figure class=&quot;highlight p
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>TCP的长连接和短连接</title>
    <link href="http://yoursite.com/2020/02/17/TCP%E7%9A%84%E9%95%BF%E8%BF%9E%E6%8E%A5%E5%92%8C%E7%9F%AD%E8%BF%9E%E6%8E%A5/"/>
    <id>http://yoursite.com/2020/02/17/TCP的长连接和短连接/</id>
    <published>2020-02-17T07:30:43.000Z</published>
    <updated>2020-02-17T07:45:40.141Z</updated>
    
    <content type="html"><![CDATA[<p>本文参考： <a href="https://www.cnblogs.com/georgexu/p/10909814.html" target="_blank" rel="noopener">https://www.cnblogs.com/georgexu/p/10909814.html</a> </p><p>在TCP真正的读写之前，server和client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时它们可以释放这个连接，连接的建立通过三次握手，释放时需要四次握手，每个连接的建立都是需要资源消耗和时间消耗。</p><h2 id="TCP短连接"><a href="#TCP短连接" class="headerlink" title="TCP短连接"></a>TCP短连接</h2><p>模拟一种TCP短连接的情况:</p><ol><li>client 向 server 发起连接请求</li><li>server 接到请求，双方建立连接</li><li>client 向 server 发送消息</li><li>server 回应 client</li><li>一次读写完成，此时双方任何一个都可以发起 close 操作</li></ol><p>在步骤5中，一般都是 client 先发起 close 操作。当然也不排除有特殊的情况。</p><p>从上面的描述看，短连接一般只会在 client/server 间传递一次读写操作！</p><h2 id="TCP长连接"><a href="#TCP长连接" class="headerlink" title="TCP长连接"></a>TCP长连接</h2><p>再模拟一种长连接的情况:</p><ol><li>client 向 server 发起连接</li><li>server 接到请求，双方建立连接</li><li>client 向 server 发送消息</li><li>server 回应 client</li><li>一次读写完成，连接不关闭</li><li>后续读写操作…</li><li>长时间操作之后client发起关闭请求</li></ol><h2 id="TCP长-短连接操作过程"><a href="#TCP长-短连接操作过程" class="headerlink" title="TCP长/短连接操作过程"></a>TCP长/短连接操作过程</h2><h3 id="短连接的操作步骤是："><a href="#短连接的操作步骤是：" class="headerlink" title="短连接的操作步骤是："></a>短连接的操作步骤是：</h3><p>建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接</p><h3 id="长连接的操作步骤是："><a href="#长连接的操作步骤是：" class="headerlink" title="长连接的操作步骤是："></a>长连接的操作步骤是：</h3><p>建立连接——数据传输…（保持连接）…数据传输——关闭连接</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适用长连接。</p><p>client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。</p><p>短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文参考： &lt;a href=&quot;https://www.cnblogs.com/georgexu/p/10909814.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/georgexu/p/109098
      
    
    </summary>
    
      <category term="网络知识" scheme="http://yoursite.com/categories/%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="网络知识" scheme="http://yoursite.com/tags/%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>泰坦尼克号获救预测实战</title>
    <link href="http://yoursite.com/2020/02/14/%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E8%8E%B7%E6%95%91%E9%A2%84%E6%B5%8B%E5%AE%9E%E6%88%98/"/>
    <id>http://yoursite.com/2020/02/14/泰坦尼克号获救预测实战/</id>
    <published>2020-02-14T07:59:46.000Z</published>
    <updated>2020-02-15T08:24:20.726Z</updated>
    
    <content type="html"><![CDATA[<p>在本次实战，会用到numpy和pandas等模块，不会的话，请补完自己的知识空区。</p><p><a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">本次数据集的下载网址</a></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data=pd.read_csv(<span class="string">"train.csv"</span>)</span><br><span class="line"><span class="comment">#查看数据集的字段</span></span><br><span class="line">print(data.columns)</span><br></pre></td></tr></table></figure><p>数据字段含义如下：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200214190725.png" alt=""></p><p>接着我们把一些没有什么用的字段删掉，比如Name、Ticket</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=data[[<span class="string">"Survived"</span>,<span class="string">"Pclass"</span>,<span class="string">"Sex"</span>,<span class="string">"Age"</span>,<span class="string">"SibSp"</span>,<span class="string">"Parch"</span>,<span class="string">"Fare"</span>,<span class="string">"Cabin"</span>,<span class="string">"Embarked"</span>]]</span><br></pre></td></tr></table></figure><p>接着，我们发现有一些NAN的数据，我们用年龄的平均填充一下，即使使用平均年龄填充并不是很好</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"Age"</span>]=data[<span class="string">"Age"</span>].fillna(data[<span class="string">"Age"</span>].mean())</span><br></pre></td></tr></table></figure><p>对Cabin字段数值化，数值化返回的是一个元祖，元祖后的第一项就是数值化后的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"Cabin"</span>]=pd.factorize(data.Cabin)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>对其它的NAN数据都填充为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.fillna(<span class="number">0</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>对Sex特征进行数值化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"Sex"</span>]=[<span class="number">1</span> <span class="keyword">if</span> x==<span class="string">"male"</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> data.Sex]</span><br></pre></td></tr></table></figure><p>对Pclass进行one-hot编码，添加三个特征，将Pclass特征删除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"p1"</span>]=np.array(data[<span class="string">"Pclass"</span>]==<span class="number">1</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">"p2"</span>]=np.array(data[<span class="string">"Pclass"</span>]==<span class="number">2</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">"p3"</span>]=np.array(data[<span class="string">"Pclass"</span>]==<span class="number">3</span>).astype(np.int32)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">"Pclass"</span>]</span><br></pre></td></tr></table></figure><p>对Embarked，首先知道它有三个港口，0是我们刚刚填充NAN的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(data.Embarked.unique())</span><br><span class="line"><span class="comment">#['S' 'C' 'Q' 0]</span></span><br></pre></td></tr></table></figure><p>接着对Embarked进行one-hot编码，再删除Embarked特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">"e1"</span>]=np.array(data[<span class="string">"Embarked"</span>]==<span class="string">"S"</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">"e2"</span>]=np.array(data[<span class="string">"Embarked"</span>]==<span class="string">"C"</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">"e3"</span>]=np.array(data[<span class="string">"Embarked"</span>]==<span class="string">"Q"</span>).astype(np.int32)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">"Embarked"</span>]</span><br></pre></td></tr></table></figure><p>数据的预处理就结束了</p><h3 id="提取训练数据"><a href="#提取训练数据" class="headerlink" title="提取训练数据"></a>提取训练数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train=data[[<span class="string">"Sex"</span>,<span class="string">"Age"</span>,<span class="string">"SibSp"</span>,<span class="string">"Parch"</span>,<span class="string">"Fare"</span>,<span class="string">"Cabin"</span>,<span class="string">"p1"</span>,<span class="string">"p2"</span>,<span class="string">"p3"</span>,<span class="string">"e1"</span>,<span class="string">"e2"</span>,<span class="string">"e3"</span>]]</span><br></pre></td></tr></table></figure><h3 id="数据标签"><a href="#数据标签" class="headerlink" title="数据标签"></a>数据标签</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_target=data[<span class="string">"Survived"</span>].values.reshape(len(data),<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="literal">None</span>, <span class="number">12</span>])</span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">weight = tf.Variable(tf.random_normal([<span class="number">12</span>, <span class="number">1</span>]))</span><br><span class="line">bias = tf.Variable(tf.random_normal([<span class="number">1</span>]))</span><br><span class="line">output = tf.matmul(x, weight) + bias</span><br><span class="line">pred = tf.cast(tf.sigmoid(output) &gt; <span class="number">0.5</span>, tf.float32)</span><br><span class="line">loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=output))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line">accurary = tf.reduce_mean(tf.cast(tf.equal(pred, y), tf.float32))</span><br></pre></td></tr></table></figure><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.plot(loss_train, <span class="string">'k-'</span>)</span><br><span class="line">plt.title(<span class="string">'train loss'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, <span class="string">'b-'</span>, label=<span class="string">'train_acc'</span>)</span><br><span class="line">plt.plot(test_acc, <span class="string">'r--'</span>, label=<span class="string">'test_acc'</span>)</span><br><span class="line">plt.title(<span class="string">'train and test accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="全部代码"><a href="#全部代码" class="headerlink" title="全部代码"></a>全部代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">data = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">data = data[[<span class="string">'Survived'</span>, <span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Cabin'</span>, <span class="string">'Embarked'</span>]]</span><br><span class="line">data[<span class="string">'Age'</span>] = data[<span class="string">'Age'</span>].fillna(data[<span class="string">'Age'</span>].mean())</span><br><span class="line">data[<span class="string">'Cabin'</span>] = pd.factorize(data[<span class="string">'Cabin'</span>])[<span class="number">0</span>]</span><br><span class="line">data.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data[<span class="string">'Sex'</span>] = [<span class="number">1</span> <span class="keyword">if</span> x==<span class="string">'male'</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Sex'</span>]]</span><br><span class="line">data[<span class="string">'p1'</span>] = np.array(data[<span class="string">'Pclass'</span>]==<span class="number">1</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">'p2'</span>] = np.array(data[<span class="string">'Pclass'</span>]==<span class="number">2</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">'p3'</span>] = np.array(data[<span class="string">'Pclass'</span>]==<span class="number">3</span>).astype(np.int32)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">'Pclass'</span>]</span><br><span class="line">data[<span class="string">'e1'</span>] = np.array(data[<span class="string">'Embarked'</span>]==<span class="string">'S'</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">'e2'</span>] = np.array(data[<span class="string">'Embarked'</span>]==<span class="string">'C'</span>).astype(np.int32)</span><br><span class="line">data[<span class="string">'e3'</span>] = np.array(data[<span class="string">'Embarked'</span>]==<span class="string">'Q'</span>).astype(np.int32)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">'Embarked'</span>]</span><br><span class="line"></span><br><span class="line">data_train = data[[ <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Cabin'</span>, <span class="string">'p1'</span>, <span class="string">'p2'</span>, <span class="string">'p3'</span>, <span class="string">'e1'</span>, <span class="string">'e2'</span>, <span class="string">'e3'</span>]]</span><br><span class="line">data_target = data[<span class="string">'Survived'</span>].values.reshape(len(data), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="literal">None</span>, <span class="number">12</span>])</span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">weight = tf.Variable(tf.random_normal([<span class="number">12</span>, <span class="number">1</span>]))</span><br><span class="line">bias = tf.Variable(tf.random_normal([<span class="number">1</span>]))</span><br><span class="line">output = tf.matmul(x, weight) + bias</span><br><span class="line">pred = tf.cast(tf.sigmoid(output) &gt; <span class="number">0.5</span>, tf.float32)</span><br><span class="line">loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=output))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line">accurary = tf.reduce_mean(tf.cast(tf.equal(pred, y), tf.float32))</span><br><span class="line"> </span><br><span class="line">data_test = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">data_test = data_test[[<span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Cabin'</span>, <span class="string">'Embarked'</span>]]</span><br><span class="line">data_test[<span class="string">'Age'</span>] = data_test[<span class="string">'Age'</span>].fillna(data_test[<span class="string">'Age'</span>].mean())</span><br><span class="line">data_test[<span class="string">'Cabin'</span>] = pd.factorize(data_test[<span class="string">'Cabin'</span>])[<span class="number">0</span>]</span><br><span class="line">data_test.fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data_test[<span class="string">'Sex'</span>] = [<span class="number">1</span> <span class="keyword">if</span> x==<span class="string">'male'</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> data_test[<span class="string">'Sex'</span>]]</span><br><span class="line">data_test[<span class="string">'p1'</span>] = np.array(data_test[<span class="string">'Pclass'</span>]==<span class="number">1</span>).astype(np.int32)</span><br><span class="line">data_test[<span class="string">'p2'</span>] = np.array(data_test[<span class="string">'Pclass'</span>]==<span class="number">2</span>).astype(np.int32)</span><br><span class="line">data_test[<span class="string">'p3'</span>] = np.array(data_test[<span class="string">'Pclass'</span>]==<span class="number">3</span>).astype(np.int32)</span><br><span class="line"><span class="keyword">del</span> data_test[<span class="string">'Pclass'</span>]</span><br><span class="line">data_test[<span class="string">'e1'</span>] = np.array(data_test[<span class="string">'Embarked'</span>]==<span class="string">'S'</span>).astype(np.int32)</span><br><span class="line">data_test[<span class="string">'e2'</span>] = np.array(data_test[<span class="string">'Embarked'</span>]==<span class="string">'C'</span>).astype(np.int32)</span><br><span class="line">data_test[<span class="string">'e3'</span>] = np.array(data_test[<span class="string">'Embarked'</span>]==<span class="string">'Q'</span>).astype(np.int32)</span><br><span class="line"><span class="keyword">del</span> data_test[<span class="string">'Embarked'</span>]</span><br><span class="line"></span><br><span class="line">test_label = pd.read_csv(<span class="string">'gender_submission.csv'</span>)</span><br><span class="line">test_label = np.reshape(test_label[<span class="string">'Survived'</span>].values.astype(np.float32), (<span class="number">418</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">loss_train=[]</span><br><span class="line">train_acc = []</span><br><span class="line">test_acc = []</span><br><span class="line"><span class="comment">#这句代码不可缺</span></span><br><span class="line">data_train = data_train.values</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">25000</span>):</span><br><span class="line">    index = np.random.permutation(len(data_target))</span><br><span class="line">    data_train = data_train[index]</span><br><span class="line">    data_target = data_target[index]</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(len(data_target)//<span class="number">100</span> + <span class="number">1</span>):</span><br><span class="line">      batch_xs = data_train[n*<span class="number">100</span>:n*<span class="number">100</span>+<span class="number">100</span>]</span><br><span class="line">      batch_ys = data_target[n*<span class="number">100</span>:n*<span class="number">100</span>+<span class="number">100</span>]</span><br><span class="line">      sess.run(train_step, feed_dict=&#123;x:batch_xs, y:batch_ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">      loss_temp = sess.run(loss, feed_dict=&#123;x:batch_xs, y:batch_ys&#125;)</span><br><span class="line">      loss_train.append(loss_temp)</span><br><span class="line">      train_acc_temp = sess.run(accurary, feed_dict=&#123;x:batch_xs, y:batch_ys&#125;)</span><br><span class="line">      train_acc.append(train_acc_temp)</span><br><span class="line">      test_acc_temp = sess.run(accurary, feed_dict=&#123;x:data_test, y:test_label&#125;)</span><br><span class="line">      test_acc.append(test_acc_temp)</span><br><span class="line">      print(loss_temp,train_acc_temp,test_acc_temp)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.plot(loss_train, <span class="string">'k-'</span>)</span><br><span class="line">plt.title(<span class="string">'train loss'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, <span class="string">'b-'</span>, label=<span class="string">'train_acc'</span>)</span><br><span class="line">plt.plot(test_acc, <span class="string">'r--'</span>, label=<span class="string">'test_acc'</span>)</span><br><span class="line">plt.title(<span class="string">'train and test accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在本次实战，会用到numpy和pandas等模块，不会的话，请补完自己的知识空区。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/c/titanic/data&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;本次数据集的下载网
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>何为端到端？</title>
    <link href="http://yoursite.com/2020/02/11/%E4%BD%95%E4%B8%BA%E7%AB%AF%E5%88%B0%E7%AB%AF%EF%BC%9F/"/>
    <id>http://yoursite.com/2020/02/11/何为端到端？/</id>
    <published>2020-02-11T03:36:08.000Z</published>
    <updated>2020-02-11T07:11:17.181Z</updated>
    
    <content type="html"><![CDATA[<p>传统机器学习的流程往往由多个独立的模块组成。比如在自然语言处理问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，每个任务结果的好坏会影响到下一步骤，从而影响整个训练的结果，这是非端到端。</p><p>端到端学习到底是什么呢？上面说了，在一些问题需要多个阶段的处理，那么端到端学习就是忽略所有不同的阶段，用单个神经网络代替它。从输入端到输出端会得到一个预测结果，与真实结果相比较会得到一个误差，这个误差会在模型中的每一层反向传播，每一层的表示都会根据这个误差再做调整，直到模型收敛或达到模型预期效果才结束，中间所有的操作都包含在神经网络内部，不再分成多个模块处理。由原始数据输入，到结果输出，从输入端到输出端，中间的神经网络自成一体，这就是端到端学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;传统机器学习的流程往往由多个独立的模块组成。比如在自然语言处理问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，每个任务结果的好坏会影响到下一步骤，从而影响整个训练的结果，这是非端到端。&lt;/p&gt;
&lt;p&gt;端到端学习到底是什么呢？上面说了，
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的模型训练技巧</title>
    <link href="http://yoursite.com/2020/01/15/Tensorflow%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/"/>
    <id>http://yoursite.com/2020/01/15/Tensorflow中的模型训练技巧/</id>
    <published>2020-01-15T07:07:04.000Z</published>
    <updated>2020-01-16T02:17:34.972Z</updated>
    
    <content type="html"><![CDATA[<h2 id="优化卷积核"><a href="#优化卷积核" class="headerlink" title="优化卷积核"></a>优化卷积核</h2><p>在实际的卷积训练中，为了加快速度，常常把卷积层裁开。比如一个3x3的过滤器，可以裁成3x1和1x3两个过滤器，分别对原有输入做卷积操作，这样可以大大提升运算速度</p><p>原理：在浮点运算中乘法消耗的资源较多，我们的目的就是尽量减小乘法运算</p><p>比如：对一个5x2的原始图片进行一次3x3的同卷积，相当于生成的5x2像素中每一个都要经历3x3次乘法，一共90次。同样一张图片，如果先进行一次3x1的同卷积需要30次运算，再进行一次1x3的同卷积还是30次，一共60次。</p><p>看下面例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)</span><br></pre></td></tr></table></figure><p>修改为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W_conv21 = weight_variable([<span class="number">5</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b_conv21 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv21 = tf.nn.relu(conv2d(h_pool1, W_conv21) + b_conv21)</span><br><span class="line">W_conv2 = weight_variable([<span class="number">1</span>, <span class="number">5</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2=tf.nn.relu(conv2d(h_conv21,W_conv2)+b_conv2)</span><br></pre></td></tr></table></figure><h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>批量归一化，简称BN算法。一般用在全连接层或卷积神经网络。它的作用是要最大限度地保证每次的正向传播输出在同一分布上， 这样反向计算时参照的数据样本分布就会与正向计算时的数据分布一样了。 保证了分布统一， 对权重的调整才会更有意义。  </p><p>批量归一化的做法很简单，即将每一层运算出来的数据都归一化成均值为0、方差为1的标准高斯分布。这样就会在保留样本分布特征的同时，又消除了层与层之间的分布差异</p><p>Tensorflow中自带的BN函数定义：</p><p>tf.nn.batch_normalization(x,mean,variance,offset,scale,variance_epsion,name=None)，参数说明如下：</p><p>x：代表输入</p><p>mean：代表样本的均值</p><p>variance：代表方差</p><p>offset：代表偏移，即相加一个转化值，后面我们会用激活函数来转换，所以直接使用0</p><p>scale：代表缩放，即乘以一个转化值。一般用1</p><p>variance_epsilon：为了避免分布为0的情况，给分母加一个极小值</p><p>要想使用上面这个BN函数，必须由另一个函数配合使用——tf.nn.moments，由他来计算均值和方差，然后就可以使用BN。tf.nn.moments定义如下：</p><p>tf.nn.moments(x,axes,name=None,keep_dims=False)</p><p>axes主要是指定哪个轴来求均值与方差</p><p>有了上面的两个函数还不够，为了有更好的效果，我们希望使用平滑指数衰减的方法来优化每次的均值与方差，于是就用到了tf.train.ExponentialMovingAverage函数。它的左右是上一次的值对本次的值有个衰减后的影响，从而使每次的值连起来后会相对平滑一些。展开后可以用下列等式表示：</p><p>shadow_variable = decay * shadow_variable + (1- decay) * variable，各参数说明如下：</p><p>decay：代表衰减指数，直接指定的。比如0.9</p><p>variable：代表本批次样本中的值</p><p>等式右边的shadow_variable：代表上次总样本的值</p><p>等式左边的shadow_variable：代表计算出来的本次总样本的值</p><p>上面的函数需要联合使用，于是在Tensorflow中的layers模块又实现了一次BN函数，相当于把上面几个函数结合在一起。使用时，首先将头文件引入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.contrib.layers.python.layers <span class="keyword">import</span> barch_norm</span><br></pre></td></tr></table></figure><p>batch_norm(inputs,decay=0.999,center=True,scale=False,epsilon=0.001,activation_fn=None,param_initializers=None,param_regularizers=None,updates_collections=ops.GraphKeys.UPDATE_OPS,is_training=True,reuse=None,variables_collections=None,outputs_collections=None,trainable=True,batch_weights=None,fused=False,data_format=DATA_FORMAT_NHWC,zero_debias_moving_mean=False,scope=None,renorm=False,renorm_clipping=None,renorm_decay=0.9)，参数看得眼都花了，参数说明如下：</p><p>inputs：代表输入</p><p>decay：代表移动平均值的衰败速度，是使用了一种叫做平滑指数衰减的方法更新均值方差，一般设为0.9；值太小会导致均值和方差更新太快，而值太大又会导致几乎没有衰减，容易出现过拟合。</p><p>scale：是否进行变化（通过乘一个gamma值进行缩放），一般设为False。</p><p>epsilon：是为了避免分母为0的情况，给分母加一个极小值，默认即可。</p><p>is_training：当它为True，代表是训练过程，这时会不断更新样本集的均值与方差。测试时，设为False，就会使用测试样本集的均值与方差</p><p>updatas_collections：默认为tf.GraphKeys.UPDARE_OPS，在训练时提供一种内置的均值方差更新机制，即通过图中的tf.GraphKeys.UPDATE_OPS变量来更新。但它是在每次当前批次训练完成后才更新均值和方差，这样导致当前数据总是使用前一次的均值和方差，没有得到最新的更新。所以都会将其设为None，让均值和方差即时更新。</p><p>reuse：支持共享变量，与下面的scope参数联合使用</p><p>scope：指定变量的作用域variable_scope</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;优化卷积核&quot;&gt;&lt;a href=&quot;#优化卷积核&quot; class=&quot;headerlink&quot; title=&quot;优化卷积核&quot;&gt;&lt;/a&gt;优化卷积核&lt;/h2&gt;&lt;p&gt;在实际的卷积训练中，为了加快速度，常常把卷积层裁开。比如一个3x3的过滤器，可以裁成3x1和1x3两个过滤器，分别对
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>梯度</title>
    <link href="http://yoursite.com/2020/01/15/%E6%A2%AF%E5%BA%A6/"/>
    <id>http://yoursite.com/2020/01/15/梯度/</id>
    <published>2020-01-15T01:15:11.000Z</published>
    <updated>2020-01-16T02:06:24.435Z</updated>
    
    <content type="html"><![CDATA[<p>在反向传播过程中，神经网络需要对每一个loss对应的学习参数求偏导，算出的这个值叫做梯度，用来乘以学习率然后更新学习参数使用的</p><h2 id="求单变量偏导"><a href="#求单变量偏导" class="headerlink" title="求单变量偏导"></a>求单变量偏导</h2><p>它是通过tf.gradients函数来实现的。<br>tf.gradients(ys,xs,grad_ys=None,name=’gradients’,colocate_gradients_with_ops=False,gate_gradients=False,  aggregation_method=None,stop_gradients=None)</p><p>第一个参数为求导公式的结果，第二个参数为指定公式中的哪个变量来求偏导。实现第一个参数对第二个参数求导。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w1=tf.Variable([[<span class="number">1</span>,<span class="number">2</span>]],dtype=tf.float32)</span><br><span class="line">w2=tf.Variable([[<span class="number">3</span>,<span class="number">4</span>]],dtype=tf.float32)</span><br><span class="line">y=tf.matmul(w1,tf.convert_to_tensor([[<span class="number">9</span>],[<span class="number">10</span>]],dtype=tf.float32))</span><br><span class="line">grads=tf.gradients(y,[w1])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  print(<span class="string">"梯度为："</span>,sess.run(grads))</span><br><span class="line"></span><br><span class="line"> <span class="comment">#梯度为： [array([[ 9., 10.]], dtype=float32)]</span></span><br></pre></td></tr></table></figure><p>上面例子中，由于y是由w1与[[9],[10]]相乘而来，所以导数就是[[9],[10]]，也就是斜率</p><h2 id="求多变量偏导"><a href="#求多变量偏导" class="headerlink" title="求多变量偏导"></a>求多变量偏导</h2><p>这就需要用到tf.gradients的第三个参数，grad_ys。grad_ys也是一个list，其长度等于len(ys)。这个参数的意义在于对第一个参数中的每个元素的求导加权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="comment">#随机生成一个形状为2的变量</span></span><br><span class="line">w1 = tf.get_variable(<span class="string">'w1'</span>, shape=[<span class="number">2</span>])</span><br><span class="line">w2 = tf.get_variable(<span class="string">'w2'</span>, shape=[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">w3 = tf.get_variable(<span class="string">'w3'</span>, shape=[<span class="number">2</span>])</span><br><span class="line">w4 = tf.get_variable(<span class="string">'w4'</span>, shape=[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">y1 = w1 + w2+ w3</span><br><span class="line">y2 = w3 + w4</span><br><span class="line"><span class="comment">#不考虑参数grad_ys</span></span><br><span class="line">gradients= tf.gradients([y1, y2], [w1, w2, w3, w4])</span><br><span class="line"><span class="comment">#考虑参数grad_ys</span></span><br><span class="line">gradients1 = tf.gradients([y1, y2], [w1, w2, w3, w4], grad_ys=[tf.convert_to_tensor([<span class="number">1.</span>,<span class="number">2.</span>]),</span><br><span class="line">                                                          tf.convert_to_tensor([<span class="number">3.</span>,<span class="number">4.</span>])])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(gradients1))</span><br><span class="line">    print(sess.run(gradients))</span><br></pre></td></tr></table></figure><h2 id="梯度停止"><a href="#梯度停止" class="headerlink" title="梯度停止"></a>梯度停止</h2><p>对于反向传播过程中某种特殊情况需要停止梯度运算时，在tensorflow中提供了一个tf.stop_gradient函数，被它定义过的节点将没有梯度运算功能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">w1 = tf.get_variable(<span class="string">'w1'</span>, shape=[<span class="number">2</span>])</span><br><span class="line">w2 = tf.get_variable(<span class="string">'w2'</span>, shape=[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">w3 = tf.get_variable(<span class="string">'w3'</span>, shape=[<span class="number">2</span>])</span><br><span class="line">w4 = tf.get_variable(<span class="string">'w4'</span>, shape=[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">y1 = w1 + w2+ w3</span><br><span class="line">y2 = w3 + w4</span><br><span class="line"></span><br><span class="line">a = w1+w2</span><br><span class="line">a_stoped = tf.stop_gradient(a)</span><br><span class="line">y3= a_stoped+w3</span><br><span class="line"></span><br><span class="line">gradients = tf.gradients([y1, y2], [w1, w2, w3, w4], grad_ys=[tf.convert_to_tensor([<span class="number">1.</span>,<span class="number">2.</span>]),</span><br><span class="line">                                                          tf.convert_to_tensor([<span class="number">3.</span>,<span class="number">4.</span>])])                                                      </span><br><span class="line">gradients2 = tf.gradients(y3, [w1, w2, w3], grad_ys=tf.convert_to_tensor([<span class="number">1.</span>,<span class="number">2.</span>]))                                                          </span><br><span class="line">print(gradients2) </span><br><span class="line"> </span><br><span class="line">gradients3 = tf.gradients(y3, [w3], grad_ys=tf.convert_to_tensor([<span class="number">1.</span>,<span class="number">2.</span>])) </span><br><span class="line">                                                       </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(gradients))</span><br><span class="line">    <span class="comment">#print(sess.run(gradients2))#报错，因为w1和w2梯度停止了</span></span><br><span class="line">    print(sess.run(gradients3))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在反向传播过程中，神经网络需要对每一个loss对应的学习参数求偏导，算出的这个值叫做梯度，用来乘以学习率然后更新学习参数使用的&lt;/p&gt;
&lt;h2 id=&quot;求单变量偏导&quot;&gt;&lt;a href=&quot;#求单变量偏导&quot; class=&quot;headerlink&quot; title=&quot;求单变量偏导&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>反卷积和反池化</title>
    <link href="http://yoursite.com/2020/01/11/%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8F%8D%E6%B1%A0%E5%8C%96/"/>
    <id>http://yoursite.com/2020/01/11/反卷积和反池化/</id>
    <published>2020-01-11T09:13:49.000Z</published>
    <updated>2020-01-15T07:02:58.500Z</updated>
    
    <content type="html"><![CDATA[<h2 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h2><p>反卷积是指通过测量输出已知输入重构未知输入的过程。在神经网络中，反卷积过程并不具备学习的能力，仅仅是用于可视化一个已经训练好的卷积网络模型，没有学习训练的过程。如下图：即为VGG16反卷积神经网络的结构，展示了一个卷积网络与反卷积网络结合的过程。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200111184504.png" alt=""></p><p>反卷积就是将中间的数据，按照前面卷积、池化等过程，完全相反地做一遍，从而得到类似原始输入的数据。</p><h3 id="反卷积原理"><a href="#反卷积原理" class="headerlink" title="反卷积原理"></a>反卷积原理</h3><p>反卷积可以理解为卷积操作的逆操作。<strong>千万不要将反卷积操作可以复原卷积操作的输入值，</strong>反卷积并没有这个功能，它仅仅是将卷积变换过程中的步骤反向变换一次。通过将卷积核转置，与卷积后的结果再做一遍卷积，所以反卷积还有个名字叫做转置卷积。</p><p>反卷积操作：</p><p>（1） 首先是将卷积核反转（ 并不是转置，而是上下左右方向进行递序操作）。也就是对卷积核做180<sup>o</sup>翻转。<br>（2） 再将卷积结果作为输入， 做补0的扩充操作， 即往每一个元素后面补0。 这一步是根据步长来的， 对每一个元素沿着步长的方向补（ 步长-1） 个0。 例如， 步长为1就不用补0了。<br>（3） 在扩充后的输入基础上再对整体补0。以原始输入的shape作为输出， 按照前面介绍的卷积padding规则， 计算pading的补0位置及个数， 得到的补0位置要上下和左右各自颠倒一下。<br>（4） 将补0后的卷积结果作为真正的输入，反转后的卷积核为filter， 进行步长为1的卷积操作。  </p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200111193249.png" alt=""></p><p>需要注意的是，通过反卷积并不能还原卷积之前的矩阵，只能从大小上进行还原，反卷积的本质还是卷积，只是在进行卷积之前，会进行一个自动的padding补0，从而使得输出的矩阵与指定输出矩阵的shape相同。框架本身，会根据我们自己设定的反卷积值来计算输入矩阵的尺寸，如果shape不符合，会出现报错。</p><h3 id="反卷积函数"><a href="#反卷积函数" class="headerlink" title="反卷积函数"></a>反卷积函数</h3><p>在Tensorflow中，反卷积是通过函数tf.nn.conv2d_transpose来实现的。</p><p>conv2d_transpose(value,filter,output_shape,strides,padding=”SAME”,data_format=”NHWC”,name=None)：</p><p>value：代表卷积操作之后的张量，需要进行反卷积的矩阵</p><p>filter：代表卷积核，参数格式[height,width,output_channels,in_channels]</p><p>output_shape：设置反卷积输出矩阵的shape</p><p>strides：反卷积步长</p><p>padding：补0方式，SAME和VALID方式</p><p>data_format：string类型的量，’NHWC’和’NCHW’其中之一，这是tensorflow新版本中新加的参数，它说明了value参数的数据格式。’NHWC’指tensorflow标准的数据格式[batch, height, width, in_channels]，‘NCHW’指Theano的数据格式,[batch, in_channels，height, width]，默认值是’NHWC’。</p><p>name：操作名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line">img = tf.Variable(tf.constant(<span class="number">1.0</span>,shape = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">1</span>])) </span><br><span class="line"></span><br><span class="line">filter =  tf.Variable(tf.constant([<span class="number">1.0</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>],shape = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">conv = tf.nn.conv2d(img, filter, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)  </span><br><span class="line">cons = tf.nn.conv2d(img, filter, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">print(conv.shape)</span><br><span class="line">print(cons.shape)</span><br><span class="line"> </span><br><span class="line">contv= tf.nn.conv2d_transpose(conv, filter, [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'VALID'</span>)</span><br><span class="line">conts = tf.nn.conv2d_transpose(cons, filter, [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  </span><br><span class="line">    sess.run(tf.global_variables_initializer() )  </span><br><span class="line">    print(<span class="string">"img"</span>,sess.run(img))</span><br><span class="line">    print(<span class="string">"conv:\n"</span>,sess.run([conv,filter])) </span><br><span class="line">    print(<span class="string">"cons:\n"</span>,sess.run([cons]))    </span><br><span class="line">    print(<span class="string">"contv:\n"</span>,sess.run([contv])) </span><br><span class="line">    print(<span class="string">"conts:\n"</span>,sess.run([conts]))</span><br></pre></td></tr></table></figure><h3 id="反卷积应用场景"><a href="#反卷积应用场景" class="headerlink" title="反卷积应用场景"></a>反卷积应用场景</h3><p>由于反卷积的特性，可以用于信道均衡、图像恢复等问题。而在神经网络的研究中， 反卷积更多的是充当可视化的作用。 对于一个复杂的深度卷积网络，通过每层若干个卷积核的变换， 我们无法知道每个卷积核关注的是什么， 变换后的特征是什么样子。 通过反卷积的还原， 可以对这些问题有个清晰的可视化， 以各层得到的特征图作为输入， 进行反卷积， 得到反卷积结果， 用以验证显示各层提取到的特征图。  </p><h2 id="反池化"><a href="#反池化" class="headerlink" title="反池化"></a>反池化</h2><p>反池化属于池化的逆操作，是无法通过池化的结果还原出全部的原始数据。因为池化的过程就是只保留主要信息，舍去部分信息。如想从池化后的这些主要信息恢复出全部信息，则存在信息缺失，这时只能通过补位来实现最大程度的信息完整</p><h3 id="反池化原理"><a href="#反池化原理" class="headerlink" title="反池化原理"></a>反池化原理</h3><p>池化有两种最大池化和平均池化，反池化与其对应。</p><p>反平均池化比较简单。首先还原成原来的大小，然后将池化结果中的每个值都填入其对应于原始数据区域中的相应未知即可。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200112123007.png" alt=""></p><p>反最大池画要求在池化过程中记录最大激活值的坐标位置，然后在反池化时，只把池化过程中最大激活值所在位置坐标的值激活，其它的值置为0。这个过程只是一种近似，因为在池化过程中，除了最大值所在的位置，其它的值是不为0的</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200112123548.png" alt=""></p><h3 id="反池化操作"><a href="#反池化操作" class="headerlink" title="反池化操作"></a>反池化操作</h3><p>Tensorflow中没有反池化操作的函数。对于最大池化，也不支持输出最大激活值的位置，但是同样有个池化的反向传播函数tf.nn.max_pool_with_argmax。该函数可以输出位置，需要我们自己封装一个反池化函数。</p><p>首先重新定义最大池化函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_with_argmax</span><span class="params">(net, stride)</span>:</span></span><br><span class="line">    _, mask = tf.nn.max_pool_with_argmax( net,ksize=[<span class="number">1</span>, stride, stride, <span class="number">1</span>], strides=[<span class="number">1</span>, stride, stride, <span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    mask = tf.stop_gradient(mask)</span><br><span class="line">    net = tf.nn.max_pool(net, ksize=[<span class="number">1</span>, stride, stride, <span class="number">1</span>],strides=[<span class="number">1</span>, stride, stride, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) </span><br><span class="line">    <span class="keyword">return</span> net, mask</span><br></pre></td></tr></table></figure><p>在上面代码里，先调用tf.nn.max_pool_with_argmax函数获得每个最大值的位置mask，再将反向传播的mask梯度停止，接着再用tf.nn.max_pool函数计算最大池化操作，然后将mask和池化结果一起返回。</p><p><strong>注意</strong>：tf.nn.max_pool_with_argmax的方法只支持GPU操作，不能在cpu机器上使用。</p><p>接下来定义一个数组，并使用最大池化函数对其进行池化操作，比较一下tensorflow自带的tf.nn.max_pool函数是否一样，看看输出的mask</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">img=tf.constant([  </span><br><span class="line">        [[<span class="number">0.0</span>,<span class="number">4.0</span>],[<span class="number">0.0</span>,<span class="number">4.0</span>],[<span class="number">0.0</span>,<span class="number">4.0</span>],[<span class="number">0.0</span>,<span class="number">4.0</span>]],  </span><br><span class="line">        [[<span class="number">1.0</span>,<span class="number">5.0</span>],[<span class="number">1.0</span>,<span class="number">5.0</span>],[<span class="number">1.0</span>,<span class="number">5.0</span>],[<span class="number">1.0</span>,<span class="number">5.0</span>]],  </span><br><span class="line">        [[<span class="number">2.0</span>,<span class="number">6.0</span>],[<span class="number">2.0</span>,<span class="number">6.0</span>],[<span class="number">2.0</span>,<span class="number">6.0</span>],[<span class="number">2.0</span>,<span class="number">6.0</span>]],  </span><br><span class="line">        [[<span class="number">3.0</span>,<span class="number">7.0</span>],[<span class="number">3.0</span>,<span class="number">7.0</span>], [<span class="number">3.0</span>,<span class="number">7.0</span>],[<span class="number">3.0</span>,<span class="number">7.0</span>]]</span><br><span class="line">    ]) </span><br><span class="line">img=tf.reshape(img,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">pooling=tf.nn.max_pool(img,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">encode,mask=max_pool_with_argmax(img,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"image:"</span>,sess.run(img))</span><br><span class="line">  print(<span class="string">"pooling:"</span>,sess.run(pooling))</span><br><span class="line">  print(<span class="string">"encode"</span>,sess.run([encode,mask]))</span><br></pre></td></tr></table></figure><p>从输出结果可以看到，定义的最大池化与原来的版本输出是一样的。mask的值是将整个数据flat(扁平化)后的索引，但却保持与池化结果一致的shape。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpool</span><span class="params">(net, mask, stride)</span>:</span></span><br><span class="line"></span><br><span class="line">    ksize = [<span class="number">1</span>, stride, stride, <span class="number">1</span>]</span><br><span class="line">    input_shape = net.get_shape().as_list()</span><br><span class="line">    <span class="comment">#计算new shape</span></span><br><span class="line">    output_shape = (input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>] * ksize[<span class="number">1</span>], input_shape[<span class="number">2</span>] * ksize[<span class="number">2</span>], input_shape[<span class="number">3</span>])</span><br><span class="line">    <span class="comment">#计算索引</span></span><br><span class="line">    one_like_mask = tf.ones_like(mask)</span><br><span class="line">    batch_range = tf.reshape(tf.range(output_shape[<span class="number">0</span>], dtype=tf.int64), shape=[input_shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    b = one_like_mask * batch_range</span><br><span class="line">    y = mask // (output_shape[<span class="number">2</span>] * output_shape[<span class="number">3</span>])</span><br><span class="line">    x = mask % (output_shape[<span class="number">2</span>] * output_shape[<span class="number">3</span>]) // output_shape[<span class="number">3</span>]</span><br><span class="line">    feature_range = tf.range(output_shape[<span class="number">3</span>], dtype=tf.int64)</span><br><span class="line">    f = one_like_mask * feature_range</span><br><span class="line">    <span class="comment">#转置索引</span></span><br><span class="line">    updates_size = tf.size(net)</span><br><span class="line">    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [<span class="number">4</span>, updates_size]))</span><br><span class="line">    values = tf.reshape(net, [updates_size])</span><br><span class="line">    ret = tf.scatter_nd(indices, values, output_shape)</span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>上面代码的思路是找到mask对应的索引，将max的值填到指定地方。接着直接调用上面代码的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img2=unpool(encode,mask,<span class="number">2</span>)</span><br><span class="line"> <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   result=sess.run(img2)  </span><br><span class="line">   <span class="keyword">print</span> (<span class="string">"reslut:\n"</span>,result)</span><br></pre></td></tr></table></figure><p>反最大池化整体代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_with_argmax</span><span class="params">(net, stride)</span>:</span></span><br><span class="line">    _, mask = tf.nn.max_pool_with_argmax( net,ksize=[<span class="number">1</span>, stride, stride, <span class="number">1</span>], strides=[<span class="number">1</span>, stride, stride, <span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    mask = tf.stop_gradient(mask)</span><br><span class="line">    net = tf.nn.max_pool(net, ksize=[<span class="number">1</span>, stride, stride, <span class="number">1</span>],strides=[<span class="number">1</span>, stride, stride, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) </span><br><span class="line">    <span class="keyword">return</span> net, mask</span><br><span class="line">img=tf.constant([  </span><br><span class="line">        [[<span class="number">0.0</span>,<span class="number">4.0</span>],[<span class="number">0.0</span>,<span class="number">4.0</span>],[<span class="number">0.0</span>,<span class="number">4.0</span>],[<span class="number">0.0</span>,<span class="number">4.0</span>]],  </span><br><span class="line">        [[<span class="number">1.0</span>,<span class="number">5.0</span>],[<span class="number">1.0</span>,<span class="number">5.0</span>],[<span class="number">1.0</span>,<span class="number">5.0</span>],[<span class="number">1.0</span>,<span class="number">5.0</span>]],  </span><br><span class="line">        [[<span class="number">2.0</span>,<span class="number">6.0</span>],[<span class="number">2.0</span>,<span class="number">6.0</span>],[<span class="number">2.0</span>,<span class="number">6.0</span>],[<span class="number">2.0</span>,<span class="number">6.0</span>]],  </span><br><span class="line">        [[<span class="number">3.0</span>,<span class="number">7.0</span>],[<span class="number">3.0</span>,<span class="number">7.0</span>], [<span class="number">3.0</span>,<span class="number">7.0</span>],[<span class="number">3.0</span>,<span class="number">7.0</span>]]</span><br><span class="line">    ]) </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpool</span><span class="params">(net, mask, stride)</span>:</span></span><br><span class="line"></span><br><span class="line">    ksize = [<span class="number">1</span>, stride, stride, <span class="number">1</span>]</span><br><span class="line">    input_shape = net.get_shape().as_list()</span><br><span class="line">    <span class="comment">#计算new shape</span></span><br><span class="line">    output_shape = (input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>] * ksize[<span class="number">1</span>], input_shape[<span class="number">2</span>] * ksize[<span class="number">2</span>], input_shape[<span class="number">3</span>])</span><br><span class="line">    <span class="comment">#计算索引</span></span><br><span class="line">    one_like_mask = tf.ones_like(mask)</span><br><span class="line">    batch_range = tf.reshape(tf.range(output_shape[<span class="number">0</span>], dtype=tf.int64), shape=[input_shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    b = one_like_mask * batch_range</span><br><span class="line">    y = mask // (output_shape[<span class="number">2</span>] * output_shape[<span class="number">3</span>])</span><br><span class="line">    x = mask % (output_shape[<span class="number">2</span>] * output_shape[<span class="number">3</span>]) // output_shape[<span class="number">3</span>]</span><br><span class="line">    feature_range = tf.range(output_shape[<span class="number">3</span>], dtype=tf.int64)</span><br><span class="line">    f = one_like_mask * feature_range</span><br><span class="line">    <span class="comment">#转置索引</span></span><br><span class="line">    updates_size = tf.size(net)</span><br><span class="line">    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [<span class="number">4</span>, updates_size]))</span><br><span class="line">    values = tf.reshape(net, [updates_size])</span><br><span class="line">    ret = tf.scatter_nd(indices, values, output_shape)</span><br><span class="line">    <span class="keyword">return</span> ret</span><br><span class="line">img=tf.reshape(img,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">pooling=tf.nn.max_pool(img,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">encode,mask=max_pool_with_argmax(img,<span class="number">2</span>)</span><br><span class="line">img2=unpool(encode,mask,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"image:"</span>,sess.run(img))</span><br><span class="line">  print(<span class="string">"pooling:"</span>,sess.run(pooling))</span><br><span class="line">  print(<span class="string">"encode"</span>,sess.run([encode,mask]))</span><br><span class="line"></span><br><span class="line">  result,mask2=sess.run([encode, mask])  </span><br><span class="line">  <span class="keyword">print</span> (<span class="string">"encode:\n"</span>,result,mask2)</span><br><span class="line">  result=sess.run(img2)  </span><br><span class="line">  <span class="keyword">print</span> (<span class="string">"reslut:\n"</span>,result)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;反卷积&quot;&gt;&lt;a href=&quot;#反卷积&quot; class=&quot;headerlink&quot; title=&quot;反卷积&quot;&gt;&lt;/a&gt;反卷积&lt;/h2&gt;&lt;p&gt;反卷积是指通过测量输出已知输入重构未知输入的过程。在神经网络中，反卷积过程并不具备学习的能力，仅仅是用于可视化一个已经训练好的卷积网
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的队列</title>
    <link href="http://yoursite.com/2020/01/10/Tensorflow%E4%B8%AD%E7%9A%84%E9%98%9F%E5%88%97/"/>
    <id>http://yoursite.com/2020/01/10/Tensorflow中的队列/</id>
    <published>2020-01-10T10:43:32.000Z</published>
    <updated>2020-02-03T02:39:06.746Z</updated>
    
    <content type="html"><![CDATA[<p>Tensorflow提供了一个队列机制，通过多线程将读取数据与计算数据分开，因为在处理海量数据集的训练时，无法把数据集一次全部载入内存中，需要以便从硬盘中读取，一边训练计算。</p><h2 id="队列（queue）"><a href="#队列（queue）" class="headerlink" title="队列（queue）"></a>队列（queue）</h2><h3 id="启动线程"><a href="#启动线程" class="headerlink" title="启动线程"></a>启动线程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.start_queue_runners()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200111144227.png" alt=""></p><p>那什么时候程序会进入挂起状态呢？</p><p>源于上面第四行代码，意思是我们要从队列中拿出指定批次的数据，但是队列里没有数据，所以程序会进入挂起状态</p><h3 id="在session内部的退出机制"><a href="#在session内部的退出机制" class="headerlink" title="在session内部的退出机制"></a>在session内部的退出机制</h3><p>如果把session部分改为with语法：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200111141238.png" alt=""></p><p>再次运行程序后，程序虽然能够正常运行，但是结束后会报错。原因是with语法的session是自动关闭的，  当运行结束后session自动关闭的同时会把里面所有的操作都关掉， 而此时的队列还在等待另一个进程往里写数据， 所以就会报错。   </p><p>解决方法：使用sess=tf.InteractiveSession()实现，或者像第一张图片一样创建</p><p><strong>tf.InteractiveSession()和tf.Session()的区别：</strong></p><p>使用InteractiveSession()来创建会话，我们要先构建Session()然后定义操作。如果使用Session来创建会话，我们需要在会话之前定义好全部的操作然后再构建会话。</p><p>上面的代码在单例程序中没什么问题， 资源会随着程序关闭而整体销毁。 但如果在复杂的代码中， 需要某个线程自动关闭， 而不是依赖进程的结束而销毁， 这种情况下需要使用tf.train.Coordinator函数来创建一个协调器， 以信号量的方式来协调线程间的关系， 完成线程间的同步。  </p><h2 id="协调器"><a href="#协调器" class="headerlink" title="协调器"></a>协调器</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL19.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL20.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/DL@1.png" alt=""></p><p>下面这个例子是先建立一个100大小的队列，主线程使用计数器不停的加1，队列线程把主线程里的计数器放到队列中，当队列为空时，主线程会在sess.run(queue.dequeue())语句位置挂起。当队列线程写入队列中时，主线程的计数器同步开始工作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment">#创建一个长度为100的队列</span></span><br><span class="line">queue=tf.FIFOQueue(<span class="number">100</span>,<span class="string">"float"</span>)</span><br><span class="line">c=tf.Variable(<span class="number">0.0</span>)   <span class="comment">#计数器</span></span><br><span class="line"><span class="comment">#c+1.0</span></span><br><span class="line">op=tf.assign_add(c,tf.constant(<span class="number">1.0</span>))</span><br><span class="line"><span class="comment">#将计数器的结果加入队列</span></span><br><span class="line">enqueue_op=queue.enqueue(c)</span><br><span class="line"><span class="comment">#创建一个队列管理器queueRunner,用上面这两个操作向queue中添加元素。</span></span><br><span class="line">qr=tf.train.QueueRunner(queue,enqueue_ops=[op,enqueue_op])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  coord=tf.train.Coordinator()</span><br><span class="line">  <span class="comment">#启动入队进程</span></span><br><span class="line">  enqueue_threads=qr.create_threads(sess,coord=coord,start=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment">#主线程</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">    print(sess.run(queue.dequeue()))</span><br><span class="line">  <span class="comment">#通知其它线程关闭，其它所有线程关闭后，这一函数才返回</span></span><br><span class="line">  coord.request_stop()</span><br></pre></td></tr></table></figure><p>还可以使用coord.join(enqueue_threads)指定等待某个进程结束</p><p>为session中的队列加上协调器，只需要将上例的coord放到启动队列中。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20200111163544.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Tensorflow提供了一个队列机制，通过多线程将读取数据与计算数据分开，因为在处理海量数据集的训练时，无法把数据集一次全部载入内存中，需要以便从硬盘中读取，一边训练计算。&lt;/p&gt;
&lt;h2 id=&quot;队列（queue）&quot;&gt;&lt;a href=&quot;#队列（queue）&quot; class
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>CNN的相关函数</title>
    <link href="http://yoursite.com/2019/11/29/CNN%E7%9A%84%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/11/29/CNN的相关函数/</id>
    <published>2019-11-29T12:22:09.000Z</published>
    <updated>2020-01-10T07:25:13.122Z</updated>
    
    <content type="html"><![CDATA[<h2 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h2><p>tf.nn.conv2d(input,filter,strides,padding,use_cudnn_on_gpu=None,name=None)</p><p>除去用以指定该操作名字的name参数，与方法有关的共有5个参数。如下:</p><p>input： 指需要做卷积的输入图像， 它要求是一个Tensor， 具有[batch， in_height， in_width，in_channels]这样的形状（shape） ， 具体含义是“训练时一个batch的图片数量， 图片高度， 图片宽度， 图像通道数” ， 注意这是一个四维的<br>Tensor， 要求类型为float32和float64其中之一。</p><p>filter： 相当于CNN中的卷积核， 它要求是一个Tensor， 具有[filter_height， filter_width，in_channels， out_channels]这样的shape， 具体含义是“卷积核的高度， 滤波器的宽度， 图像通道数， 滤波器个数” ， 要求类型与参数input相同。有一个地方需要注意， 第三维in_channels， 就是参数input的第四维。</p><p>strides： 卷积时在图像每一维的步长， 这是一个一维的向量， 长度为4。</p><p>padding： 定义元素边框与元素内容之间的空间。 string类型的量， 只能是SAME和VALID其中之一， 这个值决定了不同的卷积方式， padding的值为’VALID’时， 表示边缘不填充， 当其为’SAME’时， 表示填充到滤波器可以到达图像边缘。</p><p>use_cudnn_on_gpu： bool类型， 是否使用cudnn加速， 默认为true。  </p><h2 id="池化函数"><a href="#池化函数" class="headerlink" title="池化函数"></a>池化函数</h2><p>tf.nn.max_pool(input,ksize,strides,padding,name=None)</p><p>tf.nn.avg_pool(input,ksize,strides,padding,name=None)</p><p>上面这两个函数中的四个参数和卷积参数类似，如下：</p><p>input：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch,height,width,channels]这样的shape。</p><p>ksize：池化窗口的大小，取一个四维向量，一般是[1,height,width,1]，我们不想在batch和channels上做池化，所以这两个维度设为1</p><p>strides：步长，一般也是[1,strides,strides,1]</p><p>padding：和卷积一样，VALID是不进行padding操作，SAME是padding操作</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;卷积函数&quot;&gt;&lt;a href=&quot;#卷积函数&quot; class=&quot;headerlink&quot; title=&quot;卷积函数&quot;&gt;&lt;/a&gt;卷积函数&lt;/h2&gt;&lt;p&gt;tf.nn.conv2d(input,filter,strides,padding,use_cudnn_on_gpu=Non
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow中如何预防过拟合</title>
    <link href="http://yoursite.com/2019/11/26/tensorflow%E4%B8%AD%E5%A6%82%E4%BD%95%E9%A2%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    <id>http://yoursite.com/2019/11/26/tensorflow中如何预防过拟合/</id>
    <published>2019-11-26T14:26:29.000Z</published>
    <updated>2020-02-23T09:18:21.805Z</updated>
    
    <content type="html"><![CDATA[<h2 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h2><p>避免过拟合的方法有很多，常用的方法有early stopping、数据集扩增、正则化、dropout。下面就概述一下，具体请参照优化算法(1)</p><p>early stoping：在发生过拟合之前提前结束。理论上是可以的，但是这个点不好把握。</p><p>数据集扩增：就是让模型见到更多的情况，可以最大化地满足全样本，但实际应用中对于未来事件的预测不理想。</p><p>正则化：通过映入范数，增强模型的泛化能力。包括L1、L2.</p><p>dropout：是网络模型中的一种方法。每次训练时舍去一些节点来增强泛化能力。</p><p>下面我们来具体看看如何实现后两种方法。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>所谓的正则化， 其实就是在神经网络计算损失值的过程中， 在损失后面再加一项。 这样损失值所代表的输出与标准结果间的误差就会受到干扰， 导致学习参数w和b无法按照目标方向来调整， 实现模型无法与样本完全拟合的结果， 从而达到防止过拟合的效果  </p><p>这个干扰项一定要有下面这样的特性：</p><p>1、当欠拟合时，希望它对模型误差的影响越小越好，以便让模型快速拟合实际</p><p>2、当过拟合时，希望他对模型误差的影响越大越好，以便让模型不要产生过拟合的情况。</p><p>由上面两个特性，引入两个范数：L1和L2</p><p>L1：所有学习参数w的绝对值的和。</p><p>L2：所有学习参数w的平方和然后求平方根</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191127210328.png" alt=""></p><p>上图中的第一个式子是L1范式，另一个就是L2范式。loss为等式左边的结果，less(0)代表真实的loss值，less(0)后面的那一项就代表正则化，&lambda;为一个可以调整的参数，用来控制正则化对loss的影响。对于L2，将其乘以1/2是为了反向传播时对其求导可以将数据规整。</p><h2 id="tensorflow中的正则化"><a href="#tensorflow中的正则化" class="headerlink" title="tensorflow中的正则化"></a>tensorflow中的正则化</h2><p>L1正则化：tf.contrib.layers.l1_regularizer(lambda)(w)：lambda是正则化参数</p><p>L2正则化：tf.contrib.layers.l2_regularizer(lambda)(w)：lambda是正则化参数</p><p>但是对于高版本的tensorflow的contrib不稳定，从而在高级一点的版本中删除了contrib这个库。如果想使用L2正则化就得用下面的办法：</p><p>L2的正则化函数为：tf.nn.l2_loss(t,name=None)</p><p>L1的正则化函数在tensorflow是没有自己组装的，可以自己写：tf.reduce_sum(tf.abs(w))</p><h2 id="通过正则化改善过拟合"><a href="#通过正则化改善过拟合" class="headerlink" title="通过正则化改善过拟合"></a>通过正则化改善过拟合</h2><h3 id="使用contrib里的方法："><a href="#使用contrib里的方法：" class="headerlink" title="使用contrib里的方法："></a>使用contrib里的方法：</h3><p>tf.add_to_collection(‘list1’,var)：将变量加入到列表list1中。</p><p> tf.get_collection(‘list1’)：获得列表list1里面的所有变量。</p><p> tf.add_n(list1)：将列表list1中所有的变量加起来并返回。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, tf.constant(<span class="number">2.2</span>))</span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, tf.constant(<span class="number">3.</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.get_collection(<span class="string">'losses'</span>)))   </span><br><span class="line">    print(sess.run(tf.add_n(tf.get_collection(<span class="string">'losses'</span>))))</span><br><span class="line"><span class="comment">#[2.2, 3.0]</span></span><br><span class="line"><span class="comment">#5.2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape , lambda=<span class="number">0.0001</span>)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.random_normal( shape ), dtype = tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(<span class="keyword">lambda</span>)(var))</span><br><span class="line">    <span class="keyword">return</span> var</span><br><span class="line"></span><br><span class="line">mse_loss = tf.reduce_mean( tf.square(y_ - y))</span><br><span class="line">tf.add_to_collection(<span class="string">'losses'</span>, mse_loss)</span><br><span class="line">loss = tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br></pre></td></tr></table></figure><h3 id="使用nn里的方法："><a href="#使用nn里的方法：" class="headerlink" title="使用nn里的方法："></a>使用nn里的方法：</h3><p>使用正则化非常简单，只需要在计算损失值时加上loss的正则化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reg=<span class="number">0.01</span></span><br><span class="line">loss=tf.reduce_mean((y_pred-y)**<span class="number">2</span>)+tf.nn.l2_loss(weights[<span class="string">'h1'</span>])*reg+tf.nn.l2_loss(weight[<span class="string">'h2'</span>])*reg</span><br></pre></td></tr></table></figure><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>还有一种防止过拟合的方法是dropout。这个方法的做法是：在训练过程中，每次随机选择一部分节点不要去“学习”</p><p>因为从样本数据的分析来看，数据本身是不可能很纯净的，也就是任何一个模型不能100%把数据完全分开，在某一类中一定会有一些异常数据，过拟合的问题恰恰是把这些异常数据当初规律来学习了。我们希望把异常数据过滤掉，只关心有用的规律数据。</p><p>异常数据的特点是，它与主流样本中的规律都不同，但是量非常少，相当于在一个样本中出现的概率比主流数据出现的概率低很多。我们可以利用这一点，通过在每次模型中忽略一些节点的数据学习，将小概率的异常数据获得学习的机会降低，这样这些异常数据对模型的影响就会更小了。</p><p><strong>注意</strong>：由于dropout让一部分节点不去“学习”，所以在增加模型的泛化能力的同时，会使学习速度降低，使模型不太容易学成。所以在使用的过程中需要合理地调节到底丢弃多少节点，并不是丢弃的节点越多越好。</p><h2 id="tensorflow中的dropout"><a href="#tensorflow中的dropout" class="headerlink" title="tensorflow中的dropout"></a>tensorflow中的dropout</h2><p>tf.nn.dropout(x,keep_prob,noise_shape=None,seed=None,name=None)</p><p>x： 输入的模型节点。<br>keep_prob： 保持率。 如果为1， 则代表全部进行学习； 如果为0.8， 则代表丢弃20%的节点， 只让80%的节点参与学习。<br>noise_shape： 代表指定x中， 哪些维度可以使用dropout技术。 为None时， 表示所有维度都使用dropout技术。 也可以将某个维度标志为1， 来代表该维度使用dropout技术。 例如： x的形状为[n， len， w， ch]， 使用noise_shape为[n， 1， 1，ch]， 这表明会对x中的第二维度len和第三维度w进行dropout。<br>seed： 随机选取节点的过程中随机数的种子值。</p><h2 id="全连接网络的深浅关系"><a href="#全连接网络的深浅关系" class="headerlink" title="全连接网络的深浅关系"></a>全连接网络的深浅关系</h2><p>在实际中，如果想使用浅层神经网络拟合复杂非线性函数，就需要靠增加的神经元个数来实现。神经元过多意味着需要训练的参数过多，这会增加网络的学习难度，并影响网络的泛化能力。因此，在增加网络结构时，一般倾向于使用更多的模型，来减少网络中所需要神经元的数量，使网络有更好的泛化能力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;方法概述&quot;&gt;&lt;a href=&quot;#方法概述&quot; class=&quot;headerlink&quot; title=&quot;方法概述&quot;&gt;&lt;/a&gt;方法概述&lt;/h2&gt;&lt;p&gt;避免过拟合的方法有很多，常用的方法有early stopping、数据集扩增、正则化、dropout。下面就概述一下，具体请
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>numpy(2)</title>
    <link href="http://yoursite.com/2019/11/24/numpy(2)/"/>
    <id>http://yoursite.com/2019/11/24/numpy(2)/</id>
    <published>2019-11-24T06:58:11.000Z</published>
    <updated>2019-11-24T11:31:45.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="np-concatenate"><a href="#np-concatenate" class="headerlink" title="np.concatenate"></a>np.concatenate</h2><p>数组拼接函数，concatenate((a1,a2,……),axis=0)</p><p>参数a1，a2……为要拼接的数组，axis为在哪个维度上进行拼接。默认为0</p><p>>&gt;&gt;import numpy as np<br>>&gt;&gt;a=np.array([[1,2],[3,4]])<br>>&gt;&gt;b=np.array([[5,6]])</p><p>>&gt;&gt;np.concatenate((a,b),axis=0)<br>array([[1, 2],<br>       [3, 4],<br>       [5, 6]])</p><p>>&gt;&gt;np.concatenate((a,b.T),axis=1)<br>array([[1, 2, 5],<br>       [3, 4, 6]])</p><h2 id="np-eye"><a href="#np-eye" class="headerlink" title="np.eye"></a>np.eye</h2><p>生成对角矩阵</p><p>numpy.eye(N,M=None, k=0, dtype=<type 'float'>)</p><p>第一个参数：输出方阵（行数=列数）的规模，即行数或列数</p><p>第三个参数：默认情况下输出的是对角线全“1”，其余全“0”的方阵，如果k为正整数，则在右上方第k条对角线全“1”其余全“0”，k为负整数则在左下方第k条对角线全“1”其余全“0”。</p><p>>&gt;&gt;import numpy as np<br>>&gt;&gt;np.eye(1,3,dtype=int)<br>array([[1, 0, 0]])<br>>&gt;&gt;np.eye(2,3,dtype=int)<br>array([[1, 0, 0],<br>       [0, 1, 0]])<br>>&gt;&gt;np.eye(2,2,dtype=int)<br>array([[1, 0],<br>       [0, 1]])<br>>&gt;&gt;np.eye(4,4,dtype=int)<br>array([[1, 0, 0, 0],<br>       [0, 1, 0, 0],<br>       [0, 0, 1, 0],<br>       [0, 0, 0, 1]])</p><p>>&gt;&gt;np.eye(4,4,k=2,dtype=int)<br>array([[0, 0, 1, 0],<br>       [0, 0, 0, 1],<br>       [0, 0, 0, 0],<br>       [0, 0, 0, 0]])</p><h2 id="np-random-multivariate-normal"><a href="#np-random-multivariate-normal" class="headerlink" title="np.random.multivariate_normal"></a>np.random.multivariate_normal</h2><p>multivariate_normal(mean,cov,size=None,check_valid=None,tol=None)：用于根据实际情况生成一个多元正态分布矩阵</p><p>其中mean和cov为必要的传参而size，check_valid以及tol为可选参数。</p><p>mean：mean是多维分布的均值维度为1；</p><p>cov：协方差矩阵，注意：协方差矩阵必须是对称的且需为半正定矩阵；</p><p>size：指定生成的正态分布矩阵的维度（例：若size=(1, 1, 2)，则输出的矩阵的shape即形状为 1X1X2XN（N为mean的长度））。</p><p>check_valid：这个参数用于决定当cov即协方差矩阵不是半正定矩阵时程序的处理方式，它一共有三个值：warn，raise以及ignore。当使用warn作为传入的参数时，如果cov不是半正定的程序会输出警告但仍旧会得到结果；当使用raise作为传入的参数时，如果cov不是半正定的程序会报错且不会计算出结果；当使用ignore时忽略这个问题即无论cov是否为半正定的都会计算出结果。3种情况的console打印结果如下：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;np-concatenate&quot;&gt;&lt;a href=&quot;#np-concatenate&quot; class=&quot;headerlink&quot; title=&quot;np.concatenate&quot;&gt;&lt;/a&gt;np.concatenate&lt;/h2&gt;&lt;p&gt;数组拼接函数，concatenate((a1
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
      <category term="数学计算" scheme="http://yoursite.com/categories/python/%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="numpy" scheme="http://yoursite.com/categories/python/%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97/numpy/"/>
    
    
      <category term="python的数学计算模块" scheme="http://yoursite.com/tags/python%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9D%97/"/>
    
  </entry>
  
  <entry>
    <title>Maxout网络</title>
    <link href="http://yoursite.com/2019/11/23/Maxout%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/11/23/Maxout网络/</id>
    <published>2019-11-23T08:18:18.000Z</published>
    <updated>2019-11-23T11:12:54.214Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Maxout介绍"><a href="#Maxout介绍" class="headerlink" title="Maxout介绍"></a>Maxout介绍</h2><p>Maxout网络可以理解为单个神经元的扩展，主要是扩展单个神经元里面的激活函数。Maxout是将激活函数变成一个网络选择器，原理就是将多个神经元并列地放在一起，从它们的输出结果中找到最大的那个，代表对特征响应最敏感，然后取这个神经元的结束参与后面的运算。</p><p>下图是单个神经元和Maxout网络的区别：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/6.jpg" alt=""></p><p>Maxout的公式可以理解为：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123163141.png" alt=""></p><p>这个的做法就是相当于同时使用多个神经元放在一起， 哪个有效果就用哪个。 所以这样的网络会有更好的拟合效果。  </p><h2 id="Maxout网络实现MNIST分类"><a href="#Maxout网络实现MNIST分类" class="headerlink" title="Maxout网络实现MNIST分类"></a>Maxout网络实现MNIST分类</h2><p>Maxout网络的构建方法：通过reduce_max函数对多个神经元的输出来计算Max值，将Max值当作输入按照神经元正反传播方向进行计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据:'</span>,mnist.train.images)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据打shape:'</span>,mnist.train.images.shape)</span><br><span class="line"><span class="keyword">import</span> pylab </span><br><span class="line">im = mnist.train.images[<span class="number">1</span>]</span><br><span class="line">im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">pylab.imshow(im)</span><br><span class="line">pylab.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据打shape:'</span>,mnist.test.images.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'输入数据打shape:'</span>,mnist.validation.images.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment">#导入tensorflow库</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>]) <span class="comment"># mnist data维度 28*28=784</span></span><br><span class="line">y = tf.placeholder(tf.int32, [<span class="literal">None</span>]) <span class="comment"># 0-9 数字=&gt; 10 classes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.random_normal([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">z= tf.matmul(x, W) + b</span><br><span class="line">cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=z))</span><br><span class="line">maxout=tf.reduce_max(z,axis=<span class="number">1</span>,keep_dims=<span class="literal">True</span>)</span><br><span class="line">W2=tf.Variable(tf.truncated_normal([<span class="number">1</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2=tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">pred=tf.nn.softmax(tf.matmul(maxout,W2)+b2)</span><br><span class="line">learning_rate = <span class="number">0.04</span></span><br><span class="line"><span class="comment"># 使用梯度下降优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">training_epochs = <span class="number">200</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())<span class="comment"># Initializing OP</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启动循环开始训练</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># 遍历全部数据集</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># 显示训练中的详细信息</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print( <span class="string">" Finished!"</span>)</span><br></pre></td></tr></table></figure><p>Maxout的拟合功能很强大，但是也会有节点过多，参数过多，训练过慢的缺点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Maxout介绍&quot;&gt;&lt;a href=&quot;#Maxout介绍&quot; class=&quot;headerlink&quot; title=&quot;Maxout介绍&quot;&gt;&lt;/a&gt;Maxout介绍&lt;/h2&gt;&lt;p&gt;Maxout网络可以理解为单个神经元的扩展，主要是扩展单个神经元里面的激活函数。Maxout
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow的学习率退化和随机初始化</title>
    <link href="http://yoursite.com/2019/11/23/Tensorflow%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%80%E5%8C%96%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://yoursite.com/2019/11/23/Tensorflow的学习率退化和随机初始化/</id>
    <published>2019-11-23T03:47:08.000Z</published>
    <updated>2020-02-03T10:48:09.891Z</updated>
    
    <content type="html"><![CDATA[<h2 id="退化学习率"><a href="#退化学习率" class="headerlink" title="退化学习率"></a>退化学习率</h2><p>设置学习率的大小，是在精度和速度之间找到一个平衡。如果学习率的值比较大，则训练速度快，但结果的精度不够。如果学习率的值比较小，精度虽然提升了，但训练会花太多时间。</p><p>退化学习率又叫学习率衰减， 它的本意是希望在训练过程中对于学习率大和小的优点都能够为我们所用， 也就是当训练刚开始时使用大的学习率加快速度， 训练到一定程度后使用小的学习率来提高精度， 这时可以使用学习率衰减的方法。</p><p>exponential_decay(learning_rate,global_step,decay_rate,staircase=False,name=None)，学习率的衰减速度是由global_step和decay_steps来决定的。计算公式如下：</p><p>decayed_learning_rate=learning_rate*decay_rate^(global_step/decay_step)。staircase值默认为False。当为True，将没有衰减功能，只是使用上面的公式初始化一个学习率的值而已。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learning_rate=tf.train.exponential_decay(starter_learning_rate,global_step,<span class="number">100000</span>,<span class="number">0.96</span>)</span><br></pre></td></tr></table></figure><p>这种方式定义的学习率就是退化学习率， 它的意思是当前迭代到global_step步， 学习率每一步都按照每10万步缩小到0.96%的速度衰退。  </p><p>通过增大批次处理样本的数量也可以起到退化学习率的效果。但是这种方法要求训练时的最小批次要与实际应用中的最小批次一致。一旦满足该条件，建议优先选择增大批次数量的方法。因为这样会省去一些开发量和训练中的计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">global_step=tf.Variable(<span class="number">0</span>,trainable=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#初始学习率为0.1</span></span><br><span class="line">initial_learning_rate=<span class="number">0.1</span></span><br><span class="line"><span class="comment">#每十次衰减0.9</span></span><br><span class="line">learning_rate=tf.train.exponential_decay(initial_learning_rate,global_step=global_step,</span><br><span class="line">                                        decay_steps=<span class="number">10</span>,</span><br><span class="line">                                        decay_rate=<span class="number">0.9</span>)</span><br><span class="line">opt=tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line"><span class="comment">#相当于global_step+1</span></span><br><span class="line">add_global=global_step.assign_add(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  print(sess.run(learning_rate))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    g,rate=sess.run([add_global,learning_rate])</span><br><span class="line">    print(g,rate)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：  这是一种常用的训练策略， 在训练神经网络时， 通常在训练刚开始时使用较大的learning rate， 随着训练的进行， 会慢慢减小learning rate。 在使用时， 一定要把当前迭代次数global_step传进去， 否则不会有退化的功能。  </p><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在定义学习参数时，可以通过get_variable和Variable两个方式。在使用get_variable时，tf.get_variable(name,shape,initializer)，当然还有其它参数，可以自己上网找一下。参数initializer就是初始化参数。可以去下表中列出的相关函数</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123153950.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123154010.png" alt=""></p><h3 id="初始化为常量"><a href="#初始化为常量" class="headerlink" title="初始化为常量"></a>初始化为常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">value = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">init = tf.constant_initializer(value)</span><br><span class="line">x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">8</span>], initializer=init)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line">  <span class="comment">#sess.run(tf.global_variables_initializer())</span></span><br><span class="line"><span class="comment">#print(sess.run(x))</span></span><br><span class="line"><span class="comment">#输出:</span></span><br><span class="line"><span class="comment">#[ 0.  1.  2.  3.  4.  5.  6.  7.]</span></span><br></pre></td></tr></table></figure><h3 id="初始化为正态分布"><a href="#初始化为正态分布" class="headerlink" title="初始化为正态分布"></a>初始化为正态分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">init_random = tf.random_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=<span class="literal">None</span>, dtype=tf.float32)</span><br><span class="line">init_truncated = tf.truncated_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, seed=<span class="literal">None</span>, dtype=tf.float32)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_random)</span><br><span class="line">  y = tf.get_variable(<span class="string">'y'</span>, shape=[<span class="number">10</span>], initializer=init_truncated)</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  y.initializer.run()</span><br><span class="line"> </span><br><span class="line">  print(x.eval())</span><br><span class="line">  print(y.eval())</span><br></pre></td></tr></table></figure><h3 id="初始化为均匀分布"><a href="#初始化为均匀分布" class="headerlink" title="初始化为均匀分布"></a>初始化为均匀分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">init_uniform = tf.random_uniform_initializer(minval=<span class="number">0</span>, maxval=<span class="number">10</span>, seed=<span class="literal">None</span>, dtype=tf.float32</span><br><span class="line">x = tf.get_variable(<span class="string">'x'</span>, shape=[<span class="number">10</span>], initializer=init_uniform)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  x.initializer.run()</span><br><span class="line">  print(x.eval())</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># [ 6.93343639  9.41196823  5.54009819  1.38017178  1.78720832  5.38881063</span></span><br><span class="line"><span class="comment">#   3.39674473  8.12443542  0.62157512  8.36026382]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;退化学习率&quot;&gt;&lt;a href=&quot;#退化学习率&quot; class=&quot;headerlink&quot; title=&quot;退化学习率&quot;&gt;&lt;/a&gt;退化学习率&lt;/h2&gt;&lt;p&gt;设置学习率的大小，是在精度和速度之间找到一个平衡。如果学习率的值比较大，则训练速度快，但结果的精度不够。如果学习率的
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的损失函数和梯度下降</title>
    <link href="http://yoursite.com/2019/11/22/Tensorflow%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://yoursite.com/2019/11/22/Tensorflow中的损失函数和梯度下降/</id>
    <published>2019-11-22T12:52:58.000Z</published>
    <updated>2019-11-23T03:34:10.889Z</updated>
    
    <content type="html"><![CDATA[<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="均值平方差"><a href="#均值平方差" class="headerlink" title="均值平方差"></a>均值平方差</h3><p>均值平方差(Mean Squared Error，MSE)，也称”均方误差”，在神经网络中主要是表达预测值与真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的期望值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122210450.png" alt=""></p><p>均方误差的值越小，表明模型越好。类似的损失算法还有均方误差RMSE(将MSE开平方)，平均绝对值误差MAD(对一个真实值与预测值相减的绝对值取平均值)。</p><p><strong>注意</strong>：在神经网络计算时，预测值要与真实值控制在同样的数据分布内，假设将预测值经过Sigmoid激活函数得到取值范围在0~1之间，那么真实值也归一化成0~1之间。</p><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>交叉熵(crossentropy)也是loss算法的一种，一般用于分类问题，表达的意思为预测输入样本属于某一类的概率。其中y代表真实值分类(0或1)，a代表预测值。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122211858.png" alt=""></p><p>交叉熵也只值越小，代表预测结果越准。</p><p><strong>注意</strong>：这里用于计算的a也是通过分布同一化处理的（或者是经过Sigmoid函数激活的），取值范围0~1。</p><h3 id="损失算法的选取"><a href="#损失算法的选取" class="headerlink" title="损失算法的选取"></a>损失算法的选取</h3><p>损失函数的选取取决于输入标签数据的类型： 如果输入的是实数、 无界的值， 损失函数使用平方差； 如果输入标签是位矢量（分类标志） ， 使用交叉熵会更适合。  </p><h3 id="Tensorflow的loss函数"><a href="#Tensorflow的loss函数" class="headerlink" title="Tensorflow的loss函数"></a>Tensorflow的loss函数</h3><h4 id="均值平方差-1"><a href="#均值平方差-1" class="headerlink" title="均值平方差"></a>均值平方差</h4><p>在Tensorflow没有单独的MSE函数，不过由于公式比较简单，往往都是自己写函数。也有多种写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MSE=tf.reduce_mean(tf.pow(tf.sub(logits,outputs),<span class="number">2.0</span>))</span><br><span class="line">MSE=tf.reduce_mean(tf.square(tf.sub(logits,outputs)))</span><br><span class="line">MSE=tf.reduce_mean(tf.square(logits-outputs))</span><br></pre></td></tr></table></figure><p>logits代表标签值，outputs代表预测值。同样也可以组合其它类似loss，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rmse=tf.sqrt(tf.reduce_mean(tf.pow(tf.sub(logits,outputs),<span class="number">2.0</span>)))</span><br><span class="line">mad=tf.reduce_mean(tf.complex_abs(logits,outputs))</span><br></pre></td></tr></table></figure><h4 id="交叉熵-1"><a href="#交叉熵-1" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>在tensorflow中常见的交叉熵函数有：Sigmoid交叉熵、softmax交叉熵、Sparse交叉熵、加权Sigmoid交叉熵</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122215154.png" alt=""></p><p>当然，我们也可以像MSE那样使用自己组合的公式计算交叉熵。对于softmax后的结果logits我们可以对其使用公式-tf.reduce_sum(labels*tf.log(logis),1)，就等同于softmax_cross_entropy_with_logits得到结果。（注意有个负号）</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="softmax交叉熵"><a href="#softmax交叉熵" class="headerlink" title="softmax交叉熵"></a>softmax交叉熵</h4><p>标签是one-hot编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">labels=[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],]</span><br><span class="line">logits=[[<span class="number">2</span>,<span class="number">0.5</span>,<span class="number">6</span>],[<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">3</span>]]</span><br><span class="line"><span class="comment">#进行第一次softmax</span></span><br><span class="line">logits_scaled=tf.nn.softmax(logits)</span><br><span class="line"><span class="comment">#进行第二次softmax</span></span><br><span class="line">logits_scaled2=tf.nn.softmax(logits_scaled)</span><br><span class="line"><span class="comment">#用第一次的softmax进行交叉熵计算</span></span><br><span class="line">result1=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)</span><br><span class="line"><span class="comment">#用第二次的softmax进行交叉熵计算</span></span><br><span class="line">result2=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits_scaled)</span><br><span class="line"><span class="comment">#用自建公式实验</span></span><br><span class="line">result3=-tf.reduce_sum(labels*tf.log(logits_scaled),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  print(<span class="string">"logits_scaled："</span>,sess.run(logits_scaled))</span><br><span class="line">  print(<span class="string">"logits_scaled2"</span>,sess.run(logits_scaled2))</span><br><span class="line">  print(<span class="string">"result1："</span>,sess.run(result1))</span><br><span class="line">  print(<span class="string">"result2："</span>,sess.run(result2))</span><br><span class="line">  print(<span class="string">"result3："</span>,sess.run(result3))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123095531.png" alt=""></p><p>从结果看，logits里面的值原本加和都是大于1的，但是经过softmax之后，总和变成了1。logits中的第一个是跟标签分类相符的，第二个与标签分类不符，所以第一个的交叉熵比较小，是0.02215518。第二个交叉熵比较大，是3.09967351。</p><p><strong>总结</strong>：</p><blockquote><p>比较scaled和scaled2可以看到： 经过第二次的softmax后， 分布概率会有变化， 而scaled才是我们真实转化的softmax值。 比较rel1和rel2可以看到： 传入softmax_cross_entropy_with_logits的logits是不需要进行softmax的。 如果将softmax后的值scaled传入softmax_cross_entropy_with_ logits就相当于进行了两次的softmax转换。</p></blockquote><p>对于已经用softmax转换过的scaled，在计算loss的时候不能再使用softmax_cross_entropy_with_logits了。应该自己写一个函数，如上面代码的result3。</p><p>下面用一组总和为1但是数组中每个值都不等于0或1的数组来代替标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">labels2=[[<span class="number">0.4</span>,<span class="number">0.1</span>,<span class="number">0.5</span>],[<span class="number">0.3</span>,<span class="number">0.6</span>,<span class="number">0.1</span>]]</span><br><span class="line">result4=tf.nn.softmax_cross_entropy_with_logits(labels=labels2,logits=logits)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"result4："</span>,result4)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">result4 [<span class="number">2.1721554</span> <span class="number">2.7696736</span>]</span><br></pre></td></tr></table></figure><p>与前面的result1对比发现，标准的one-hot的结果比较明显。</p><h4 id="sparse交叉熵"><a href="#sparse交叉熵" class="headerlink" title="sparse交叉熵"></a>sparse交叉熵</h4><p>使用sparse_softmax_cross_entropy_with_logits函数的用法，他需要使用非one-hot的标签，所以要把前面的标签换成具体的数值[2,1]。PS：这个labels能不能换成[1,2]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">labels3=[<span class="number">2</span>,<span class="number">1</span>]<span class="comment">#表明labels共有3个类，0、1、2。[2,1]等价于one-hot编码的001与010</span></span><br><span class="line">result5=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels3,logits=logits)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"result5："</span>,result5)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line">result5 [<span class="number">0.02215516</span> <span class="number">3.0996735</span> ]</span><br></pre></td></tr></table></figure><p>result5与result1完全一样。</p><h3 id="计算loss值"><a href="#计算loss值" class="headerlink" title="计算loss值"></a>计算loss值</h3><p>对于softmax_cross_entropy_with_logits后的结果求loss直接取均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss=tf.reduce_mean(result1)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"loss"</span>,sess.run(loss))</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="comment">#loss 1.5609143</span></span><br></pre></td></tr></table></figure><p>对于softmax后的结果，先使用-tf.reduce_sum(labels*tf.log(logits_scaled))，接着求均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss2=tf.reduce_mean(-tf.reduce_sum(labels*tf.log(logits_scaled),<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  print(<span class="string">"loss2:"</span>,loss2)</span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="comment">#loss 1.5609144</span></span><br></pre></td></tr></table></figure><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降法是一个最优化算法， 通常也称为最速下降法， 常用于机器学习和人工智能中递归性地逼近最小偏差模型， 梯度下降的方向也就是用负梯度方向为搜索方向， 沿着梯度下降的方向求解极小值。  </p><p>在训练过程中，每次的正向传播后都会得到输出值与真实值的损失值。这个损失值越小越好，代表模型越好。于是梯度下降的算法就用在这里，帮助寻找最小的那个损失值，从而可以反推出对应的学习参数b和w，达到优化模型的效果。</p><p>常用的梯度下降方法可以分为：批量梯度下降、随机梯度下降、小批量梯度下降。</p><p>批量梯度下降：  遍历全部数据集算一次损失函数， 然后算函数对各个参数的梯度和更新梯度。 这种方法每更新一次参数， 都要把数据集里的所有样本看一遍， 计算量大， 计算速度慢， 不支持在线学习，称为batch gradient descent。</p><p>随机梯度下降：每看一个数据就算一下损失函数，然后求梯度更新参数。  这称为stochastic gradient descent， 随机梯度下降。 这个方法速度比较快， 但是收敛性能不太好， 可能在最优点附近晃来晃去， 命中不到最优点。 两次参数的更新也有可能互相抵消， 造成目标函数震荡比较剧烈  </p><p>小批量梯度下降：为了克服上面两种方法的缺点， 一般采用一种折中手段——小批的梯度下降。 这种方法把数据分为若干个批， 按批来更新参数， 这样一批中的一组数据共同决定了本次梯度的方向， 下降起来就不容易跑偏， 减少了随机性。 另一方面因为批量的样本数与整个数据集相比小了很多， 计算量也不是很大。  </p><h3 id="tensorflow中的梯度下降函数"><a href="#tensorflow中的梯度下降函数" class="headerlink" title="tensorflow中的梯度下降函数"></a>tensorflow中的梯度下降函数</h3><p>在tensorflow中是通过一个叫做Optimizer的优化器进行训练优化的。对于不同的优化器，在tensorflow会有不同的类：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191123112610.png" alt=""></p><p>在训练过程中，先实例化一个优化函数，并基于一定的学习率进行梯度优化训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer=tf.train.GradientDescentOptimizer(learning_rate)</span><br></pre></td></tr></table></figure><p>接着使用minimize()操作，接着传入损失值loss到这个操作。优化器就会按照循环的次数一次次沿着loss最小值的方向优化参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;损失函数&quot;&gt;&lt;a href=&quot;#损失函数&quot; class=&quot;headerlink&quot; title=&quot;损失函数&quot;&gt;&lt;/a&gt;损失函数&lt;/h2&gt;&lt;h3 id=&quot;均值平方差&quot;&gt;&lt;a href=&quot;#均值平方差&quot; class=&quot;headerlink&quot; title=&quot;均值平方差&quot;&gt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow中的激活函数和分类函数</title>
    <link href="http://yoursite.com/2019/11/22/Tensorflow%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%92%8C%E5%88%86%E7%B1%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/11/22/Tensorflow中的激活函数和分类函数/</id>
    <published>2019-11-22T08:36:54.000Z</published>
    <updated>2020-02-03T04:05:28.784Z</updated>
    
    <content type="html"><![CDATA[<p>关于激活函数，我已经在一篇博客上讲解了它的常见种类和作用，详情点击<a href="https://brickexperts.github.io/2019/09/03/激活函数/" target="_blank" rel="noopener">激活函数</a>。这篇博客一起来看下在tensorflow下的激活函数，并补充一些激活函数。顺提一下分类函数。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数的作用就是用来加入非线性因素的，以解决线性模型表达能力不足的缺陷。常用的激活函数有sigmoid，tanh，relu。</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>sigmoid在tensorflow下的对应函数为：</p><p>tf.nn.sigmoid(x,name=None)。从sigmoid的图像来看，随着x趋近正负无穷大，y对应的值越来越接近1或-1，这种情况叫做饱和。处于饱和态的激活函数意味着，当x=100和x=1000时的反映都是一样的，这样的特性转换相当于将1000大于100十倍这个信息丢失了。</p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>tanh在tensorflow下的对应函数为：</p><p>tf.nn.tanh(x,name=None)。  x取值也是从正无穷到负无穷， 但其对应的y值变为-1～1之间， 相对于Sigmoid函数有更广的值域。  但同样也拥有饱和问题。</p><h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><p>relu在tensorflow下的对应函数为：</p><p>tf.nn.relu(x,name=None)。该函数非常简单，大于0的留下，否则一律为0。relu函数运算简单，大大提升了机器的运行效率。还有tf.nn.relu6(x,name=None)，这是以6为阈值的relu函数。与relu函数类似的还有softplus函数，二者的区别是：Softplus函数会更加平滑，但是计算量很大。</p><blockquote><p>softplus的函数公式：f(x)=ln(1+e<sup>x</sup>)。在tensorflow中，Softplus函数对应的函数是tf.nn.softplus(x,name=None)</p></blockquote><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122191433.png" alt=""></p><p>虽然ReLU函数在信号响应上有很多优势， 但这仅仅在正向传播方面。 由于其对负值的全部舍去， 因此很容易使模型输出全零从而无法再进行训练。 例如， 随机初始化的w加入值中有个值是负值， 其对应的正值输入值特征也就被全部屏蔽了， 同理， 对应负值输入值反而被激活了。 这显然不是我们想要的结果。 于是在基于ReLU的基础上又演化出了一些变种函数， 举例如下：  </p><blockquote><p>Noise relus：为max中的x加了一个高斯分布的噪声</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122192702.png" alt=""></p><p>Leaky relus：在relu的基础上，保留一部分负值，让x为负时乘a，a小于等于1。也就是Leaky relus对负信号不是一昧的拒绝，而是缩小。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122192303.png" alt=""></p><p>在TensorFlow中， Leaky relus公式没有专门的函数， 不过可以利用现有函数组成而得到：</p><p>tf.maximum(x,leak*x,name=name) #leakl为传入的参数，可以设为0.01等。</p><p>Elus：当x小于0时，做了更复杂的变换。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122192444.png" alt=""></p><p>在tensorflow中，Elus函数对应的函数，tf.nn.elu(x,name=None)</p></blockquote><h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><p>Swish函数是谷歌公司发现的一个效果更优于Relu的激活函数。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122194816.png" alt=""></p><p>其中&beta;为x的缩放参数，一般情况默认为1即可。在tensorflow的低版本中，没有单独的Swish函数，可以手动封装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Swish</span><span class="params">(x,beta=<span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> x*tf.nn.sigmoid(x*beta)</span><br></pre></td></tr></table></figure><h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><p>对于上面讲的激活函数，其输出值只有两种（0、1，或-1、1，0、x），而现实生活中需要对某一问题进行某种分类，这时就需要使用softmax算法。</p><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>softmax， 就是如果判断输入属于某一个类的概率大于属于其他类的概率， 那么这个类对应的值就逼近于1， 其他类的值就逼近于0。 该算法的主要应用就是多分类， 而且是互斥的， 即只能属于其中的一个类。 与sigmoid类的激活函数不同的是， 一般的激活函数只能分两类，所以可以理解成Softmax是Sigmoid类的激活函数的扩展。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122201749.png" alt=""></p><p> 把所有值用e的n次方计算出来， 求和后算每个值占的比率， 保证总和为1。一般就可以认为softmax得出的就是概率。</p><p>举个例子， 训练的模型可能推测一张包含9的图片代表数字9的概率是80%， 但是判断它是8的概率是5%（因为8和9都有上半部分相似的小圆） ，判断它代表其他数字的概率值更小。 于是取最大    概率的对应数值， 就是这个图片的分类了。 这是一个softmax回归。  </p><h3 id="常用的分类函数"><a href="#常用的分类函数" class="headerlink" title="常用的分类函数"></a>常用的分类函数</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191122203519.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于激活函数，我已经在一篇博客上讲解了它的常见种类和作用，详情点击&lt;a href=&quot;https://brickexperts.github.io/2019/09/03/激活函数/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;激活函数&lt;/a&gt;。这篇博客一起
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>识别图中模糊的手写数字</title>
    <link href="http://yoursite.com/2019/11/21/%E8%AF%86%E5%88%AB%E5%9B%BE%E4%B8%AD%E6%A8%A1%E7%B3%8A%E7%9A%84%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97/"/>
    <id>http://yoursite.com/2019/11/21/识别图中模糊的手写数字/</id>
    <published>2019-11-21T08:57:06.000Z</published>
    <updated>2020-02-03T03:52:11.493Z</updated>
    
    <content type="html"><![CDATA[<p>MNIST是一个入门级的计算机视觉数据集。<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST数据集的官网</a>，我们可以手动下载数据集。</p><h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><p>除了上面的手动下载数据集，tensorflow提供了一个库，可以直接用来自动下载MNIST。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>运行上面的代码，会自动下载数据集并将文件解压到当前代码所在同级目录下的MNIST_data文件夹下。</p><p><strong>注意</strong>：代码中的one_hot=True，表示将样本标签转化为one_hot编码。解释one_hot编码，假如一共10类。0的one_hot为1000000000，1的one_hot编码为0100000000，2的one_hot编码为0010000000，等等。只有一个位是1，1所在的位置就代表第几类，从零开始数。</p><p>MNIST数据集中的图片是28x28Pixel，所以，每一幅图就是1行784列的数据，每一个值代表一个像素。</p><p>如果是黑白的图片，图片中的黑色的地方数值为0，有图案的地方数值为0~255之间的数字，代表其颜色的深度。</p><p>如果是彩色的图片，一个像素由三个值表示RGB（红，黄，蓝）。</p><h2 id="显示数据集信息"><a href="#显示数据集信息" class="headerlink" title="显示数据集信息"></a>显示数据集信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"输入数据为："</span>,mnist.train.images)</span><br><span class="line">print(<span class="string">"输入数据打印shape："</span>,mnist.train.images.shape)</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line">im=mnist.train.images[<span class="number">1</span>]</span><br><span class="line">im=im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">pylab.imshow(im)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure><p>运行代码得出结果：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191121201128.png" alt=""></p><p>这是一个55000行、784列的矩阵，即：这个数据集有55000张图片。</p><h2 id="MNIST数据集组成"><a href="#MNIST数据集组成" class="headerlink" title="MNIST数据集组成"></a>MNIST数据集组成</h2><p>在MNIST训练数据集中，mnist.train.images是一个形状为[55000,784]的张量。其中，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0~255之间。</p><p>MNIST数据集里包含三个数据集：训练集、测试集、验证集。训练集用于训练，测试集用于评估训练过程中的准确度，验证集用于评估最终模型的准确度。可使用以下命令查看里面的数据信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"测试集的shape："</span>,mnist.test.images.shape)</span><br><span class="line">print(<span class="string">"验证集的shape："</span>,mnist.validation.images.shape)</span><br></pre></td></tr></table></figure><p>运行完上面代码，可以发现在测试数据集里有10000条样本图片，验证集有5000个图片。</p><p>三个数据集还有分别对应的三个标签文件，用来标注每个图片上的数字是几。把图片和标签放在一起，称为“样本”。</p><p>MNIST数据集的标签是介于0～9之间的数字， 用来描述给定图片里表示的数字。标签数据是“one-hot vectors”： 一个one-hot向量，除了某一位的数字是1外， 其余各维度数字都是0。 例如， 标签0将表示为（[1， 0， 0， 0， 0， 0，0， 0， 0， 0， 0]） 。 因此， mnist.train.labels是一个[55000， 10]的数字矩阵。  </p><h2 id="定义变量"><a href="#定义变量" class="headerlink" title="定义变量"></a>定义变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">x=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line"><span class="comment">#标签</span></span><br><span class="line">y=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>代码中的None，代表此张量的第一个维度可以是任意长度的。</p><h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><h3 id="定义学习参数"><a href="#定义学习参数" class="headerlink" title="定义学习参数"></a>定义学习参数</h3><p>模型也需要权重值和偏置值，它们被统一称为学习参数。在Tensorflow，使用Variable来定义学习参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W=tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b=tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><h3 id="定义正向传播"><a href="#定义正向传播" class="headerlink" title="定义正向传播"></a>定义正向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred=tf.nn.softmax(tf.matual(x,W)+b)</span><br></pre></td></tr></table></figure><h3 id="定义反向传播"><a href="#定义反向传播" class="headerlink" title="定义反向传播"></a>定义反向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#定义超参数</span></span><br><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)</span><br></pre></td></tr></table></figure><p>首先，将正向传播生成的pred与样本标签y进行一次交叉熵的运算，然后取平均值。接着将cost作为一次正向传播的误差，通过梯度下降的优化方法找到能够使这个误差最小化的W和b。整个过程就是不断让损失值变小。因为损失值越小，才能表明输出的结果跟标签数据越接近。</p><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#要把整个训练样本集迭代25次</span></span><br><span class="line">train_epochs=<span class="number">25</span></span><br><span class="line"><span class="comment">#代表在训练过程中一次取100条数据进行训练</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line"><span class="comment">#每训练一次就把具体的中间状态显示出来</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment">#初始化op</span></span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    avg_cost=<span class="number">0</span></span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">      batch_xs,batch_ys=mnist.train.next_batch(batch_size)</span><br><span class="line">      _,c=sess.run([optimizer,cost],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">      avg_cost+=c/total_batch</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%display_step==<span class="number">0</span>:</span><br><span class="line">      print(<span class="string">"Epoch:"</span>,<span class="string">"%04d"</span> % (epoch+<span class="number">1</span>),<span class="string">"cost="</span>,<span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">  print(<span class="string">"~~~~finish~~~~"</span>)</span><br></pre></td></tr></table></figure><h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><p>测试错误率的算法是：直接判断预测的结果与真实的标签是否相同，如是相同的就表示是正确的，如是不相同的，就表示是错误的。然后将正确的个数除以总个数，得到的值即为正确率。由于是one-hot编码，这里使用了tf.argmax函数返回one-hot编码中数值为1的哪个元素的下标。1代表横轴，0代表纵轴</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试 model</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><p>在代码的两处区域加入以下代码：</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191121221831.png" alt=""></p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191121221920.png" alt=""></p><h2 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"开始第二个会话"</span>)</span><br><span class="line">print(<span class="string">"Starting 2nd session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># Restore model weights from previously saved model</span></span><br><span class="line">    saver.restore(sess, model_path)</span><br><span class="line">    </span><br><span class="line">     <span class="comment"># 测试 model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    </span><br><span class="line">    output = tf.argmax(pred, <span class="number">1</span>)</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">2</span>)</span><br><span class="line">    outputval,predv = sess.run([output,pred], feed_dict=&#123;x: batch_xs&#125;)</span><br><span class="line">    print(outputval,predv,batch_ys)</span><br><span class="line"></span><br><span class="line">    im = batch_xs[<span class="number">0</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    <span class="comment">#pylab.imshow(im)</span></span><br><span class="line">    <span class="comment">#pylab.show()</span></span><br><span class="line">    plt.imshow(im)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    im = batch_xs[<span class="number">1</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    pylab.imshow(im)</span><br><span class="line">    pylab.show()</span><br></pre></td></tr></table></figure><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">mnist=input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">x=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">y=tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W=tf.Variable(tf.random_normal([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b=tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment">#正向传播</span></span><br><span class="line">pred=tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),))</span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line">optimizer=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"><span class="comment">#模型保存</span></span><br><span class="line">saver=tf.train.Saver()</span><br><span class="line">model_path=<span class="string">"./model.ckpt"</span></span><br><span class="line">train_epochs=<span class="number">50</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">display_step=<span class="number">1</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment">#初始化op</span></span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  <span class="keyword">for</span> epoch <span class="keyword">in</span> range(train_epochs):</span><br><span class="line">    avg_cost=<span class="number">0</span></span><br><span class="line">    total_batch=int(mnist.train.num_examples/batch_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">      batch_xs,batch_ys=mnist.train.next_batch(batch_size)</span><br><span class="line">      <span class="comment">#c代表每一个batch_size总的损失</span></span><br><span class="line">      _,c=sess.run([optimizer,cost],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">      avg_cost+=c/total_batch</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%display_step==<span class="number">0</span>:</span><br><span class="line">      print(<span class="string">"Epoch:"</span>,<span class="string">"%04d"</span> % (epoch+<span class="number">1</span>),<span class="string">"cost="</span>,<span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">  print(<span class="string">"~~~~finish~~~~"</span>)</span><br><span class="line">  <span class="comment"># 测试 model</span></span><br><span class="line">  correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">  <span class="comment"># 计算准确率，cast函数用于类型转换</span></span><br><span class="line">  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">  <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">  <span class="comment">#模型保存</span></span><br><span class="line">  save_path = saver.save(sess, model_path)</span><br><span class="line">  print(<span class="string">"Model saved in file: %s"</span> % save_path)</span><br><span class="line">print(<span class="string">"开始第二个会话"</span>)</span><br><span class="line">print(<span class="string">"Starting 2nd session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># Restore model weights from previously saved model</span></span><br><span class="line">    saver.restore(sess, model_path)</span><br><span class="line">    </span><br><span class="line">     <span class="comment"># 测试 model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line">    </span><br><span class="line">    output = tf.argmax(pred, <span class="number">1</span>)</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">2</span>)</span><br><span class="line">    outputval,predv = sess.run([output,pred], feed_dict=&#123;x: batch_xs&#125;)</span><br><span class="line">    print(outputval,predv,batch_ys)</span><br><span class="line"></span><br><span class="line">    im = batch_xs[<span class="number">0</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    <span class="comment">#pylab.imshow(im)</span></span><br><span class="line">    <span class="comment">#pylab.show()</span></span><br><span class="line">    plt.imshow(im)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    im = batch_xs[<span class="number">1</span>]</span><br><span class="line">    im = im.reshape(<span class="number">-1</span>,<span class="number">28</span>)</span><br><span class="line">    pylab.imshow(im)</span><br><span class="line">    pylab.show()</span><br></pre></td></tr></table></figure><p>第70、71行的代码可以用68、69行的代码代替。pylab库结合了pyplot模块和numpy模块。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;MNIST是一个入门级的计算机视觉数据集。&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MNIST数据集的官网&lt;/a&gt;，我们可以手动下载数据集。&lt;/p&gt;
&lt;h2 id=&quot;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="识别数字" scheme="http://yoursite.com/tags/%E8%AF%86%E5%88%AB%E6%95%B0%E5%AD%97/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow的eval用法</title>
    <link href="http://yoursite.com/2019/11/20/tensorflow%E7%9A%84eval%E7%94%A8%E6%B3%95/"/>
    <id>http://yoursite.com/2019/11/20/tensorflow的eval用法/</id>
    <published>2019-11-20T13:23:03.000Z</published>
    <updated>2019-11-20T13:26:25.219Z</updated>
    
    <content type="html"><![CDATA[<p>eval()其实就是tf.Tensor的session.run()的另一种写法。</p><p>1、eval()也是启动计算的一种方式。基于tensorflow基本原理，首先需要定义图，然后计算图，其中计算图的函数有常见的run()函数，如sess.run()，eval()也是类似。</p><p>2、eval()只能用于tf.tensor类对象，也就是有输出的operaton。没有输出的operation，使用session.run()。<br>t.eval() 等价于 tf.get_default_session().run(t)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;eval()其实就是tf.Tensor的session.run()的另一种写法。&lt;/p&gt;
&lt;p&gt;1、eval()也是启动计算的一种方式。基于tensorflow基本原理，首先需要定义图，然后计算图，其中计算图的函数有常见的run()函数，如sess.run()，eval()
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="框架" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A1%86%E6%9E%B6/tensorflow/"/>
    
    
      <category term="框架" scheme="http://yoursite.com/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下安装qt5</title>
    <link href="http://yoursite.com/2019/11/11/ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85qt5/"/>
    <id>http://yoursite.com/2019/11/11/ubuntu下安装qt5/</id>
    <published>2019-11-11T11:45:29.000Z</published>
    <updated>2019-11-19T06:49:11.077Z</updated>
    
    <content type="html"><![CDATA[<p>在官网下载相关文件，<a href="http://download.qt.io/archive/qt/" target="_blank" rel="noopener">官网</a></p><p>我下载的是qt5.11.1版本。点击进入，</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119142332.png" alt=""></p><p>下载安装程序时需注意，只能下载qt-opensource-linux-x64-5.11.1.run，因为只有这个是在linux环境下的安装程序。新建一个名为Qt5.11.1的文件夹，将这个文件放进去。</p><p>在文件目录下打开终端，输入./run进行安装。接着点击next。可能会有要求填邮箱。点击skip跳过输入邮箱步骤。接下来更改程序安装目录，按照自己的需求修改。之后在select components步骤时，建议Select All。在License Agreement步骤时，选择Qt Installer LGPL Agreement。接着等待。进入安装目录，发现如下内容即为成功安装。</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119144311.png" alt=""></p><p>此时安装完后，仍就无法运行。我们需要安装相应的工具，以使得程序正常运行。</p><blockquote><p>sudo apt-get install gcc g++</p><p>sudo apt-get install libqt4-dev</p><p>sudo apt-get install build-essential</p></blockquote><p>以上内容全部安装完毕，进入目录下的Tools/Qtcreator/bin目录下，在终端输入./qtcreator</p><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed2/master/20191119144742.png" alt=""></p><p>接下来的使用就要靠各位读者自己摸索和学习了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在官网下载相关文件，&lt;a href=&quot;http://download.qt.io/archive/qt/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我下载的是qt5.11.1版本。点击进入，&lt;/p&gt;
&lt;p&gt;&lt;img src=
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/categories/Linux/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
</feed>
