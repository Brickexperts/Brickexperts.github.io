<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: 'search.xml'
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="线性回归原理回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目标根本不是求解出标签，注意加以区别。 线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这些参数中并没有一个是必填的，更没有对我们">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="http://yoursite.com/2019/09/07/线性回归/index.html">
<meta property="og:site_name" content="DY的个人博客">
<meta property="og:description" content="线性回归原理回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目标根本不是求解出标签，注意加以区别。 线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这些参数中并没有一个是必填的，更没有对我们">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML82.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML83.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line40.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML85.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML88.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML89.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML90.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML75.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML76.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML77.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML78.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML79.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML80.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML81.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML74.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML91.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML42.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML43.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML45.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML44.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML93.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML94.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line4.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line5.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line6.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line7.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line8.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line9.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line10.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line11.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line14.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line15.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line16.png">
<meta property="og:image" content="c:%5CUsers%5C%E6%B9%9B%E8%93%9D%E6%98%9F%E7%A9%BA%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1568183504767.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line19.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line20.png">
<meta property="og:updated_time" content="2019-09-20T08:34:18.770Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性回归">
<meta name="twitter:description" content="线性回归原理回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目标根本不是求解出标签，注意加以区别。 线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这些参数中并没有一个是必填的，更没有对我们">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png">
  <link rel="alternate" href="/atom.xml" title="DY的个人博客" type="application/atom+xml">
  <link rel="canonical" href="http://yoursite.com/2019/09/07/线性回归/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>线性回归 | DY的个人博客</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DY的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        <li class="menu-item menu-item-search">
          <a href="javascript:;" class="popup-trigger">
          
            <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>


    </div>
</nav>

</div>
    </header>

    

  <a href="https://github.com/Brickexperts" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/07/线性回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="湛蓝星空">
      <meta itemprop="description" content="这个人贼菜">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DY的个人博客">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">线性回归

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-09-07 16:09:19" itemprop="dateCreated datePublished" datetime="2019-09-07T16:09:19+08:00">2019-09-07</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-20 16:34:18" itemprop="dateModified" datetime="2019-09-20T16:34:18+08:00">2019-09-20</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="线性回归原理"><a href="#线性回归原理" class="headerlink" title="线性回归原理"></a>线性回归原理</h2><p>回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向<br>量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目<br>标根本不是求解出标签，注意加以区别。</p>
<p>线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这<br>些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决<br>于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型<br>变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归<br>可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。</p>
<p>线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个<br>有n个特征的样本i而言，它的回归结果可以写作一个几乎人人熟悉的方程 ：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML95.png" alt></p>
<p>&omega;被统称为模型的参数。&omega;0被称为截距&omega;<sub>1</sub>~&omega;<sub>n</sub>被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 X<sub>i1</sub>-X<sub>in</sub>是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML82.png" alt></p>
<p>我们可以使用矩阵来表示这个方程：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML83.png" alt></p>
<p>y=X&omega;，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中&omega;可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量&omega;。</p>
<p>在多元线性回归中，我们在损失函数如下定义：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line40.png" alt></p>
<p>在这个平方结果下，我们的真实标签和预测值分别如上图表示，也就是说，这个损失函数是在计算我们的真实标签和预测值之间的距离。因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML85.png" alt></p>
<p><strong>第一次看不明白上面红字，后面看了书，才明白。这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数</strong>。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。</p>
<p>现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对&omega;求导。</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML88.png" alt></p>
<p>我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML89.png" alt></p>
<h2 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h2><p>sklearn中的线性模型模块是linear_model。</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML90.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML75.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML76.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML77.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML78.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML79.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML80.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML81.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML74.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML91.png" alt></p>
<p>coding：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML42.png" alt></p>
<h2 id="模型评估指标"><a href="#模型评估指标" class="headerlink" title="模型评估指标"></a>模型评估指标</h2><p>回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等<br>评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算<br>法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不<br>同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。<br>第一，我们是否预测到了正确的数值。<br>第二，我们是否拟合到了足够的信息。 </p>
<p>sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML43.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML45.png" alt></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML44.png" alt></p>
<p>​            <strong>注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="keyword">as</span> LR</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue=fch()</span><br><span class="line">xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line">std_x=StandardScaler()</span><br><span class="line">xtrain=std_x.fit_transform(xtrain)</span><br><span class="line">xtest=std_x.transform(xtest)</span><br><span class="line">std_y=StandardScaler()</span><br><span class="line">ytrain=std_y.fit_transform(ytrain.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">ytest=std_y.transform(ytest.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">reg=LR().fit(xtrain,ytrain)</span><br><span class="line">ypredict=reg.predict(xtest)</span><br><span class="line">print(std_y.inverse_transform(ypredict))</span><br><span class="line">print(<span class="string">"均方误差："</span>,mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))</span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"mean_squared_error"</span>))</span><br></pre></td></tr></table></figure>

<p>这里如果我们运行上面的代码，会在第十九行报错：</p>
<p>我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评<br>判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，<br>会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，<br>所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是<br>neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"neg_mean_squared_error"</span>))</span><br></pre></td></tr></table></figure>

<p>为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助:</p>
<p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML93.png" alt></p>
<p>在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。</p>
<p>R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种<br>是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两<br>种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"R2"</span>,r2_score(ypredict,ytest))</span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML94.png" alt></p>
<p>????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是<strong>预测值在分子，真实值在分母</strong>。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接用r2_score</span></span><br><span class="line">print(<span class="string">"R2"</span>,r2_score(y_true=ytest,y_pred=ypredict))</span><br><span class="line"><span class="comment">#利用接口score</span></span><br><span class="line">print(reg.score(xtest,ytest))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#EVS的两种调用方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> explained_variance_score <span class="keyword">as</span> evs</span><br><span class="line"><span class="comment">#第一种</span></span><br><span class="line">print(cross_val_score(reg,housevalue.data,housevalue.target,cv=<span class="number">10</span>,scoring=<span class="string">"explained_variance"</span>))</span><br><span class="line"><span class="comment">#第二种</span></span><br><span class="line">print(<span class="string">"evs"</span>,evs(ytest,ypredict))</span><br></pre></td></tr></table></figure>

<h2 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h2><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line1.png" alt></p>
<p>矩阵A中第一行和第三行的关系，被称为“精确相关关系”，即完全相关，一行可使另一行为0。在这种精确相关关系下，矩阵A的行列式为0，则矩阵A的逆不可能存在。在我们的最小二乘法中，如果矩阵中存在这种精确相关关系，则逆不存在，最小二乘法完全无法使用，线性回归会无法求出结果。 </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line2.png" alt></p>
<p>矩阵B中第一行和第三行的关系不太一样，他们之间非常接近于”精确相关关系“，但又不是完全相关，一行不能使另一行为0，这种关系被称为”高度相关关系“。在这种高度相关关系下，矩阵的行列式不为0，但是一个非常接近0数，矩阵A的逆存在，不过接近于无限大。在这种情况下，最小二乘法可以使用，不过得到的逆会很大，直接影响我们对参数向量w的求解：</p>
<p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line3.png" alt></p>
<p>这样求解出来的参数向量w会很大，因此会影响建模的结果，造成模型有偏差或者不可用。精确相关关系和高度相关关系并称为”多重共线性”。在多重共线性下，模型无法建立，或者模型不可用。</p>
<p>相对的，矩阵C的行之间结果相互独立，梯形矩阵看起来非常正常，它的对角线上没有任何元素特别接近于0，因此其行列式也就不会接近0或者为0，因此矩阵C得出的参数向量w就不会有太大偏差，对于我们拟合而言是比较理想的。 </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line4.png" alt></p>
<p>从上面的所有过程我们可以看得出来，<strong>一个矩阵如果要满秩，则要求矩阵中每个向量之间不能存在多重共线性</strong>。这也构成了线性回归算法对于特征矩阵的要求。</p>
<h3 id="多重共线性与相关性"><a href="#多重共线性与相关性" class="headerlink" title="多重共线性与相关性"></a>多重共线性与相关性</h3><p>多重共线性如果存在，则线性回归就无法使用最小二乘法来进行求解，或者求解就会出现偏差。幸运的是，不能存在<br>多重共线性，不代表不能存在相关性——机器学习不要求特征之间必须独立，必须不相关，只要不是高度相关或者精<br>确相关就好。</p>
<p><strong>关键概念：</strong>多重共线性与相关性</p>
<p>多重共线性是一种统计现象，是指线性模型中的特征（解释变量）之间由于存在精确相关关系或高度相关关系，多重共线性的存在会使模型无法建立，或者估计失真。多重共线性使用指标方差膨胀因子（variance inflation factor，VIF）来进行衡量（from statsmodels.stats.outliers_influence import variance_inflation_factor），通常当我们提到“共线性”，都特指多重共线性。</p>
<p>相关性是衡量两个或多个变量一起波动的程度的指标，它可以是正的，负的或者0。当我们说变量之间具有相关性，通常是指线性相关性，线性相关一般由皮尔逊相关系数进行衡量，非线性相关可以使用斯皮尔曼相关系数或者互信息法进行衡量。 </p>
<h3 id="处理多重共线性的方法"><a href="#处理多重共线性的方法" class="headerlink" title="处理多重共线性的方法"></a>处理多重共线性的方法</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line5.png" alt></p>
<p>这三种手段中，第一种相对耗时耗力，需要较多的人工操作，并且会需要混合各种统计学中的知识和检验来进行使<br>用。第二种手段在现实中应用较多，不过由于理论复杂，效果也不是非常高效。我们的核心是使用第三种方法：改进线性回归来处理多重共线性。为此，岭回归、Lasso、弹性网就被研究出来了。</p>
<h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>在线性模型之中，除了线性回归之外，最知名的就是岭回归与Lasso了。这两个算法非常神秘，他们的原理和应用都不像其他算法那样高调，学习资料也很少。这可能是因为<strong>这两个算法不是为了提升模型表现，而是为了修复漏洞而设计的。</strong></p>
<p>岭回归在多元线性回归的损失函数上加上了正则项，表达为系数w的L2范式（即系数w的平方项）乘以正则化系数&alpha;。岭回归的损失函数的完整表达式写作：</p>
<p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line6.png" alt></p>
<h3 id="linear-model-Ridge"><a href="#linear-model-Ridge" class="headerlink" title="linear_model.Ridge"></a>linear_model.Ridge</h3><p>在sklearn中，岭回归由线性模型库中的Ridge类来调用 </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line7.png" alt></p>
<p>和线性回归相比，岭回归的参数稍微多了那么一点点，但是真正核心的参数就是我们的<strong>正则项的系数&alpha;</strong> ，其他的参数是当我们希望使用最小二乘法之外的求解方法求解岭回归的时候才需要的，通常我们完全不会去触碰这些参数。所以,大家只需要了解&alpha;的用法就可以了。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, LinearRegression, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line"><span class="comment">#print(housevalue.data)</span></span><br><span class="line">print(X.head())</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#数据集索引恢复</span></span><br><span class="line">print(Xtest.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [Xtrain,Xtest]:</span><br><span class="line">    i.index = range(i.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#使用岭回归来进行建模</span></span><br><span class="line">reg = Ridge(alpha=<span class="number">1</span>).fit(Xtrain,Ytrain)</span><br><span class="line">reg.score(Xtest,Ytest)</span><br><span class="line">alpharange = np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>)</span><br><span class="line">ridge, lr = [], []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpharange:</span><br><span class="line">    reg = Ridge(alpha=alpha)</span><br><span class="line">    linear = LinearRegression()</span><br><span class="line">    regs = cross_val_score(reg,X,y,cv=<span class="number">5</span>,scoring = <span class="string">"r2"</span>).mean()</span><br><span class="line">    linears = cross_val_score(linear,X,y,cv=<span class="number">5</span>,scoring = <span class="string">"r2"</span>).mean()</span><br><span class="line">    ridge.append(regs)</span><br><span class="line">    lr.append(linears)</span><br><span class="line">plt.plot(alpharange,ridge,color=<span class="string">"red"</span>,label=<span class="string">"Ridge"</span>)</span><br><span class="line">plt.plot(alpharange,lr,color=<span class="string">"orange"</span>,label=<span class="string">"LR"</span>)</span><br><span class="line">plt.title(<span class="string">"Mean"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行上面代码，可以看出，加利佛尼亚数据集上，岭回归的结果轻微上升，随后骤降。可以说，加利佛尼亚房屋价值数据集带有很轻微的一部分共线性，这种共线性被正则化参数 消除后，模型的效果提升了一点点，但是对于整个模型而言是杯水车薪。在过了控制多重共线性的点后，模型的效果飞速下降，显然是正则化的程度太重，挤占了参数 本来的估计空<br>间。从这个结果可以看出，加利佛尼亚数据集的核心问题不在于多重共线性，岭回归不能够提升模型表现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#观察模型方差</span></span><br><span class="line">alpharange = np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>)</span><br><span class="line">ridge, lr = [], []</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alpharange:</span><br><span class="line">    reg = Ridge(alpha=alpha)</span><br><span class="line">    linear = LinearRegression()</span><br><span class="line">    varR = cross_val_score(reg,X,y,cv=<span class="number">5</span>,scoring=<span class="string">"r2"</span>).var()</span><br><span class="line">    varLR = cross_val_score(linear,X,y,cv=<span class="number">5</span>,scoring=<span class="string">"r2"</span>).var()</span><br><span class="line">    ridge.append(varR)</span><br><span class="line">    lr.append(varLR)</span><br><span class="line">plt.plot(alpharange,ridge,color=<span class="string">"red"</span>,label=<span class="string">"Ridge"</span>)</span><br><span class="line">plt.plot(alpharange,lr,color=<span class="string">"orange"</span>,label=<span class="string">"LR"</span>)</span><br><span class="line">plt.title(<span class="string">"Variance"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>可以发现，模型的方差上升快速。虽然岭回归和Lasso不是设计来提升模型表现，而是专注于解决多重共线性问题的，但当&alpha;在一定范围内变动的时候，消除多重共线性也许能够一定程度上提高模型的泛化能力。 <strong>不是很明白这句话，后面补</strong></p>
<h3 id="选取最佳的正则化参数"><a href="#选取最佳的正则化参数" class="headerlink" title="选取最佳的正则化参数"></a>选取最佳的正则化参数</h3><p>既然要选择&alpha;的范围，我们就不可避免的进行最优参数的选择。在各种机器学习教材中，总是教导使用岭迹图来判断正则项参数的最佳取值。传统的岭迹图长成下图：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line8.png" alt></p>
<p>这一个以正则化参数为横坐标，线性模型求解的系数&alpha;为纵坐标的图像，其中每一条彩色的线都是一个系数&alpha;。其目标是建立正则化参数与系数&alpha;之间的直接关系，以此来观察正则化参数的变化如何影响了系数w的拟合。岭迹图认为，线条交叉越多，则说明特征之间的多重共线性越高。我们应该选择系数较为平稳的喇叭口所对应的&alpha;取值作为最佳的正则化参数的取值。绘制岭迹图的方法非常简单，代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="comment">#创造10*10的希尔伯特矩阵</span></span><br><span class="line">X = <span class="number">1.</span> / (np.arange(<span class="number">1</span>, <span class="number">11</span>) + np.arange(<span class="number">0</span>, <span class="number">10</span>)[:, np.newaxis])</span><br><span class="line">y = np.ones(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#计算横坐标</span></span><br><span class="line">n_alphas = <span class="number">200</span></span><br><span class="line">alphas = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, n_alphas)</span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alphas:</span><br><span class="line">    ridge = linear_model.Ridge(alpha=a, fit_intercept=<span class="literal">False</span>)</span><br><span class="line">    ridge.fit(X, y)</span><br><span class="line">    coefs.append(ridge.coef_)</span><br><span class="line"><span class="comment">#绘图展示结果</span></span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.plot(alphas, coefs)</span><br><span class="line">ax.set_xscale(<span class="string">'log'</span>)</span><br><span class="line">ax.set_xlim(ax.get_xlim()[::<span class="number">-1</span>]) <span class="comment">#将横坐标逆转</span></span><br><span class="line">plt.xlabel(<span class="string">'正则化参数alpha'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'系数w'</span>)</span><br><span class="line">plt.title(<span class="string">'岭回归下的岭迹图'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>我非常不建议大家使用岭迹图来作为寻找最佳参数的标准。</strong><br>有这样的两个理由：</p>
<p>1、岭迹图的很多细节，很难以解释。比如为什么多重共线性存在会使得线与线之间有很多交点？当&alpha;很大了之后看上去所有的系数都很接近于0，难道不是那时候线之间的交点最多吗？</p>
<p>2、岭迹图的评判标准，非常模糊。哪里才是最佳的喇叭口？哪里才是所谓的系数开始变得”平稳“的时候？一千个读者一千个哈姆雷特的画像？未免也太不严谨了</p>
<p>我们应该使用交叉验证来选择最佳的正则化系数。在sklearn中，我们有带交叉验证的岭回归可以使用：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line9.png" alt></p>
<p>RidgeCV的重要参数、属性和接口：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line10.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV, LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Ridge_ = RidgeCV(alphas=np.arange(<span class="number">1</span>,<span class="number">1001</span>,<span class="number">100</span>),store_cv_values=<span class="literal">True</span>).fit(X, y)</span><br><span class="line"><span class="comment">#无关交叉验证的岭回归结果</span></span><br><span class="line">print(<span class="string">"没有交叉验证的岭回归："</span>,Ridge_.score(X,y))</span><br><span class="line"><span class="comment">#调用所有交叉验证的结果</span></span><br><span class="line">print(<span class="string">"调用所有交叉验证："</span>,Ridge_.cv_values_.shape)</span><br><span class="line"><span class="comment">#进行平均后可以查看每个正则化系数取值下的交叉验证结果</span></span><br><span class="line">print(<span class="string">"平均后的每个正则化系数交叉验证结果："</span>,max(Ridge_.cv_values_.mean(axis=<span class="number">0</span>)))</span><br><span class="line"><span class="comment">#查看被选择出来的最佳正则化系数</span></span><br><span class="line">print(<span class="string">"最佳正则化系数："</span>,Ridge_.alpha_)</span><br></pre></td></tr></table></figure>

<h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><p>除了岭回归之外，最常被人们提到还有模型Lasso。Lasso全称最小绝对收缩和选择算子（least absolute shrinkage and selection operator），由于这个名字过于复杂所以简称为Lasso。和岭回归一样，Lasso是被创造来作用于多重共线性问题的算法，不过Lasso使用的是系数w的L1范式（L1范式则是系数w的绝对值）乘以正则化系数&alpha; ，所以,Lasso的损失函数表达式为：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line11.png" alt></p>
<p><strong>岭回归VSLasso</strong>：岭回归可以解决特征间的精确相关关系导致的最小二乘法无法使用的问题，而Lasso不行。</p>
<p><strong>Lasso不是从根本上解决多重共线性问题，而是限制多重共线性带来的影响。</strong></p>
<h3 id="Lasso的核心作用：特征选择"><a href="#Lasso的核心作用：特征选择" class="headerlink" title="Lasso的核心作用：特征选择"></a>Lasso的核心作用：特征选择</h3><p>sklearn中我们使用类Lasso来调用lasso回归，众多参数中我们需要比较在意的就是参数&alpha; ，正则化系数。另外需要注意的就是参数positive。当这个参数为”True”的时候，是我们要求Lasso回归出的系数必须为正数，以此来保证我们的&alpha;一定以增大来控制正则化的程度。 需要注意的是，在sklearn中我们的Lasso使用的损失函数是：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line14.png" alt></p>
<p>红色框框住的只是作为系数存在。用来消除我们对损失函数求导后多出来的那个2的（求解w时所带的1/2），然后对整体的RSS求了一个平均而已，无论时从损失函数的意义来看还是从Lasso的性质和功能来看，这个变化没有造成任何影响，只不过计算上会更加简便一些。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, LinearRegression, Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="keyword">as</span> TTS</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">housevalue = fch()</span><br><span class="line">X = pd.DataFrame(housevalue.data)</span><br><span class="line">y = housevalue.target</span><br><span class="line">X.columns = [<span class="string">"住户收入中位数"</span>,<span class="string">"房屋使用年代中位数"</span>,<span class="string">"平均房间数目"</span></span><br><span class="line">,<span class="string">"平均卧室数目"</span>,<span class="string">"街区人口"</span>,<span class="string">"平均入住率"</span>,<span class="string">"街区的纬度"</span>,<span class="string">"街区的经度"</span>]</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#恢复索引</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [Xtrain,Xtest]:</span><br><span class="line">    i.index = range(i.shape[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#线性回归进行拟合</span></span><br><span class="line">reg = LinearRegression().fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"线性回归："</span>,(reg.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#岭回归进行拟合</span></span><br><span class="line">Ridge_ = Ridge(alpha=<span class="number">0</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"岭回归："</span>,(Ridge_.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#Lasso进行拟合</span></span><br><span class="line">lasso_ = Lasso(alpha=<span class="number">0</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"Lasso"</span>,(lasso_.coef_*<span class="number">100</span>).tolist())</span><br></pre></td></tr></table></figure>

<p>可以看到，岭回归没有报出错误，但Lasso就不一样了，虽然依然对系数进行了计算，但是报出了整整三个警告：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line15.png" alt></p>
<p>这三条分别是这样的内容：</p>
<p>​    1、正则化系数为0，这样算法不可收敛！如果你想让正则化系数为0，请使用线性回归吧</p>
<p>​    2、没有正则项的坐标下降法可能会导致意外的结果，不鼓励这样做！</p>
<p>​    3、目标函数没有收敛，你也许想要增加迭代次数，使用一个非常小的alpha来拟合模型可能会造成精确度问题！ </p>
<p>sklearn不推荐我们使用0这样的正则化系数。如果我们的确希望取到0，那我们可以使用一个比较很小的数，比如0.01这样的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#岭回归拟合</span></span><br><span class="line">Ridge_ = Ridge(alpha=<span class="number">0.01</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print((Ridge_.coef_*<span class="number">100</span>).tolist())</span><br><span class="line"><span class="comment">#Lasso进行拟合</span></span><br><span class="line">lasso_ = Lasso(alpha=<span class="number">0.01</span>).fit(Xtrain,Ytrain)</span><br><span class="line">print((lasso_.coef_*<span class="number">100</span>).tolist())</span><br></pre></td></tr></table></figure>

<p>这样就不会报任何警告了。</p>
<h3 id="选取最佳的正则化"><a href="#选取最佳的正则化" class="headerlink" title="选取最佳的正则化"></a>选取最佳的正则化</h3><p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line16.png" alt></p>
<p><img src="C:%5CUsers%5C%E6%B9%9B%E8%93%9D%E6%98%9F%E7%A9%BA%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1568183504767.png" alt="1568183504767"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing <span class="keyword">as</span> fch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">housevalue=fch()</span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest=train_test_split(housevalue.data,housevalue.target,test_size=<span class="number">0.3</span>,random_state=<span class="number">420</span>)</span><br><span class="line"><span class="comment">#自己建立Lasso进行alpha选择的范围</span></span><br><span class="line">alpharange = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, <span class="number">200</span>,base=<span class="number">10</span>)</span><br><span class="line">lasso_ = LassoCV(alphas=alpharange <span class="comment">#自行输入的alpha的取值范围</span></span><br><span class="line">,cv=<span class="number">5</span> <span class="comment">#交叉验证的折数</span></span><br><span class="line">).fit(Xtrain, Ytrain)</span><br><span class="line"><span class="comment">#查看被选择出来的最佳正则化系数</span></span><br><span class="line">print(<span class="string">"最佳的正则化系数："</span>,lasso_.alpha_)</span><br><span class="line"><span class="comment">#调用所有交叉验证的结果</span></span><br><span class="line">print(<span class="string">"所有交叉验证的结果："</span>,lasso_.mse_path_)</span><br><span class="line">print(lasso_.mse_path_.shape) <span class="comment">#返回每个alpha下的五折交叉验证结果</span></span><br><span class="line">print(lasso_.mse_path_.mean(axis=<span class="number">1</span>)) <span class="comment">#有注意到在岭回归中我们的轴向是axis=0吗？</span></span><br><span class="line"><span class="comment">#在岭回归当中我们的交叉验证结果返回的是，每一个样本在每个alpha下的交叉验证结果</span></span><br><span class="line"><span class="comment">#因此我们要求每个alpha下的交叉验证均值，就是axis=0，跨行求均值</span></span><br><span class="line"><span class="comment">#而在这里，我们返回的是，每一个alpha取值下，每一折交叉验证的结果</span></span><br><span class="line"><span class="comment">#因此我们要求每个alpha下的交叉验证均值，就是axis=1，跨列求均值</span></span><br><span class="line"><span class="comment">#最佳正则化系数下获得的模型的系数结果</span></span><br><span class="line">print(<span class="string">"最佳正则化系数获得的模型结果："</span>,lasso_.coef_)</span><br><span class="line">print(<span class="string">"最佳正则化系数的准确率："</span>,lasso_.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#与线性回归相比如何？</span></span><br><span class="line">reg = LinearRegression().fit(Xtrain,Ytrain)</span><br><span class="line">print(<span class="string">"线性回归的准确率："</span>,reg.score(Xtest,Ytest))</span><br><span class="line"><span class="comment">#使用lassoCV自带的正则化路径长度和路径中的alpha个数来自动建立alpha选择的范围</span></span><br><span class="line">ls_ = LassoCV(eps=<span class="number">0.00001</span></span><br><span class="line">,n_alphas=<span class="number">300</span></span><br><span class="line">,cv=<span class="number">5</span></span><br><span class="line">).fit(Xtrain, Ytrain)</span><br><span class="line">print(ls_.alpha_)</span><br><span class="line">print(ls_.alphas_) </span><br><span class="line">print(ls_.alphas_.shape)</span><br><span class="line">print(ls_.score(Xtest,Ytest))</span><br><span class="line">print(ls_.coef_)</span><br></pre></td></tr></table></figure>

<p>模型效果上表现和普通的Lasso没有太大的区别，不过他们都在各个方面对原有的Lasso做了一些相应的改进（比如说提升了本来就已经很快的计算速度，增加了模型选择的维度，因为均方误差作为损失函数只考虑了偏差，不考虑方差的在。除了解决多重共线性这个核心问题之外，<strong>线性模型还有更重要的事情要做：提升模型表现</strong>。这才是机器学习最核心的需求，而Lasso和岭回归不是为此而设计的。<strong>为了提升模型表现而做出的改进：多项式回归</strong>。 </p>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>首先，“线性”这个词用于描述不同事物时有着不同的含义，我们最常使用的线性是指“变量之间的线性关系(linear relationship)”。它表示两个变量之间的关系可以展示为一条直线，即可以使用方程y=ax+b来进行拟合。要探索两个变量之间的关系是否线性，最简单的方式就是绘制散点图，如果散点图能够相对均匀地分布在一条直线的两端，则说明这两个变量之间的关系是线性的。从线性关系这个概念出发，我们有了一种说法叫做“线性数据”。通常来说，一组数据由多个特征和标签组成。当这些特征分别与标签存在线性关系的时候，我们就说这一组数据是线性数据。</p>
<p>但当我们在进行分类的时候，我们的数据分布往往是这样的：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line19.png" alt></p>
<p>这些数据都不能由一条直线来进行拟合，他们也没有均匀分布在某一条线的周围，那我们怎么判断，这些数据是线性数据还是非线性数据呢？在这里就要注意了，当我们在回归中绘制图像时，绘制的是特征与标签的关系图，横坐标是特征，纵坐标是标签，我们的标签是连续型的，所以我们可以通过是否能够使用一条直线来拟合图像判断数据究竟属于线性还是非线性。然而在分类中，我们绘制的是数据分布图，横坐标是其中一个特征，纵坐标是另一个特征，标签则是数据点的颜色。因此在分类数据中，我们使用“是否线性可分”（linearly separable）这个概念来划分分类数据集。<strong>当分类数据的分布上可以使用一条直线来将两类数据分开时，我们就说数据是线性可分的。反之，数据不是线性可分的。</strong> ps：上面那张图我也不知道是不是线性可分的，大家知道以下这个概念就好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">rnd = np.random.RandomState(<span class="number">42</span>) <span class="comment">#设置随机数种子</span></span><br><span class="line">X = rnd.uniform(<span class="number">-3</span>, <span class="number">3</span>, size=<span class="number">100</span>) <span class="comment">#random.uniform，从输入的任意两个整数中取出size个随机数</span></span><br><span class="line"><span class="comment">#生成y的思路：先使用NumPy中的函数生成一个sin函数图像，然后再人为添加噪音</span></span><br><span class="line">y = np.sin(X) + rnd.normal(size=len(X)) / <span class="number">3</span> <span class="comment">#random.normal，生成size个服从正态分布的随机数</span></span><br><span class="line"><span class="comment">#使用散点图观察建立的数据集是什么样子</span></span><br><span class="line">plt.scatter(X, y,marker=<span class="string">'o'</span>,c=<span class="string">'k'</span>,s=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#为后续建模做准备：sklearn只接受二维以上数组作为特征矩阵的输入</span></span><br><span class="line">X = X.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#使用原始数据进行建模</span></span><br><span class="line">LinearR = LinearRegression().fit(X, y)</span><br><span class="line">TreeR = DecisionTreeRegressor(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line"><span class="comment">#放置画布</span></span><br><span class="line">fig, ax1 = plt.subplots(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#创建测试数据：一系列分布在横坐标上的点</span></span><br><span class="line">line = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">1000</span>, endpoint=<span class="literal">False</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#将测试数据带入predict接口，获得模型的拟合效果并进行绘制</span></span><br><span class="line">ax1.plot(line, LinearR.predict(line), linewidth=<span class="number">2</span>, color=<span class="string">'green'</span>,</span><br><span class="line">label=<span class="string">"linear regression"</span>)</span><br><span class="line">ax1.plot(line, TreeR.predict(line), linewidth=<span class="number">2</span>, color=<span class="string">'red'</span>,</span><br><span class="line">label=<span class="string">"decision tree"</span>)</span><br><span class="line"><span class="comment">#将原数据上的拟合绘制在图像上</span></span><br><span class="line">ax1.plot(X[:, <span class="number">0</span>], y, <span class="string">'o'</span>, c=<span class="string">'k'</span>)</span><br><span class="line"><span class="comment">#其他图形选项</span></span><br><span class="line">ax1.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"Regression output"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"Input feature"</span>)</span><br><span class="line">ax1.set_title(<span class="string">"Result before discretization"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>从图像上可以看出，线性回归无法拟合出这条带噪音的正弦曲线的真实面貌，只能够模拟出大概的趋势，而决策树却<br>通过建立复杂的模型将几乎每个点都拟合出来了。可见，使用线性回归模型来拟合非线性数据的效果并不好，而决策<br>树这样的模型却拟合得太细致，相比之下，还是决策树的拟合效果更好一些。<strong>线性模型可以用来拟合非线性数据，而非线性模型也可以用来拟合线性数据，更神奇的是，有的算法没有模型也可以处理各类数据，而有的模型可以既可以是线性，也可以是非线性模型。</strong></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/line20.png" alt></p>

    </div>

    
    
    
<div>
	
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">----本文结束，感谢您的阅读。如有错，请指正。----</div>
    
</div>

	
</div>
    
      <div>
        <div id="reward-container">
  <div>大哥大嫂过年好！支持我一下呗</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="湛蓝星空 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="湛蓝星空 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      </div>

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/09/07/模型的保存和加载/" rel="next" title="模型的保存和加载">
                <i class="fa fa-chevron-left"></i> 模型的保存和加载
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/09/07/逻辑回归/" rel="prev" title="逻辑回归">
                逻辑回归 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="湛蓝星空">
  <p class="site-author-name" itemprop="name">湛蓝星空</p>
  <div class="site-description motion-element" itemprop="description">这个人贼菜</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">79</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/Brickexperts" title="GitHub &rarr; https://github.com/Brickexperts" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归原理"><span class="nav-text">线性回归原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归API"><span class="nav-text">线性回归API</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型评估指标"><span class="nav-text">模型评估指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多重共线性"><span class="nav-text">多重共线性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#多重共线性与相关性"><span class="nav-text">多重共线性与相关性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理多重共线性的方法"><span class="nav-text">处理多重共线性的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#岭回归"><span class="nav-text">岭回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-model-Ridge"><span class="nav-text">linear_model.Ridge</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选取最佳的正则化参数"><span class="nav-text">选取最佳的正则化参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lasso"><span class="nav-text">Lasso</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lasso的核心作用：特征选择"><span class="nav-text">Lasso的核心作用：特征选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选取最佳的正则化"><span class="nav-text">选取最佳的正则化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多项式回归"><span class="nav-text">多项式回归</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">湛蓝星空</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>

    

  </div>

  
    
    
  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>

  
  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>



  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>



  <script src="/js/next-boot.js?v=7.3.0"></script>

  

  

  


  























  <script src="/js/local-search.js?v=7.3.0"></script>














</body>
</html>
