<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="generator" content="Hexo 4.0.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    search: {
      root: '/',
      path: 'search.xml'
    },
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="​    前些天我把python的处理图像的库——opencv总结了一下，但这终究是传统方法处理图像。现在都是用深度学习网络处理图像。所以，在学深度学习之前，我看了些机器学习的知识。但在看机器学习的算法前，我们先来看看特征工程。 sklearn中的数据堪称完美，各大教材的数据也是一样。但是到我们现实应用中时，发现模型调用效果差，这是因为现实中的数据离完美的数据集差十万八千里。所以数据预处理和特征工">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="数据预处理与特征工程">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2019&#x2F;08&#x2F;09&#x2F;%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B&#x2F;index.html">
<meta property="og:site_name" content="DY的个人博客">
<meta property="og:description" content="​    前些天我把python的处理图像的库——opencv总结了一下，但这终究是传统方法处理图像。现在都是用深度学习网络处理图像。所以，在学深度学习之前，我看了些机器学习的知识。但在看机器学习的算法前，我们先来看看特征工程。 sklearn中的数据堪称完美，各大教材的数据也是一样。但是到我们现实应用中时，发现模型调用效果差，这是因为现实中的数据离完美的数据集差十万八千里。所以数据预处理和特征工">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML121.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML68.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML1.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML3.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML4.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML6.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML7.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML8.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML9.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML10.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML12.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML11.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML13.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML14.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML15.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML16.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML17.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML18.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML19.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML60.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML20.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML69.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML66.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML70.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML122.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML123.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML124.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML125.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML126.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML127.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML128.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML129.png">
<meta property="og:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML130.png">
<meta property="og:updated_time" content="2019-09-15T06:31:33.127Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Brickexperts&#x2F;Figurebed&#x2F;master&#x2F;ML121.png">
  <link rel="alternate" href="/atom.xml" title="DY的个人博客" type="application/atom+xml">
  <link rel="canonical" href="http://yoursite.com/2019/08/09/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">


<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>数据预处理与特征工程 | DY的个人博客</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DY的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
    <ul id="menu" class="menu">
        
        
        
          
          <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        <li class="menu-item menu-item-search">
          <a href="javascript:;" target="_blank" rel="noopener" class="popup-trigger">
          
            <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>


    </div>
</nav>

</div>
    </header>

    

  <a href="https://github.com/Brickexperts" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/09/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="湛蓝星空">
      <meta itemprop="description" content="这个人贼菜">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DY的个人博客">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">数据预处理与特征工程

              
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-09 10:53:28" itemprop="dateCreated datePublished" datetime="2019-08-09T10:53:28+08:00">2019-08-09</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-15 14:31:33" itemprop="dateModified" datetime="2019-09-15T14:31:33+08:00">2019-09-15</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          <br>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>​    前些天我把python的处理图像的库——opencv总结了一下，但这终究是传统方法处理图像。现在都是用深度学习网络处理图像。所以，在学深度学习之前，我看了些机器学习的知识。但在看机器学习的算法前，我们先来看看特征工程。</p>
<p>sklearn中的数据堪称完美，各大教材的数据也是一样。但是到我们现实应用中时，发现模型调用效果差，这是因为现实中的数据离完美的数据集差十万八千里。所以数据预处理和特征工程是必要的。</p>
<p>sklearn中六大板块有两大板块是关于数据预处理和特征工程的：也就是黄色标识的两个模块</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML121.png" alt=""></p>
<h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p>在机器学习中，大多数算法都只能处理数值型数据，不能处理文字。在sklearn当中，除了专用来处理文字的算法，其它的算法在fit的时候全部要求输入数组和矩阵，也不能导入文字型数据。然而在现实生活中，许多标签和特征在数据收集完毕后，都不是以数字来表现的。为了让数据适应算法，我们必须将数据进行编码，即将文字型数据转换为数值型。</p>
<p><strong>preprocessing.LabelEncoder</strong>：标签专用，能够将分类转换为分类数值。这个没运行成功，好像是我的数据集有问题。我自己设了个数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame,Series</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data=DataFrame(data=[[<span class="number">-1</span>,<span class="number">2</span>],[<span class="number">-0.5</span>,<span class="number">6</span>],[<span class="number">0</span>,<span class="number">18</span>],[<span class="number">1</span>,<span class="number">18</span>]])</span><br><span class="line">print(data)</span><br><span class="line">y=data.iloc[:,<span class="number">-1</span>]<span class="comment">#要输入的是标签，所以容许一维数组</span></span><br><span class="line">print(y)</span><br><span class="line">le=LabelEncoder()</span><br><span class="line">le.fit(y)       <span class="comment">#导入数据，调取结果</span></span><br><span class="line">label=le.transform(y)</span><br><span class="line"><span class="comment">#print(le.fit_transform(y))</span></span><br><span class="line">print(<span class="string">"调取结果是："</span>,label)</span><br><span class="line">print(le.classes_)     <span class="comment">#查看标签中有多少类型</span></span><br><span class="line">data.iloc[:,<span class="number">-1</span>]=label</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML68.png" alt=""></p>
<p><strong>preprocessing.OrdinalEncoder</strong>：特征专用，能够将分类特征转换为分类数值</p>
<p>字典特征抽取，把数据中的以字符串标记的数据转化为one-hot编码。</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML1.png" alt=""></p>
<p>由上面代码可得下面的稀疏（sparse）矩阵：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML2.png" alt=""></p>
<p>将稀疏矩阵转为矩阵：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML3.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML4.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML6.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML7.png" alt=""></p>
<p>将矩阵转化为字典：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML8.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML9.png" alt=""></p>
<p>文本特征抽取：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML10.png" alt=""></p>
<p>和字典特征抽取不一样的是CountVectorizer没有sparse参数，只能通过矩阵的toarray转化为数组</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML12.png" alt=""></p>
<p>可得：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML11.png" alt=""></p>
<p><strong>注意：单个字母不统计。因为单个英文字母没有依据</strong></p>
<p>如果文本是中文，根据需要的词频给文本添加空格。同样，单个字无法统计</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML13.png" alt=""></p>
<p>利用jieba分词对数据进行one-hot编码：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML14.png" alt=""></p>
<p>TF-IDF特征抽取：用以评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML15.png" alt=""></p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="数据归一化处理"><a href="#数据归一化处理" class="headerlink" title="数据归一化处理"></a>数据归一化处理</h3><p>通过对原始数据进行交换把数据映射到（默认为[0,1]）之间</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML16.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML17.png" alt=""></p>
<p>归一化缺点：注意在特定场景最大值最小值是变化的。另外，最大值与最小值非常容易受异常点的影响，所以这种方法的鲁棒性差，只适合传统精确小数据场景。</p>
<h3 id="数据标准化处理"><a href="#数据标准化处理" class="headerlink" title="数据标准化处理"></a>数据标准化处理</h3><p>通过对原始数据进行变换把数据变换到均值为0，方差为1的范围。</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML18.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML19.png" alt=""></p>
<p>标准化总结：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。</p>
<p>除了StandardScaler和MinMaxScaler之外，sklearn也提供了各种其他的缩放处理：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML60.png" alt=""></p>
<h3 id="缺失值处理方法"><a href="#缺失值处理方法" class="headerlink" title="缺失值处理方法"></a>缺失值处理方法</h3><p>老版本用Imputer，新版本用SimpleImputer</p>
<p>删除：如果每列或者行数据缺失值达到一定比例时，建议放弃整行或者整列</p>
<p>插补：可以通过缺失值每行或者每列的平均值、中位数来填充</p>
<p>Imputer：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML20.png" alt=""></p>
<p>SimpleImputer：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML69.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML66.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line">data=pd.read_csv(<span class="string">"train.csv"</span>,index_col=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#打印行数，列数，列索引，列非空值个数，列类型，内存占用</span></span><br><span class="line">print(data.info())</span><br><span class="line">Age = data.loc[:,<span class="string">"Age"</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>)         <span class="comment">#sklearn当中特征矩阵必须是二维</span></span><br><span class="line">imp_mean = SimpleImputer()                          <span class="comment">#实例化，默认均值填补</span></span><br><span class="line">imp_mean = imp_mean.fit_transform(Age)             <span class="comment">#fit_transform一步完成调取结果</span></span><br><span class="line">data.loc[:,<span class="string">"Age"</span>]=imp_mean                             <span class="comment">#使用中位数填补Age</span></span><br><span class="line">print(data[<span class="string">"Age"</span>])</span><br></pre></td></tr></table></figure>
<h3 id="处理连续型特征"><a href="#处理连续型特征" class="headerlink" title="处理连续型特征"></a>处理连续型特征</h3><p>二值化和分段</p>
<p>sklearn.preprocessing.Binarizer</p>
<p>根据阈值将数据二值化(将特征值设置为0或1)，用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射为1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Binarizer</span><br><span class="line">Age=data.loc[:,<span class="string">"Age"</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">Bin=Binarizer(threshold=<span class="number">30</span>).fit_transform(Age)</span><br><span class="line">print(Bin)</span><br></pre></td></tr></table></figure>
<p>sklearn.preprocessing.KBinsDiscretizer</p>
<p>这是将连续变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。三个重要参数：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML70.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line">x=data.loc[:,<span class="string">"Age"</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">'ordinal'</span>, strategy=<span class="string">'uniform'</span>)</span><br><span class="line">est.fit_transform(x)</span><br><span class="line"><span class="comment">#查看转换后分的箱:变成了一列中的三箱 set(est.fit_transform(X).ravel())    ravel降维</span></span><br><span class="line">est = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">'onehot'</span>, strategy=<span class="string">'uniform'</span>) <span class="comment">#查看转换后分的箱变成了哑变量</span></span><br><span class="line">result=est.fit_transform(x).toarray()</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>当数据预处理完成后，我们就要开始进行特征工程了 </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML122.png" alt=""></p>
<p>我们有四种方法可以用来选择特征：过滤法，嵌入法，包装法，和降维算法。 </p>
<h3 id="过滤法"><a href="#过滤法" class="headerlink" title="过滤法"></a>过滤法</h3><h4 id="方差过滤"><a href="#方差过滤" class="headerlink" title="方差过滤"></a>方差过滤</h4><p>这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征。VarianceThreshold有重要参数threshold，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line">print(X.shape)</span><br><span class="line">selector = VarianceThreshold() <span class="comment">#实例化，不填参数默认方差为0</span></span><br><span class="line">X_var0 = selector.fit_transform(X) <span class="comment">#获取删除不合格特征之后的新特征矩阵</span></span><br><span class="line"><span class="comment">#也可以直接写成 X = VairanceThreshold().fit_transform(X)</span></span><br><span class="line">print(X_var0.shape)</span><br></pre></td></tr></table></figure>
<p>可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)</span><br><span class="line">np.median(X.var().values)</span><br><span class="line">print(X_fsvar.shape)</span><br></pre></td></tr></table></figure>
<p>当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为：</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML123.png" alt=""></p>
<p>其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line"><span class="comment">#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征</span></span><br><span class="line">X_bvar = VarianceThreshold(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)).fit_transform(X)</span><br><span class="line">print(X_bvar.shape)</span><br></pre></td></tr></table></figure>
<p>如果你用knn和随机森林对没过滤的数据和过滤后的数据进行交叉验证，会发现，随机森林的准确率略低于knn，但随机森林运行的速度比knn快很多。为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？这是由于两种算法的原理中涉及到的计算量不同。最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选<br>择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。<strong>因此，过滤法的主要对象是：需要遍历特征或升维的算法们，而过滤法的主要目的是：在维持算法表现的前提下，帮助算法们降低计算成本。</strong> </p>
<p>对受影响的算法来说，我们可以将方差过滤的影响总结如下：</p>
<p> <img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML124.png" alt=""></p>
<p>我们怎样知道，方差过滤掉的到底时噪音还是有效特征呢？过滤后模型到底会变好还是会变坏呢？答案是：每个数<br>据集不一样，只能自己去尝试。这里的方差阈值，其实相当于是一个超参数，要选定最优的超参数，我们可以画学<br>习曲线，找模型效果最好的点。 </p>
<h4 id="相关性过滤"><a href="#相关性过滤" class="headerlink" title="相关性过滤"></a>相关性过滤</h4><p>方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息。 </p>
<h5 id="卡方过滤"><a href="#卡方过滤" class="headerlink" title="卡方过滤"></a>卡方过滤</h5><p>卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data=pd.read_csv(<span class="string">"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)</span><br><span class="line"><span class="comment">#假设在这里我一直我需要300个特征</span></span><br><span class="line">X_fschi = SelectKBest(chi2, k=<span class="number">300</span>).fit_transform(X_fsvar, y)</span><br><span class="line">print(X_fschi.shape)</span><br><span class="line">print(cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean())</span><br></pre></td></tr></table></figure>
<p>可以看出，模型的效果降低了，这说明我们在设定k=300的时候删除了与模型相关且有效的特征，我们的K值设置得太小，要么我们需要调整K值，要么我们必须放弃相关性过滤。当然，如果模型的表现提升，则说明我们的相关性过滤是有效的，是过滤掉了模型的噪音的，这时候我们就保留相关性过滤的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>,<span class="number">390</span>,<span class="number">10</span>):</span><br><span class="line">    X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y)</span><br><span class="line">    once = cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.plot(range(<span class="number">200</span>,<span class="number">390</span>,<span class="number">10</span>),score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML125.png" alt=""></p>
<p>通过这条曲线，我们可以观察到，随着K值的不断增加，模型的表现不断上升，这说明，K越大越好，数据中所有的特征都是与标签相关的。但是运行这条曲线的时间同样也是非常地长。还有一种更好的k的方法：看p值选择k</p>
<p>卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和P值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平，即p值判断的边界，具体我们可以这样来看： </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML126.png" alt=""></p>
<p><strong>调用SelectKBest之前，我们可以直接从chi2实例化后的模型中获得各个特征所对应的卡方值和P值</strong>。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line">chivalue, pvalues_chi = chi2(X_fsvar,y)</span><br><span class="line"><span class="comment">#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征</span></span><br><span class="line"><span class="comment">#pvalues_chi就是p值</span></span><br><span class="line">print(pvalues_chi)<span class="comment">#从此看出所有特征的p值为0</span></span><br><span class="line">k = chivalue.shape[<span class="number">0</span>] - (pvalues_chi &gt; <span class="number">0.05</span>).sum()</span><br><span class="line">print(k)</span><br><span class="line">X_fschi = SelectKBest(chi2, k=k).fit_transform(X_fsvar, y)</span><br><span class="line">print((cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fschi,y,cv=<span class="number">5</span>).mean()))</span><br></pre></td></tr></table></figure>
<p>可以观察到，所有特征的p值都是0，这说明对于digit recognizor这个数据集来说，方差验证已经把所有和标签无关的特征都剔除了，或者这个数据集本身就不含与标签无关的特征。在这种情况下，舍弃任何一个特征，都会舍弃对模型有用的信息，而使模型表现下降，因此在我们对计算速度感到满意时，我们不需要使用相关性过滤来过滤我们的数据。 </p>
<h5 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a>F检验</h5><p>F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式。 </p>
<p>F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，<strong>我们希望选取p值小于0.05或0.01的特征</strong>，这些特征与标签时显著线性相关的，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接上面的代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif</span><br><span class="line">F, pvalues_f = f_classif(X_fsvar,y)</span><br><span class="line">k = F.shape[<span class="number">0</span>] - (pvalues_f &gt; <span class="number">0.05</span>).sum()</span><br><span class="line">print(k)</span><br><span class="line">X_fsF = SelectKBest(f_classif, k=k).fit_transform(X_fsvar, y)</span><br><span class="line">print(cross_val_score(RFC(n_estimators=<span class="number">10</span>,random_state=<span class="number">0</span>),X_fsF,y,cv=<span class="number">5</span>).mean())</span><br></pre></td></tr></table></figure>
<p>得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。 </p>
<h5 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h5><p>互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和<br>feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。</p>
<p>所有特征的互信息量估计都大于0，因此所有特征都与标签相关。</p>
<h3 id="Embedder嵌入法"><a href="#Embedder嵌入法" class="headerlink" title="Embedder嵌入法"></a>Embedder嵌入法</h3><p>嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性。我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。 </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML127.png" alt=""></p>
<p>SelectFromModel是一个元变换器，可以与任何在拟合后具有coef_，feature_importances_属性或参数中可选惩罚项的评估器一起使用（比如随机森林和树模型就具有属性feature_importances_，逻辑回归就带有l1和l2惩罚项，线性支持向量机也支持l2惩罚项） </p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML128.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data=pd.read_csv(<span class="string">"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">X_embedded = SelectFromModel(RFC_,threshold=<span class="number">0.005</span>).fit_transform(X,y)</span><br><span class="line">threshold = np.linspace(<span class="number">0</span>,(RFC_.fit(X,y).feature_importances_).max(),<span class="number">20</span>)</span><br><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> threshold:</span><br><span class="line">    X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)</span><br><span class="line">    once = cross_val_score(RFC_,X_embedded,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.plot(threshold,score)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML129.png" alt=""></p>
<p>从图像上来看，随着阈值越来越高，模型的效果逐渐变差，被删除的特征越来越多，信息损失也逐渐变大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#细化学习曲线</span></span><br><span class="line">score2 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.linspace(<span class="number">0</span>,<span class="number">0.002</span>,<span class="number">20</span>):</span><br><span class="line">    X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y)</span><br><span class="line">    once = cross_val_score(RFC_,X_embedded,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score2.append(once)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(np.linspace(<span class="number">0</span>,<span class="number">0.002</span>,<span class="number">20</span>),score2)</span><br><span class="line">plt.xticks(np.linspace(<span class="number">0</span>,<span class="number">0.002</span>,<span class="number">20</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="wrapper包装法"><a href="#wrapper包装法" class="headerlink" title="wrapper包装法"></a>wrapper包装法</h3><p>包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。 区别于过滤法和嵌入法的一次是过滤法和嵌入法训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。</p>
<p><img src="https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML130.png" alt=""></p>
<p>参数estimator是需要填写的实例化后的评估器，n_features_to_select是想要选择的特征个数，step表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，.support_：返回所有的特征的是否最后被选中的布尔矩阵，以及.ranking_返回特征的按数次迭代中综合重要性的排名。类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data=pd.read_csv(<span class="string">"digit recognizor.csv"</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">1</span>:]</span><br><span class="line">y = data.iloc[:,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier <span class="keyword">as</span> RFC</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line">RFC_ = RFC(n_estimators =<span class="number">10</span>,random_state=<span class="number">0</span>)</span><br><span class="line">selector = RFE(RFC_, n_features_to_select=<span class="number">340</span>, step=<span class="number">50</span>).fit(X, y)</span><br><span class="line">print(<span class="string">"返回所有的特征的最后被选中的布尔矩阵个数："</span>,selector.support_.sum())</span><br><span class="line">print(<span class="string">"返回特征的按数次迭代中综合重要性的排名："</span>,selector.ranking_)</span><br><span class="line">X_wrapper = selector.transform(X)</span><br><span class="line">print(cross_val_score(RFC_,X_wrapper,y,cv=<span class="number">5</span>).mean())</span><br><span class="line">score = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">751</span>,<span class="number">50</span>):</span><br><span class="line">    X_wrapper = RFE(RFC_,n_features_to_select=i, step=<span class="number">50</span>).fit_transform(X,y)</span><br><span class="line">    once = cross_val_score(RFC_,X_wrapper,y,cv=<span class="number">5</span>).mean()</span><br><span class="line">    score.append(once)</span><br><span class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">5</span>])</span><br><span class="line">plt.plot(range(<span class="number">1</span>,<span class="number">751</span>,<span class="number">50</span>),score)</span><br><span class="line">plt.xticks(range(<span class="number">1</span>,<span class="number">751</span>,<span class="number">50</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="特征选择总结"><a href="#特征选择总结" class="headerlink" title="特征选择总结"></a>特征选择总结</h2><p>至此，我们讲完了降维之外的所有特征选择的方法。这些方法的代码都不难，但是每种方法的原理都不同，并且都涉及到不同调整方法的超参数。经验来说，过滤法更快速，但更粗糙。包装法和嵌入法更精确，比较适合具体到算法去调整，但计算量比较大，运行时间长。当数据量很大的时候，优先使用方差过滤和互信息法调整，再上其他特征选择方法。使用逻辑回归时，优先使用嵌入法。使用支持向量机时，优先使用包装法。迷茫的时候，从过滤法走起，看具体数据具体分析。</p>

    </div>

    
    
    
<div>
	
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">----本文结束，感谢您的阅读。如有错，请指正。----</div>
    
</div>

	
</div>
    
      <div>
        <div id="reward-container">
  <div>大哥大嫂过年好！支持我一下呗</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="湛蓝星空 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="湛蓝星空 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      </div>

    <footer class="post-footer">
          
        
        <div class="post-tags">
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          
        </div>
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
              <a href="/2019/08/09/opencv-7/" rel="next" title="opencv(7)">
                <i class="fa fa-chevron-left"></i> opencv(7)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
              <a href="/2019/08/09/ML%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2/" rel="prev" title="机器学习算法分类和数据分割">
                机器学习算法分类和数据分割 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
    </footer>
  </div>
  
  
  
  </article>

  </div>


          </div>
          


        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="湛蓝星空">
  <p class="site-author-name" itemprop="name">湛蓝星空</p>
  <div class="site-description motion-element" itemprop="description">这个人贼菜</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/%20%7C%7C%20archive">
        
          <span class="site-state-item-count">107</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/%20%7C%7C%20th">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/%20%7C%7C%20tags">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">46</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/Brickexperts" title="GitHub &rarr; https://github.com/Brickexperts" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征抽取"><span class="nav-text">特征抽取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据预处理"><span class="nav-text">数据预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据归一化处理"><span class="nav-text">数据归一化处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据标准化处理"><span class="nav-text">数据标准化处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缺失值处理方法"><span class="nav-text">缺失值处理方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理连续型特征"><span class="nav-text">处理连续型特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程"><span class="nav-text">特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#过滤法"><span class="nav-text">过滤法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#方差过滤"><span class="nav-text">方差过滤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#相关性过滤"><span class="nav-text">相关性过滤</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#卡方过滤"><span class="nav-text">卡方过滤</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#F检验"><span class="nav-text">F检验</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#互信息法"><span class="nav-text">互信息法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedder嵌入法"><span class="nav-text">Embedder嵌入法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wrapper包装法"><span class="nav-text">wrapper包装法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择总结"><span class="nav-text">特征选择总结</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">湛蓝星空</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>

    

  </div>

  
    
    
  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  <script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>

  
  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>



  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>



  <script src="/js/next-boot.js?v=7.3.0"></script>

  

  

  


  





  



















  <script src="/js/local-search.js?v=7.3.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->














</body>
</html>
