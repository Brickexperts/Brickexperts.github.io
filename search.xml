<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tensorflow中的模型训练技巧]]></title>
    <url>%2F2020%2F01%2F15%2FTensorflow%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[优化卷积核在实际的卷积训练中，为了加快速度，常常把卷积层裁开。比如一个3x3的过滤器，可以裁成3x1和1x3两个过滤器，分别对原有输入做卷积操作，这样可以大大提升运算速度 原理：在浮点运算中乘法消耗的资源较多，我们的目的就是尽量减小乘法运算 比如：对一个5x2的原始图片进行一次3x3的同卷积，相当于生成的5x2像素中每一个都要经历3x3次乘法，一共90次。同样一张图片，如果先进行一次3x1的同卷积需要30次运算，再进行一次1x3的同卷积还是30次，一共60次。 看下面例子： 123W_conv2 = weight_variable([5, 5, 64, 64])b_conv2 = bias_variable([64])h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2) 修改为： 123456W_conv21 = weight_variable([5, 1, 64, 64])b_conv21 = bias_variable([64])h_conv21 = tf.nn.relu(conv2d(h_pool1, W_conv21) + b_conv21)W_conv2 = weight_variable([1, 5, 64, 64])b_conv2 = bias_variable([64])h_conv2=tf.nn.relu(conv2d(h_conv21,W_conv2)+b_conv2) 批量归一化批量归一化，简称BN算法。一般用在全连接层或卷积神经网络。它的作用是要最大限度地保证每次的正向传播输出在同一分布上， 这样反向计算时参照的数据样本分布就会与正向计算时的数据分布一样了。 保证了分布统一， 对权重的调整才会更有意义。 批量归一化的做法很简单，即将每一层运算出来的数据都归一化成均值为0、方差为1的标准高斯分布。这样就会在保留样本分布特征的同时，又消除了层与层之间的分布差异 Tensorflow中自带的BN函数定义： tf.nn.batch_normalization(x,mean,variance,offset,scale,variance_epsion,name=None)，参数说明如下： x：代表输入 mean：代表样本的均值 variance：代表方差 offset：代表偏移，即相加一个转化值，后面我们会用激活函数来转换，所以直接使用0 scale：代表缩放，即乘以一个转化值。一般用1 variance_epsilon：为了避免分布为0的情况，给分母加一个极小值 要想使用上面这个BN函数，必须由另一个函数配合使用——tf.nn.moments，由他来计算均值和方差，然后就可以使用BN。tf.nn.moments定义如下： tf.nn.moments(x,axes,name=None,keep_dims=False) axes主要是指定哪个轴来求均值与方差 有了上面的两个函数还不够，为了有更好的效果，我们希望使用平滑指数衰减的方法来优化每次的均值与方差，于是就用到了tf.train.ExponentialMovingAverage函数。它的左右是上一次的值对本次的值有个衰减后的影响，从而使每次的值连起来后会相对平滑一些。展开后可以用下列等式表示： shadow_variable = decay * shadow_variable + (1- decay) * variable，各参数说明如下： decay：代表衰减指数，直接指定的。比如0.9 variable：代表本批次样本中的值 等式右边的shadow_variable：代表上次总样本的值 等式左边的shadow_variable：代表计算出来的本次总样本的值 上面的函数需要联合使用，于是在Tensorflow中的layers模块又实现了一次BN函数，相当于把上面几个函数结合在一起。使用时，首先将头文件引入： 1from tensorflow.contrib.layers.python.layers import barch_norm batch_norm(inputs,decay=0.999,center=True,scale=False,epsilon=0.001,activation_fn=None,param_initializers=None,param_regularizers=None,updates_collections=ops.GraphKeys.UPDATE_OPS,is_training=True,reuse=None,variables_collections=None,outputs_collections=None,trainable=True,batch_weights=None,fused=False,data_format=DATA_FORMAT_NHWC,zero_debias_moving_mean=False,scope=None,renorm=False,renorm_clipping=None,renorm_decay=0.9)，参数看得眼都花了，参数说明如下： inputs：代表输入 decay：代表移动平均值的衰败速度，是使用了一种叫做平滑指数衰减的方法更新均值方差，一般设为0.9；值太小会导致均值和方差更新太快，而值太大又会导致几乎没有衰减，容易出现过拟合。 scale：是否进行变化（通过乘一个gamma值进行缩放），一般设为False。 epsilon：是为了避免分母为0的情况，给分母加一个极小值，默认即可。 is_training：当它为True，代表是训练过程，这时会不断更新样本集的均值与方差。测试时，设为False，就会使用测试样本集的均值与方差 updatas_collections：默认为tf.GraphKeys.UPDARE_OPS，在训练时提供一种内置的均值方差更新机制，即通过图中的tf.GraphKeys.UPDATE_OPS变量来更新。但它是在每次当前批次训练完成后才更新均值和方差，这样导致当前数据总是使用前一次的均值和方差，没有得到最新的更新。所以都会将其设为None，让均值和方差即时更新。 reuse：支持共享变量，与下面的scope参数联合使用 scope：指定变量的作用域variable_scope]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度]]></title>
    <url>%2F2020%2F01%2F15%2F%E6%A2%AF%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[在反向传播过程中，神经网络需要对每一个loss对应的学习参数求偏导，算出的这个值叫做梯度，用来乘以学习率然后更新学习参数使用的 求单变量偏导它是通过tf.gradients函数来实现的。tf.gradients(ys,xs,grad_ys=None,name=’gradients’,colocate_gradients_with_ops=False,gate_gradients=False, aggregation_method=None,stop_gradients=None) 第一个参数为求导公式的结果，第二个参数为指定公式中的哪个变量来求偏导。实现第一个参数对第二个参数求导。 12345678910import tensorflow as tfw1=tf.Variable([[1,2]],dtype=tf.float32)w2=tf.Variable([[3,4]],dtype=tf.float32)y=tf.matmul(w1,tf.convert_to_tensor([[9],[10]],dtype=tf.float32))grads=tf.gradients(y,[w1])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("梯度为：",sess.run(grads)) #梯度为： [array([[ 9., 10.]], dtype=float32)] 上面例子中，由于y是由w1与[[9],[10]]相乘而来，所以导数就是[[9],[10]]，也就是斜率 求多变量偏导这就需要用到tf.gradients的第三个参数，grad_ys。grad_ys也是一个list，其长度等于len(ys)。这个参数的意义在于对第一个参数中的每个元素的求导加权重 1234567891011121314151617181920212223import tensorflow as tftf.reset_default_graph()#随机生成一个形状为2的变量w1 = tf.get_variable('w1', shape=[2])w2 = tf.get_variable('w2', shape=[2])w3 = tf.get_variable('w3', shape=[2])w4 = tf.get_variable('w4', shape=[2])y1 = w1 + w2+ w3y2 = w3 + w4#不考虑参数grad_ysgradients= tf.gradients([y1, y2], [w1, w2, w3, w4])#考虑参数grad_ysgradients1 = tf.gradients([y1, y2], [w1, w2, w3, w4], grad_ys=[tf.convert_to_tensor([1.,2.]), tf.convert_to_tensor([3.,4.])])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(w1)) print(sess.run(gradients1)) print(sess.run(gradients)) 梯度停止对于反向传播过程中某种特殊情况需要停止梯度运算时，在tensorflow中提供了一个tf.stop_gradient函数，被它定义过的节点将没有梯度运算功能 123456789101112131415161718192021222324252627import tensorflow as tftf.reset_default_graph()w1 = tf.get_variable('w1', shape=[2])w2 = tf.get_variable('w2', shape=[2])w3 = tf.get_variable('w3', shape=[2])w4 = tf.get_variable('w4', shape=[2])y1 = w1 + w2+ w3y2 = w3 + w4a = w1+w2a_stoped = tf.stop_gradient(a)y3= a_stoped+w3gradients = tf.gradients([y1, y2], [w1, w2, w3, w4], grad_ys=[tf.convert_to_tensor([1.,2.]), tf.convert_to_tensor([3.,4.])]) gradients2 = tf.gradients(y3, [w1, w2, w3], grad_ys=tf.convert_to_tensor([1.,2.])) print(gradients2) gradients3 = tf.gradients(y3, [w3], grad_ys=tf.convert_to_tensor([1.,2.])) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(gradients)) #print(sess.run(gradients2))#报错，因为w1和w2梯度停止了 print(sess.run(gradients3))]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反卷积和反池化]]></title>
    <url>%2F2020%2F01%2F11%2F%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%8F%8D%E6%B1%A0%E5%8C%96%2F</url>
    <content type="text"><![CDATA[反卷积反卷积是指通过测量输出已知输入重构未知输入的过程。在神经网络中，反卷积过程并不具备学习的能力，仅仅是用于可视化一个已经训练好的卷积网络模型，没有学习训练的过程。如下图：即为VGG16反卷积神经网络的结构，展示了一个卷积网络与反卷积网络结合的过程。 反卷积就是将中间的数据，按照前面卷积、池化等过程，完全相反地做一遍，从而得到类似原始输入的数据。 反卷积原理反卷积可以理解为卷积操作的逆操作。千万不要将反卷积操作可以复原卷积操作的输入值，反卷积并没有这个功能，它仅仅是将卷积变换过程中的步骤反向变换一次。通过将卷积核转置，与卷积后的结果再做一遍卷积，所以反卷积还有个名字叫做转置卷积。 反卷积操作： （1） 首先是将卷积核反转（ 并不是转置，而是上下左右方向进行递序操作）。也就是对卷积核做180o翻转。（2） 再将卷积结果作为输入， 做补0的扩充操作， 即往每一个元素后面补0。 这一步是根据步长来的， 对每一个元素沿着步长的方向补（ 步长-1） 个0。 例如， 步长为1就不用补0了。（3） 在扩充后的输入基础上再对整体补0。以原始输入的shape作为输出， 按照前面介绍的卷积padding规则， 计算pading的补0位置及个数， 得到的补0位置要上下和左右各自颠倒一下。（4） 将补0后的卷积结果作为真正的输入，反转后的卷积核为filter， 进行步长为1的卷积操作。 需要注意的是，通过反卷积并不能还原卷积之前的矩阵，只能从大小上进行还原，反卷积的本质还是卷积，只是在进行卷积之前，会进行一个自动的padding补0，从而使得输出的矩阵与指定输出矩阵的shape相同。框架本身，会根据我们自己设定的反卷积值来计算输入矩阵的尺寸，如果shape不符合，会出现报错。 反卷积函数在Tensorflow中，反卷积是通过函数tf.nn.conv2d_transpose来实现的。 conv2d_transpose(value,filter,output_shape,strides,padding=”SAME”,data_format=”NHWC”,name=None)： value：代表卷积操作之后的张量，需要进行反卷积的矩阵 filter：代表卷积核，参数格式[height,width,output_channels,in_channels] output_shape：设置反卷积输出矩阵的shape strides：反卷积步长 padding：补0方式，SAME和VALID方式 data_format：string类型的量，’NHWC’和’NCHW’其中之一，这是tensorflow新版本中新加的参数，它说明了value参数的数据格式。’NHWC’指tensorflow标准的数据格式[batch, height, width, in_channels]，‘NCHW’指Theano的数据格式,[batch, in_channels，height, width]，默认值是’NHWC’。 name：操作名称 12345678910111213141516171819202122import numpy as npimport tensorflow as tf img = tf.Variable(tf.constant(1.0,shape = [1, 4, 4, 1])) filter = tf.Variable(tf.constant([1.0,0,-1,-2],shape = [2, 2, 1, 1]))conv = tf.nn.conv2d(img, filter, strides=[1, 2, 2, 1], padding='VALID') cons = tf.nn.conv2d(img, filter, strides=[1, 2, 2, 1], padding='SAME')print(conv.shape)print(cons.shape) contv= tf.nn.conv2d_transpose(conv, filter, [1,4,4,1],strides=[1, 2, 2, 1], padding='VALID')conts = tf.nn.conv2d_transpose(cons, filter, [1,4,4,1],strides=[1, 2, 2, 1], padding='SAME') with tf.Session() as sess: sess.run(tf.global_variables_initializer() ) print("img",sess.run(img)) print("conv:\n",sess.run([conv,filter])) print("cons:\n",sess.run([cons])) print("contv:\n",sess.run([contv])) print("conts:\n",sess.run([conts])) 反卷积应用场景由于反卷积的特性，可以用于信道均衡、图像恢复等问题。而在神经网络的研究中， 反卷积更多的是充当可视化的作用。 对于一个复杂的深度卷积网络，通过每层若干个卷积核的变换， 我们无法知道每个卷积核关注的是什么， 变换后的特征是什么样子。 通过反卷积的还原， 可以对这些问题有个清晰的可视化， 以各层得到的特征图作为输入， 进行反卷积， 得到反卷积结果， 用以验证显示各层提取到的特征图。 反池化反池化属于池化的逆操作，是无法通过池化的结果还原出全部的原始数据。因为池化的过程就是只保留主要信息，舍去部分信息。如想从池化后的这些主要信息恢复出全部信息，则存在信息缺失，这时只能通过补位来实现最大程度的信息完整 反池化原理池化有两种最大池化和平均池化，反池化与其对应。 反平均池化比较简单。首先还原成原来的大小，然后将池化结果中的每个值都填入其对应于原始数据区域中的相应未知即可。 反最大池画要求在池化过程中记录最大激活值的坐标位置，然后在反池化时，只把池化过程中最大激活值所在位置坐标的值激活，其它的值置为0。这个过程只是一种近似，因为在池化过程中，除了最大值所在的位置，其它的值是不为0的 反池化操作Tensorflow中没有反池化操作的函数。对于最大池化，也不支持输出最大激活值的位置，但是同样有个池化的反向传播函数tf.nn.max_pool_with_argmax。该函数可以输出位置，需要我们自己封装一个反池化函数。 首先重新定义最大池化函数： 12345def max_pool_with_argmax(net, stride): _, mask = tf.nn.max_pool_with_argmax( net,ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1],padding='SAME') mask = tf.stop_gradient(mask) net = tf.nn.max_pool(net, ksize=[1, stride, stride, 1],strides=[1, stride, stride, 1], padding='SAME') return net, mask 在上面代码里，先调用tf.nn.max_pool_with_argmax函数获得每个最大值的位置mask，再将反向传播的mask梯度停止，接着再用tf.nn.max_pool函数计算最大池化操作，然后将mask和池化结果一起返回。 注意：tf.nn.max_pool_with_argmax的方法只支持GPU操作，不能在cpu机器上使用。 接下来定义一个数组，并使用最大池化函数对其进行池化操作，比较一下tensorflow自带的tf.nn.max_pool函数是否一样，看看输出的mask 12345678910111213img=tf.constant([ [[0.0,4.0],[0.0,4.0],[0.0,4.0],[0.0,4.0]], [[1.0,5.0],[1.0,5.0],[1.0,5.0],[1.0,5.0]], [[2.0,6.0],[2.0,6.0],[2.0,6.0],[2.0,6.0]], [[3.0,7.0],[3.0,7.0], [3.0,7.0],[3.0,7.0]] ]) img=tf.reshape(img,[1,4,4,2])pooling=tf.nn.max_pool(img,[1,2,2,1],[1,2,2,1],padding="SAME")encode,mask=max_pool_with_argmax(img,2)with tf.Session() as sess: print("image:",sess.run(img)) print("pooling:",sess.run(pooling)) print("encode",sess.run([encode,mask])) 从输出结果可以看到，定义的最大池化与原来的版本输出是一样的。mask的值是将整个数据flat(扁平化)后的索引，但却保持与池化结果一致的shape。 1234567891011121314151617181920def unpool(net, mask, stride): ksize = [1, stride, stride, 1] input_shape = net.get_shape().as_list() #计算new shape output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]) #计算索引 one_like_mask = tf.ones_like(mask) batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1]) b = one_like_mask * batch_range y = mask // (output_shape[2] * output_shape[3]) x = mask % (output_shape[2] * output_shape[3]) // output_shape[3] feature_range = tf.range(output_shape[3], dtype=tf.int64) f = one_like_mask * feature_range #转置索引 updates_size = tf.size(net) indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size])) values = tf.reshape(net, [updates_size]) ret = tf.scatter_nd(indices, values, output_shape) return ret 上面代码的思路是找到mask对应的索引，将max的值填到指定地方。接着直接调用上面代码的函数： 1234img2=unpool(encode,mask,2) with tf.Session() as sess: result=sess.run(img2) print ("reslut:\n",result) 反最大池化整体代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import tensorflow as tfdef max_pool_with_argmax(net, stride): _, mask = tf.nn.max_pool_with_argmax( net,ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1],padding='SAME') mask = tf.stop_gradient(mask) net = tf.nn.max_pool(net, ksize=[1, stride, stride, 1],strides=[1, stride, stride, 1], padding='SAME') return net, maskimg=tf.constant([ [[0.0,4.0],[0.0,4.0],[0.0,4.0],[0.0,4.0]], [[1.0,5.0],[1.0,5.0],[1.0,5.0],[1.0,5.0]], [[2.0,6.0],[2.0,6.0],[2.0,6.0],[2.0,6.0]], [[3.0,7.0],[3.0,7.0], [3.0,7.0],[3.0,7.0]] ]) def unpool(net, mask, stride): ksize = [1, stride, stride, 1] input_shape = net.get_shape().as_list() #计算new shape output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]) #计算索引 one_like_mask = tf.ones_like(mask) batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1]) b = one_like_mask * batch_range y = mask // (output_shape[2] * output_shape[3]) x = mask % (output_shape[2] * output_shape[3]) // output_shape[3] feature_range = tf.range(output_shape[3], dtype=tf.int64) f = one_like_mask * feature_range #转置索引 updates_size = tf.size(net) indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size])) values = tf.reshape(net, [updates_size]) ret = tf.scatter_nd(indices, values, output_shape) return retimg=tf.reshape(img,[1,4,4,2])pooling=tf.nn.max_pool(img,[1,2,2,1],[1,2,2,1],padding="SAME")encode,mask=max_pool_with_argmax(img,2)img2=unpool(encode,mask,2)with tf.Session() as sess: print("image:",sess.run(img)) print("pooling:",sess.run(pooling)) print("encode",sess.run([encode,mask])) result,mask2=sess.run([encode, mask]) print ("encode:\n",result,mask2) result=sess.run(img2) print ("reslut:\n",result)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow中的队列]]></title>
    <url>%2F2020%2F01%2F10%2FTensorflow%E4%B8%AD%E7%9A%84%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[Tensorflow提供了一个队列机制，通过多线程将读取数据与计算数据分开，因为在处理海量数据集的训练时，无法把数据集一次全部载入内存中，需要以便从硬盘中读取，一边训练计算。 队列（queue）启动线程1tf.train.start_queue_runners() 那什么时候程序会进入挂起状态呢？ 源于上面第四行代码，意思是我们要从队列中拿出指定批次的数据，但是队列里没有数据，所以程序会进入挂起状态 在session内部的退出机制如果把session部分改为with语法： 再次运行程序后，程序虽然能够正常运行，但是结束后会报错。原因是with语法的session是自动关闭的， 当运行结束后session自动关闭的同时会把里面所有的操作都关掉， 而此时的队列还在等待另一个进程往里写数据， 所以就会报错。 解决方法：使用sess=tf.InteractiveSession()实现，或者像第一张图片一样创建 tf.InteractiveSession()和tf.Session()的区别： 使用InteractiveSession()来创建会话，我们要先构建Session()然后定义操作。如果使用Session来创建会话，我们需要在会话之前定义好全部的操作然后再构建会话。 上面的代码在单例程序中没什么问题， 资源会随着程序关闭而整体销毁。 但如果在复杂的代码中， 需要某个线程自动关闭， 而不是依赖进程的结束而销毁， 这种情况下需要使用tf.train.Coordinator函数来创建一个协调器， 以信号量的方式来协调线程间的关系， 完成线程间的同步。 协调器 下面这个例子是先建立一个100大小的队列，主线程使用计数器不停的加1，队列线程把主线程里的计数器放到队列中，当队列为空时，主线程会在sess.run(queue.dequeue())语句位置挂起。当队列线程写入队列中时，主线程的计数器同步开始工作。 1234567891011121314151617181920import tensorflow as tf#创建一个长度为100的队列queue=tf.FIFOQueue(100,"float")c=tf.Variable(0.0) #计数器#c+1.0op=tf.assign_add(c,tf.constant(1.0))#将计数器的结果加入队列enqueue_op=queue.enqueue(c)#创建一个队列管理器queueRunner,用上面这两个操作向queue中添加元素。qr=tf.train.QueueRunner(queue,enqueue_ops=[op,enqueue_op])with tf.Session() as sess: sess.run(tf.global_variables_initializer()) coord=tf.train.Coordinator() #启动入队进程 enqueue_threads=qr.create_threads(sess,coord=coord,start=True) #主线程 for i in range(0,10): print(sess.run(queue.dequeue())) #通知其它线程关闭，其它所有线程关闭后，这一函数才返回 coord.request_stop() 还可以使用coord.join(enqueue_threads)指定等待某个进程结束 为session中的队列加上协调器，只需要将上例的coord放到启动队列中。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN的相关函数]]></title>
    <url>%2F2019%2F11%2F29%2FCNN%E7%9A%84%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[卷积函数tf.nn.conv2d(input,filter,strides,padding,use_cudnn_on_gpu=None,name=None) 除去用以指定该操作名字的name参数，与方法有关的共有5个参数。如下: input： 指需要做卷积的输入图像， 它要求是一个Tensor， 具有[batch， in_height， in_width，in_channels]这样的形状（shape） ， 具体含义是“训练时一个batch的图片数量， 图片高度， 图片宽度， 图像通道数” ， 注意这是一个四维的Tensor， 要求类型为float32和float64其中之一。 filter： 相当于CNN中的卷积核， 它要求是一个Tensor， 具有[filter_height， filter_width，in_channels， out_channels]这样的shape， 具体含义是“卷积核的高度， 滤波器的宽度， 图像通道数， 滤波器个数” ， 要求类型与参数input相同。有一个地方需要注意， 第三维in_channels， 就是参数input的第四维。 strides： 卷积时在图像每一维的步长， 这是一个一维的向量， 长度为4。 padding： 定义元素边框与元素内容之间的空间。 string类型的量， 只能是SAME和VALID其中之一， 这个值决定了不同的卷积方式， padding的值为’VALID’时， 表示边缘不填充， 当其为’SAME’时， 表示填充到滤波器可以到达图像边缘。 use_cudnn_on_gpu： bool类型， 是否使用cudnn加速， 默认为true。 池化函数tf.nn.max_pool(input,ksize,strides,padding,name=None) tf.nn.avg_pool(input,ksize,strides,padding,name=None) 上面这两个函数中的四个参数和卷积参数类似，如下： input：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch,height,width,channels]这样的shape。 ksize：池化窗口的大小，取一个四维向量，一般是[1,height,width,1]，我们不想在batch和channels上做池化，所以这两个维度设为1 strides：步长，一般也是[1,strides,strides,1] padding：和卷积一样，VALID是不进行padding操作，SAME是padding操作]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow中如何预防过拟合]]></title>
    <url>%2F2019%2F11%2F26%2Ftensorflow%E4%B8%AD%E5%A6%82%E4%BD%95%E9%A2%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88%2F</url>
    <content type="text"><![CDATA[方法概述避免过拟合的方法有很多，常用的方法有early stopping、数据集扩增、正则化、dropout。下面就概述一下，具体请参照 优化算法(1)/#more](https://brickexperts.github.io/2019/10/11/优化算法(2)/#more) early stoping：在发生过拟合之前提前结束。理论上是可以的，但是这个点不好把握。 数据集扩增：就是让模型见到更多的情况，可以最大化地满足全样本，但实际应用中对于未来事件的预测不理想。 正则化：通过映入范数，增强模型的泛化能力。包括L1、L2. dropout：是网络模型中的一种方法。每次训练时舍去一些节点来增强泛化能力。 下面我们来具体看看如何实现后两种方法。 正则化所谓的正则化， 其实就是在神经网络计算损失值的过程中， 在损失后面再加一项。 这样损失值所代表的输出与标准结果间的误差就会受到干扰， 导致学习参数w和b无法按照目标方向来调整， 实现模型无法与样本完全拟合的结果， 从而达到防止过拟合的效果 这个干扰项一定要有下面这样的特性： 1、当欠拟合时，希望它对模型误差的影响越小越好，以便让模型快速拟合实际 2、当过拟合时，希望他对模型误差的影响越大越好，以便让模型不要产生过拟合的情况。 由上面两个特性，引入两个范数：L1和L2 L1：所有学习参数w的绝对值的和。 L2：所有学习参数w的平方和然后求平方根 上图中的第一个式子是L1范式，另一个就是L2范式。loss为等式左边的结果，less(0)代表真实的loss值，less(0)后面的那一项就代表正则化，&lambda;为一个可以调整的参数，用来控制正则化对loss的影响。对于L2，将其乘以1/2是为了反向传播时对其求导可以将数据规整。 tensorflow中的正则化L2的正则化函数为: tf.nn.l2_loss(t,name=None) L1的正则化函数在tensorflow是没有自己组装的，可以自己写： tf.reduce_sum(tf.abs(w)) 通过正则化改善过拟合使用正则化非常简单，只需要在计算损失值时加上loss的正则化。 12reg=0.01loss=tf.reduce_mean((y_pred-y)**2)+tf.nn.l2_loss(weights['h1'])*reg+tf.nn.l2_loss(weight['h2'])*reg dropout还有一种防止过拟合的方法是dropout。这个方法的做法是：在训练过程中，每次随机选择一部分节点不要去“学习” 因为从样本数据的分析来看，数据本身是不可能很纯净的，也就是任何一个模型不能100%把数据完全分开，在某一类中一定会有一些异常数据，过拟合的问题恰恰是把这些异常数据当初规律来学习了。我们希望把异常数据过滤掉，只关心有用的规律数据。 异常数据的特点是，它与主流样本中的规律都不同，但是量非常少，相当于在一个样本中出现的概率比主流数据出现的概率低很多。我们可以利用这一点，通过在每次模型中忽略一些节点的数据学习，将小概率的异常数据获得学习的机会降低，这样这些异常数据对模型的影响就会更小了。 注意：由于dropout让一部分节点不去“学习”，所以在增加模型的泛化能力的同时，会使学习速度降低，使模型不太容易学成。所以在使用的过程中需要合理地调节到底丢弃多少节点，并不是丢弃的节点越多越好。 tensorflow中的dropouttf.nn.dropout(x,keep_prob,noise_shape=None,seed=None,name=None) x： 输入的模型节点。keep_prob： 保持率。 如果为1， 则代表全部进行学习； 如果为0.8， 则代表丢弃20%的节点， 只让80%的节点参与学习。noise_shape： 代表指定x中， 哪些维度可以使用dropout技术。 为None时， 表示所有维度都使用dropout技术。 也可以将某个维度标志为1， 来代表该维度使用dropout技术。 例如： x的形状为[n， len， w， ch]， 使用noise_shape为[n， 1， 1，ch]， 这表明会对x中的第二维度len和第三维度w进行dropout。seed： 随机选取节点的过程中随机数的种子值。 全连接网络的深浅关系在实际中，如果想使用浅层神经网络拟合复杂非线性函数，就需要靠增加的神经元个数来实现。神经元过多意味着需要训练的参数过多，这会增加网络的学习难度，并影响网络的泛化能力。因此，在增加网络结构时，一般倾向于使用更多的模型，来减少网络中所需要神经元的数量，使网络有更好的泛化能力。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy(2)]]></title>
    <url>%2F2019%2F11%2F24%2Fnumpy(2)%2F</url>
    <content type="text"><![CDATA[np.concatenate数组拼接函数，concatenate((a1,a2,……),axis=0) 参数a1，a2……为要拼接的数组，axis为在哪个维度上进行拼接。默认为0 >&gt;&gt;import numpy as np>&gt;&gt;a=np.array([[1,2],[3,4]])>&gt;&gt;b=np.array([[5,6]]) >&gt;&gt;np.concatenate((a,b),axis=0)array([[1, 2], [3, 4], [5, 6]]) >&gt;&gt;np.concatenate((a,b.T),axis=1)array([[1, 2, 5], [3, 4, 6]]) np.eye生成对角矩阵 numpy.eye(N,M=None, k=0, dtype=) 第一个参数：输出方阵（行数=列数）的规模，即行数或列数 第三个参数：默认情况下输出的是对角线全“1”，其余全“0”的方阵，如果k为正整数，则在右上方第k条对角线全“1”其余全“0”，k为负整数则在左下方第k条对角线全“1”其余全“0”。 >&gt;&gt;import numpy as np>&gt;&gt;np.eye(1,3,dtype=int)array([[1, 0, 0]])>&gt;&gt;np.eye(2,3,dtype=int)array([[1, 0, 0], [0, 1, 0]])>&gt;&gt;np.eye(2,2,dtype=int)array([[1, 0], [0, 1]])>&gt;&gt;np.eye(4,4,dtype=int)array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]) >&gt;&gt;np.eye(4,4,k=2,dtype=int)array([[0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0]]) np.random.multivariate_normalmultivariate_normal(mean,cov,size=None,check_valid=None,tol=None)：用于根据实际情况生成一个多元正态分布矩阵 其中mean和cov为必要的传参而size，check_valid以及tol为可选参数。 mean：mean是多维分布的均值维度为1； cov：协方差矩阵，注意：协方差矩阵必须是对称的且需为半正定矩阵； size：指定生成的正态分布矩阵的维度（例：若size=(1, 1, 2)，则输出的矩阵的shape即形状为 1X1X2XN（N为mean的长度））。 check_valid：这个参数用于决定当cov即协方差矩阵不是半正定矩阵时程序的处理方式，它一共有三个值：warn，raise以及ignore。当使用warn作为传入的参数时，如果cov不是半正定的程序会输出警告但仍旧会得到结果；当使用raise作为传入的参数时，如果cov不是半正定的程序会报错且不会计算出结果；当使用ignore时忽略这个问题即无论cov是否为半正定的都会计算出结果。3种情况的console打印结果如下：]]></content>
      <categories>
        <category>python</category>
        <category>数学计算</category>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>python的数学计算模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maxout网络]]></title>
    <url>%2F2019%2F11%2F23%2FMaxout%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[Maxout介绍Maxout网络可以理解为单个神经元的扩展，主要是扩展单个神经元里面的激活函数。Maxout是将激活函数变成一个网络选择器，原理就是将多个神经元并列地放在一起，从它们的输出结果中找到最大的那个，代表对特征响应最敏感，然后取这个神经元的结束参与后面的运算。 下图是单个神经元和Maxout网络的区别： Maxout的公式可以理解为： 这个的做法就是相当于同时使用多个神经元放在一起， 哪个有效果就用哪个。 所以这样的网络会有更好的拟合效果。 Maxout网络实现MNIST分类Maxout网络的构建方法：通过reduce_max函数对多个神经元的输出来计算Max值，将Max值当作输入按照神经元正反传播方向进行计算 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/")print ('输入数据:',mnist.train.images)print ('输入数据打shape:',mnist.train.images.shape)import pylab im = mnist.train.images[1]im = im.reshape(-1,28)pylab.imshow(im)pylab.show()print ('输入数据打shape:',mnist.test.images.shape)print ('输入数据打shape:',mnist.validation.images.shape)import tensorflow as tf #导入tensorflow库tf.reset_default_graph()# tf Graph Inputx = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784y = tf.placeholder(tf.int32, [None]) # 0-9 数字=&gt; 10 classes# Set model weightsW = tf.Variable(tf.random_normal([784, 10]))b = tf.Variable(tf.zeros([10]))z= tf.matmul(x, W) + bcost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=z))maxout=tf.reduce_max(z,axis=1,keep_dims=True)W2=tf.Variable(tf.truncated_normal([1,10],stddev=0.1))b2=tf.Variable(tf.zeros([1]))pred=tf.nn.softmax(tf.matmul(maxout,W2)+b2)learning_rate = 0.04# 使用梯度下降优化器optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)training_epochs = 200batch_size = 100display_step = 1# 启动sessionwith tf.Session() as sess: sess.run(tf.global_variables_initializer())# Initializing OP # 启动循环开始训练 for epoch in range(training_epochs): avg_cost = 0. total_batch = int(mnist.train.num_examples/batch_size) # 遍历全部数据集 for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # Run optimization op (backprop) and cost op (to get loss value) _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;) # Compute average loss avg_cost += c / total_batch # 显示训练中的详细信息 if (epoch+1) % display_step == 0: print ("Epoch:", '%04d' % (epoch+1), "cost=", "&#123;:.9f&#125;".format(avg_cost)) print( " Finished!") Maxout的拟合功能很强大，但是也会有节点过多，参数过多，训练过慢的缺点。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow的学习率退化和随机初始化]]></title>
    <url>%2F2019%2F11%2F23%2FTensorflow%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%80%80%E5%8C%96%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[退化学习率设置学习率的大小，是在精度和速度之间找到一个平衡。如果学习率的值比较大，则训练速度快，但结果的精度不够。如果学习率的值比较小，精度虽然提升了，但训练会花太多时间。 退化学习率又叫学习率衰减， 它的本意是希望在训练过程中对于学习率大和小的优点都能够为我们所用， 也就是当训练刚开始时使用大的学习率加快速度， 训练到一定程度后使用小的学习率来提高精度， 这时可以使用学习率衰减的方法。 1def exponential_decay(learning_rate,global_step,decay_rate,staircase=False,name=None) 学习率的衰减速度是由global_step和decay_steps来决定的。计算公式如下： decayed_learning_rate=learning_rate*decay_rate^(global_step/decay_step)。staircase值默认为False。当为True，将没有衰减功能，只是使用上面的公式初始化一个学习率的值而已。 1learning_rate=tf.train.exponential_decay(starter_learning_rate,global_step,10000,0.96) 这种方式定义的学习率就是退化学习率， 它的意思是当前迭代到global_step步， 学习率每一步都按照每10万步缩小到0.96%的速度衰退。 通过增大批次处理样本的数量也可以起到退化学习率的效果。但是这种方法要求训练时的最小批次要与实际应用中的最小批次一致。一旦满足该条件，建议优先选择增大批次数量的方法。因为这样会省去一些开发量和训练中的计算量。 123456789101112131415161718import tensorflow as tfglobal_step=tf.Variable(0,trainable=False)#初始学习率为0.1initial_learning_rate=0.1#每十次衰减0.9learning_rate=tf.train.exponential_decay(initial_learning_rate,global_step=global_step, decay_steps=10, decay_rate=0.9)opt=tf.train.GradientDescentOptimizer(learning_rate)#相当于global_step+1add_global=global_step.assign_add(1)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(learning_rate)) for i in range(20): g,rate=sess.run([add_global,learning_rate]) print(g,rate) 注意： 这是一种常用的训练策略， 在训练神经网络时， 通常在训练刚开始时使用较大的learning rate， 随着训练的进行， 会慢慢减小learning rate。 在使用时， 一定要把当前迭代次数global_step传进去， 否则不会有退化的功能。 随机初始化在定义学习参数时，可以通过get_variable和Variable两个方式。在使用get_variable时，tf.get_variable(name,shape,initializer)，当然还有其它参数，可以自己上网找一下。参数initializer就是初始化参数。可以去下表中列出的相关函数 初始化为常量1234567891011import tensorflow as tfvalue = [0, 1, 2, 3, 4, 5, 6, 7]init = tf.constant_initializer(value)x = tf.get_variable('x', shape=[8], initializer=init)with tf.Session() as sess: x.initializer.run() print(x.eval()) #sess.run(tf.global_variables_initializer()) #print(sess.run(x))#输出:#[ 0. 1. 2. 3. 4. 5. 6. 7.] 初始化为正态分布123456789101112import tensorflow as tfinit_random = tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32)init_truncated = tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32) with tf.Session() as sess: x = tf.get_variable('x', shape=[10], initializer=init_random) y = tf.get_variable('y', shape=[10], initializer=init_truncated) x.initializer.run() y.initializer.run() print(x.eval()) print(y.eval()) 初始化为均匀分布1234567891011import tensorflow as tf init_uniform = tf.random_uniform_initializer(minval=0, maxval=10, seed=None, dtype=tf.float32x = tf.get_variable('x', shape=[10], initializer=init_uniform)with tf.Session() as sess: x.initializer.run() print(x.eval()) # 输出结果# [ 6.93343639 9.41196823 5.54009819 1.38017178 1.78720832 5.38881063# 3.39674473 8.12443542 0.62157512 8.36026382]]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow中的损失函数和梯度下降]]></title>
    <url>%2F2019%2F11%2F22%2FTensorflow%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[损失函数均值平方差均值平方差(Mean Squared Error，MSE)，也称”均方误差”，在神经网络中主要是表达预测值与真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的期望值。 均方误差的值越小，表明模型越好。类似的损失算法还有均方误差RMSE(将MSE开平方)，平均绝对值误差MAD(对一个真实值与预测值相减的绝对值取平均值)。 注意：在神经网络计算时，预测值要与真实值控制在同样的数据分布内，假设将预测值经过Sigmoid激活函数得到取值范围在0~1之间，那么真实值也归一化成0~1之间。 交叉熵交叉熵(crossentropy)也是loss算法的一种，一般用于分类问题，表达的意思为预测输入样本属于某一类的概率。其中y代表真实值分类(0或1)，a代表预测值。 交叉熵也只值越小，代表预测结果越准。 注意：这里用于计算的a也是通过分布同一化处理的（或者是经过Sigmoid函数激活的），取值范围0~1。 损失算法的选取损失函数的选取取决于输入标签数据的类型： 如果输入的是实数、 无界的值， 损失函数使用平方差； 如果输入标签是位矢量（分类标志） ， 使用交叉熵会更适合。 Tensorflow的loss函数均值平方差在Tensorflow没有单独的MSE函数，不过由于公式比较简单，往往都是自己写函数。也有多种写法： 123MSE=tf.reduce_mean(tf.pow(tf.sub(logits,outputs),2.0))MSE=tf.reduce_mean(tf.square(tf.sub(logits,outputs)))MSE=tf.reduce_mean(tf.square(logits-outputs)) logits代表标签值，outputs代表预测值。同样也可以组合其它类似loss， 12rmse=tf.sqrt(tf.reduce_mean(tf.pow(tf.sub(logits,outputs),2.0)))mad=tf.reduce_mean(tf.complex_abs(logits,outputs)) 交叉熵在tensorflow中常见的交叉熵函数有：Sigmoid交叉熵、softmax交叉熵、Sparse交叉熵、加权Sigmoid交叉熵 当然，我们也可以像MSE那样使用自己组合的公式计算交叉熵。对于softmax后的结果logits我们可以对其使用公式-tf.reduce_sum(labels*tf.log(logis),1)，就等同于softmax_cross_entropy_with_logits得到结果。（注意有个负号） 实验softmax交叉熵标签是one-hot编码。 1234567891011121314151617181920import tensorflow as tflabels=[[0,0,1],[0,1,0],]logits=[[2,0.5,6],[0.1,0,3]]#进行第一次softmaxlogits_scaled=tf.nn.softmax(logits)#进行第二次softmaxlogits_scaled2=tf.nn.softmax(logits_scaled)#用第一次的softmax进行交叉熵计算result1=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)#用第二次的softmax进行交叉熵计算result2=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits_scaled)#用自建公式实验result3=-tf.reduce_sum(labels*tf.log(logits_scaled),1)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("logits_scaled：",sess.run(logits_scaled)) print("logits_scaled2",sess.run(logits_scaled2)) print("result1：",sess.run(result1)) print("result2：",sess.run(result2)) print("result3：",sess.run(result3)) 从结果看，logits里面的值原本加和都是大于1的，但是经过softmax之后，总和变成了1。logits中的第一个是跟标签分类相符的，第二个与标签分类不符，所以第一个的交叉熵比较小，是0.02215518。第二个交叉熵比较大，是3.09967351。 总结： 比较scaled和scaled2可以看到： 经过第二次的softmax后， 分布概率会有变化， 而scaled才是我们真实转化的softmax值。 比较rel1和rel2可以看到： 传入softmax_cross_entropy_with_logits的logits是不需要进行softmax的。 如果将softmax后的值scaled传入softmax_cross_entropy_with_ logits就相当于进行了两次的softmax转换。 对于已经用softmax转换过的scaled，在计算loss的时候不能再使用softmax_cross_entropy_with_logits了。应该自己写一个函数，如上面代码的result3。 下面用一组总和为1但是数组中每个值都不等于0或1的数组来代替标签。 123456labels2=[[0.4,0.1,0.5],[0.3,0.6,0.1]]result4=tf.nn.softmax_cross_entropy_with_logits(labels=labels2,logits=logits)with tf.Session() as sess: print("result4：",result4)#输出结果result4 [2.1721554 2.7696736] 与前面的result1对比发现，标准的one-hot的结果比较明显。 sparse交叉熵使用sparse_softmax_cross_entropy_with_logits函数的用法，他需要使用非one-hot的标签，所以要把前面的标签换成具体的数值[2,1]。PS：这个labels能不能换成[1,2] 123456labels3=[2,1]#表明labels共有3个类，0、1、2。[2,1]等价于one-hot编码的001与010result5=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels3,logits=logits)with tf.Session() as sess: print("result5：",result5)#输出结果result5 [0.02215516 3.0996735 ] result5与result1完全一样。 计算loss值对于softmax_cross_entropy_with_logits后的结果求loss直接取均值。 12345loss=tf.reduce_mean(result1)with tf.Session() as sess: print("loss",sess.run(loss))#输出结果#loss 1.5609143 对于softmax后的结果，先使用-tf.reduce_sum(labels*tf.log(logits_scaled))，接着求均值。 12345loss2=tf.reduce_mean(-tf.reduce_sum(labels*tf.log(logits_scaled),1))with tf.Session() as sess: print("loss2:",loss2)#输出结果#loss 1.5609144 梯度下降梯度下降法是一个最优化算法， 通常也称为最速下降法， 常用于机器学习和人工智能中递归性地逼近最小偏差模型， 梯度下降的方向也就是用负梯度方向为搜索方向， 沿着梯度下降的方向求解极小值。 在训练过程中，每次的正向传播后都会得到输出值与真实值的损失值。这个损失值越小越好，代表模型越好。于是梯度下降的算法就用在这里，帮助寻找最小的那个损失值，从而可以反推出对应的学习参数b和w，达到优化模型的效果。 常用的梯度下降方法可以分为：批量梯度下降、随机梯度下降、小批量梯度下降。 批量梯度下降： 遍历全部数据集算一次损失函数， 然后算函数对各个参数的梯度和更新梯度。 这种方法每更新一次参数， 都要把数据集里的所有样本看一遍， 计算量大， 计算速度慢， 不支持在线学习，称为batch gradient descent。 随机梯度下降：每看一个数据就算一下损失函数，然后求梯度更新参数。 这称为stochastic gradient descent， 随机梯度下降。 这个方法速度比较快， 但是收敛性能不太好， 可能在最优点附近晃来晃去， 命中不到最优点。 两次参数的更新也有可能互相抵消， 造成目标函数震荡比较剧烈 小批量梯度下降：为了克服上面两种方法的缺点， 一般采用一种折中手段——小批的梯度下降。 这种方法把数据分为若干个批， 按批来更新参数， 这样一批中的一组数据共同决定了本次梯度的方向， 下降起来就不容易跑偏， 减少了随机性。 另一方面因为批量的样本数与整个数据集相比小了很多， 计算量也不是很大。 tensorflow中的梯度下降函数在tensorflow中是通过一个叫做Optimizer的优化器进行训练优化的。对于不同的优化器，在tensorflow会有不同的类： 在训练过程中，先实例化一个优化函数，并基于一定的学习率进行梯度优化训练： 1optimizer=tf.train.GradientDescentOptimizer(learning_rate) 接着使用minimize()操作，接着传入损失值loss到这个操作。优化器就会按照循环的次数一次次沿着loss最小值的方向优化参数。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow中的激活函数和分类函数]]></title>
    <url>%2F2019%2F11%2F22%2FTensorflow%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%92%8C%E5%88%86%E7%B1%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[关于激活函数，我已经在一篇博客上讲解了它的常见种类和作用，详情点击激活函数)。这篇博客一起来看下在tensorflow下的激活函数，并补充一些激活函数。顺提一下分类函数。 激活函数激活函数的作用就是用来加入非线性因素的，以解决线性模型表达能力不足的缺陷。常用的激活函数有sigmoid，tanh，relu。 Sigmoidsigmoid在tensorflow下的对应函数为： tf.nn.sigmoid(x,name=None)。从sigmoid的图像来看，随着x趋近正负无穷大，y对应的值越来越接近1或-1，这种情况叫做饱和。处于饱和态的激活函数意味着，当x=100和x=1000时的反映都是一样的，这样的特性转换相当于将1000大于100十倍这个信息丢失了。 Tanhtanh在tensorflow下的对应函数为： tf.nn.tanh(x,name=None)。 x取值也是从正无穷到负无穷， 但其对应的y值变为-1～1之间， 相对于Sigmoid函数有更广的值域。 但同样也拥有饱和问题。 Relurelu在tensorflow下的对应函数为： tf.nn.relu(x,name=None)。该函数非常简单，大于0的留下，否则一律为0。relu函数运算简单，大大提升了机器的运行效率。还有tf.nn.relu6(x,name=None)，这是以6为阈值的relu函数。与relu函数类似的还有softplus函数，二者的区别是：Softplus函数会更加平滑，但是计算量很大。 softplus的函数公式：f(x)=ln(1+ex)。在tensorflow中，Softplus函数对应的函数是tf.nn.softplus(x,name=None) 虽然ReLU函数在信号响应上有很多优势， 但这仅仅在正向传播方面。 由于其对负值的全部舍去， 因此很容易使模型输出全零从而无法再进行训练。 例如， 随机初始化的w加入值中有个值是负值， 其对应的正值输入值特征也就被全部屏蔽了， 同理， 对应负值输入值反而被激活了。 这显然不是我们想要的结果。 于是在基于ReLU的基础上又演化出了一些变种函数， 举例如下： Noise relus：为max中的x加了一个高斯分布的噪声 Leaky relus：在relu的基础上，保留一部分负值，让x为负时乘a，a小于等于1。也就是Leaky relus对负信号不是一昧的拒绝，而是缩小。 在TensorFlow中， Leaky relus公式没有专门的函数， 不过可以利用现有函数组成而得到： tf.maximum(x,leak*x,name=name) #leakl为传入的参数，可以设为0.01等。 Elus：当x小于0时，做了更复杂的变换。 在tensorflow中，Elus函数对应的函数，tf.nn.elu(x,name=None) SwishSwish函数是谷歌公司发现的一个效果更优于Relu的激活函数。 其中&beta;为x的缩放参数，一般情况默认为1即可。在tensorflow的低版本中，没有单独的Swish函数，可以手动封装。 12def Swish(x,beta=1): return x*tf.nn.sigmoid(x*beta) 分类算法对于上面讲的激活函数，其输出值只有两种（0、1，或-1、1，0、x），而现实生活中需要对某一问题进行某种分类，这时就需要使用softmax算法。 softmaxsoftmax， 就是如果判断输入属于某一个类的概率大于属于其他类的概率， 那么这个类对应的值就逼近于1， 其他类的值就逼近于0。 该算法的主要应用就是多分类， 而且是互斥的， 即只能属于其中的一个类。 与sigmoid类的激活函数不同的是， 一般的激活函数只能分两类，所以可以理解成Softmax是Sigmoid类的激活函数的扩展。 把所有值用e的n次方计算出来， 求和后算每个值占的比率， 保证总和为1。一般就可以认为softmax得出的就是概率。 举个例子， 训练的模型可能推测一张包含9的图片代表数字9的概率是80%， 但是判断它是8的概率是5%（因为8和9都有上半部分相似的小圆） ，判断它代表其他数字的概率值更小。 于是取最大 概率的对应数值， 就是这个图片的分类了。 这是一个softmax回归。 常用的分类函数]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[识别图中模糊的手写数字]]></title>
    <url>%2F2019%2F11%2F21%2F%E8%AF%86%E5%88%AB%E5%9B%BE%E4%B8%AD%E6%A8%A1%E7%B3%8A%E7%9A%84%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[MNIST是一个入门级的计算机视觉数据集。MNIST数据集的官网是http://yann.lecun.com/exdb/mnist/，我们可以手动下载数据集。 下载数据集除了上面的手动下载数据集，tensorflow提供了一个库，可以直接用来自动下载MNIST。代码如下： 12from tensorflow.examples.tutorials.mnist import input_datamnist=input_data.read_data_sets("MNIST_data/",one_hot=True) 运行上面的代码，会自动下载数据集并将文件解压到当前代码所在同级目录下的MNIST_data文件夹下。 注意：代码中的one_hot=True，表示将样本标签转化为one_hot编码。解释one_hot编码，假如一共10类。0的one_hot为1000000000，1的one_hot编码为0100000000，2的one_hot编码为0010000000，等等。只有一个位是1，1所在的位置就代表第几类，从零开始数。 MNIST数据集中的图片是28x28Pixel，所以，每一幅图就是1行784列的数据，每一个值代表一个像素。 如果是黑白的图片，图片中的黑色的地方数值为0，有图案的地方数值为0~255之间的数字，代表其颜色的深度。 如果是彩色的图片，一个像素由三个值表示RGB（红，黄，蓝）。 显示数据集信息1234567print("输入数据为：",mnist.train.images)print("输入数据打印shape：",mnist.train.images.shape)import pylabim=mnist.train.images[1]im=im.reshape(-1,28)pylab.imshow(im)pylab.show() 运行代码得出结果： 这是一个55000行、784列的矩阵，即：这个数据集有55000张图片。 MNIST数据集组成在MNIST训练数据集中，mnist.train.images是一个形状为[55000,784]的张量。其中，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0~255之间。 MNIST数据集里包含三个数据集：训练集、测试集、验证集。训练集用于训练，测试集用于评估训练过程中的准确度，验证集用于评估最终模型的准确度。可使用以下命令查看里面的数据信息 12print("测试集的shape：",mnist.test.images.shape)print("验证集的shape：",mnist.validation.images.shape) 运行完上面代码，可以发现在测试数据集里有10000条样本图片，验证集有5000个图片。 三个数据集还有分别对应的三个标签文件，用来标注每个图片上的数字是几。把图片和标签放在一起，称为“样本”。 MNIST数据集的标签是介于0～9之间的数字， 用来描述给定图片里表示的数字。标签数据是“one-hot vectors”： 一个one-hot向量，除了某一位的数字是1外， 其余各维度数字都是0。 例如， 标签0将表示为（[1， 0， 0， 0， 0， 0，0， 0， 0， 0， 0]） 。 因此， mnist.train.labels是一个[55000， 10]的数字矩阵。 定义变量12345678import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist=input_data.read_data_sets("MNIST_data/",one_hot=True)import pylabtf.reset_default_graph()x=tf.placeholder(tf.float32,[None,784])#标签y=tf.placeholder(tf.float32,[None,10]) 代码中的None，代表此张量的第一个维度可以是任意长度的。 构建模型定义学习参数模型也需要权重值和偏置值，它们被统一称为学习参数。在Tensorflow，使用Variable来定义学习参数。 12W=tf.Variable(tf.random_normal([784,10]))b=tf.Variable(tf.zeros([10])) 定义正向传播1pred=tf.nn.softmax(tf.matual(x,W)+b) 定义反向传播123456#损失函数cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=1))#定义超参数learning_rate=0.01#梯度下降optimizer=tf.train.GradientDescentOptimizer(learn_rate).minimize(cost) 首先，将正向传播生成的pred与样本标签y进行一次交叉熵的运算，然后取平均值。接着将cost作为一次正向传播的误差，通过梯度下降的优化方法找到能够使这个误差最小化的W和b。整个过程就是不断让损失值变小。因为损失值越小，才能表明输出的结果跟标签数据越接近。 训练模型12345678910111213141516171819#要把整个训练样本集迭代25次train_epochs=25#代表在训练过程中一次取100条数据进行训练batch_size=100#每训练一次就把具体的中间状态显示出来display_step=1with tf.Session() as sess: #初始化op sess.run(tf.global_variables_initializer()) for epoch in range(train_epochs): avg_cost=0 total_batch=int(mnist.train.num_examples/batch_size) for i in range(total_batch): batch_xs,batch_ys=mnist.train.next_batch(batch_size) _,c=sess.run([optimizer,cost],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;) avg_cost+=c/total_batch if (epoch+1)%display_step==0: print("Epoch:","%04d" % (epoch+1),"cost=","&#123;:.9f&#125;".format(avg_cost)) print("~~~~finish~~~~") 测试模型测试错误率的算法是：直接判断预测的结果与真实的标签是否相同，如是相同的就表示是正确的，如是不相同的，就表示是错误的。然后将正确的个数除以总个数，得到的值即为正确率。由于是one-hot编码，这里使用了tf.argmax函数返回one-hot编码中数值为1的哪个元素的下标。 12345# 测试 modelcorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))# 计算准确率accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))print ("Accuracy:", accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;)) 保存模型在代码的两处区域加入以下代码： 读取模型123456789101112131415161718192021222324252627282930print("开始第二个会话")print("Starting 2nd session...")with tf.Session() as sess: # Initialize variables sess.run(tf.global_variables_initializer()) # Restore model weights from previously saved model saver.restore(sess, model_path) # 测试 model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # 计算准确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print ("Accuracy:", accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;)) output = tf.argmax(pred, 1) batch_xs, batch_ys = mnist.train.next_batch(2) outputval,predv = sess.run([output,pred], feed_dict=&#123;x: batch_xs&#125;) print(outputval,predv,batch_ys) im = batch_xs[0] im = im.reshape(-1,28) #pylab.imshow(im) #pylab.show() plt.imshow(im) plt.show() im = batch_xs[1] im = im.reshape(-1,28) pylab.imshow(im) pylab.show() 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport matplotlib.pyplot as pltmnist=input_data.read_data_sets("MNIST_data/",one_hot=True)import pylabtf.reset_default_graph()x=tf.placeholder(tf.float32,[None,784])y=tf.placeholder(tf.float32,[None,10])W=tf.Variable(tf.random_normal([784,10]))b=tf.Variable(tf.zeros([10]))#正向传播pred=tf.nn.softmax(tf.matmul(x,W)+b)#反向传播cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),))learning_rate=0.001#梯度下降optimizer=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)#模型保存saver=tf.train.Saver()model_path="./model.ckpt"train_epochs=50batch_size=100display_step=1with tf.Session() as sess: #初始化op sess.run(tf.global_variables_initializer()) for epoch in range(train_epochs): avg_cost=0 total_batch=int(mnist.train.num_examples/batch_size) for i in range(total_batch): batch_xs,batch_ys=mnist.train.next_batch(batch_size) #c代表每一个batch_size总的损失 _,c=sess.run([optimizer,cost],feed_dict=&#123;x:batch_xs,y:batch_ys&#125;) avg_cost+=c/total_batch if (epoch+1)%display_step==0: print("Epoch:","%04d" % (epoch+1),"cost=","&#123;:.9f&#125;".format(avg_cost)) print("~~~~finish~~~~") # 测试 model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # 计算准确率，cast函数用于类型转换 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print ("Accuracy:", accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;)) #模型保存 save_path = saver.save(sess, model_path) print("Model saved in file: %s" % save_path)print("开始第二个会话")print("Starting 2nd session...")with tf.Session() as sess: # Initialize variables sess.run(tf.global_variables_initializer()) # Restore model weights from previously saved model saver.restore(sess, model_path) # 测试 model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # 计算准确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print ("Accuracy:", accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;)) output = tf.argmax(pred, 1) batch_xs, batch_ys = mnist.train.next_batch(2) outputval,predv = sess.run([output,pred], feed_dict=&#123;x: batch_xs&#125;) print(outputval,predv,batch_ys) im = batch_xs[0] im = im.reshape(-1,28) #pylab.imshow(im) #pylab.show() plt.imshow(im) plt.show() im = batch_xs[1] im = im.reshape(-1,28) pylab.imshow(im) pylab.show() 第70、71行的代码可以用68、69行的代码代替。pylab库结合了pyplot模块和numpy模块。 把模糊数字换成我们自己的图片研究以下，补。。。。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>识别数字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow的eval用法]]></title>
    <url>%2F2019%2F11%2F20%2Ftensorflow%E7%9A%84eval%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[eval()其实就是tf.Tensor的session.run()的另一种写法。 1、eval()也是启动计算的一种方式。基于tensorflow基本原理，首先需要定义图，然后计算图，其中计算图的函数有常见的run()函数，如sess.run()，eval()也是类似。 2、eval()只能用于tf.tensor类对象，也就是有输出的operaton。没有输出的operation，使用session.run()。t.eval() 等价于 tf.get_default_session().run(t)]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下安装qt5]]></title>
    <url>%2F2019%2F11%2F11%2Fubuntu%E4%B8%8B%E5%AE%89%E8%A3%85qt5%2F</url>
    <content type="text"><![CDATA[在官网下载相关文件，官网 我下载的是qt5.11.1版本。点击进入， 下载安装程序时需注意，只能下载qt-opensource-linux-x64-5.11.1.run，因为只有这个是在linux环境下的安装程序。新建一个名为Qt5.11.1的文件夹，将这个文件放进去。 在文件目录下打开终端，输入./run进行安装。接着点击next。可能会有要求填邮箱。点击skip跳过输入邮箱步骤。接下来更改程序安装目录，按照自己的需求修改。之后在select components步骤时，建议Select All。在License Agreement步骤时，选择Qt Installer LGPL Agreement。接着等待。进入安装目录，发现如下内容即为成功安装。 此时安装完后，仍就无法运行。我们需要安装相应的工具，以使得程序正常运行。 sudo apt-get install gcc g++ sudo apt-get install libqt4-dev sudo apt-get install build-essential 以上内容全部安装完毕，进入目录下的Tools/Qtcreator/bin目录下，在终端输入./qtcreator 接下来的使用就要靠各位读者自己摸索和学习了]]></content>
      <categories>
        <category>Linux</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用darknet训练自己的数据]]></title>
    <url>%2F2019%2F11%2F09%2F%E7%94%A8darknet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[本篇博客采用darknet训练自己的数据，那么在训练自己的数据之前，我们得先拥有自己数据，怎么得到呢?只能自己做了 安装labelImg我用过精灵标注助手和labelImg两款标注工具，标注后得出得XML不一样。本篇博客采用labelImg工具标注图片。 环境：python3、ubuntu18.04 sudo apt-get install pyqt5-dev-toolssudo pip3 install lxml 下载labelImg源码 git clone https://github.com/tzutalin/labelImg.git 进入labelImg目录下 cd labelImg 再make qt5py3，建议不要make all。出现下面这种结果即为成功 然后python3 labelImg.py。出现界面即为成功。woc，这是我最顺利的一次。 制作自己的数据集首先进入darknet目录下，再目录下新建文件夹VOC2019，并在VOC2019下新建Annotations，ImageSets，JPEGImages三个文件夹。在ImageSets新建Main文件夹。 将自己的数据集图片放到JPEGImages目录下，将标注文件放到Annotations目录下。接着开始标注数据。过程就随便说以下。[Open Dir]或Ctrl+u选择要标注的图片所在的根目录，[CreateRectBox]或w开始标注，鼠标框选目标区域后选择对应的标签类别,按空格或Ctrl+s保存，[Next Image]或d切换到下一张图片，标注错误的选框可选中后按[Delete]删除。要注意的是，如果不是使用原有的目标检测物体的类别，我们要打开data/predefined_classes.txt，修改默认类别为要检测的类别。 接着再VOC2019下新建test.py文件，将以下代码拷贝进去。在ImageSets的Maxin文件夹下将生成四个文件：train.txt，val.txt，test.txt，trainval.txt。 1234567891011121314151617181920212223242526272829303132333435import randomimport ostrainval_percent = 0.1train_percent = 0.9xmlfilepath = 'Annotations'txtsavepath = 'ImageSets\Main'total_xml = os.listdir(xmlfilepath)num = len(total_xml)list = range(num)tv = int(num * trainval_percent)tr = int(tv * train_percent)trainval = random.sample(list, tv)train = random.sample(trainval, tr)ftrainval = open('ImageSets/Main/trainval.txt', 'w')ftest = open('ImageSets/Main/test.txt', 'w')ftrain = open('ImageSets/Main/train.txt', 'w')fval = open('ImageSets/Main/val.txt', 'w')for i in list: name = total_xml[i][:-4] + '\n' if i in trainval: ftrainval.write(name) if i in train: ftest.write(name) else: fval.write(name) else: ftrain.write(name)ftrainval.close()ftrain.close()fval.close()ftest.close() YOLOV3的label标注的一行五个数分别代表类别（从 0 开始编号）， BoundingBox 中心 X 坐标，中心 Y 坐标，宽，高。这些坐标都是 0～1 的相对坐标。和我们刚才标注的label不同，因此我们需要下面的py文件帮我们转换label。 wget https://pjreddie.com/media/files/voc_label.py 也可以在windows下好了拷到ubuntu下。总之把这个文件放到darknet文件夹下。打开voc_label.py文件，修改sets和classes。sets如下，classes根据自己的类别需要修改。 打开终端输入python voc_label.py，于是在当前目录生成三个txt文件2019_train.txt，2019_val.txt，2019_test.txt。在VOCdevkit文件夹下的VOC2019也会多生成一个文件夹labels。点开里面的文件就会发现以及转化成YOLOv3需要的格式了。数据集的制作完成，bingo！！！ 局部修改1、打开darknet下的cfg文件夹，修改voc.data。 根据自己的需要修改classes类别个数，train和valid的地址。names和backup不用修改。 2、修改data/voc.names和coco.names。打开对应的文件发现都是原本数据集里的类别，改成自己需求的类别就行。 3、修改参数文件cfg/yolov3-voc.cfg，用ctrl+f搜 yolo, 总共会搜出3个含有yolo的地方。每个地方都必须要改2处， filters：3*（5+len（classes））和classes类别数。 可修改：random，原本是1，显存小改为0。（是否要多尺度输出。） 报错&amp;训练首先下载darknet53的预训练模型： wget https://pjreddie.com/media/files/darknet53.conv.74 开始训练： ./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74 你以为就这样结束了吗？我就知道没怎么简单。又报错了，报错信息如下。 检查文件和路径，完全正确。 上网找了解决方案，如下 下载一个notepad++，打开文件。 选择 视图 -&gt; 显示符号 -&gt; 显示所有符号； 选择 编辑 -&gt; 文档格式转换 -&gt; 转换为UNIX（LF）格式； 转换完成后的格式如下： 注： 1.注意检查最后一行是否有LF标志。 2.为保证不出错，将所有训练过程中使用到的相关文件都修改。 我使用了上面的方法，发现我的文件格式本来就是对的。不需要改。那是什么问题呢？后来和一位同学一起瞎改cfg目录下voc.data文件，将train和valid的路径改成如下这样： 才开始运行。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>darknet</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下安装darknet]]></title>
    <url>%2F2019%2F11%2F09%2Fubuntu%E4%B8%8B%E5%AE%89%E8%A3%85darknet%2F</url>
    <content type="text"><![CDATA[首先下载源码 12git clone https://github.com/pjreddie/darknet.gitcd darknet 进入darknet目录后，打开Makefile。用到那个将对应的0改为1。 比如我没有GPU，就不用修改GPU为1。但我用到了opencv，将opencv的0改为1。 编译源码 1make 测试是否安装成功 1./darknet 此时看到如下信息即为安装成功。 我在执行这步的时候报错了，报错信息如下： 解决方案如下，在终端执行下面命令： 12sudo /bin/bash -c 'echo "/usr/local/lib" &gt; /etc/ld.so.conf.d/opencv.conf'sudo ldconfig 安装成功后，可以先下载预训练模型测试效果。 12 wget https://pjreddie.com/media/files/yolov3.weights ./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg 我在这里又报错了，报错信息如下： 解决方案：sudo apt-get install libcanberra-gtk-module 再运行一次 可以看到YOLO的detection图。到这里，YOLOV3已经走通了，是时候加入自己的数据了。 请看下回分解。。。。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>darknet</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下编译安装opencv]]></title>
    <url>%2F2019%2F11%2F09%2Fubuntu%E4%B8%8B%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85opencv%2F</url>
    <content type="text"><![CDATA[本篇博客使用的ubuntu版本是18.04和opencv3.2 首先安装CMAKE sudo apt-get install cmake 接着安装一些依赖项 sudo apt-get install build-essential pkg-config 安装视频I/O包： sudo apt-get install libgtk2.0-dev libavcodec-dev libavformat-dev libswscale-dev 安装gtk2.0： sudo apt-get install libgtk2.0-dev 安装常用图像工具包： sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev libdc1394-22-dev 如果没有报错，那是最好的。但是我报错了。报错信息如下： E: Package &#39;libpng12-dev&#39; has no installation candidateE: Unable to locate package libjasper-dev 上网找了解决方案，首先先解决第一个错： libpng12-dev在Ubuntu16.04之后就被丢弃了，所以放弃用这个吧。把 libpng12-dev 换成 libpng-dev 就行了 接着是第二个错误： sudo add-apt-repository &quot;deb http://security.ubuntu.com/ubuntu xenial-security main&quot;sudo apt updatesudo apt install libjasper1 libjasper-dev 然后从官网下载源码，直接git clone。也可以从windows拷到虚拟机（要装VM tools）。网址 git clone https://github.com/opencv/opencv.git 在解压后的文件夹中新建build文件夹，用来存放编译文件。 mkdir buildcd build 在电脑上有多个版本的python时，可以通过-D PYTHON_DEFAULT_EXECUTABLE=$(which python3)来确定安装在哪个版本python上。 cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON_DEFAULT_EXECUTABLE=​\$(which python3) .. 如果执行上面代码没有问题，直接make。接着再执行sudo make install。 我又报错了。。。。报错信息如下： — ICV: Downloading ippicv_linux_20151201.tgz…CMake Error at 3rdparty/ippicv/downloader.cmake:73 (file):file DOWNLOAD HASH mismatch for file: [/root/library/opencv/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/ippicv_linux_20151201.tgz]expected hash: [808b791a6eac9ed78d32a7666804320e]actual hash: [d41d8cd98f00b204e9800998ecf8427e]status: [1;”Unsupported protocol”] Call Stack (most recent call first):3rdparty/ippicv/downloader.cmake:110 (_icv_downloader)cmake/OpenCVFindIPP.cmake:243 (include)cmake/OpenCVFindLibsPerf.cmake:37 (include)CMakeLists.txt:558 (include) CMake Error at 3rdparty/ippicv/downloader.cmake:77 (message):ICV: Failed to download ICV package: ippicv_linux_20151201.tgz.Status=1;”Unsupported protocol”Call Stack (most recent call first):3rdparty/ippicv/downloader.cmake:110 (_icv_downloader)cmake/OpenCVFindIPP.cmake:243 (include)cmake/OpenCVFindLibsPerf.cmake:37 (include)CMakeLists.txt:558 (include) — Configuring incomplete, errors occurred!See also “/root/library/opencv/opencv-3.2.0/build/CMakeFiles/CMakeOutput.log”.See also “/root/library/opencv/opencv-3.2.0/build/CMakeFiles/CMakeError.log”. 百度一下，这是因为我们在编译opencv的时候需要下载ippicv_linux_20151201.tgz，但是由于网络的原因，经常下载失败。解决方案： 手动下载ippicv_linux_20151201.tgz，网上都有资源。读者找一下。接着放入相关路径，我的路径是 home/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/。读者根据自己的路径放。 在重新执行cmake。 接着执行完cmake后，make编译，sudo make install 安装。 到这里还没装完我们要编译的文件，还有一些模块保留在opencv_contrib的资源库中。所以这个我们也要编译。 将opencv_contrib下到build的同级目录下，在build目录下打开终端或者在终端进入build目录， cd buildcmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local -D PYTHON_DEFAULT_EXECUTABLE=$(which python3) -D OPENCV_EXTRA_MODULES_PATH=../opencv_contrib-3.2.0/modules/ ..makesudo make install 在终端运行python3，import cv2。没有报错，opencv安装就成功了。 我装过好几次opencv，这是我最顺的一次。。。。]]></content>
      <categories>
        <category>Linux</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu下装python3和库]]></title>
    <url>%2F2019%2F11%2F09%2Fubuntu%E4%B8%8B%E8%A3%85python3%2F</url>
    <content type="text"><![CDATA[ubuntu下的环境配得我要吐了，全都是坑。一定要写博客避坑。后面还有编译opencv、tensorflow等环境。 本片博客安装的python版本是python3.5。不用卸载python2，不用卸载python2，不用卸载python2 首先更新软件包： sudo apt-get update 接着执行一下命令： sudo apt-get install python3.5 安装pip3： sudo apt-get install python3-pip 在这里我就报错了： 上网找了解决方案： 第一种情况：进程中存在与apt相关的正在运行的进程 首先检查是否在运行apt，apt-get相关的进程。 ps aux | grep -i apt 如果存在与apt相关的正在运行的进程，kill掉 sudo kill -9 &lt;进程号&gt; 或者简单粗暴的直接kill掉： sudo killall apt apt-get 再执行一次sudo apt-get install python3-pip。如果这还不行，那就是第二种情况了 第二种情况： 产生错误的根本原因是lock file。 loack file用于防止两个或多个进程使用相同的数据。 当运行apt或apt-commands时，它会在几个地方创建lock files。 当前一个apt命令未正确终止时，lock file未被删除，因此它们会阻止任何新的apt / apt-get命令实例，比如正在执行apt-get upgrade，在执行过程中直接ctrl+c取消了该操作，很有可能就会造成这种情况。要解决此问题，首先要删除lock file。 首先使用lsof命令获取持有lock file的进程的ID： 如果三个命令都没有返回值，则说明没有正在运行的进程。如果返回了相应的进程，则需要kill掉。 接着删除所有的lock file： 最后重新配置一下dpkg： sudo dpkg --configure -a 到了这一步，没有报错的话，完事大吉。再执行sudo apt-get python3-pip 但是，屋漏偏逢连阴雨。太难了。又报错了 接着找出正在锁定lock file的进程： lsof /var/lib/dpkg/lock-frontend 如果上述命令返回进程，kill掉输出的进程。 sudo kill -9 进程号 删除lock file 并重新配置dpkg： 12sudo rm /var/lib/dpkg/lock-frontendsudo dpkg --configure -a 再重新配置一下dpkg。 后面几步的命令集合： 到了这里，就全部完成了。 接下来安装一些库： 先安装build依赖包： 接着就可以安装python库了。 sudo pip3 install numpy 等等一些库]]></content>
      <categories>
        <category>Linux</category>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yolo算法实战]]></title>
    <url>%2F2019%2F10%2F31%2FYolo%E7%AE%97%E6%B3%95%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[coco介绍本次实战采用coco数据集。这个数据集是由微软团队提供的。下载网址 annotations是采用json格式标注的，包含三个信息：object instances(物体当前的实例信息)、object keypoints(关键点信息)、image caption(图像信息) 下图是标注文件的整体结构： info是基本信息，image是图像信息，annotations是针对于图像数据的标注信息。下图是除了annotations字段其它字段展开后的具体信息： 我们要具体关注annotations的信息。annotations信息如下： 对于annotations，如果我们标注当前的图片为单个物体或多个物体时我们需要修改iscrowd和segmentation。 单个object时： iscrowd=0 segmentation=polygon 多个objects时： iscrowd=1 segmentation=RLE 检测模型的搭建Darknet的搭建Darknet是一个较为轻型的完全基于C与CUDA的开源深度学习框架。支持CPU和GPU两种计算方式。容易安装，且没有任何依赖项。 在windows上我是看这篇博客配好darknet的。https://blog.csdn.net/lvsehaiyang1993/article/details/81032826 注意在使用darknet的时候最后如果出现Couldn’t open file: D:/darknet/data/coco.names。重新下一次zip文件、解压再编译一次。博客中说会有predictions.png文件，我并没有找到。百度是说，电脑内存不够运算。 后面训练的时候，有报错 在网上没有找到解决方案。所以更换了系统再配一次。 在ubuntu上我配了一次。配得我都吐了，最后还把opencv装到python2上。过程不写了，网上都是。也都不是。坑踩踩就好了。我的博客有教程 Darknet解读下面的部分图片是windows。 Darknet文件结构： src、include、obj：存放了darknet的源码和编译后的文件 cfg：存放了作者提供好的各种各样的网络配置文件 打开cfg文件夹下的yolov3.cfg文件，下图网络配置信息的含义。 接下去的就是主干网络。一组中括号加上网络层的名字定义当前的网络属于哪一层。接着是训练过程中用到的信息。然后就是中括号加上convolutional定义卷积层和卷积层的参数，同样定义了池化层和池化层参数。略过，我们直接看Yolo层 接着打开coco.data文件，配置如下： 在windows下，我没有找到train和valid两个文件列表。linux也没有找到 data：存放了接下来用到的数据 script：存放了一些脚本 python：存放了darknet编译出来针对于python的接口 examples：存放了我们可能会用到的函数接口 backup：存放了模型训练时中间结果。比如说，在训练1000次的时候存一个model，10000次的时候存一个model。这个训练多少次保存一次模型，也是可以修改的。在examples文件夹下的detector.c文件的138行修改。 注意：每次修改darknet配置，都要重新编译。 Darknet的使用安装完darknet后，在终端运行./darknet， 在window上运行darknet就行了 这句话告诉我们调用darknet来进行模型训练，我们需要采用的格式为./darknet 函数来完成调用。 在ubuntu进行数据增强： 在windows上显示不出来。我怀疑是因为我的windows上没有编译运行opencv，编译过后应该可以运行的。大家可以试一下。 训练命令： ./darknet detector train cfg/coco.data cfg/yolov3.cfg 采用预训练的模型： ./darknet detector train cfg/coco.data cfg/yolov3.cfg cfg/yolov3_20000.weights 测试命令： ./darknet detector test cfg/coco.data cfg/yolov3.cfg backup/yolov3_20000.weights data/giraffe.jpg -thresh 0.4 -thresh用于阈值筛选 在example下，darknet有提供python的接口。我们也可以利用这个接口完成模型测试。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
        <category>Yolo</category>
      </categories>
      <tags>
        <tag>Yolo算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yolo系列算法]]></title>
    <url>%2F2019%2F10%2F29%2FYolo-%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[对于目标检测，我们最多到底能检测多少个类别呢？对于Yolo来说，是9000个。这是非常厉害的，所以，接下来看看Yolo的三代算法Yolo v1、Yolo v2、Yolo v3。 目标检测经历了一个高度的符合人类的直觉的过程。既需要识别出目标的位置，将图片划分成小图片扔进算法中去，当算法认为某物体在这个小区域上之时，那么检测完成。那我们就认为这个物体在这个小图片上了。而这个思路，正是比较早期的目标检测思路，比如R-CNN。后来的Fast R-CNN，Faster R-CNN虽有改进，比如不再是将图片一块块的传进CNN提取特征，而是整体放进CNN提取特征图后，再做进一步处理，但依旧是整体流程分为 ‘区域提取’和‘目标分类’两部分（two-stage），这样做的一个特点是虽然确保了精度，但速度非常慢。而Yolo将物体检测任务当作一个regression（回归）问题来处理，每张图像只需要“看一眼”就能得出图像中都有哪些物体和这些物体的位置。其实Yolo展开就是you only look once。Yolo是One-stage算法。 Yolo v1基本思想YOLO v1的核心思想在于将目标检测作为回归问题解决 ，YOLO v1首先会把原始图片放缩到448×448的尺寸，放缩到这个尺寸是为了后面整除来的方便。然后将图片划分成SxS个区域，注意这个区域的概念不同于上文提及将图片划分成N个区域扔进算法的区域不同。上文提及的区域是将图片进行剪裁，或者说把图片的某个局部的像素输入算法中，而这里的划分区域，只的是逻辑上的划分。 如果一个对象的中心落在某个单元格上，那么这个单元格负责预测这个物体。每个单元格需要预测B(超参数)个边界框（bbox）值(bbox值包括坐标和宽高)，同时为每个bbox值预测一个置信度(confidence scores)。此后以每个单元格为单位进行预测分析。 这个置信度并不只是该边界框是待检测目标的概率，而是该边界框是待检测目标的概率乘上该边界框和真实位置的IoU（框之间的交集除以并集）的积。通过乘上这个交并比，反映出该边界框预测位置的精度。如下式所示： 每个边界框对应于5个输出，分别是x，y，w，h和置信度。其中x，y代表边界框的中心离开其所在网格单元格边界的偏移。w，h代表边界框真实宽高相对于整幅图像的比例。x，y，w，h这几个参数都已经被限制到了区间[0,1]上。除此以外，每个单元格还产生C个概率(有多少类物体，C就是多少)。注意，我们不管B的大小，每个单元格只产生一组这样的概率。 然后我们让每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score: P(Classi)就是每个网络预测的类别信息。 这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。 网络模型架构 网络结构借鉴了 GoogLeNet 。24个卷积层，2个全连接层。 损失函数YOLO v1全部使用了均方差（mean squared error）作为损失（loss）函数。由三部分组成：坐标误差、IOU误差和分类误差。 考虑到每种loss的贡献率，YOLO v1给坐标误差（coordErr）设置权重λcoord=5。在计算IoU误差时，包含物体的格子与不包含物体的格子（此处的‘包含’是指存在一个物体，它的中心坐标落入到格子内），二者的IOU误差对网络loss的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的置信度值近似为0，变相放大了包含物体的格子的置信度误差，在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用λnoobj（置信度误差）=0.5修正iouErr。 对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（w和h）进行求平方根来改进这个问题，但并不能完全解决这个问题。 所以，Loss计算公式： PS：连数学符号都没认全，我太难了 后面看别人的博客，才知道公式意思如下： 在激活函数上： 在最后一层使用的是标准的线性激活函数，其他的层都使用leaky rectified 线性激活函数。 Yolo v2/Yolo 9000YOLO v1对于bounding box的定位不是很好，在精度上比同类网络还有一定的差距，所以YOLOv2对于速度和精度做了很大的优化，并且吸收了同类网络的优点，一步步做出尝试。 YOLO v2在v1基础上做出改进后提出。其受到Faster RCNN方法的启发，引入了anchor（先验框）。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中。并且修改了网络结构，去掉了全连接层，改成了全卷积结构。在训练时引入了世界树（WordTree）结构，将检测和分类问题做成了一个统一的框架，并且提出了一种层次性联合训练方法，将ImageNet分类数据集和COCO检测数据集同时对模型训练。 预测更准确batch normalizationYOLOv2对每批数据都做了一个归一化预处理。通过在每一个卷积层后添加batch normalization，极大的改善了收敛速度同时减少了对其它正则方法的依赖（Yolo v2不在使用dropout），使得mAP获得了提升。（mAP：平均精度均值（mean Average Precision）） 通常，一次训练会输入一批样本（batch）进入神经网络。批规一化在神经网络的每一层，在网络（线性变换）输出后和激活函数（非线性变换）之前增加一个批归一化层（BN），BN层进行如下变换：①对该批样本的各特征量（对于中间层来说，就是每一个神经元）分别进行归一化处理，分别使每个特征的数据分布变换为均值0，方差1。从而使得每一批训练样本在每一层都有类似的分布。这一变换不需要引入额外的参数。②对上一步的输出再做一次线性变换，假设上一步的输出为Z，则Z1=γZ + β。这里γ、β是可以训练的参数。增加这一变换是因为上一步骤中强制改变了特征数据的分布，可能影响了原有数据的信息表达能力。增加的线性变换使其有机会恢复其原本的信息。 使用高分辨率图像YOLOv1在分辨率为224×224的图片上进行预训练，在正式训练时将分辨率提升到448×448，这需要模型去适应新的分辨率。但是YOLOv2是直接使用448×448的输入， 所以YOLO2在采用 224*224 图像进行分类模型预训练后，再采用 448*448 的高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448*448 的分辨率。然后再使用 448*448 的检测样本进行训练，缓解了分辨率突然切换造成的影响。 采用先验框anchor在预测框的数量上，由于YOLOv2将网络的输入分辨率调整到416×416，下采样率为32，多次卷积后得到13×13的特征图（feature map）。在这上面使用9种anchor boxes，得到13×13×9=1521个，这比YOLOv1大多了。PS：我也不知道为什么突然就变416了？ YOLOv1利用全连接层的数据完成边框的预测，会导致丢失较多的空间信息，使定位不准。在YOLOv2中作者借鉴了Faster R-CNN中的anchor思想，来改善全连接层带来的影响。 Anchor是RPN（region proposal network）网络在Faster R-CNN中的一个关键步骤，是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的候选框。 为了引入anchor boxes来预测候选框，作者在网络中去掉了全连接层。并去掉了最后的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416 * 416，目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个中心框（center cell）。YOLO算法的作者观察到，大物体通常占据了图像的中间位置，可以只用中心的一个框来预测这些物体的位置，否则就要用中间的4个格子来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（采样因子为32），使得输入卷积网络的416 * 416图片最终得到13 * 13的卷积特征图（416/32=13） 聚类提取先验框尺度。使用anchor的时候，anchor boxes的宽和高往往是人工选定的。虽然在训练过程中，网络也会调整宽和高。但一开始就选择了更好的宽和高，网络就更容易得到准确的预测位置。为了使网络更易学到准确的预测位置，作者使用了K-means聚类方法类训练bounding boxes，可以自动找到更好的框宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的框会比较小的框产生更多的误差，聚类结果可能会偏离。为此，作者采用IOU得分作为评价标准，这样的话，误差就和框的尺度无关。最终的距离函数为： centroid是聚类时被选作中心的边框，box就是其它边框，d就是两者间的“距离”。IOU越大，“距离”越近。YOLO2给出的聚类分析结果如下图所示： 上图左边是选择不同的聚类k值情况下，得到的k个centroid边框，计算样本中标注的边框与各centroid的Avg IOU。显然，边框数k越多，Avg IOU越大。YOLO2选择k=5作为边框数量与IOU的折中。对比手工选择的先验框，使用5个聚类框即可达到61 Avg IOU，相当于9个手工设置的先验框60.9 Avg IOU。 约束预测边框的位置 借鉴于Faster RCNN的先验框方法，在训练的早期阶段，其位置预测容易不稳定。其位置预测公式为： 其中，x，y是预测边框的中心，xa，ya是先验框(anchor)的中心点坐标，&omega;a，ha是先验框(anchor)的宽和高，tx，ty是要学习的参数。在Yolo论文中写的是x=(tx*&omega;a)-xa，根据Faster RCNN，应该是“+”。 由于tx，ty的取值没有任何约束，因此预测边框的中心可能出现在任何位置，导致训练早期阶段不容易稳定。Yolo 调整了预测公式，将预测边框的中心约束在特定的grid网格内。 其中，bx、by、bw、bh是预测边框的中心和宽高。Pr(object) * IOU(b,object)是预测边框的置信度，Yolo v1是直接预测置信度的值，这里对预测参数t0进行&sigma;(sigmoid)变换后作为置信度的值。cx，cy是当前网格左上角的距离，要先将网格大小归一化，即令一个网格的宽=1，高=1。pw，ph是先验框的宽和高。tx、ty、tw、th、to是要学习的参数，分别用于预测边框的中心和宽高，以及置信度。 参考上图，由于σ函数将tx、ty约束在(0,1)范围内，所以根据上面的计算公式，预测边框的蓝色中心点被约束在蓝色背景的网格内。约束边框位置使得模型更容易学习，且预测更为稳定。 passthrough层检测细粒度特征对象检测面临的一个问题是图像中对象会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLO2中输入416*416经过卷积网络下采样最后输出是13*13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。 YOLO2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26*26*512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。 一拆4是什么拆的呢？举个例子，如下： 还有其它拆法。就不具述了。 多尺度图像训练 因为去掉了全连接层，YOLO2可以输入任何尺寸的图像。因为整个网络下采样倍数是32，作者采用了{320,352,…,608}等10种输入图像的尺寸，这些尺寸的输入图像对应输出的特征图宽和高是{10,11,…19}。训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。 速度更快大多数目标检测的框架是建立在VGG-16上的。为了进一步提升速度，Yolo v2提出了Darknet-19网络结构。Darknet-19(19层卷积层和5层池化层)比VGG-16小一些，精度不弱于VGG-16，但浮点运算量减少到约1/5。 YOLO2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224*224 ，共训练160个epochs。然后第二阶段将网络的输入调整为 448*448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs。第三个阶段就是修改Darknet-19分类模型为检测模型，移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个 3*3*1024卷积层，同时增加了一个passthrough层，最后使用 11 卷积层输出预测结果，输出的channels数为：**num_anchors\(5+num_classes)** ，和训练采用的数据集有关系。对于VOC数据集(20种分类对象)，假如anchor数为5，输出的channels就是125。 识别对象更多论文提出了一种联合训练的机制：使用识别数据集训练模型识别相关部分，使用分类数据集训练模型分类相关部分。 众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。所以在YOLOv1中，边界框的预测其实并不依赖于物体的标签，YOLOv2实现了在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。 作者选择在COCO和ImageNet数据集上进行联合训练，遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的词树（WordTree）如下图所示： WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个路径，然后计算路径上各个节点的概率之积。 在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是 ，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。 Yolo v3YOLO v3没有太多的创新，主要是借鉴一些好的方案融合到YOLO里面。不过效果还是不错的，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。 新的网络结构在基本的图像特征提取方面，Yolo v3采用了称之为Darknet-53的网络结构(含有53个卷积层)，它借鉴了残差网络residual network的做法，在一些层之间设置了快捷链路(shortcut connections)。 利用多尺度特征进行对象预测YOLO2曾采用passthrough结构来检测细粒度特征，在YOLO3更进一步采用了3个不同尺度的特征图来进行对象检测。 结合上图看，卷积网络在79层后，经过下方几个黄色的卷积层得到一种尺度的检测结果。相比输入图像，这里用于检测的特征图有32倍的下采样。比如输入是416*416的话，这里的特征图就是13*13了。由于下采样倍数高，这里特征图的感受野比较大，因此适合检测图像中尺寸比较大的对象。 为了实现细粒度的检测，第79层的特征图又开始作上采样（从79层往右开始上采样卷积），然后与第61层特征图融合（Concatenation），这样得到第91层较细粒度的特征图，同样经过几个卷积层后得到相对输入图像16倍下采样的特征图。它具有中等尺度的感受野，适合检测中等尺度的对象。 最后，第91层特征图再次上采样，并与第36层特征图融合（Concatenation），最后得到相对输入图像8倍下采样的特征图。它的感受野最小，适合检测小尺寸的对象。 9种尺度的先验框随着输出的特征图的数量和尺度的变化，先验框的尺寸也需要相应的调整。YOLO2已经开始采用K-means聚类得到先验框的尺寸，YOLO3延续了这种方法，为每种下采样尺度设定3种先验框，总共聚类出9种尺寸的先验框。在COCO数据集这9个先验框是：(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。 分配上，在最小的13*13特征图上（有最大的感受野）应用较大的先验框(116x90)，(156x198)，(373x326)，适合检测较大的对象。中等的26*26特征图上（中等感受野）应用中等的先验框(30x61)，(62x45)，(59x119)，适合检测中等大小的对象。较大的52*52特征图上（较小的感受野）应用较小的先验框(10x13)，(16x30)，(33x23)，适合检测较小的对象。 感受一下9种先验框的尺寸，下图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格。 13*13: 26*26: 52*52: 对象分类softmax改成logistic预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象（比如一个人有Woman 和 Person两个标签）。 输入映射到输出 不考虑神经网络结构细节的话，总的来说，对于一个输入图像，YOLO3将其映射到3个尺度的输出张量，代表图像各个位置存在各种对象的概率。 我们看一下YOLO3共进行了多少个预测。对于一个416*416的输入图像，在每个尺度的特征图的每个网格设置3个先验框，总共有 13*13*3 + 26*26*3 + 52*52*3 = 10647 个预测。每一个预测是一个(4+1+80)=85维向量，这个85维向量包含边框坐标（4个数值），边框置信度（1个数值），对象类别的概率（对于COCO数据集，有80种对象）。 对比一下，YOLO2采用13*13*5 = 845个预测，YOLO3的尝试预测边框数量增加了10多倍，而且是在不同分辨率上进行，所以mAP以及对小物体的检测效果有一定的提升。 本篇文章使用了较多的他人的图片，如有侵权，请联系我删掉。QQ：1171708687]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
        <category>Yolo</category>
      </categories>
      <tags>
        <tag>Yolo算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch的可视化]]></title>
    <url>%2F2019%2F10%2F26%2Fpytorch%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[可视化Visdom是一个灵活的可视化工具，可以实时显示新创建的数据。Visdom的目的是促进远程数据的可视化，支持科学实验。可以发送可视化图像和文本。通过UI为实时数据创建dashboards，检查实验的结果。 Panes(窗格)UI刚开始是个白板，我们可以用图像和文本填充它。这些填充的数据出现在Panes，我们可以对这些Panes进行拖放、删除、调整大小和销毁操作。Panes是保存在Environment(环境)中的，Environment(环境)的状态存储在会话之间。可以使用浏览器的放大缩小功能来调整UI的大小。 Environment(环境)可以使用Envs对可视化空间进行分区。每个用户都会有一个叫做main的Envs。可以通过编程或UI创建新的Envs，Envs的状态是长期保存的。可以通过https://localhost.com:8097/env/main访问特定的ENV。如果服务器是被托管的，那么可以将此URL分享给别人，那么其他人也会看到可视化结果， 在初始化服务器的时候，Envs默认通过$HOME/.visdom/加载。也可以将自定义的路径当作命令行参数传入。如果移除了Envs文件家下的.json文件，那么相应的环境也会删除。 State(状态)一旦创建了一些可视化，状态是被保存的。服务器自动缓存我们的可视化，如果重新加载网页，可视化就会重新出现。 Save：可以通过点击save按钮手动保存Envs。它首先会序列化Envs的状态，然后以.json文件的形式保存到硬盘上，包括窗口的位置。 Fork：输入一个新的Envs名字，“保存”会建立一个新的Envs，有效的分割之前的状态。 当前用不到可视化，先学后面的。后面有用到就补]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>深度学习框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch(1)]]></title>
    <url>%2F2019%2F10%2F22%2Fpytorch-1%2F</url>
    <content type="text"><![CDATA[torch.nn包里的Functional包含convolution函数，pooling函数，非线性函数、激活函数等函数，torch.nn.optim包含各种优化算法，Momentum、RMSProp等。 Tensortorch.Tensor是一种包含单一数据类型元素的多维矩阵。 Torch定义了七种CPU tensor类型和八种 tensor类型： torch.Tensor是默认的tensor类型（torch.FloatTensor）的简称。 一个张量tensor可以从Python的list或序列构建： >&gt;&gt;torch.FloatTensor([[1,2,3],[4,5,6]])tensor([[1., 2., 3.], [4., 5., 6.]]) >&gt;&gt;torch.Tensor([[3,2,1],[6,5,4]])tensor([[3., 2., 1.],[6., 5., 4.]]) 空张量tensor可以通过规定其大小来构建： >&gt;&gt;torch.IntTensor(2,4).zero_()tensor([[0, 0, 0, 0], [0, 0, 0, 0]], dtype=torch.int32) 可以用python的索引和切片来获取和修改一个张量tensor中的内容： >&gt;&gt;x=torch.FloatTensor([[1,2,3],[4,5,6]])>&gt;&gt;print(x[1][2])tensor(6.)>&gt;&gt;print(x[0][1])tensor(2.)>&gt;&gt;x[0][1]=8>&gt;&gt;print(x)tensor([[1., 8., 3.], [4., 5., 6.]]) 每一个张量tensor都有一个相应的tourch.Storage用来保存其数据。 注意：会改变tensor的函数操作会用一个下划线后缀来表示。比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而torch.FloatTensor.abs()将会在一个新的tensor中计算结果。 创建张量时有个参数是requires_grad，如果设置 requires_grad为 True，那么将会追踪所有对于该张量的操作吗，默认False。 当完成计算后通过调用 .backward()，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 .grad属性。 torch.squeezetorch.squeeze(input,dim=None,out=None)：将输入张量形状中的1去除并返回。当给定dim时，那么挤压操作只在给定维度上。 >&gt;&gt;x=torch.zeros(2,1)>&gt;&gt;xtensor([[0.], [0.]])>&gt;&gt;y=torch.squeeze(x)>&gt;&gt;ytensor([0., 0.]) torch.abstorch.abs(input,out=None)：计算输入张量的每个元素的绝对值 >&gt;&gt;torch.abs(torch.Tensor([-1,-2,-3]),out=z)tensor([1., 2., 3.])>&gt;&gt;ztensor([1., 2., 3.]) torch.addtorch.add(input,value,out=None)，对输入张量input逐元素加上标量值value，并返回结果得到一个新的张量out。 >&gt;&gt;a=torch.rand(4)>&gt;&gt;atensor([0.0694, 0.6366, 0.6938, 0.8872])>&gt;&gt;torch.add(a,20)tensor([20.0694, 20.6366, 20.6938, 20.8872]) torch.meantorch.mean(input,dim,out=None)：返回输入张量给定维度dim上每行的均值。0是列，1是行。 >&gt;&gt;a=torch.rand(4,4)>&gt;&gt;atensor([[0.7413, 0.5828, 0.0070, 0.0146], [0.1668, 0.6566, 0.1865, 0.0700], [0.7603, 0.8017, 0.3065, 0.8772], [0.7918, 0.8798, 0.7355, 0.1142]])>&gt;&gt;torch.mean(a,1)tensor([0.3364, 0.2700, 0.6864, 0.6303]) 比较运算符torch.eqtorch.eq(input,other,out=None)：比较元素相等性。第二个参数可以为一个数或与第一个参数同类型形状的张量。 >&gt;&gt;torch.eq(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))tensor([[ True, False], [False, True]]) 如果两个张量有相同的形状和元素值，则返回True，否则返回False。 >&gt;&gt;torch.equal(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))False >&gt;&gt;torch.equal(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,2],[3,4]]))True torch.getorch.ge(input,other,out=None)：逐元素比较input和other。即是否 input&gt;=otherinput&gt;=other。如果两个张量有相同的形状和元素值，则返回True。否则返回False。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。 >&gt;&gt;torch.ge(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))tensor([[ True, True], [False, True]]) torch.gttorch.gt(input,other,out=None)：逐元素比较input和other。即input&gt;otherinput&gt;other。如果两个张量有相同的形状和元素值，则返回True。否则返回False。第二个参数可以为一个数或与第一个参数相同形状和类型的张量。PS:这个和ge有什么区别？ >&gt;&gt;torch.gt(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))tensor([[False, True], [False, False]]) torch.letorch.le(input, other, out=None)：逐元素比较input和other ， 即是否input&lt;=otherinput&lt;=other 第二个参数可以为一个数或与第一个参数相同形状和类型的张量 >&gt;&gt;torch.le(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))tensor([[ True, False], [ True, True]]) torch.lttorch.lt(input, other, out=None)：逐元素比较input和other ， 即是否 input&lt;otherinput&lt;other。第二个参数可以为一个数或与第一个参数相同形状和类型的张量 >&gt;&gt;torch.lt(torch.Tensor([[1,2],[3,4]]),torch.Tensor([[1,1],[4,4]]))tensor([[False, False], [ True, False]]) VariableVariable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。 首先Variable是在torch.autograd.Variable中，要将tensor变成Variable，只需要Variable(a)就可以了。Variable有三个比较重要的组成属性：data、grad、grad_fn。data可以取出variable里面的tensor值，grad_fn表示的是得到这个Variable的操作(op)，grad是这个Variable的反向传播梯度。 >&gt;&gt;from torch.autograd import Variable>&gt;&gt;x=Variable(torch.Tensor([1]),requires_grad=True)>&gt;&gt;w=Variable(torch.Tensor([2]),requires_grad=True)>&gt;&gt;b=Variable(torch.Tensor([3]),requires_grad=True)>&gt;&gt;y=w*x+b>&gt;&gt;ytensor([5.], grad_fn=)>&gt;&gt;print(y.backward())None>&gt;&gt;print(x.grad)tensor([2.])>&gt;&gt;print(w.grad)tensor([1.])>&gt;&gt;print(b.grad)tensor([1.])>&gt;&gt;print(x.grad_fn)None>&gt;&gt;print(y.grad_fn) 构建Variable，我们传入了一个参数requires_grad=True，这个参数表示是否对这个变量求梯度，默认的是False，也就是不对这个变量求梯度。 parametersparamerters(memo=None)：返回一个包含模型所有参数的迭代器。一般用来当作optimizer的参数。 backward如果需要计算导数，可以在Tensor上调用.backward()。 如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数， 但是如果它有更多的元素，需要指定一个gradient参数来匹配张量的形状。 zero_gradzero_grad()：清空所有被优化过的Variable的梯度 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667## view 返回一个有相同数据但大小不同的tensor。 返回的tensor必须有与原tensor相同的数据和相同数目的元素，但可以有不同的大小。一个tensor必须是连续的`contiguous()`才能被查看。 &gt; \&gt;&gt;&gt;import torch&gt; \&gt;&gt;&gt;x=torch.randn(4,4)&gt; \&gt;&gt;&gt;x.size()&gt; torch.Size([4, 4])&gt; \&gt;&gt;&gt;y=x.view(16)&gt; \&gt;&gt;&gt;y.size()&gt; torch.Size([16])&gt; \&gt;&gt;&gt;z=x.view(-1,8)&gt; \&gt;&gt;&gt;z.size()&gt; torch.Size([2, 8])## zero_zero_(tensor)：用0填充一个tensor&gt; \&gt;&gt;&gt;torch.zero_(x)&gt; tensor([[0., 0., 0., 0.],&gt; [0., 0., 0., 0.],&gt; [0., 0., 0., 0.],&gt; [0., 0., 0., 0.]])## 模型的保存和加载在pytorch里面使用torch.save来保存模型的结构和参数，有两种保存方式，当然加载模型有两种方式对应于保存模型的方式：（1）保存这个模型的结构信息和参数信息，保存的对象是模型model。加载完整的模型结构和参数信息，在网络较大的时候加载的时间比较长，同时存储空间也比较大。torch.save(the_model,path)the_model=torch.load(&quot;path&quot;)（2）保存模型的参数，保存的对象是模型的状态model.state_dict()。加载模型时加载的是参数信息。torch.save(the_model.state_dict(),path)the_model=TheModeClass(\*args,\**kwargs)the.model.load_state_dict(path)## 优化算法优化算法分两类：### 一阶优化算法这种算法使用各个参数的梯度值来更新参数，最常用的一阶优化算法是梯度下降。所谓的梯度就是导数的多变量表达式，同时也是一个方向，这个方向上方向导数最大，且等于梯度。梯度下降的功能是通过寻找最小值，控制方差，更新模型参数，最终使模型收敛。### 二阶优化算法二阶优化算法使用了二阶导数来最小化或最大化损失函数，主要基于牛顿法。但是由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。torch.optim是一个实现各种优化算法的包，大多数常见的算法都能通过这个包调用。### torch.optim为了使用torch.optim，我们需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。#### 构建optimizer为了构建一个optimizer，我们需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，可以设置optimizer的参数选项，比如学习率，权重衰减，等等。 举个例子： ```optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) 单独设置参数我们还可以为每个参数单独设置选项。Optimizer也支持为每个参数单独设置选项。若想这么做，不要直接传入Variable的iterable，而是传入dict的iterable。每一个dict都分别定 义了一组参数，并且包含一个param键，这个键对应参数的列表。其他的键应该optimizer所接受的其他参数的关键字相匹配，并且会被用于对这组参数的优化。 你仍然能够传递选项作为关键字参数。在未重写这些选项的组中，它们会被用作默认值。当你只想改动一个参数组的选项，但其他参数组的选项不变时，这是非常有用的。 例如，当我们想指定每一层的学习率时，这是非常有用的： optim.SGD([{&#39;params&#39;: model.base.parameters()}, {&#39;params&#39;: model.classifier.parameters(), &#39;lr&#39;:1e-3], lr=1e-2, momentum=0.9) 单次优化使用optimizer.step()，这是大多数optimizer支持的版本，一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。 for input, target in dataset: ​ optimizer.zero_grad() ​ output = model(input) ​ loss = loss_fn(output, target) ​ loss.backward() ​ optimizer.step() 补充我看到优化算法时，发现中文文档好像没有调整学习率的函数。补充一下。torch.optim.lr_scheduler提供了几种方法来根据epoches的数量调整学习速率。 两种机制：LambdaLR机制和StepLR机制； LambdaLR机制：class torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda,last_epoch=-1) 将每一个参数组的学习率设置为初始学习率lr的某个函数倍.当last_epoch=-1时,设置初始学习率为lr. 参数: ​ optimizer(Optimizer对象)—优化器 ​ lr_lambda(是一个函数,或者列表(list))— 当是一个函数时,需要给其一个整数参数,使其计算出一个乘数因子,用于调整学习率,通常该输入参数是epoch数目或者是一组上面的函数组成的列表, ​ last_epoch(int类型):最后一次epoch的索引,默认为-1. StepLR机制class torch.optim.lr_scheduler.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1) 设置每个参数组的学习率为 lr*&lambda;n, n=epoch/step_size。当last_epoch=-1时,令lr=lr 参数: ​ optimizer(Optimizer对象)—优化器 ​ step_size(整数类型): 调整学习率的步长,每过step_size次,更新一次学习率 ​ gamma(float 类型):学习率下降的乘数因子 ​ last_epoch(int类型):最后一次epoch的索引,默认为-1. 其它API其它的API不想做搬运工了，上网址。pytorch中文文档 看文档我发现torch.nn包里面和nn.functional都有卷积等一些函数。其实nn.functional中的函数仅仅定义了一些具体的基本操作，不能构成pytorch中的一个Layer。当我们需要自定义一些非标准Layer时，可以在其中调用nn.functional中的操作。例如，relu仅仅是一个函数，参数包括输入和计算需要的参数，返回计算的结果，它不能存储任何上下文的信息。 torch.nn包里面只是包装好了神经网络架构的类，nn.functional 与torch.nn包相比，nn.functional是可以直接调用函数的。]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[超参数调试、batch正则化]]></title>
    <url>%2F2019%2F10%2F13%2F%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81batch%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[调试处理关于训练深度最难的事情之一是要处理的参数的数量，从学习速率到Momentum（动量梯度下降法）的参数。如果使用Momentum或Adam优化算法的参数，&beta;1，&beta;2和ξ，也许你还得选择层数，也许还得选择不同层中隐藏单元的数量，也许还想使用学习率衰减。所以，使用的不是单一的学习率a。接着，当然可能还需要选择mini-batch的大小。 &beta;1、&beta;2、ξ推荐使用0.9、0.999、10-10。a是学习速率，学习速率是需要调试的最重要的超参数。 现在，如果我们尝试调整一些超参数，该如何选择调试值呢？在早一代的机器学习算法中，如果有两个超参数，这里会称之为超参1，超参2，常见的做法是在网格中取样点，像这样，然后系统的研究这些数值。这里放置的是5×5的网格，实践证明，网格可以是5×5，也可多可少，但对于这个例子，我们可以尝试这所有的25个点，然后选择哪个参数效果最好。当参数的数量相对较少时，这个方法很实用。 在深度学习领域，吴恩达老师推荐我们使用随机选择点方法，所以我们可以选择同等数量的点，对吗？25个点，接着，用这些随机取的点试验超参数的效果。之所以这么做是因为，对于要解决的问题而言，你很难提前知道哪个超参数最重要，正如之前看到的，一些超参数的确要比其它的更重要。 假如我们拥有三个超参数呢？这时我们搜索的就不只是一个方格了，而是一个立方体。超参数3代表第三维，接着，在三维立方体中取值，我们会实验更多的值。 实践中，需要搜索的可能不止三个超参数有时很难预知，哪个是最重要的超参数，对于具体应用而言，随机取值而不是网格取值表明，我们要探究了更多重要超参数的潜在值，无论结果是什么。 当我们给超参数取值时，另一个惯例是采用由粗糙到精细的策略。 比如在二维的那个例子中，你进行了取值，也许你会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内），然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索，如果你怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，你会知道接下来应该聚焦到更小的方格中。在更小的方格中，你可以更密集得取点。所以这种从粗到细的搜索也经常使用。 为超参数选择合适的范围讲个例子，假如我们在搜索超参数a，假设我们怀疑其值最小是0.0001，最大是1。假如我们把这个取值范围当作数轴，沿其随机均匀取值，那90%的值都会落在0.1到1之间。只有10%的搜索资源在0.0001到0.1之间。反而，用数标尺搜索超参数的方式会更合理，因此不使用线性轴。分别依次取0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在0.0001到0.001之间，就会有更多的搜索资源可用。 在python中，这可以通过numpy模块实现。 r=-4 x np.random.rand() a=10r 对a随机取值，由上面第一行代码得出r的取值在[-4,0]之间，a的取值[10-4,100]之间。如果我们在10a和10b之间取值，在此例中，我们可以通过0.0001算出a的值为-4，b的值为0。我们要做的就是在[a,b]区间随机均匀的给r取值，然后设置a的值。所以总结一下，在对数坐标下取值，取最小值的对数就得到a的值，取最大值的对数就得到b值，所以现在你在对数轴上的10a到10b区间取值，在a，b间随意均匀的选取值，将超参数设置为10r，这就是在对数轴上取值的过程。 最后，还有一个例子是对&beta;取值。用于计算指数的加权平均数。假设我们认为&beta;是0.9到0.999之间的某个值，这也是我们想搜索的范围。 上面哪个例子说了如果想在0.9到0.999区间搜索，那就不能用线性轴取值。不要随机均匀在此区间取值，所以考虑这个问题最好的方法就是，我们要探究的是1-&beta;，此值在0.1到0.001区间内，所以我们会给1-&beta;取值，大概是从0.1到0.001。所以我们要做的是在[-3,-1]里随机均匀的给r取值。由1-&beta;=10r推出&beta;=1-10r，然后就变成了在特定的选择范围内超参数的随机取值。 超参数调试的方法两种方法： 一种是你照看一个模型，通常是有庞大的数据组，但没有许多计算资源或足够的CPU和GPU的前提下，基本而言，只可以一次负担起试验一个模型或一小批模型，在这种情况下，即使当它在试验时，也可以逐渐改良。这是一个人照料一个模型的方法，观察它的表现，耐心的调试学习率。 另一种方法则是同时试验多种模型，你设置了一些超参数，尽管让它自己运行，或者是一天甚至多天，然后你会获得像这样的学习曲线，这可以是损失函数J或实验误差或损失或数据误差的损失，但都是你曲线轨迹的度量。同时你可以开始一个有着不同超参数设定的不同模型。 所以这两种方式的选择，是由你拥有的计算资源决定的，如果你拥有足够的计算机去平行试验许多模型，那绝对采用第二种方式，尝试许多不同的超参数，看效果怎么样。 归一化的激活函数在深度学习兴起后，最重要的一个思想是它的一个算法——Batch归一化。batch归一化会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易。接下来看一下原理。 之前的逻辑回归，我们讲过归一化输入特征可以加快学习过程。计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化数据集。那么更深的模型呢？我们不仅输入了特征值x，而且第一层有激活值a[1]，第二层有激活层a[2]等。那么如果我们想训练&omega;[3]，b[3]，那归一化a[2]岂不是更好？便于我们训练&omega;[3]和b[3]。 那么，我们能归一化每个隐藏层的a值吗？比如a[2]，但不仅仅是a[2]，可以是任何隐藏层的。a[2]的值是下一层的输入值，所以a[2]会影响&omega;[3]，b[3]的训练。这就是batch归一化的作用。严格来说，归一化的是z[2]，并不是a[2]。 在神经网络中，假设你有一些隐藏单元值从z[1]到z[m]，这些来源于隐藏层，所以这样写会更准确，即z[ l ]( i )为隐藏层，i从1到m，所以已知这些值。如下，你要计算平均值，强调一下，所有这些都是针对 l 层，但已省略 l 及方括号,计算方法如下： 分母中加上ε以防止δ为0的情况。 所以现在我们已把这些z值标准化，化为含平均值 0 和标准单位方差，所以z的每一个分量都含有平均值 0 和方差 1，但我们不想让隐藏单元总是含有平均值 0 和方差 1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算，我们称之为z~(i)： 这里γ和β是模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法，比如 Momentum 或者 Nesterov， Adam，接着更新γ和β，正如更新神经网络的权重一样。 γ和β的作用是可以随意设置z~(i) 的平均值，事实上，如果： 那么： Batch 归一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏层。应用 Batch 归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，也许不想隐藏单元值必须是平均值 0 和方差 1。γ和β参数控制使得均值和方差可以是 0 和 1，也可以是其它值。 注意：均值不是平均值，是数学期望。 将Batch Norm拟合进神经网络 假设有一个这样的神经网络，之前的知识说过，我们可以认为每个单元负责计算两件事。第一，它先计算z，然后应用其到激活函数中再计算a，所以可以认为，每个圆圈代表着两步的计算过程。同样的，对于下一层而言，那就是$z^{[2]}_{1}$和$z^{[2]}_{2}$等。所以如果没有应用Batch归一化，会把输入X拟合到第一隐藏层，然后首先计算z[1]，这是由&omega;[1]和b[1]两个参数控制的。接着，通常而言，会把z[1]拟合到激活函数以计算a[1]。但Batch归一化的做法是将z[1]值进行Batch归一化，简称BN，此过程将由&beta;[1]和&gamma;[1]两参数控制，这一操作会给你一个新的规范化的z[1]值（z~[1]），然后将其输入激活函数中得到a[1]，即a[1]=g[1](z~[1])。 现在，你已在第一层进行了计算，此时Batch归一化发生在z的计算和a之间，接下来，你需要应用a[1]值来计算，此过程是由&omega;[1]和b[1]控制的。与第一层所做的类似，也会将进行Batch归一化，现在我们简称BN，这是由下一层的Batch归一化参数所管制的，即&beta;[2]和&gamma;[2]，现在你得到z~[2]，再通过激活函数计算出a[2]。 所以，得出结论的是batch归一化是发生在计算z和a之间的。与其使用没有归一化的z值，不如用归一化的z~值，也就是第一层的z~[1]。第二层同理，也是与其应用z值，不如应用z~[2]值。所以，我们以前的网络参数&omega;[1]、b[1]、&omega;[2]、b[2]将加上&beta;[1]、&beta;[2]、&gamma;[1]、&gamma;[2]等参数。注意：这里的&beta;[1]、&beta;[2]和超参数&beta;没有关系 &beta;1]、&beta;[2]、&gamma;[1]、&gamma;[2]等是算法的新参数。接下来就是使用梯度下降法来执行它。举个例子，对于给定层，计算d&beta;[1]，接着更新参数为&beta;[1]=&beta;[1]-&alpha;d&beta;[1]。你也可以使用Adam或RMSprop或Momentum，以更新参数&beta;和&gamma;，并不是只应用梯度下降法。 实践中，我们常将batch归一化和mibi-bacth一起使用。我们用第一个mini-batch{X[1]}，然后应用&omega;[1]和b[1]计算z[1]，接着用batch归一化得到z~[1]，再应用激活函数得到a[1]。然后接着用&omega;[2]和b[2]计算z[2]。 类似的工作，你会在第二个mini-batch（X{2}）上计算z[1]，然后用Batch归一化来计算，所以Batch归一化的此步中，用的是第二个mini-batch（X{2}）中的数据使归一化。然后在mini-batch（X{3}）上同样这样做，继续训练。 先前说过每层的参数是&omega;[ l ]和b[ l ]，还有&beta;[ l ]和&gamma;[ l ]，请注意计算z的方式如下，z[ l ]=&omega;[ l ] * a[l-1]+b[ l ]，但Batch归一化做的是，要看这个mini-batch，先将z[ l ]归一化，结果为均值0和标准方差，再由&beta;和&gamma;重缩放，但这意味着，无论b[ l ]的值是多少，都是要被减去的，因为在Batch归一化的过程中，要计算z[ l ]的均值，再减去平均值，在此例中的mini-batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消。 所以，我们在使用batch归一化时，我们可以消除b[ l ]这个参数。或者也可以设置为0。那么式子将从z[ l ]=&omega;[ l ] * a[l-1]+b[ l ]变为z[ l ]=&omega;[ l ] * a[l-1]。然后归一化z[ l ]，得z~[ l ]=&gamma;[ l ] * z[ l ]+&beta;[ l ]，所以最后我们会用&beta;[ l ]，以便决定z~[ l ]的取值。 总结一下关于如何用 Batch 归一化来应用梯度下降法： 假设使用 mini-batch梯度下降法，运行t = 1到 batch 数量的 for 循环，会在 mini-batchX{t}上应用正向 prop，每个隐藏层都应用正向 prop，用 Batch 归一化代替z[l]为z~[l]。接下来，它确保在这个 mini-batch 中，z值有归一化的均值和方差，归一化均值和方差后是z~[ l ] ，然后，你用反向 prop 计算：dw[ l ]，db[l]，dβ[ l ] ，dγ[ l ]。 尽管严格来说，因为要去掉b，这部分其实已经去掉了。最后，更新这些参数： batch norm为什么会管用？一个原因是，你已经看到如何归一化输入特征值x，使其均值为0，方差1，它又是怎样加速学习的，有一些从0到1而不是从1到1000的特征值，通过归一化所有的输入特征值x，以获得类似范围的值，可以加速学习。所以Batch归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值，这只是Batch归一化作用的冰山一角，还有些深层的原理，它会有助于你对Batch归一化的作用有更深的理解，让我们一起来看看吧。 Batch归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层，比如，第10层的权重相比于神经网络中前层的权重更能经受得住变化。 看上面的素材。假设我们已经在某个网络上训练了所有黑猫的图像，如果我们将这个网络应用于有色猫。这种情况下，我们的效果可能不是很好。因为正面的例子不止左边的黑猫，还有右边其它颜色的猫。 所以我们要使数据改变分布，想法名字叫“Covariate shift”。想法是这样的，如果你已经学习了x到y的映射，如果x的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由x到y映射保持不变，正如此例中，因为真实函数是此图片是否是一只猫，训练你的函数的需要变得更加迫切，如果真实函数也改变，情况就更糟了。 看上图，看第三层网络。此网络已经学习了参数&omega;[3]和b[3]。它还得到了一些值，称为$a^{[2]}_{1}$、$a^{[2]}_{2}$、$a^{[2]}_{3}$、$a^{[2]}_{4}$，但这些值也可以变为x1、x2、x3、x4。第三层隐藏层要做的是，找到一种方式使这些值映射到y帽。再看网络左边前三层(包括输入层)，这个网络还有参数&omega;[2]、b[2]、&omega;[1]、b[1]，如果这些参数改变，这些a[2]的值也会变。所以从第三层隐藏层的角度来看，这些隐藏单元的值在不断地改变，所以它就有了“Covariate shift”的问题。 Batch归一化做的，是它减少了这些隐藏值分布变化的数量。batch归一化讲的是当神经网络层更新参数时，batch归一化可以确保无论$z^{[2]}_{1}$、$z^{[2]}_{2}$、$z^{[2]}_{3}$、$z^{[2]}_{4}$怎么变化它们的均值和方差都保持不变。均值和方差是由&beta;[2]和&gamma;[2]决定的值，如果神经网络选择的话，可强制均值为0，方差为1，或其它任何均值和方差。 Batch归一化减少了输入值改变的问题，它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，我们可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。 batch归一化还有一个作用，它有轻微的正则化效果。在mini-batch计算中，由均值和方差缩放的，因为是在mini-batch上计算的均值和方差，而不是在整个数据集上，它只是由一小部分数据估计得出的。所以和dropout相似，它往每个隐藏层的激活值上增加了噪音，dropout有增加噪音的方式，它使一个隐藏的单元，以一定的概率乘以0，以一定的概率乘以1，所以你的dropout含几重噪音，因为它乘以0或1。对比而言，Batch归一化含几重噪音，因为标准偏差的缩放和减去均值带来的额外噪音。这里的均值和标准差的估计值也是有噪音的，所以类似于dropout，Batch归一化有轻微的正则化效果，因为给隐藏单元添加了噪音，这迫使后部单元不过分依赖任何一个隐藏单元，类似于dropout。batch归一化给隐藏层增加了噪音，因此有轻微的正则化效果。因为添加的噪音很微小，所以并不是巨大的正则化效果。如果想得到dropout更大的正则化效果，可以将Batch归一化和dropout一起使用。 测试时的batch normbatch归一化将数据以mini-batch的形式逐一处理，但在测试时，需要对每个样本逐一处理。 在训练时，这些就是用来执行Batch归一化的等式。在一个mini-batch中，你将mini-batch的z(i)值求和，计算均值，所以这里你只把一个mini-batch中的样本都加起来，我用m来表示这个mini-batch中的样本数量，而不是整个训练集。然后计算方差，再算$z^{(i)}_{norm}$，即用均值和标准差来调整，加上ξ是为了数值稳定性。z~(i)是用&gamma;和&beta;再次调整$z^{(i)}_{norm}$得到的。 请注意用于调节计算的μ和σ2是在整个mini-batch上进行计算，但是在测试时，不能将一个mini-batch中的6428或2056个样本同时处理，因此需要用其它方式来得到μ和σ2，而且如果只有一个样本，一个样本的均值和方差没有意义。那么实际上，为了将神经网络运用于测试，就需要单独估算μ和σ2，在典型的Batch归一化运用中，需要用一个指数加权平均来估算，这个平均数涵盖了所有mini-batch，接下来是具体解释。 选择 l 层，假设我们用mini-batch，X[1],X[2],X[3]……以及对应的y值等等，那么在 l 层训练X{1}时，就得到了μ{1}[ l ]。当我们训练第二个mini-batch，我们就得到了μ{2}[ l ]，第三个mini-batch，就得到了μ{3}[1]值。正如我们之前用的指数加权平均来计算&theta;1，&theta;2，&theta;3的均值，当时是试着计算当前气温的指数加权平均，你会这样来追踪你看到的这个均值向量的最新平均值，于是这个指数加权平均就成了你对这一隐藏层的z均值的估值。同样的，你可以用指数加权平均来追踪你在这一层的第一个mini-batch中所见的的σ2值，以及第二个mini-batch中所见的的σ2值等等。因此在用不同的mini-batch训练神经网络的同时，能够得到你所查看的每一层的μ和σ2的平均数的实时数值。最后在测试时，我们只需要z值来计算$z^{(i)}_{norm}$，用μ和σ2的指数加权平均，用手头的最新数值来做调整，然后就可以用刚算出来的znorm和在神经网络训练过程中得到的&beta;和&gamma;参数来计算那个测试样本的z~值。 Softmax回归前面的讲过逻辑回归属于二分类，如果我们有多种可能的类型呢？ 假设我们不止需要识别猫，而是想识别猫、狗和小鸡。把猫当作类1，把狗当作类2，把小鸡当作类3。如果不属于以上任何一类，就分到“其它”或“以上均不符合”这一类，我把它叫做类0。所以上面图中第一张图为3类，第二张图为1类，第三张图为2类，第四张图为0类，依次类推。我们使用C来表示输入会被分入的类别总个数。在这个例子中，我们有四种可能的类别。当有四个分类时，指示类别的数字0-C-1。就是0、1、2、3。 最后一层的隐藏单元为4，为所分的类的数目。输出的值表示属于每个类的概率。它们加起来等于一。 Softmax的具体步骤如下： 在神经网络的最后一层，我们将会想往常一样计算各层的线性部分，z[ l ]是最后一层的z变量。z[ l ]=W[ l ] * a[l-1]+b[ l ]，算出了z后，我们需要应用Softmax激活函数。它的作用是这样的： 我们要计算一个临时变量，我们把它叫做t。它等于ez[ l ]&lt;/sup&gt;，这适用于每个元素。而这里的z[ l ]，在上面这个例子中，维度是4x1的。四维向量t=ez[ l ]，这是对所有元素求幂，t也是一个4x1维向量，然后输出a[ l ]，基本上就是向量t，但是会归一化，使和为1。 换句话说，a[ l ]也是一个4x1维向量。而这个思维向量的第 i 个元素： 讲个例子：假设我们算出了z[ l ]，这是一个四维向量。假设为： 我们要做的就是用这个元素取幂方法来计算t，所以： 接着利用计算器计算得到以下值： 如果把t的元素都加起来，把这四个数字加起来，得到176.3。最终： 看下图： 例如第一个节点，会输出e5/176.3=0.842。这样来说，对于这张图片，它是0类的概率就是84.2%。下个节点输出0.042，也就是4.2%的几率是1类。其它类别以这种规律推出。 神经网络的输出a[ l ]，也就是y帽。是一个4x1维向量，如下： 所以这种算法通过向量z计算出总和为1的四个概率。 之前，我们的激活函数都是接受单行数值输入，例如Sigmoid和ReLu激活函数，输入一个实数，输出一个实数。Softmax激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。 下面是分类的几个例子： 这是一个没有隐藏层的神经网络。他所做的就是z[1]=W[1] * x+b[1]，而输出的a[ l ]=g(z[1])，就是Softmax激活函数。 上面三张图的C=3，下面三张图从左到右的C等于4、5、6。 这显示了Softmax分类器在没有隐藏层的情况下能够做到的事情，当然更深的神经网络会有x，然后是一些隐藏单元，以及更多隐藏单元等等，你就可以学习更复杂的非线性决策边界，来区分多种不同分类。 训练一个Softmax分类器回忆我们之前举得例子，输出层计算的z[ l ]。我们有四个分类，z[ l ]可以是4x1维向量，我们计算了临时变量t。 如果我们的激活函数是Softmax，那么输出是这样的: 简单来说就是用临时变量t将它归一化，使总和为1。于是这就变成了a[ l ]。在这个向量z中，最大的元素是5。最大的概率是0.842。 Softmax这个是与所谓的hardmax对比，hardmax会把z变量变为[1 0 0 0]T，hardmax会观察z的元素，然后在z中最大元素的位置放上1，其它的输出都放0。 Softmax回归或Softmax激活函数将logistic激活函数推广到C类，而不仅仅是两类。而当C=2，那么的Softmax实际上变回了logistic回归，那么输出层将会输出两个数字。如果C=2的话，也许输出0.842和0.158，对吧？这两个数字加起来要等于1，因为它们的和必须为1，其实它们是冗余的，也许你不需要计算两个，而只需要计算其中一个，结果就是你最终计算那个数字的方式又回到了logistic回归计算单个输出的方式。 接下来我们来看怎样训练带有Softmax输出层的神经网络，具体而言，我们先定义训练神经网络使会用到的损失函数。举个例子，我们来看看训练集中某个样本的目标输出，真实标签是[0 1 0 0]T，用上一个视频中讲到过的例子，这表示这是一张猫的图片，因为它属于类1，现在我们假设你的神经网络输出的是 y帽，y帽是一个包括总和为1的概率的向量，y帽=[0.3 0.2 0.1 0.4]T，可以看到总和为1，这就是a[ l ]。对于这个样本神经网络的表现不佳，这实际上是一只猫，但却只分配到20%是猫的概率，所以在本例中表现不佳。 那么我们使用什么损失函数来训练这个神经网络？看下图： 注意在这个样本中，y1=y2=y3=0，因为这些都是0，只有y2=1，如果你看这个求和，所有含有值为0的yi的项都等于0，最后只剩下 -y2logy2帽，因为当你按照下标 j 全部加起来，所有的项都为0，除了j=2，因为y2=1，所以它就等于 -logy2帽。 这就意味着，如果你的学习算法试图将它变小，因为梯度下降法是用来减少训练集的损失的，要使它变小的唯一方式就是使 -logy2帽变小，要想做到这一点，就需要使 y2帽 尽可能大。也就是[0.3 0.2 0.1 0.4]T中的第二个元素。 接下来看看，我们在有Softmax的输出层如何实现梯度下降法。输出层会计算z[1]，它是C x 1维的。在上个例子中，是4 x1维的。然后用Softmax激活函数来得到a[ l ]或者y帽。然后计算出损失。反向传播的关键步骤是这个表达式dz[ l ]=y帽-y。我们用y帽这个4x1向量减去y这个4x1向量。这是对z[ l ]的损失函数的偏导数dz[ l ]=∂J/∂z[ l ]，然后开始反向传播的过程，计算整个神经网络中所需要的所有导数。]]></content>
      <categories>
        <category>深度学习</category>
        <category>改善深层神经网络</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化算法（2）]]></title>
    <url>%2F2019%2F10%2F11%2F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(2)%2F</url>
    <content type="text"><![CDATA[Mini-batch 梯度下降我们之前学过，向量化能够让我们有效地对所有m个样本进行计算，所以我们要把训练样本放到巨大的矩阵X中。 X=[x(1) x(2) x(3) x(4) …… x(n) ]。Y也是如此，Y=[y(1) y(2) y(3) y(4) …… y(n) ]。所以X的维度是(nx,m)，Y的维度是(1,m)。向量化能够让我们相对较快地处理所有样本，如果m很大的话，处理速度仍然缓慢。 相比于mini-batch梯度下降法，我们大家更熟悉的应该是batch梯度下降法，即梯度下降法。那batch梯度下降法和mini-batch梯度下降法有什么区别吗？其实它俩的区别就存在于名字中，一个是batch，即进行梯度下降训练时，使用全部的训练集，而mini-batch，表示比batch小一些，就是指在进行梯度下降训练时，并不使用全部的训练集，只使用其中一部分数据集。 我们知道，不论是梯度下降法还是mini-batch梯度下降法，我们都可以通过向量化(vectorization)更加有效地计算所有样本。既然已经有了梯度下降法，我们为什么还要提出mini-batch梯度下降法呢？在实际计算中，我们可能会遇到特别大的数据集，这时再使用梯度下降法，每次迭代都要计算所有的数据集，计算量太大，效率低下，而mini-batch梯度下降法允许我们拿出一小部分数据集来运行梯度下降法，能够大大提高计算效率。 我们可以把训练集分割成小一点的子集训练，这些子集被取名为mini-batch，假设每一个子集中只要1000个样本，那么把x(1)到x(1000)取出来，将其称为第一个子训练集，也叫做mini-batch，然后我们再取出接下来的1000个样本，从x(1001)到x(2000)，然后再取1000个样本，以此类推。 接下来是吴恩达老师的一个新的符号，把x(1)到x(1000)称为X{1}，x(1001)到x(2000)称为X{2}，如果我们的训练样本一共有500万个，每个mini-batch都有1000个样本。也就是说，你有5000个mini-batch，因为5000乘以1000就是500万。最后得到的是X{5000}，对y进行相同的处理。 mini-batch的数量t组成X{t}和Y{t}，这就是10000个训练样本。包括相应的输入输出对。如果X{1}是一个有1000个样本的训练集，X{1}的维度应该是(nx,1000)，X[2]的应该也是(nx，1000)，以此类推，所有的子集维数都是(nx,1000)，对于Y也是一样的。 之前我们执行前向传播，就是执行z[1]=W[1]X+b[1]。变成mini-batch后呢，把X替换成X[t]，即z[1]=W[1]X{t}+b[1]，然后执行A[1]k=g[1](Z[1])，依次类推，直到A[L]=g[L](Z[L])，这就是我们的预测值。注意：这里一次性处理的是1000个样本不是500万个样本。接着计算成本函数，因为子集规模是1000，所以J=1/1000$\sum_{i=1}^{1000}$L(y(i) 帽,y(i))。 如果我们使用了正则化， 你也会注意到，我们做的一切似曾相识，其实跟之前我们执行梯度下降法如出一辙，除了现在的对象不是X，Y，而是X{t}和Y{t}。接下来，执行反向传播来计算J{t}的梯度，你只是使用X{t}和Y{t}，然后更新加权值，W实际上是W[l]，更新为W[l]=W[l]-adW[l]。对b做相同处理，b[l]=b[l]-adb[l]。这是使用mini-batch梯度下降法训练样本的一步。被称为进行“一代”（1 epoch）的训练。一代这个词意味着只是遍历了一次训练集。我们可以在外围加一个for循环，从1到5000，因为我们有5000个各有1000个样本的组。 使用batch梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用mini-batch梯度下降法，一次遍历训练集，能让你做5000个梯度下降。 理解mini-batch使用batch梯度下降法时，每次迭代都需要历遍整个训练集，可以预期每次迭代成本都会下降，所以如果成本函数是迭代次数的一个函数，它应该会随着每次迭代而减少，如果在某次迭代中增加了，那肯定出了问题，也许学习率太大。 使用mini-batch梯度下降法，如果你作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是X{t}和Y{t}，如果要作出成本函数J{t}的图，而J{t}只和X{t}，Y{t}有关，也就是每次迭代下你都在训练不同的样本集或者说训练不同的mini-batch。 我们需要决定的变量之一是mini-batch的大小，m就是训练集的大小。其中一个极端情况下，mini-batch梯度下降法就是batch梯度下降法。另一个极端情况，假设mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch，当你看第一个mini-batch，也就是X{1}和Y{1}，如果mini-batch大小为1，它就是你的第一个训练样本。接着再看第二个mini-batch，也就是第二个训练样本，采取梯度下降步骤，然后是第三个训练样本，以此类推，一次只处理一个。 实际上你选择的mini-batch大小在二者之间，大小在1和m之间，而1太小了，m太大了。如果取n，每个迭代需要处理大量训练样本，单次迭代耗时太长。如果训练样本不大，batch梯度下降法运行地很好。相反，如果使用随机梯度下降法，如果你只要处理一个样本，那这个方法很好，这样做没有问题，通过减小学习率，噪声会被改善或有所减小，但随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的mini-batch尺寸，实际上学习率达到最快。 首先，如果训练集较小，直接使用batch梯度下降法，样本集较小就没必要使用mini-batch梯度下降法，可以快速处理整个训练集，所以使用batch梯度下降法也很好，这里的少是说小于2000个样本，这样比较适合使用batch梯度下降法。不然，样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini-batch大小是2的n次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。所以我经常把mini-batch大小设成2的次方。 最后需要注意的是在你的mini-batch中，要确保X{t}和Y{t}要符合CPU/GPU内存，取决于你的应用方向以及训练集的大小。如果不符合内存，无论采取什么方法处理数据，结果变得惨不忍睹。 指数加权平均数指数加权平均也叫指数加权移动平均，是一种常用的序列数据处理方式。 它的计算公式是： 其中， θ_t：为第 t 天的实际观察值， V_t: 是要代替 θ_t 的估计值，也就是第 t 天的指数加权平均值， β： 为 V_{t-1} 的权重，是可调节的超参。( 0 &lt; β &lt; 1 ) 下面是一组气温数据，图中横轴为一年中的第几天，纵轴为气温： 直接看上面的数据图会发现噪音很多， 这时，我们可以用 指数加权平均 来提取这组数据的趋势， 按照前面的公式计算： 这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t，将计算后得到的V_t表示出来，得到下图红色线： 可以看出，红色的数据比蓝色的原数据更加平滑，少了很多噪音，并且刻画了原数据的趋势。 指数加权平均，作为原数据的估计值，不仅可以 1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来。 我们可以改变&beta;值，当&beta;=0.98时，得出下图的绿线。当&beta;=0.5，结果是下图的黄线。 理解指数加权平均数上个小节，我们得出下面公式。 我们进一步分析，来理解如何计算出每日温度的平均值 使&beta;=0.9，得出下面公式： 将第二个公式带到第一个，第三个公式带到第二个公式，一次类推，把这些展开： 我们可以将v0，v1，v2等等写成明确的变量，不过在实际中执行的话，你要做的是，一开始将v0初始化为0，然后在第一天使v=&beta;v+(1-&beta;)&theta;1，然后第二天，更新v值，v=&beta;v+(1-&beta;)&theta;2，以此类推，有些人会把v加下标，来表示v是用来计算数据的指数加权平均数。 指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。 指数加权平均的偏差修正偏差修正可以让平均数运算更加准确 在前几节中，如上图，红色曲线对应&beta;为0.9，绿色曲线对应的&beta;=0.98。吴恩达老师说执行下面这个公式： vt=&beta;vt-1+(1-&beta;)&theta;t 得到的就是紫色曲线，而不是绿色曲线。后面紫色和绿色有大部分重合。PS：为什么？同时0.98，紫色曲线的起点低。 计算移动平均数时，初始化v0，v1=0.98v0+0.02&theta;1，因为v0=0，所以0.98v0=0。所以v1=0.02&theta;1，如果第一天温度时40，v1=0.02 x 40=8。因此得到的值会小很多，所以第一天的温度估测不准。 v1=0.98v0+0.02&theta;1，如果带入v1，然后相乘，v2=0.98 x 0.02&theta;1 + 0.02&theta;2，假如&theta;1和&theta;2都是正数，计算后的v2要远小于&theta;2和&theta;1，所以不能很好预估这一年前两天的温度。 有个办法可以修改这一估测，让估测变得更好，更准确，特别是在估测初期，也就是不用vt，而是用vt/1-&beta;t，t就是现在的天数。举个具体例子，当t=2时，1-&beta;t=1-0.982=0.0396，因此对第二天温度的估测变成了 v2/0.0396=(0.0196&theta;1+0.02&theta;2)/0.0396 也就是和的加权平均数，并去除了偏差。你会发现随着t增加，&beta;t接近于0，所以当t很大的时候，偏差修正几乎没有作用.因此当t较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。 动量梯度下降法还有一种算法叫做Momentum，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法。基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新的权重。 如果优化成本函数，函数形状如图，红点代表最小值的位置，假设从这里（蓝色点）开始梯度下降法，如果进行梯度下降法的一次迭代，无论是batch或mini-batch下降法，也许会指向这里，现在在椭圆的另一边，计算下一步梯度下降，结果或许如此，然后再计算一步，再一步，计算下去。我们发现梯度下降法要很多计算步骤。 慢慢摆动到最小值，这种上下波动减慢了梯度下降法的速度，就无法使用更大的学习率，如果要用较大的学习率（紫色箭头），结果可能会偏离函数的范围，为了避免摆动过大，就要用一个较小的学习率。 另一个看待问题的角度是，在竖直方向上，我们希望学习慢一点，因为我们不想要这些摆动。但在水平方向，我们希望快速从左向右移动，移向最小值，移向红点。所以使用动力梯度下降法，在每次迭代中，都会计算dW，db。我们要做的是计算vdW=&beta;vdW+(1-&beta;)dW，接着同样计算vdb=&beta;vdb+(1-&beta;)db。然后重新赋值权重，W=W-avdW，同样b=b-avdW，从而减缓梯度下降的幅度。 举个例子，如果你站在一个地方不动，让你立刻向后转齐步走，你可以迅速向后转然后就向相反的方向走了起来，批梯度下降和随机梯度下降就是这样，某一时刻的梯度只与这一时刻有关，改变方向可以做到立刻就变。而如果你正在按照某个速度向前跑，再让你立刻向后转，可以想象得到吧，此时你无法立刻将速度降为0然后改变方向，你由于之前的速度的作用，有可能会慢慢减速然后转一个弯。 动量梯度下降是同理的，每一次梯度下降都会有一个之前的速度的作用，如果我这次的方向与之前相同，则会因为之前的速度继续加速；如果这次的方向与之前相反，则会由于之前存在速度的作用不会产生一个急转弯，而是尽量把路线向一条直线拉过去。 这就解决了文中第一个图的那个在普通梯度下降中存在的下降路线折来折去浪费时间的问题。 我们有两个超参数，学习率&alpha;以及&beta;参数。&beta;是指数加权平均数，常用值为0.9。 RMSprop前面我们知道动力梯度下降法可以加快梯度下降，还有一个叫做RMSprop的算法，全称是root mean square prop算法，它也可以加速梯度下降，我们来看看它是如何运作的。 复习前面的内容，如果我们执行梯度下降，虽然横轴方向正在推进，但纵轴方向会有大幅动摆动。所以，我们想减缓纵轴方向的学习，同时加快横轴方向的学习(至少不减慢)。RMSprop算法可以实现这个目标。 在第t次迭代中，该算法会找出计算mini-batch的微分dW，db。这里我们不用vdW，而是用到新符号SdW。因此： SdW=&beta;SdW+(1-&beta;)(dW)2，这里的平方是对整个dW平方。 Sdb=&beta;Sdb+(1-&beta;)db2，这里的平方也是对整个db平方。 接着，RMSprop会更新参数值。如下： 我们希望，SdW相对较少，Sdb较大。因为垂直方向的微分要比水平方向的大得多，所以斜率在垂直方向特别大。所以db较大，dW较小。db的平方较大，所以Sdb的平方较大。dW会小，SdW也会小。结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动，而水平方向的更新则被较小的数相除。 RMSprop的影响就是你的更新最后会变成这样（绿色线），纵轴方向上摆动较小，而横轴方向继续推进。还有个影响就是，你可以用一个更大学习率a，然后加快学习，而无须在纵轴上垂直方向偏离。 Adam优化算法Adam优化算法基本上就是将Momentum和RMSprop结合在一起，那么来看看如何使用Adam算法。 首先，初始化 然后用当前的 mini-batch 计算出 dW 和 db 接下来计算 Momentum 指数加权平均数： 上图中用到的&beta;1是为了和下面的RMSprop公式中用到的&beta;2相互区分 一般运用 Adam 算法的时候，我们还要对 v 和 S 的偏差进行修正： 然后就是权重的更新： 完整的如下所示： 本算法有很多超参数，分别有a、&beta;1、&beta;2、ξ四个超参数。a需要调试，尝试一系列的值，然后看哪个有效。&beta;1常用的值为0.9。至于&beta;2，推荐使用0.999，ξ为10-8。 学习率衰减加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。 假设使用mini-batch梯度下降法，mini-batch数量不大，大概64或者128个样本，在迭代过程中会有噪音（蓝色线），下降朝向这里的最小值，但是不会精确地收敛，所以你的算法最后在附近摆动，并不会真正收敛，因为你用的a是固定值，不同的mini-batch中有噪音。 但是如果要减少学习率的话，在初期的时候，a学习率还比较大，我们的学习还是相对较快，但随着a变小，我们的步伐也会变慢。而不是在训练过程中，大幅度在最小值附近摆动。 所以慢慢减少a的本质在于，在学习初期，承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。 我们应该拆分成不同的mini-batch。第一次遍历训练集叫做第一代，第二代就是第二代。以此类推，我们得出公式： decay-rate称为衰减率，epoch-num为代数，&alpha;0为初始学习率），注意这个衰减率是另一个你需要调整的超参数。 当然还有其它衰减： 方法二：指数衰减 方法三： 方法四： 方法五：手动衰减 如果一次只训练一个模型，如果花上数小时或数天来训练，看看自己的模型训练，耗上数日，学习速率变慢了，然后把&alpha;调小一点。但这种方法只是在模型数量小的时候有用。 局部最优也许我们想优化一些参数，我们把它们称之为W1和W2，平面的高度就是损失函数。在图中似乎各处都分布着局部最优。梯度下降法或者某个算法可能困在一个局部最优中，而不会抵达全局最优。如果要作图计算一个数字，比如说这两个维度，就容易出现有多个不同局部最优的图，而这些低维的图曾经影响了我们的理解，但是这些理解并不正确。事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。 在高维度空间，我们更可能碰到鞍点。 首先，你不太可能困在极差的局部最优中，条件是你在训练较大的神经网络，存在大量参数，并且成本函数J被定义在较高的维度空间。 第二点，平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或是RMSprop，Adam这样的算法，能够加速学习算法的地方。在这些情况下，更成熟的优化算法，如Adam算法，能够加快速度，让你尽早往下走出平稳段。 名词解释：平稳段是一段区域，其中导数长时间接近于0。]]></content>
      <categories>
        <category>深度学习</category>
        <category>改善深层神经网络</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化算法（1）]]></title>
    <url>%2F2019%2F10%2F08%2F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95(1)%2F</url>
    <content type="text"><![CDATA[在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高效的神经网络。训练神经网络时，我们需要做出很多决策，例如：神经网络分多少层、每层含有多少个隐藏单元、学习速率是多少、各层采用哪些激活函数等问题。 训练、验证和测试集我们通常会将这些数据划分成三部分，一部分作为训练集（train set），一部分作为简单交叉验证集，有时也称之为验证集，方便起见，我就叫它验证集（dev set），其实都是同一个概念。最后一部分则作为测试集（test set）。 接下来，我们开始对训练执行算法，通过验证集或简单交叉验证集选择最好的模型，经过充分验证，我们选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况。 在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。 在机器学习中，如果只有一个训练集和一个验证集，而没有独立的测试集，遇到这种情况，训练集还被人们称为训练集，而验证集则被称为测试集。 那么验证集和测试集有什么区别呢？为什么要划分训练集、验证集和测试集？ 训练集用于训练模型参数，测试集用于估计模型对样本的泛化误差，验证集用于“训练”模型的超参数。 偏差（Bias）、方差（Variance） 假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（high bias）的情况，我们称为“欠拟合”（underfitting）。 相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（high variance），数据过度拟合（overfitting）。 在两者之间，可能还有复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（just right）是介于过度拟合和欠拟合中间的一类。 下面举例子： 假定训练集误差是1%，为了方便论证，假定验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，我们可能过度拟合了训练集，在某种程度上，验证集并没有充分利用交叉验证集的作用，像这种情况，我们称之为“高方差”。 通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有高方差。也就是说衡量训练集和验证集误差就可以得出不同结论。 假设训练集误差是15%，我们把训练集误差写在首行，验证集误差是16%，假设该案例中人的错误率几乎为0%，人们浏览这些图片，分辨出是不是猫。算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，就可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，所以这种算法偏差高，因为它甚至不能拟合训练集，这与上一张图最左边的图片相似。 上面的分析都是基于假设预测，假设人眼辨别的错误率接近0%。最优误差也被称为贝叶斯误差。如果最优误差或贝叶斯误差非常高，比如15%。我们再看看上面这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低。 正则化（Rugularization）如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差，下面我们就来讲讲正则化的作用原理。 L2正则化我们用逻辑回归讲解原理。 &lambda;/2m乘以&omega;范数的平方。&omega;是欧几里得范数，&omega;的平方等于&omega;j (j值从1到nx)，也可以表示为&omega;T&omega;。此方法称为L2正则化。这里的&lambda;就是正则化参数。因为这里使用了欧几里得法线，被称为向量参数&omega;的L2范式。 为什么只正则化参数&omega;呢？我们可以加上参数b吗？我们可以这么做，但是一般习惯省略不写。因为&omega;通常是一个高维参数矢量，已经可以表达高偏差问题，&omega;可能包含很多参数，我们不可能拟合所有参数，而b只是单个数字，所以&omega;几乎涵盖所有参数，而不是b。如果加了参数b，没什么影响。因为b也是众多参数的一个，想加就加，没有问题。 L2正则化是最常见的正则化类型，还有L1正则化。L1正则化加的不是L2范式，而是正则项 &lambda;/m乘以$\sum_{j=1}^n $ |&omega;|，这也被称为参数&omega;向量的L1范数。(这里的n就是项数，和L2范式的nx一样) 如果用的L1是正则化，最终会是稀疏的，也就是说&omega;向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是L1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用L2正则化。 为什么正则化有利于预防过拟合？如果正则化&lambda;设置得足够大，权重矩阵W被设置为接近于0的值（我也没看懂）。实际上不会发生这种情况的。直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近高偏差状态。 假设我们用tanh作为我们的激活函数，用g(z)表示tanh(z)。那么我们发现，只要z非常小，z只涉及少量参数。这里我们利用双曲正切函数的线性状态，只要z可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。 总结一下，如果正则化参数变得很大，参数W很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上，z的取值范围很小，这个激活函数，也就是曲线函数tanh会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。 dropout正则化除了L2正则化，还有一个非常使用的正则化方法——Dropout(随机失活)。 假设你在训练上图这样的神经网络，它存在过拟合，这就是dropout所要处理的，我们复制这个神经网络，dropout会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练。如下图： 这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。不过可想而知，我们针对每个训练样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络。 如何实施dropout？吴恩达老师讲了最常用的方法。就是inverted dropout(随机失活)。 首先定义变量d，d3表示一个三层的dropout向量: d3 = np.random.rand(a3.shape[0],a3.shape[1]) 然后看它是否小于某数，我们称之为keep-prob，keep-prob是一个具体数字，上个示例中它是0.5，而假设在本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2。keep-prob的作用是生成随机矩阵，如果对a3进行因子分解，效果也是一样的。d3是一个矩阵，其中d3中的值为1的概率都是0.8，对应为0的概率是0.2。 接下来要做的就是从第三层中获取激活函数，这里我们叫它a3，a3含有要计算的激活函数，等于上面的a3乘以d3 a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为a3*=d3。 它的作用就是让过滤d3中所有等于0的元素，而各个元素等于0的概率只有20%，乘法运算最终把d3中相应元素归零，即让d3中0元素与a3中相对元素归零。 最后，我们向外扩展a3。用它除以0.8，或者除以keep-prob参数。 a3/= (keep-prob) 解释一下最后一步，我们假设第三隐藏层上有50个神经元，保留和删除它们的概率分别为80%和20%，这意味着最后被删除或归零的单元平均有10（50×20%）个。现在我们看下z[4]，z[4]=&omega;[4]*a[3]+b[4]，我们的预期是，a[3]减少20%，也就是说a[3]中有20%的元素被归零。为了不影响z[4]的期望值，我们需要用&omega;[4]a[3]/0.8，它将会修正或弥补我们所需的那20%，a[3]的期望值不会变。 它的功能是，不论keep-prop的值是多少0.8，0.9甚至是1，如果keep-prop设置为1，那么就不存在dropout，因为它会保留所有节点。反向随机失活（inverted dropout）方法通过除以keep-prob，确保a[3]的期望值不变。 代码实现如下: 12345678910Z2 = np.dot(W2, A1) + b2A2 = relu(Z2)# 随机生成和 A2 相同维度的矩阵D2 = np.random.rand(A2.shape[0], A2.shape[1]) # 保留部分元素: 小于keep_prob的设为 True, 否则设为 0 D2 = D2 &lt; keep_prob # 对应位置的元素相乘, 可以把 True 看做 1, False 看做 0. A2 = np.multiply(A2, D2) # 如果没有下面这行就是普通的 DropoutA2 = A2/keep_prob 要注意的是, 我们不希望在测试的时候, 得到的结果也是随机的, 因此Dropout 在测试过程不使用。 理解dropoutDropout可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？ 直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，所以不愿意给任何一个输入加上太多权重。因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果。和之前讲的L2正则化类似，实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化。 实施dropout的另一个细节是，这是一个拥有三个输入特征的网络，其中一个要选择的参数是keep-prob，它代表每一层上保留单元的概率。所以不同层的keep-prob也可以变化。第一层，矩阵W[1]是7×3，第二个权重矩阵W[2]是7×7，第三个权重矩阵W[3]是3×7，以此类推，W[2]是最大的权重矩阵，因为W[2]拥有最大参数集，即7×7，为了预防矩阵的过拟合，对于这一层，它的keep-prob值应该相对较低，假设是0.5。对于其它层，过拟合的程度可能没那么严重，它们的keep-prob值可能高一些，可能是0.7甚至更高，假设这里是0.7。如果在某一层，我们不必担心其过拟合的问题，那么keep-prob可以为1。 总结：如果我们担心某些层比其他层更容易发生过拟合，可以把某些层的keep-prob值设置得比其他层更低，缺点是为了使用交叉验证，我们要搜索更多得超级参数。另一种方案是在一些层上应用dropout，而有些层不用dropout应用。dropout的层只含有keep-prob这一个超参数。 dropout一大缺点就是代价函数J不再被明确定义，因为每次迭代都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。所以吴恩达老师推荐通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。 其它正则化方法数据扩增假设我们正在拟合猫咪图片分类器，如果我们想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。 除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。 通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。 对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字4看起来是波形的，其实不用对数字4做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个4看起来有点扭曲。所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。 early stopping 因为在训练过程中，我们希望训练误差，代价函数J都在下降，通过early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升。early stopping 代表提早停止训练神经网络。 当我们还未在神经网络上运行太多迭代过程的时候，参数&omega;接近0，因为随机初始化&omega;值时，它的值可能都是较小的随机值，所以在我们长期训练神经网络之前&omega;依然很小，在迭代过程和训练过程中&omega;的值会变得越来越大，比如在这儿，神经网络中参数&omega;的值已经非常大了，所以early stopping要做就是在中间点停止迭代过程，我们得到一个&omega;值中等大小的弗罗贝尼乌斯范数，与L2正则化相似，选择参数&omega;范数较小的神经网络，但愿那时的神经网络过度拟合不严重。 early stopping 有一个缺点，接下来了解一下。 在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。在重点优化代价函数时，你只需要留意&omega;和b，J(&omega;,b)的值越小越好，你只需要想办法减小这个值，其它的不用关注。然后，预防过拟合还有其他任务，就是减少方差”。 缺点就是我们不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数J，因为现在你不再尝试降低代价函数J，所以代价函数J的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。 归一化输入假设我们有一个数据集，它有两个输入特征，所以数据是二维的。下图即为数据集散点图: 训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤： 零均值化 归一化方差； 我们希望无论是训练集还是测试集都是通过相同的μ和&sigma;2定义的数据转换。 第一步是零均值化。 μ=1/m x $\sum_{i=1}^m$x(i) μ是一个向量，x等于每个训练数据x减去μ，意思是移动数据集，直到完成零均值化。 输入数据经过零均值后，得出下面的散点图： 第二步是归一化方差，注意特征x1的方差比特征x2的方差要大得多。 μ=1/m x $\sum_{i=1}^m$(x(i))2 &sigma;2是一个向量，它的每个特征都有方差。经过归一化方差后，得出下面结果： 如果你用它来调整训练数据，那么用相同的μ和σ2来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论的μ值是什么，也不论的σ2值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估μ和σ2。因为我们希望不论是训练数据还是测试数据，都是通过相同μ和σ2定义的相同数据转换，其中μ和σ2是由训练集数据计算得来的。 为什么我们需要归一化输入特征？回想一下代价函数 J(&omega;,b)=1/m x $\sum_{i=1}^m$L(y(i) hat , y(i)) 如果输入未归一化的输入特征，代价函数如下： 然而如果归一化特征，代价函数平均起来看更对称，如果要在上图这样的代价函数上运行梯度下降法，必须使用一个非常小的学习率。因为如果是在这个位置（蓝色箭头所在位置），梯度下降法可能需要多次迭代过程，直到最后找到最小值。但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，我们可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行。 下图即为输入归一化输入特征的代价函数： 如果输入特征处于不同范围内，可能有些特征值从0到1，有些从1到1000，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。 梯度消失/梯度爆炸训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。 吴恩达老师给我们的理解是：权重W只比1略大一点（或者说比单位矩阵略大一点），深度神经网络的激活函数将爆炸式增长，如果W比单位矩阵略小一点，激活函数将以指数级递减。 神经网络的权重初始化为了避免上述的梯度爆炸，权重的初始化很重要。首先，我们来看看只有一个神经元的情况： 假设b=0，可以得到： Z=W1x1+W2x2+W3x3+……+Wnxn，b=0。 为了不让Z那么大，我们尽量让Wi小一些。实际做法如下： ​ 使用Relu函数，W的初始化方法： ​ ​ 使用tanh函数，W的初始化方法： 其中, n[l−1] 表示第l层的输入特征的个数, 也就是第 l - 1 层神经元的个数.]]></content>
      <categories>
        <category>深度学习</category>
        <category>改善深层神经网络</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸验证和神经风格转换]]></title>
    <url>%2F2019%2F10%2F05%2F%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[One—Shot学习人脸识别所面临的一个挑战就是需要解决一次学习问题，这意味着在大多数人脸识别应用中，需要通过单单一张图片或者单单一个人脸样例就能去识别这个人。而在我们的学习过程中，发现深度学习只有一个训练样例时，它的表现并不好。接下来解决一下这个问题。 假设我们的数据库有以下四张照片： 有一种办法是，将人的照片放进卷积神经网络中，使用softmax单元来输出4种，或者说5种标签，分别对应这4个人，或者4个都不是，所以softmax里我们会有5种输出。但实际上这样效果并不好，因为如此小的训练集不足以去训练一个稳健的神经网络。而且，假如有新人加入团队，我们现在将会有5个组员需要识别，所以输出就变成了6种，这时你要重新训练神经网络吗？这听起来实在不像一个好办法。 所以要让人脸识别能够做到一次学习，为了能有更好的效果，现在要做的应该是学习Similarity函数。详细地说，想要神经网络学习这样一个用d表示的函数：d(img1,img2)=degree of difference between images。 它以两张图片作为输入，然后输出这两张图片的差异值。如果你放进同一个人的两张照片，你希望它能输出一个很小的值，如果放进两个长相差别很大的人的照片，它就输出一个很大的值。所以在识别过程中，如果这两张图片的差异值小于某个阈值τ，它是一个超参数，那么这时就能预测这两张图片是同一个人，如果差异值大于τ，就能预测这是不同的两个人，这就是解决人脸验证问题的一个可行办法。 Siamese网络通过上一小节我们知道我们该怎么去做人脸识别，通过输入两张图片。它将让你解决一次学习问题。接下来我们学习如何训练我们的神经网络学会这个函数d。 看上图，我们经常看到这样的卷积网络，输入图片，然后通过一些列卷积，池化和全连接层，最终得到编号1这样的特征向量。有时这个会被送进softmax单元来做分类。但是我们关注的重点是编号1，假如它有128个数，它是由网络深层的全连接层计算出来的，我们要给这128个数命个名字，把它叫做f(x(1))。可以把f(x((1))看作是输入图像x(1)的编码，取这个输入图像（编号2），然后表示成128维的向量。 建立一个人脸识别系统的方法就是，如果你要比较两个图片的话，例如这里的第一张（编号1）和第二张图片（编号2），你要做的就是把第二张图片喂给有同样参数的同样的神经网络，然后得到一个不同的128维的向量（编号3），这个向量代表或者编码第二个图片，我们把第二张图片的编码叫做f(x(2))。这里我用x(1)和x(2)仅仅代表两个输入图片，它们没必要非是第一个和第二个训练样本，可以是任意两个图片。最后如果相信这些编码很好地代表了这两个图片，我们要做的就是定义d，将x(1)和x(2)的距离定义为这两幅图片的编码之差的范数。 那么怎么训练这个Siamese神经网络呢？不要忘了这两个网络有相同的参数，所以实际要做的就是训练一个网络，它计算得到的编码可以用于函数d，它可以告诉我们两张图片是否是同一个人。更准确地说，神经网络的参数定义了一个编码函数，如果给定输入图像，这个网络会输出的128维的编码。你要做的就是学习参数，使得如果两个图片和是同一个人，那么你得到的两个编码的距离就小。相反，如果和是不同的人，那么我们会想让它们之间的编码距离大一点。 训练神经网络训练神经网络两种方法：Triplet 损失 和 人脸识别二分类 Triplet损失要想学习神经网络参数来得到优质的人脸图片编码，方法之一就是定义三元组损失函数然后应用梯度下降。看到这个的时候，我也一脸懵逼。这是什么啊？接下来我们一起看下。 我们看下这是什么意思，为了应用三元组损失函数，你需要比较成对的图像，比如这个图片，为了学习网络的参数，你需要同时看几幅图片，比如这对图片（编号1和编号2），你想要它们的编码相似，因为这是同一个人。然而假如是这对图片（编号3和编号4），你会想要它们的编码差异大一些，因为这是不同的人。 用三元组损失的术语来说，我们要做的通常是看一个 Anchor 图片，想让Anchor图片和Positive图片（Positive意味着是同一个人）的距离很接近。然而，当Anchor图片与Negative图片（Negative意味着是非同一个人）对比时，我们会想让他们的距离离得更远一点。 这就是为什么叫做三元组损失，它代表你通常会同时看三张图片，你需要看Anchor图片、Postive图片，还有Negative图片，我要把Anchor图片、Positive图片和Negative图片简写成A、P、N。 把以上内容写成公式的话，d= ||f(A)-f(P)||2。我们希望||f(A)-f(P)||2 &lt;= ||f(A)-f(N)||2。||f(A)-f(P)||2 就是d(A,P)，||f(A)-f(N)||2时d(A,N)，我们可以把d看为距离函数。 对表达式修改一下，因为有一种情况满足这个表达式，但是没有用处，就是把所有的东西都学成0，如果总是输出0，即0-0≤0，这就是0减去0还等于0，如果所有图像的都是一个零向量，那么总能满足这个方程f。所以为了确保网络对于所有的编码不会总是输出0，也为了确保它不会把所有的编码都设成互相相等的。另一种方法能让网络得到这种没用的输出，就是如果每个图片的编码和其他图片一样，这种情况，还是得到0-0。 为了阻止网络出现上述状况，我们需要修改表达式。也就是||f(A)-f(P)||2 - ||f(A)-f(N)||2不能刚好小于等于0。应该是比0还要小，所以这个应该小于一个-a，也就是||f(A)-f(P)||2 - ||f(A)-f(N)||2 &lt;= -a。这里的a是一个超参数，是为了阻止网络输出无用的结果。 举个例子，假如间隔设置成0.2，如果在这个例子中，d(A,P)=0.5，如果 Anchor和 Negative图片的，即只大一点，比如说0.51，条件就不能满足。虽然0.51也是大于0.5的，但还是不够好，我们想要比大很多。你会想让这个值d(A,N)至少是0.7或者更高，这样间距至少达到0.2，你可以把这项调大或者这个调小。超参数a至少是0.2，在d(A,P)和d(A,N)之间至少相差0.2，这就是间隔参数的作用。 接下来定义损失函数。 这个max函数的作用就是，只要这个||f(A)-f(P)||2 - ||f(A)-f(N)||2+a &lt;= 0，那么损失函数就是0。另一方面如果||f(A)-f(P)||2 - ||f(A)-f(N)||2+a&gt;=0，然后取最大值。最后得到||f(A)-f(P)||2 - ||f(A)-f(N)||2+a，这样就会得到一个正的损失值。通过最小化这个损失函数达到的效果就是使这部分||f(A)-f(P)||2 - ||f(A)-f(N)||2+a小于等于0，只要这个损失函数小于等于0，网络不会关心它负值有多大。 这是一个三元组定义的损失，整个网络的代价函数应该是训练集中这些单个三元组损失的总和。假如你有一个10000个图片的训练集，里面是1000个不同的人的照片，你要做的就是取这10000个图片，然后生成这样的三元组，然后训练你的学习算法，对这种代价函数用梯度下降，这个代价函数就是定义在你数据集里的这样的三元组图片上。 注意，为了定义三元组的数据集你需要成对的A和P，即同一个人的成对的图片，为了训练你的系统你确实需要一个数据集，里面有同一个人的多个照片。这也是为什么在这个例子中，我说假设你有1000个不同的人的10000张照片，也许是这1000个人平均每个人10张照片，组成了你整个数据集。如果你只有每个人一张照片，那么根本没法训练这个系统。当然，训练完这个系统之后，你可以应用到你的一次学习问题上，对于你的人脸识别系统，可能你只有想要识别的某个人的一张照片。但对于训练集，你需要确保有同一个人的多个图片，至少是你训练集里的一部分人，这样就有成对的Anchor和Positive图片了。 那么我们该怎么选择这些三元组来形成训练集呢？一个问题是如果从训练集中，随机地选择A、P和N，遵守A和P是同一个人，而 A 和 N 是不同的人这一原则。有个问题就是，如果随机的选择它们，那么这个约束条件 d(A,P)+a&lt;=d(A,N) 很容易达到，因为随机选择的图片，A和N 比 A和P差别很大的概率很大。所以有很大的可能性|| f(A)-f(N) ||会比||(f(A) -f(P) )||大，而且差距远大于a，这样网络并不能从中学到什么。 所以为了构建一个数据集，要做的就是尽可能选择难训练的三元组A、P和N。具体而言，你想要所有的三元组都满足这个条件 d(A,P)+a&lt;=d(A,N)。难训练的三元组就是，你的A、P和N的选择使得很接近，即d(A,P)约等于d(A,N)。这样的学习算法会竭尽全力使d(A,N)式子变大，或者使d(A,P)变小，这样左右两边至少有一个的间隔。并且选择这样的三元组还可以增加你的学习算法的计算效率，如果随机的选择这些三元组，其中有太多会很简单，梯度算法不会有什么效果，因为网络总是很容易就得到了正确的结果，只有选择难的三元组梯度下降法才能发挥作用，使得这两边离得尽可能远。 人脸验证与二分类上述的Triplet loss是一个学习人脸识别卷积网络参数的好方法。另一个训练神经网络的方法是选取一对神经网络Siamese网络，使其同时计算这些嵌入，比如说128维的嵌入（编号1），或者更高维，然后将其输入到逻辑回归单元，然后进行预测，如果是相同的人，那么输出是1，若是不同的人，输出是0。这就把人脸识别问题转换为一个二分类问题，训练这种系统时可以替换Triplet loss的方法。 最后的逻辑回归单元输出 y帽： f(x(i))k 代表图片x(i) 的编码，下标 k代表选择这个向量中的第k个元素。| f(x(i))k -f(x(i))k |是对着两个编码取元素差的绝对值。把这128个元素当作特征，然后把他们放入逻辑回归中，最后的逻辑回归可以增加参数&omega;i 和 b，就像普通的逻辑回归一样。你将在这128个单元上训练合适的权重，用来预测两张图片是否是一个人，这是一个很合理的方法来学习预测0或者1，即是否是同一个人。 但是在这个学习公式中，输入是一对图片，这是你的训练输入x（编号1、2），输出y是0或者1，取决于你的输入是相似图片还是非相似图片。与之前类似，你正在训练一个Siamese网络，意味着上面这个神经网络拥有的参数和下面神经网络的相同（编号3和4所示的网络），两组参数是绑定的，这样的系统效果很好。 如果这是一张新图片（编号1），当员工走进门时，希望门可以自动为他们打开，这个（编号2）是在数据库中的图片，不需要每次都计算这些特征（编号6），不需要每次都计算这个嵌入，你可以提前计算好，那么当一个新员工走近时，你可以使用上方的卷积网络来计算这些编码（编号5），然后使用它，和预先计算好的编码进行比较，然后输出预测值。因此不需要存储原始图像，如果你有一个很大的员工数据库，你不需要为每个员工每次都计算这些编码。这个预先计算的思想，可以节省大量的计算。 神经风格迁移这是卷积神经网络最有趣的应用。什么是神经风格迁移？ 看例子： 为了描述如何实现神经网络迁移，我将使用来C表示内容图像，S表示风格图像，G表示生成的图像。这只是一个提出，在深入了解如何实现神经风格迁移之前，我们先看神经网络不同层之间的具体运算。 CNN特征可视化其实我一直觉得神经网络的解释性真的蛮差的。让我们接下来看一下，深度卷积网络到底在学什么？ 看不懂，之后再补。。。。。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>人脸验证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测组件]]></title>
    <url>%2F2019%2F10%2F03%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[目标定位图片分类任务我们已经熟悉了，就是算法遍历图片，判断其中的对象是不是汽车，这就是图片分类。这节课我们要学习构建神经网络的另一个问题，即定位分类问题。这意味着，我们不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来，这就是定位分类问题。其中“定位”的意思是判断汽车在图片中的具体位置。之后，我们再讲讲当图片中有多个对象时，应该如何检测它们，并确定出位置。比如，你正在做一个自动驾驶程序，程序不但要检测其它车辆，还要检测其它对象，如行人、摩托车等等，稍后我们再详细讲。 本节我们要研究的分类定位问题，通常只有一个较大的对象位于图片中间位置，我们要对它进行识别和定位。而在对象检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。因此，图片分类的思路可以帮助学习分类定位，而对象定位的思路又有助于学习对象检测，我们先从分类和定位开始讲起。 图片分类问题你已经并不陌生了，例如，输入一张图片到多层卷积神经网络。这就是卷积神经网络，它会输出一个特征向量，并反馈给softmax单元来预测图片类型。 如果你正在构建汽车自动驾驶系统，那么对象可能包括以下几类：行人、汽车、摩托车和背景，这意味着图片中不含有前三种对象，也就是说图片中没有行人、汽车和摩托车，输出结果会是背景对象，这四个分类就是softmax函数可能输出的结果。 下面是如何为监督学习任务定义目标标签y，请注意，这有四个分类，神经网络输出的是这四个数字和一个分类标签，或分类标签出现的概率。目标标签y的定义如下： 它是一个向量，第一个组件pc表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则pc=1，如果是背景，则图片中没有要检测的对象，则pc=0 。我们可以这样理解pc，它表示被检测对象属于某一分类的概率，背景分类除外。 如果检测到对象，就输出被检测对象的边界框参数bx、by、bh和bw。最后，如果存在某个对象，那么pc，同时输出c1、c2 和c3，表示该对象属于1-3类中的哪一类，是行人，汽车还是摩托车。鉴于我们所要处理的问题，我们假设图片中只含有一个对象，所以针对这个分类定位问题，图片最多只会出现其中一个对象。 我们再看几个例子，假如下图是一张训练集图片。在y当中，第一个元素pc，因为图中有一辆车，bx、by、bh和bw会指明边界框的位置，所以标签训练集需要标签的边界框。图片中是一辆车，所以结果属于分类2，因为定位目标不是行人或摩托车，而是汽车，所以c1=0，c2=1，c3=0，c1、c2和c3中最多只有一个等于1。 上图是只有一个检测对象的情况，如果图片中没有检测对象呢？看下图 这种情况下，pc=0，y的其它参数将变得毫无意义。这里我全部写成问号，表示”毫无意义“的参数，因为图片中不存在检测对象，所以不用考虑网络输出中边界框的大小。也不用考虑图片中的对象是属于c1、c2、c3中的哪一类。 最后，我们介绍一下神经网络的损失函数，其参数为类别y和网络输出y帽，如果采用平方误差策略，则 损失值等于每个元素相应差值的平方和。 如果图片中存在定位对象，那么y1 = 1，所以y1 = pc，同样地，如果图片中存在定位对象，pc = 1，损失值就是不同元素的平方和。 另一种情况是，y1=0，也就是pc=0，损失值是(y1帽 - y1)^2，因为对于这种情况，我们不用考虑其它元素，只需要关注神经网络输出pc的准确度。 特征点检测假设我们正在构建一个人脸识别应用，如下图。出于某种原因，我们希望算法可以给出眼角的具体位置。眼角坐标为(x,y)，你可以让神经网络的最后一层多输出两个数字 lx 和 ly，作为眼角的坐标值。如果你想知道两只眼睛的四个眼角的具体位置，那么从左到右，依次用四个特征点来表示这四个眼角。对神经网络稍做些修改，输出第一个特征点（l1x，l1y），第二个特征点（l2x，l2y），依此类推，这四个脸部特征点的位置就可以通过神经网络输出了。 也许除了这四个特征点，你还想得到更多的特征点输出值，这些（图中眼眶上的红色特征点）都是眼睛的特征点，你还可以根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。如下图，为了便于说明，你可以设定特征点的个数，假设脸部有64个特征点，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。 具体做法是，准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1或0，1表示有人脸，0表示没有人脸，然后输出（l1x，l1y）……直到（l64x，l64y）。这里我用 l 代表一个特征，这里有129个输出单元，其中1表示图片中有人脸，因为有64个特征，64×2=128，所以最终输出128+1=129个单元。 最后一个例子，看下图。如果你对人体姿态检测感兴趣，你还可以定义一些关键特征点，假设有32个，如胸部的中点，左肩，左肘，腰等等。然后通过神经网络标注人物姿态的关键特征点，再输出这些标注过的特征点，就相当于输出了人物的姿态动作。当然，要实现这个功能，你需要设定这些关键特征点，从胸部中心(l1x，l1y)一直向下，直到（l32x，l32y）。 目标检测学过了对象定位和特征点检测，今天我们来构建一个对象检测算法。接下来，我们将学习如何通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。 假如你想构建一个汽车检测算法，步骤是，首先创建一个标签训练集，也就是x和y表示适当剪切的汽车图片样本，这张图片（编号1）x是一个正样本，因为它是一辆汽车图片，这几张图片（编号2、3）也有汽车，但这两张（编号4、5）没有汽车。出于我们对这个训练集的期望，你一开始可以使用适当剪切的图片，就是整张图片x几乎都被汽车占据，你可以照张照片，然后剪切，剪掉汽车以外的部分，使汽车居于中间位置，并基本占据整张图片。有了这个标签训练集，你就可以开始训练卷积网络了，输入这些适当剪切过的图片（编号6），卷积网络输出y，0或1表示图片中有汽车或没有汽车。训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下。 假设上图这是一张测试图片，首先选定一个特定大小的窗口，比如图片下方这个窗口，将这个红色小方块输入卷积神经网络，卷积网络开始进行预测，即判断红色方框内有没有汽车。 滑动窗口目标检测算法接下来会继续处理第二个图像，即红色方框稍向右滑动之后的区域，并输入给卷积网络，因此输入给卷积网络的只有红色方框内的区域，再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落。 接着我们重复上述过程，不过这次我们选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，你可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或1。如下图。 再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果。 这种算法叫作滑动窗口目标检测，因为我们以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。 滑动窗口目标检测算法也有很明显的缺点，就是计算成本，因为你在图片中剪切出太多小方块，卷积网络要一个个地处理。如果你选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本。 滑动窗口的卷积实现为了构建滑动窗口的卷积应用，首先要知道如何把神经网络的全连接层转化为卷积层。 假设对象检测算法输入一个14×14×3的图像，图像很小，不过演示起来方便。在这里过滤器大小为5×5，数量是16，14×14×3的图像在过滤器处理之后映射为10×10×16。然后通过参数为2×2的最大池化操作，图像减小到5×5×16。然后添加一个连接400个单元的全连接层，接着再添加一个全连接层，最后通过softmax单元输出。为了跟下图区分开，我先做一点改动，用4个数字来表示，它们分别对应softmax单元所输出的4个分类出现的概率。这4个分类可以是行人、汽车、摩托车和背景或其它对象。 画一个这样的卷积网络，它的前几层和之前的一样，而对于下一层，也就是这个全连接层，我们可以用5×5的过滤器来实现，数量是400个（编号1所示），输入图像大小为5×5×16，用5×5的过滤器对它进行卷积操作，过滤器实际上是5×5×16，因为在卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致，输出结果为1×1。假设应用400个这样的5×5×16过滤器，输出维度就是1×1×400，我们不再把它看作一个含有400个节点的集合，而是一个1×1×400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5×5×16维度的过滤器，所以每个值都是上一层这些5×5×16激活值经过某个任意线性函数的输出结果。接着我们再添加另外一个卷积层（编号2所示），这里用的是1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，它其实就是上个网络中的这一全连接层。最后经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，我们最终得到这个1×1×4的输出层，而不是这4个数字（编号3所示）。以上就是用卷积层代替全连接层的过程，结果这几个单元集变成了1×1×400和1×1×4的维度。 掌握了卷积知识，我们再看看如何通过卷积实现滑动窗口对象检测算法。 假设向滑动窗口卷积网络输入14×14×3的图片，为了简化演示和计算过程，这里我们依然用14×14的小图片。和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4，我画得比较简单，严格来说，14×14×3应该是一个长方体，第二个10×10×16也是一个长方体。为了方便，立体部分随便画了。 上图是输入给卷积网络的图片大小是14×14×3，测试集图片是16×16×3，如下图。现在给这个输入图片加上黄色条块，在最初的滑动窗口算法中，你会把这片蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。我们在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签。 把这两个图放一起： 结果发现，这4次卷积操作中很多计算都是重复的。所以执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在这一步操作中（编号1），卷积网络运行同样的参数，使得相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化（编号2），输出结果6×6×16。照旧应用400个5×5的过滤器（编号3），得到一个2×2×400的输出层，现在输出层为2×2×400，而不是1×1×400。应用1×1过滤器（编号4）得到另一个2×2×400的输出层。再做一次全连接的操作（编号5），最终得到2×2×4的输出层，而不是1×1×4。最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），也就是这个14×14区域经过卷积网络处理后的结果，同样，右下角这个方块是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果。 具体的计算步骤是以绿色方块为例，假设你剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6） 所以该卷积操作的原理是我们不需要把输入图像分割成四个子集，分别执行前向传播，而是把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享很多计算。 以上就是在卷积层上应用滑动窗口算法的内容，它提高了整个算法的效率。不过这种算法仍然存在一个缺点，就是边界框的位置可能不够准确。 Bounding Box预测 看上面的预测结果，这些边界框没有一个能完美匹配汽车的位置。 其中一个能得到更精准边框的算法是YOLO算法。它是这么做的，比如你的输入图像是100×100的，然后在图像上放一个网格。为了介绍起来简单一些，我用3×3网格，实际实现时会用更精细的网格，可能是19×19，可能更精细。基本思路是：采用图像分类和定位算法，逐一应用到图像的九个格子中。 所以对于这里9个格子中任何一个，你都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。因为这里有3×3格子，然后对于每个格子，你都有一个8维向量，所以目标输出尺寸是3×3×8。 注意：把对象分配到一个格子的过程是，你观察对象的中点，然后将这个对象分配到其中点所在的格子。所以即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，就是3×3网络的其中一个格子，或者19×19网络的其中一个格子。如果我们在现实实践时，采用19x19的网格会更精细。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。 交并比（IOU）在对象检测任务中，你希望能够同时定位对象，所以如果实际边界框是这样的，你的算法给出这个紫色的边界框，那么这个结果是好还是坏？所以交并比（lOU）函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域（绿色阴影表示区域），而交集就是这个比较小的区域（橙色阴影表示区域），那么交并比就是交集的大小，这个橙色阴影面积，然后除以绿色阴影的并集面积。 一般约定，在计算机检测任务中，如果IOU&gt;=0.5，就说检测正确，如果预测器和实际边界框完美重叠，lOU就是1，因为交集就等于并集。但一般来说只要IOU&gt;=0.5，那么结果是可以接受的，看起来还可以。一般约定，0.5是阈值，用来判断预测的边界框是否正确。一般是这么约定，但如果你希望更严格一点，你可以将IOU定得更高，比如说大于0.6或者更大的数字，但IOU越高，边界框越精确。 非极大值抑制（NMS）到目前为止你们学到的对象检测中的一个问题是，你的算法可能对同一个对象做出多次检测，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以确保你的算法对每个对象只检测一次。 假设你需要在这张图片里检测行人和汽车，你可能会在上面放个19×19网格，理论上右边这辆车只有一个中点，所以它应该只被分配到一个格子里，左边的车子也只有一个中点，所以理论上应该只有一个格子做出有车的预测。 图中的绿色点和黄色点分别为两辆车的中点。 实践中当你运行对象分类和定位算法时，对于每个格子都运行一次，所以这个格子（编号1）可能会认为这辆车中点应该在格子内部，这几个格子（编号2、3）也会这么认为。对于左边的车子也一样，所以不仅仅是这个格子，如果这是你们以前见过的图像，不仅这个格（编号4）子会认为它里面有车，也许这个格子（编号5）和这个格子（编号6）也会，也许其他格子也会这么认为，觉得它们格子内有车。 我们分步介绍一下非极大抑制是怎么起效的，因为你要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的pc，我这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后可能会对同一个对象做出多次检测，所以非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。 所以具体上，这个算法做的是，首先看看每次报告每个检测结果相关的概率pc，首先看概率最大的那个，这个例子（右边车辆）中是0.9，然后就说这是最可靠的检测，所以我们就用高亮标记，就说我这里找到了一辆车。这么做之后，非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制。所以这两个矩形分别是0.6和0.7，这两个矩形和淡蓝色矩形重叠程度很高，所以会被抑制，变暗，表示它们被抑制了。 接下来，逐一审视剩下的矩形，找出概率最高pc= 0.8，我们就认为这里检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他lOU值很高的矩形。所以现在每个矩形都会被高亮显示或者变暗，如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些，这就是最后得到的两个预测结果。 所以这就是非极大值抑制，非最大值意味着你只输出概率最大的分类结果，但抑制很接近，但不是最大的其他预测结果，所以这方法叫做非极大值抑制。 在这里只介绍了算法检测单个对象的情况，如果你尝试同时检测三个对象，比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。 Anchor Boxes到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，你可以这么做，就是使用anchor box这个概念。 假设有这样一张图片，对于这个例子，我们继续使用3x3网格。 注意人的中点和汽车的中点几乎在同一个地方，两者都落入同一个格子。对于那个同一个格子，y输出 我们可以检测着三个类别：行人、汽车、摩托车。它将无法输出检测结果，所以我必须从两个检测结果中选一个。 而anchor box的思路是，这样子，预先定义两个不同形状的anchor box，或者anchor box形状，要做的是把预测结果和这两个anchor box关联起来。一般来说，你可能会用更多的anchor box，可能要5个甚至更多，但对于这个视频，我们就用两个anchor box，这样介绍起来简单一些。 我们要做的是定义类别标签。用的向量不再是上面那个y=[pc bx by bh bw c1 c2 c3]^T，而是重复两次，y=[pc bx by bh bw c1 c2 c3 pc bx by bh bw c1 c2 c3]^T。前面的pc bx by bh bw c1 c2 c3 是和anchor box1关联的8个参数，后面的8个参数适合anchor box2相关联。 因为行人的形状更类似于anchor box 1的形状，而不是anchor box 2的形状，所以可以用这8个数值（前8个参数），这么编码pc=1代表有个行人，用bx,by,bh,bw编码包住行人的边界框，然后用c1=1,c2=0,c3=0来说明这个对象是个行人。然后是车子，因为车子的边界框比起anchor box 1更像anchor box 2的形状。这样编码，pc=1,bx,by,bh,bw, c1=0,c2=1,c3=0。 假设车子的边界框形状是这样，更像anchor box 2，如果这里只有一辆车，行人走开了，那么anchor box 2分量还是一样的，要记住这是向量对应anchor box 2的分量和anchor box 1对应的向量分量，我们要填的就是，里面没有任何对象，所以 ，然后剩下的就是？。编码：y=[0 ? ? ? ? ? ? ? 1 bx by bh bw 0 1 0]。 最后，应该怎么选择anchor box呢？人们一般手工指定anchor box形状，我们可以选择5到10个anchor box形状，覆盖到多种不同的形状，可以涵盖我们自己想要检测的对象的各种形状。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度卷积网络]]></title>
    <url>%2F2019%2F10%2F02%2F%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[经典网络LeNet - 5LeNet-5是专门为灰度图训练的。所以他的图像样本的深度都是1。 LeNet-5的网络结构： AlexNet网络结构： 实际上论文的原文是使用224x224x3的，但经过试验发现227x227x3效果更好。 VGG-16网络结构： 从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。 可以看到 VGG16 是13个卷积层+3个全连接层叠加而成。 残差网络网络越深越难训练，因为存在梯度消失和梯度爆炸的问题。本小节学习跳远连接。跳远连接可从某一个网络层激活，然后迅速反馈给另外一层甚至是神经网络的更深层。我们可以利用跳远连接构建能够训练深度网络的ResNets。 ResNets是由残差块构建的。那什么是残差块？看下图 这是一个两层的神经网络，它在L层进行激活，得到a[ l +1]再次进行激活，两层之后得到a[ l +2]。下图中的黑色部分即为计算过程。 而在残差网络中，我们直接将a^[ l ]拷贝到蓝色箭头所指位置。在线性激活之后、Relu非线性激活之前加上a[ l ]，不在沿着原来的主路径传递。这就意味者我们主路径过程的第四个式子替换为蓝色式子，也正是这个蓝色式子中加上的a[ l ]产生了一个残差块。 为什么残差网络有用？一个网络深度越深，它在训练集上训练网络的效率会有所减弱，但对于ResNets就不完全是这样了。 上一张图已经说过a^[ l +2]=g(z^[ l +2]+a^[ l ])，添加项a^[ l ]是刚添加的跳远连接的输入。解开这个式子，得： a^[ l+2]=g(w^[ l+2] * a^[ l+1]+b^[ l+2]+a^[ l ])，这里w和b为关键值。如果w和b均为0，那a^[ l+2]=g(a^[ l ])=a^[ l ](假设这里得激活函数是Relu)。结果表明，残差块学习这个恒等式函数残差块并不难，跳远连接让我们很容易的得到a^[ l+2]=a^[ l ] 这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络。因为对它来说，学习恒等函数对它来说很简单，尽管它多了两层，也只是把a^[ l ]的值赋给a^[ l+2]。 所以，残差网络有用的原因是这些函数残差块学习恒等函数非常容易。 1x1的卷积看到这个标题，你也许会迷惑，1x1的卷积能做什么呢？不就是乘以数字吗？结果并非如此 看上图，你会觉得这个1x1的过滤器没什么用，只是对输入矩阵乘以某个数字。但这个1x1的过滤器仅仅是对于6x6x1的信道图片效果不好 。如果是一张6x6x32的图片就不一样了。 具体来说，1x1卷积所实现的功能是遍历这36个单元格。计算输入图中32个数字和过滤器中32个数字的乘积，然后应用Relu函数。我们以一个单元格为例，用着36个数字乘以这个输入层上1x1的切片，得到一个实数画在下面图中。 这个1x1x32的过滤器中的32可以这样理解，一个神经元的输入是32个数字，乘以相同高度和宽度上某个切片上的32个数字。这三十二个数字具有不同信道，乘以32个权重，然后应用Relu非线性函数。一般来说，如果过滤器不止一个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6x6x过滤器数量。所以1x1卷积可以从根本上理解为这32个单元都应用了一个全连接神经网络。全连接层的作用是输入32个数字和过滤器数量，标记为nc^[ l+1]。在36个单元上重复此过程，输出结果是6x6x过滤器数量。这种方法通常称为1x1卷积，也成为Network in Network。 下面是一个1x1卷积的应用： 假设这是一个28x28x192的输入层，我们可以利用池化层压缩它的高度和宽度。但是如果信道数量很大，我们该如何把它压缩为28x28x32维度的层呢？我们可以用32个大小为1x1的过滤器，每个过滤器的大小都是1x1x192维，因为过滤器中信道的数量必须与输入层中信道的数量一致。因此过滤器数量为32，输出层为28x28x32。这就是压缩nc的方法。 Inception网络构建卷积层是，我们需要决定过滤器的大小是3x3还是5x5或者其它大小？或者要不要添加池化层？而我们接下来要讲的Inception就是代替我们来做决定的。虽然网络结构会变得非常复杂，但网络表现得非常好。我们来看一下原理。 基本思想：不需要人为的决定使用哪个过滤器，或者是否需要池化，而是由网络自行确定这些参数。人们只需给出这些参数的所有可能值，然后把这些输出连起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合。 举个例子： 如果我们直接计算上图，我们的计算成本为28x28x32x5x5x192 但是我们用1x1卷积后： 我们的计算成本变为28x28x16x192+28x28x32x5x5x16，使用1x1卷积后计算成本是没使用前的1/10。 下面再举一个例子： 例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=”SAME”)，输出数据的大小为100x100x256。其中，卷积层的参数为5x5x128x256。假如上一层输出先经过具有32个输出的1x1卷积层，这时得图片输出为100x100x32，接着再经过具有256个输出的5x5卷积层，那么最终的输出数据的大小仍为100x100x256，但卷积参数量已经减少为1x1x128x32 + 5x5x32x256，大约减少了4倍。 在inception结构中，大量采用了1x1的矩阵，主要是两点作用：1）对数据进行降维；2）引入更多的非线性，提高泛化能力，因为卷积后要经过ReLU激活函数。 搭建inception网络： inception就是将这些模块都组合到一起。 Inception network：下面这个图就是Inception的网络： 看起来很复杂，但我们截取其中一个环节，如上图的红色框。就会发现这不正是我们搭建的inception吗。另外，再一些inception模块前网络会使用最大池化层来修改高和宽的维度。 其实上图的Inception并不完整，看下图： 多出了红色框住的部分。这些分支是做什么的呢?这些分支是通过隐藏层做出预测。 迁移学习我们在做计算机视觉的应用时，相对于从头训练权重，下载别人训练好的网络结构的权重作为我们预训练，然后转换到感兴趣的任务上。别人的训练过程可能是需要花费好几周，并且需要很多的GPU找最优的过程，这就意味着我们可以下载别人开源的权重参数并把它当做一个很好的初始化，用在我们的神经网络上。这就是迁移学习。 假如我们在做一个识别任务，却没有很多的训练集。我们就可以把别人的网络下载，冻结所有层的参数。我们只需要训练和我们Softmax层有关的参数，然后把别人Softmax改成我们自己Softmax。通过别人的训练的权重，我们可能会得到一个好的结果，即使我们的训练集并不多。 由于前面的层都冻结了，相当于一个固定函数，不需要改变，因为我们不训练它。 网络层数越多，需要冻结的层数越少，需要训练的层数就越多。 数据扩充 数据扩充也叫数据增强。因为计算机视觉相对于机器学习，数据较少。所以数据增强为了增加数据的数量。下面我们讲一下数据增强的办法： 垂直镜像对称 随机裁剪 色彩转换给RGB三个通道加上不同的失真值 这些可以轻易改变图像的颜色，但是对目标的识别还是保持不变的。所以使用这种数据增强方法使得模型对照片的颜色更改更具鲁棒性。]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN详解]]></title>
    <url>%2F2019%2F10%2F01%2FCNN%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[全连接神经网络的局限性过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为每张图片都有 3 个颜色通道，它的数据量是 64×64×3=12288。 现代计算机中，64×64 真的是很小的图片，如果想处理大一点的图片，比如1000×1000，那么最终输入层的数据量是300万，假设我们有一个隐藏层，含有1000个神经元构成的全连接网络，那么数据量将达到 1000*300万，也就是30亿。在这样的情况下很难获得足够的数据防止过拟合，并且需要的内存大小很难得到满足。 本篇讲解的卷积神经网络（也称作 ConvNets 或 CNN）不仅可以达到减少参数的作用，而且在图像分类任务中还有其他优于全连接神经网络的特性。 卷积神经网络概览一个图像的相互靠近的局部具有联系性，卷积神经网络的思想就是用不同的神经元去学习图片的局部特征，每个神经元用来感受局部的图像区域，如下图不同颜色的圆圈所示： 然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。 卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了由卷积层和子采样层构成的特征抽取器，并且在最后有几个全连接层用于对提取的特征进行分类。一个简单的卷积神经网络模型如下： 接下来我们讲解以下这几个层究竟在做的是什么。 卷积层卷积层主要是做卷积运算。本文用 “*” 表示卷积操作。 卷积运算假如我们拥有一个6x6的灰度图，矩阵如下图表示，在矩阵右边还有一个3x3的矩阵。我们称之为过滤器(filter)或核。 卷积的第一步就是将过滤器覆盖在灰度图的左上角的数字就是过滤器对应位置上的数字，如下图蓝色部分： 蓝色矩阵每个小个子的左上角的数字就是过滤器对应位置上的数字，将每个格子的两个数字相乘： 然后将得到的矩阵所有元素相加，即3+1+2+0+0+0+(-1)+(-8)+(-2)= -5，然后将这个值放到新的矩阵的左上角，最后的新的矩阵是一个4*4的矩阵。规律：n*n维的矩阵经过一个f*f过滤器后，得到一个（n-f+1）*（n-f+1）维度的新矩阵。 我们以1为步长(stride)向右移动一格，再进行类似的计算： 0 × 1 + 5 × 1 + 7 × 1 + 1 × 0 + 8 × 0 + 2 × 0 + 2 × (-1) + 9 × (-1) + 5 × (-1) = -4 写在刚刚得到的-5的右边。 重复向右移动，直到第一行都计算完成： 然后将过滤器向下移动一格，从左边开始继续运算，得到-10，写在新的矩阵第二行第一个位置上： 不断的向右、向下移动，直到计算出所有的数据，这样就完成了卷积运算，得出下图： 下面这张图也可以帮助我们理解卷积运算(黄色的矩阵式过滤器)： 最终经过卷积得到的图像，我们称之为特征图(Feature Map) 对于上述过程，也许你有很多疑问，比如过滤器是什么，为什么是3×3的，作用是什么，滑动的步长stride为什么是1，为什么得到4×4的矩阵，卷积有什么作用等，我们一一解答。 过滤器(filter)我们来看看下面的例子，本例子中我们用正数表示白色，0表示灰色，负数表示黑色。 左边 6×6 的矩阵是一个灰度图，左半部分是白色，右半部分是灰色。下面的3×3的过滤器是一种垂直边缘过滤器。 对二者进行卷积得到新的矩阵，这个矩阵的中间部分数值大于0，显示为白色，而两边为 0， 显示为灰色。 上图中三个红色箭头都是我们过滤得到的边缘。在这里，你可能会疑问为什么会有这么大的边缘，那是因为我们的原始图片太小，如果我们把原始灰度图放大一点： 再进行卷积，就可以得到： 这样就能大致看出过滤器确实过滤出了垂直边缘。如果想得到水平的边缘，将过滤器转置一下即可。而如果想得到其他角度的边缘，则需要在过滤器中设置不同的数值。下图是水平边缘过滤器： 当然很难手动设置过滤器的值去过滤某个特殊角度（比如31°）的边缘，我们通常是将过滤器中的9个数字当成9个参数。随机初始化这些参数后，通过神经网络的学习来获得一组值，用于过滤某个角度的边缘。 目前主流网络的过滤器的大小一般是1x1、3x3、5x5等，数值一般不会太大，因为我们希望这些过滤器可以获得一些更细微的边缘特征。另外，一般将长和宽都取奇数，具体原因我们讲到Padding的时候再解释。 PaddingPadding是填充。什么是填充呢？ 在前面，我们的例子一个6*6的矩阵经过一个3*3的过滤器后，得到一个4*4的新矩阵。但是这样的话会有两个缺点，第一个缺点就是我们每次做卷积运算的时候，我们的图像都会缩小。可能我们在做完几次卷积之后，我们的图像就会变得很小。第二个缺点是角落边的像素点只会被使用一次，像下图的绿色。但是如果是中间的像素点(类似图中的红色框)，就会被使用很多次。 所以，那些边缘或者角落的像素点在输出时使用较少。这意味者，我们将丢掉图片边缘区域。如果这里有重要信息，那我们就得不偿失了。 那我们该怎么解决这个问题呢？这时候就用到Padding了。我们可以在卷积操作之前填充这个图像。我们可以沿着图像的边缘再填充一层像素(通常为0，因为用0填充不影响原来的图片特征)。这样做的话，我们的原始图片像素就有6*6变成了8*8。接着我们对这幅8*8的图像进行卷积，得到的新矩阵就不是4*4的，而是6*6的。规律：n*n维的矩阵填充了P个像素点后再经过一个f*f过滤器后，得到一个（n+2p-f+1）*（n+2p-f+1）维度的新矩阵。 由上面这个规律：我们得出p=(f-1)/2，为了使p是对称填充，所以我们的f通常都是奇数。还有一个原因，是因为当我们有一个奇数的过滤器，我们会有一个中间像素点。这就是为什么前面我们说过滤器通常都是奇数维度。 那我们Padding填充的时候到底选择多少呢？1还是2还是更大的数字？ 其实Padding有两个选择：一个Valid，一个是Same。 Valid：意味不填充。也就是之前的6*6矩阵经过一个3*3的矩阵后，得出一个4*4的矩阵。 Same：意味我们的图片输出大小和输入大小是一样的。也就是上面的6*6矩阵在Padding=1之后经过一个3*3的矩阵后，得出一个6*6的矩阵。图片输出大小和输入大小一样。 卷积步长我们先用 3x3 的过滤器以2为步长卷积 7x7 的图像，看看会得到什么。 第一步，对左上角的区域进行计算： 第二步，向右移动，得： 再继续向右向下移动过滤器，就可以得到下面的结果： 规律：我们用一个ff的过滤器卷积一个n\n的图像，Padding=p，步长为s，我们得到一个(n+2p-f)/s+1*(n+2p-f)/s+1 最后一个问题：如果我们的商不是整数呢？这时我们采用向下求整的策略。 三维卷积上面的讲解你已经知道如何对灰度图进行卷积运算了，现在看看如何在有三个通道(RGB)的图像上进行卷积。 假设彩色图像尺寸 6×6×3，这里的 3 指的是三个颜色通道。我们约定图像的通道数必须和过滤器的通道数匹配，所以需要增加过滤器的个数到3个。将他们叠在一起，进行卷积操作。6x6x3的图像经过一个3x3x3过滤器得出一个新的4x4x1图像。 如下图： 具体过程就是，首先，把这个 3×3×3 的过滤器先放到最左上角的位置，依次取原图和过滤器对应位置上的各27个数相乘后求和，得到的数值放到新矩阵的左上角。 注意，由于我们是用三维的过滤器对三维的原图进行卷积操作，所以最终得到的矩阵只有一维。 然后不断向右向下进行类似操作，直到“遍历”完整个原图： 至于效果是怎么样的。结合之前的知识，我们假设了一组垂直边界过滤器： 它可以完成RGB三个通道上垂直边缘的检测工作。这也解释了为什么过滤器使用3个通道：也就是说如果你想获得RGB三个通道上的特征，那么你使用的过滤器也得是3个通道的。 如果你除了垂直边界，还想要检测更多的边界怎么办？可以使用更多 3维 甚至更多的过滤器，如下图框住的部分，我们增加了一组过滤器： 这样，两组过滤器就得到了两组边界，我们一般会将卷积后得到的图叠加在一起，得到上图左边的 4x4x2 的图像。如果你还想得到更多的边界特征，使用更多的3维的过滤器即可。 现在我们对维度进行总结： 首先我们假设我们的没有padding且步长为1，这时候使用 n × n × nc 原图和 nc’ 个 f × f × fc 的过滤器进行卷积，得到 (n-f+1) × (n-f+1) × nc’ 的图像 nc 是 原图的通道数(3)，过滤器的通道数和原图的通道数必须一样，都是 nc。 nc’ 是最终得到的图像的通道数(在上面的例子为2)，由使用的过滤器个数决定(而 nc’ 又是下一层的输入图像的通道数) 卷积层全貌 对于卷积层，我们同样需要进行以下操作，这也是前向传播的操作： z^[1] = w^[1] * a^[0]+b^[1] a^[1]=g(z^[1]) 上面所用到的变量解释： a^[0] 是我们原图的数据 X ，也就是上图 6x6x3 的RGB图。 w^[1] 是这一层的权重矩阵，由 2 个 3x3x3 的过滤器组成。 b^[1] 是第一层的偏置项，不同的过滤器对应不同的偏置值 g() 是激活函数，本例子中使用 ReLu 卷积层的工作说明： 式子中 w^[1] * a^[0] 是代表进行卷积运算得到两个特征图(也就是2个4x4的矩阵)，如下图的绿色框所示，接着再对两个特征图加上不同的偏置就得到了z^[1]，如下图红色框所示： 进一步应用激活函数就得到了两个处理过的图像，将他们叠加在一起，就得到了这一层的输出 a^[1](上图右下角)。 注意：上图中的加偏置(b1,b2)均使用了python的广播 简单来说，卷积层的工作就是：将输入的数据a^[l - 1]进行卷积操作，加上偏置，再经过激活函数非线性化处理得到a^[l]。到这里卷积层的任务就完成了。 为什么要使用卷积？卷积层的主要优势在稀疏连接就和权值共享 稀疏连接假设我们输入的是32×32×3的RGB图像。 如果是全连接网络，对于隐藏层的某个神经元，它不得不和前一层的所有输入（32×32×3）都保持连接。 但是，对卷积神经网络而言，神经元只和输入图像的局部相连接，如下图圆圈所示： 这个局部连接的区域，我们称之为“感受野”，大小等同于过滤器的大小(3*3*3)。 看下面的图： 右边的绿色或者红色的输出单元仅仅与36个输入特征中的九个相连接，其它像素值对输出不会产生任何影响。 相比之下，卷积神经网络的神经元与图像的连接要稀疏的多，我们称之为稀疏连接。 权值共享权值共享其实就是过滤器共享。特征检测如垂直边缘检测过滤器如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，如果我们用垂直过滤器扫描整张图片，就可以得到整张图片的所有垂直边缘，而换做水平过滤器就可以扫描出图片的水平边缘。 在CNN中，我们用不同的神经元(过滤器)去学习整张图片的不同特征，而不是利用不同的神经元 学习图片不同的局部特征。因为图像的不同部分也可能有相同的特征。每个特征检测过滤器都可以在输入图片的不同区域中使用同样的参数(即方格内的9个数字)，以便提取垂直边缘或其它特征。右边两张图每个位置是被同样的过滤器扫描而得的，所以权重是一样的，也就是共享。假设有100个神经元，全连接神经网络就需要32×32×3×100=307200个参数。 因为共享了权值，提取一个特征只需要 f×f 个参数，上图右边两张图的每个像素只与使用 3x3 的过滤器相关，顶多再加上一个偏置项。在本例子中，得到一个feature map 只用到了 3*3+1=10 个参数。如果想得到100个特征，也不过是用去1000个参数。这就解释了为什么利用卷积神经网络可以减少参数个数了。 当你对提取到的 水平垂直或者其他角度 的特征，再进行卷积操作，就可以得到更复杂的特征，比如一个曲线。如果对得到的曲线再进行卷积操作，又能得到更高维度的特征，比如一个车轮，如此往复，最终可以提取到整个图像全局的特征。 到这里卷积层的内容就结束了。 池化层对于一个32x32像素的图像，假设我们使用400个3x3的过滤器提取特征，每一个过滤器和图像卷积都会得到含有 (32-3 + 1) × (32 - 3 + 1) = 900 个特征的feature map，由于有 400 个过滤器，所以每个样例 (example) 都会得到 900 × 400 = 360000 个特征。而如果是96x96的图像，使用400个8x8的过滤器提取特征，最终得到一个样本的特征个数为 3168400。对于如此大的输入，训练难度大，也容易发生过拟合。池化的主要目的是降维，即在保持原有特征的基础上最大限度地将数组的维数变小。 为了减少下一个卷积层的输入，引入了采样层(也叫池化层)，采样操作分为最大值采样(Max pooling)，平均值采样(Mean pooling)。 最大值采样 举个例子，我们用3x3的过滤器以1为步长去扫描图像，每次将重叠的部分的最大值提取出来，作为这部分的特征，过程如下： 我们也可以使用2x2的过滤器。以2为步长进行特征值的提取。最终得到的图像长度和宽度都缩小一半： 平均值采样 与最大值采样不同的是 将覆盖部分特征的平均值作为特征，其他过程都一样。经过采样处理，可以提取更为抽象的特征，减少数据处理量的同时保留有效信息。做法是对里面的所有不为0的像素点取均值 下图是过滤器为2x2，步长为2进行特征值的提取： 注意：一定是不为零的像素点，这个很重要。如果把带0的像素点加上，则会增加分母。从而使整体数据变低。 注意：池化层没有需要学习的参数。且padding大多数时候为0 全连接层在经过几次卷积和池化的操作之后，卷积神经网络的最后一步是全连接层，全连接层的个数可能不止一个： 最后一个采样层到全连接层的过渡是通过卷积来实现的，比如前层输出为 3x3x5 的feature map，那我们就用 3x3x5 的过滤器对前层的这些feature map进行卷积，于是得到了一个神经元的值： 全连接层有1024个神经元，那我们就用1024个3x3x5 的过滤器进行卷积即可。 第一个全连接层到第二个全连接层的过渡也是通过卷积实现的，若前层有1024个神经元，这层有512个，那可以看做前层有1024x1x1个feature map，然后用1x1的卷积核对这些feature map进行卷积，则得到100x1x1个feature map。 卷积神经网络要做的事情，大致分为两步： 卷积层、采样层负责提取特征 全连接层用于对提取到的特性进行分类 最后一步就和普通的神经网络没有什么区别，充当一个分类器的作用，输入是不同特征的特征值，输出是分类。在全连接层后，就可以根据实际情况用softmax对图片进行分类了。]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深层神经网络]]></title>
    <url>%2F2019%2F09%2F29%2F%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[深层神经网络深层神经网络样子也许是下面这样的，也有可能是更多层的。 上图是一个四层的神经网络(输入层一般记为零层或不记住)，我们用L表示神经网络的层数，n^[l] 表示第 l 层的神经元数量 由于神经网络具有许多层，在下图中用方框代表一层，每个层都要完成各自的任务，流程图大致如下所示： 蓝框中的部分完成正向传播： 该过程的输入是 X 也就是 A[0]，一层一层向后计算，最后得到A[L]。 并且，在各层 l 计算出 A^[l] 的同时，缓存各层的输出值 A^[l] 到变量 cache 中，因为反向传播将会用上。图中这里写错了，应该是A[1]、A[2]、A[3]的。 绿框中的部分完成反向传播： 该过程的输入是dA[L]，并根据dA[L]一步步向前计算，得到各层对应的dZ[l]，dW[l]，db[l] 此外，还要计算出 dA[l - 1] 作为前一层的输入 具体过程请看浅层神经网络的正向传播) 和 浅层神经网络的反向传播) 核对矩阵的维数为了在写代码的过程中减少出现bug，尤其是在进行反向传播的时候，我们要注意核对矩阵的维数，这个知识点我们前面将 浅层神经网络的反向传播)的文章中也提到了，大致规律如下： W^[l] ： (n[ l ], n[l - 1]) Z^[l] ： (n[ l ], m), A^[l] ： (n[ l ], m) 另外，还有一个规律： 对任意层而言： dW 的维度和 W 的维度相同 dZ 的维度和 Z 的维度相同 dA 的维度和 A 的维度相同 为什么要使用深度神经网络？我们都知道深度神经网络可以解决很多问题，网络并不一定要很大，但一定要有深度，需要比较多的隐藏层。这到底为什么呢？ 如果我们在建立一个人脸识别或者检测系统，当我们输入一张脸部的图片时，我们可以把深度神经网络第一层当成一个特征探测器或者边缘检测器。在这个例子中，我们建立一个大概有20个隐藏单元的深度神经网络。看下图： 隐藏单元就是这个图里这些小方块。每个小方块都是一个隐藏单元，它会去寻找一张图的边缘方向。它可能在水平方向找、也可能在竖直方向找。这个东西在卷积神经网络我们会细讲(别问，问就是当时不会)。这里就告诉你们我们可以先把神经网络的第一层当作看图，然后去找这张图片的各个边缘。接着我们把组成图片边缘的像素放在一起看。它可以把探测到的边缘组合成面部的不同部分。比如说，可能有一个神经元回去找眼睛的部分，另外还有找鼻子或其它的部分。然后把这许多边缘结合在一起，就可以开始检测人脸的不同部分，最后再把这些部分放在一起，就可以识别或检测不同的人脸。我们把前几层的神经网络当作简单的检测函数，例如：边缘检测等，之后把它们跟后几层结合在一起。注意：边缘探测器其实相对来说都是针对图片中非常小块的面积。 具体原因移步网易云课堂的吴恩达的深度学习。链接 超参数在学习算法中还有其它参数，需要输入到学习算法中，比如学习率&alpha;、隐藏层的数量、使用的激活函数种类等都是超参数。因为它们影响着最终W和b的值。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上传项目到GitHub]]></title>
    <url>%2F2019%2F09%2F28%2F%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E5%88%B0GitHub%2F</url>
    <content type="text"><![CDATA[首先登录GitHub，没有账号的申请一个。很简单，跳过。 新建一个仓库： 记住这个网址，之后用到 进入项目的目录，点击空白处，选择git Bash。 输入git init，会发现当前目录下多了一个.git文件夹 接着输入git add . 这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把.换成这个特定的文件名即可。 输入git commit -m “first commit”，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要 输入git remote add origin 自己的仓库url地址（上面有说到） 将本地的仓库关联到github上， 输入git push -u origin master，这是把代码上传到github仓库的意思]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github的使用]]></title>
    <url>%2F2019%2F09%2F28%2FGithub%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[github的账户创建和仓库创建比较简单，就不赘述了 github添加ssh账户首先，点击账户，选择setting 直接添加就完事了。那我们怎么生成ssh公钥呢？ 先回到用户的主目录下，编辑文件.gitconfig，修改某台机器的git配置。修改为注册github的邮箱，填写用户名 接着执行命令，ssh-keygen -t rsa -C “邮箱地址”，一路yes。最后生成三个文件： id_rsa是机器私钥，自己保留。id_rsa.pub就是我们的公钥。把公钥内容复制到第一张图New SSH Key后的位置。名字可以随便取。 克隆项目 接着git clone SSH 这个再cmd下执行同样有效。 如果出错了，执行以下代码：先执行eval “$(ssh-agent -s)”，再执行ssh-add 不只是可以使用SSH，也可以使用HTTPS。 git clone 网址 在cmd上同样是有效的。 推送代码推送前： 图中的红框是分支。 推送分支就是把给分支上的所有本地提交库推送到远程库，推送时要指定本地分支，这样，git就会把该分支推送到远程库对应的远程分支上。 git push origin 分支名称 本地分支跟踪远程分支git branch —set-upstream-to=origin/远程分支名词 本地分支名称 跟踪后，如果本地分支和远程分支的进度不一样，使用命令 git status 会提醒。 拉取代码git pull orgin 分支名称]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git的分支管理]]></title>
    <url>%2F2019%2F09%2F27%2Fgit%E7%9A%84%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[分支原理git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。HEAD严格来说不是指向提交，而是指向master。master才是指向提交的版本，所以，HEAD指向的就是当前分支。 每次提交，master分支都会向前移动一步。这样，随着不断提交，master分支的线也越来越长。 当我们创建新的分支dev，git创建了一个指针叫dev。指向master相同的提交。再把HEAD指向dev，就表示当前分支在dev上。 git创建一个分支很快，因为除了增加一个dev指针，改变HEAD的指向，工作区的文件没有任何变化。 假如我们在dev上的工作完成了，就可以把dev合并到master上。怎么合并的呢？git直接把master指向dev的当前提交，就完成了合并。 合并分支也很快，就改改指针。工作区的内容不变。 合并完分支后，甚至可以删除dev分支，删除dev分支就是把dev指针给删掉。删掉后，我们只剩下一条master指针。 分支作用分支在实际中有很大用处。假设你准备开发一个新功能，但是需要两周才能完成，第一周只写了一半的代码。如果立刻提交，由于代码还没写完，不完整的代码库会导致别人无法干活，如果等代码全部写完，又存在丢失每天进度的巨大风险。有了分支，就不用怕这些事情。你创建自己的一个分支，别人看不到，也可以在原来的分支上工作。而你还在自己的分支上干活，想提交就提交。直到全部开发完，一次性合并到主分支。这样既安全，又不影响别人工作。 分支基本操作查看当前几个分支且能看到在哪个分支工作git branch 创建分支git branch 分支名 创建分支并切换到其上工作git checkout -b dev 切换回master分支git checkoout master 合并分支git merge 分支名字 注意上面的Fast-forward，也就是红色框。git告诉我们，这次合并是快速合并，也就是直接把master指向dev的当前提交，所以合并速度非常快 解决冲突合并冲突并不是一番风顺的。在两个分支上修改同一个文件并提交，就会起冲突。解决办法：手动解决冲突，再提交一次 看下图： git告诉我们，git_test2.txt文件存在冲突，必须手动解决冲突再提交。 打开刚才修改的文件，发现文件修改了 将文件中新增加的&lt;、= 和 &gt;手动删掉，再提交一次 删除分支git branch -d 分支名字 这个操作也是非常快的，直接把dev这个指针删了就好了。 分支管理策略通常，合并分支时，如果可能，git会用fast forward模式。但是有些快速合并并不能合并但不会起冲突，合并之后并做一次新的提交。在这种模式下，删除分支后，会丢掉部分信息。 如上图，merge并不会起冲突(因为不是同一个文件)。但是，会出来一个框： 为什么会出现这个框？前面我们说了，合并分支无法合并但不会起冲突且做一次提交。在这次提交中，需要输入描述信息。就在弹窗输入描述信息 PS：我太难了，我接下来一直退不出这个框。所以这个例子就这样吧。可以的跟我说一下，拜托了。 禁用快速合并： git merge —no-ff -m “描述” 分支名 Bug分支软件开发中，bug就像家常便饭，有了bug就要修复。在git中，由于分支是如此强大，所以，每个bug都可以通过一个新的临时分支来修复。修复后，合并分支，然后将临时分支删除。这个例子不难，就不贴图了。 假如你正在写代码，突然老大让你修改一个bug。你需要先保存一下工作现场，修复完bug后，再恢复工作现场。]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git介绍和基本操作]]></title>
    <url>%2F2019%2F09%2F26%2Fgit%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近经常用到git和GitHub，但是命令很多都忘记了。故写一篇博客回顾并总结一下 git介绍git是目前世界上最先进的分布式版本系统。 git的两大特点 版本控制：可以解决多人同时开发的代码问题，也可以解决找回历史代码的问题 分布式：git是分布式版本控制系统，同一个git仓库，可以分布到不同的机器上，首先找一台电脑充当服务器的角色。每天24小时开机，其它每个人都从这个“服务器”仓库克隆一份自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交，可以自己搭建服务器，也可以使用github网站。 git的安装就不赘述了，直接下好安转包。一路next就完事了(不用改安装路径)。 git基本操作初始化通过git我们可以管理一个目录下的代码，首先我们通过git init创建一个版本库 随机切到一个目录，点击空白区域。点击Git Bash Here，如图中所示： 这样我们就有一个格式为 .git 文件 版本创建首先，git add 文件名.文件格式：添加某个特定的文件 或 git add . ：将目录上所有的文件添加到仓库 接着，git commit -m “对这个版本的说明信息” 查看版本记录使用 git log 图中的commit 12c6a……代表版本序列号 随便放了一张图片进项目，第二次提交项目 如果版本记录过程，以简短形式显示： 版本回退在git中，有个HEAD指针一直指向当前的版本。用HEAD^代表HEAD前一个版本，HEAD^^代表前两个版本。其它版本类推。那前一百个要写100个^吗？答案当然是不用的，用HEAD~100。HEAD~1 等价于 HEAD^。 git reset —hard HEAD git reset —hard 版本号：这里的版本号就是commit后的版本序列号 版本号不需要写完，写完前几个号码就好了。 查看操作记录git reflog 工作区和暂存区工作区就相当于我们的目录 在工作区中，有一个隐藏目录 .git，这个不是工作区。而是git的版本库。其中存了很多东西，其中最重要的是称为stage(或者叫index)的暂存区，还有git为我们自动创建的第一个分支master，以及指向master的一个指针HEAD。 前面讲了我们把文件往git版本库里添加进去，是分两步执行的： 第一步是git add把文件添加进去，实际上就是把文件添加到暂存区 第二步是git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支 用git status查看当前的状态。如果有新的文件或有文件在修改之后、在add之前，会出现以下情况： 把新的文件add之后，得出以下情况： 经历add和commit后使用status： git管理文件的修改，它只会提交暂存区的修改来创建版本。也就是说，在修改或创建一个新文件后，不add到暂存区而直接commit，git也会说文件被修改。 撤销修改git checkout —文件名.文件格式 如果是add后，先利用reset取消暂存的变更重新回到工作区： 接着用git checkout —文件名.文件格式 丢弃工作区的改动 如果是commit了，就版本回退 对比文件的不同对比工作区和某个版本中文件的不同 git diff 某个版本库 — 比较的文件 对比两个版本间的不同 git diff 某个版本库 某个版本库 — 比较的文件 两个版本交换位置，得出的效果是不同的 删除文件 如果是add到暂存区，也是和之前一样，先reset，再checkout。]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现浅层神经网络]]></title>
    <url>%2F2019%2F09%2F24%2FPython%E5%AE%9E%E7%8E%B0%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[开始前的准备本文用到的库：numpy、sklearn、matplotlib 另外，我们还要借助一下吴恩达老师的工具函数testCases.py 和 planar_utils.py。地址) ​ testCases：提供测试样本来评估我们的模型。 ​ planar_utils：提供各种有用的函数。 这两个文件的函数就不具体阐述了，有兴趣的自己研究去吧。 导入必要的库1234567import numpy as npimport matplotlib.pyplot as pltfrom testCases import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets 数据集介绍12#X是训练集，Y是测试集X,Y=load_planar_dataset() 数据集的说明： X是维度为(2, 400)的训练集，两个维度分别代表平面的两个坐标。 Y是为(1, 400)的测试集，每个元素的值是0（代表红色），或者1（代表蓝色） 利用 matplotlib 进行数据的可视化操作，这是一个由红点和蓝色的点构成的类似花状的图像 12345 # np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉 # 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23 # 目的是确保 cost 是一个浮点数plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral);plt.show() 构建神经网络模型 对于某个样本x^[i]： 成本函数J： 接下来我们要用代码实现神经网络的结构，步骤大致如下： 1. 定义神经网络结构 2. 随机初始化参数 3. 不断迭代: - 正向传播 - 计算成本函数值 - 利用反向传播得到梯度值 - 更新参数（梯度下降） 定义神经网络的结构主要任务是描述网络各个层的节点个数 12345def layer_sizes(X, Y): n_x = X.shape[0] # 输入层单元数量 n_h = 4 # 隐藏层单元数量，在这个模型中，我们设置成4即可 n_y = Y.shape[0] # 输出层单元数量 return (n_x, n_h, n_y) 随机初始化参数参数W的随机初始化是很重要的，如果W和b都初始化成0，那么这一层的所有单元的输出值都一样，导致反向传播时，所有的单元的参数都做出完全相同的调整，这样多个单元的效果和只有一个单元的效果是相同的。那么多层神经网络就没有任何意义。为了避免这样的情况发生，我们会把参数W初始化成非零。 另外，随机初始化W之后，我们还会乘上一个较小的常数，比如 0.01 ，这样是为了保证输出值 Z 数值都不会太大。为什么这么做？设想我们用的激活函数是 sigmoid 或者 tanh ，那么，太大的 Z 会导致最终 A 会落在函数图像中比较平坦的区域内，这样的地方梯度都接近于0，会降低梯度下降的速度，因此我们会将权重初始化为较小的随机值。 1234567891011def initialize_parameters(n_x, n_h, n_y): #随机初始化 W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 循环迭代实现正向传播123456789101112131415161718def forward_propagation(X, parameters): # 从parameters中取出参数 W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # 执行正向传播操作 Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) #如果不相等，直接报错 assert (A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cache 计算成本函数J 1234567891011def compute_cost(A2, Y, parameters): m = Y.shape[1] # 样本个数 # 下一行的结果是 (1, m)的矩阵 logprobs = np.multiply(Y, np.log(A2)) + np.multiply(1 - Y, np.log(1 - A2)) # 将 (1, m)的矩阵求和后取平均值 cost = - 1 / m * np.sum(logprobs) # np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉 # 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23 # 目的是确保 cost 是一个浮点数 cost = np.squeeze(cost) return cost 利用正向传播函数返回的cache，我们可以实现反向传播了。 实现反向传播 说明： 我们使用的 g^[1] () 是 tanh ，并且 A1 = g^[1] ( Z^[1] ) ,那么求导变形后可以得到 g‘ ^[1] ( Z^[1] ) = 1-( A^[1] )^2 用python表示为： 1 - np.power(A1, 2) 我们使用的 g^[2] () 是 sigmoid ，并且 A2 = g^[2] ( Z^[2] ) ,那么求导变形后可以得到 g‘ ^[2] ( Z^[2] ) =A^2 - Y 1234567891011121314151617181920def backward_propagation(parameters, cache, X, Y): # 获取样本的数量 m = X.shape[1] # 从 parameters 和 cache 中取得参数 W1 = parameters["W1"] W2 = parameters["W2"] A1 = cache["A1"] A2 = cache["A2"] # 计算梯度 dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = 1 / m * np.dot(dZ2, A1.T) db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True) dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) dW1 = 1 / m * np.dot(dZ1, X.T) db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True) grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return grads 说明 1、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。 2、 keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。 更新参数利用上面反向传播的代码段获得的梯度去更新 (W1, b1, W2, b2)： 式子中 α 是学习率，较小的学习率的值会降低梯度下降的速度，但是可以保证成本函数 J 可以在最小值附近收敛，而较大的学习率让梯度下降速度较快，但是可能会因为下降的步子太大而越过最低点，最终无法在最低点附近出收敛。 本篇博客选择1.2作为学习率 123456789101112131415161718192021def update_parameters(parameters, grads, learning_rate=1.2): W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # 更新参数 W1 -= learning_rate * dW1 b1 -= learning_rate * db1 W2 -= learning_rate * dW2 b2 -= learning_rate * db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 构建完整的神经网络模型123456789101112131415161718192021222324252627282930313233def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): # 输入层单元数 n_x = layer_sizes(X, Y)[0] # 输出层单元数 n_y = layer_sizes(X, Y)[2] # 随机初始化参数 parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # 10000次梯度下降的迭代 for i in range(0, num_iterations): # 正向传播，得到Z1、A1、Z2、A2 A2, cache = forward_propagation(X, parameters) # 计算成本函数的值 cost = compute_cost(A2, Y, parameters) # 反向传播，得到各个参数的梯度值 grads = backward_propagation(parameters, cache, X, Y) # 更新参数 parameters = update_parameters(parameters, grads, learning_rate = 1.2) # 每迭代1000下就输出一次cost值 if print_cost and i % 1000 == 0: print ("Cost after iteration %i: %f" %(i, cost)) # 返回最终训练好的参数 return parameters 对样本进行预测我们约定 预测值大于0.5的之后取1，否则取0： 对一个矩阵M而言，如果你希望它的每个元素 如果大于某个阈值 threshold 就用1表示，小于这个阈值就用 0 表示，那么，在python中可以这么实现：M_new = (M &gt; threshold) 1234def predict(parameters, X): A2, cache = forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) return predictions 利用模型预测123456789# 利用含有4个神经元的单隐藏层的神经网络构建分类模型parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)# 可视化分类结果plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))plt.show()# 输出准确率predictions = predict(parameters, X)print('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%') 尝试换一下隐藏层的单元个数12345678910plt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print ("Accuracy for &#123;&#125; hidden units: &#123;&#125; %".format(n_h, accuracy)) 换数据集12345678910111213141516171819202122# Datasetsnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()datasets = &#123;"noisy_circles": noisy_circles, "noisy_moons": noisy_moons, "blobs": blobs, "gaussian_quantiles": gaussian_quantiles&#125;# 在这里选择你要选用的数据集dataset = "noisy_circles"X, Y = datasets[dataset]X, Y = X.T, Y.reshape(1, Y.shape[0])# make blobs binaryif dataset == "blobs": Y = Y%2# 可视化数据plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral);plt.show()# 测试代码如下：parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# 画出边界plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))plt.show()]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅层神经网络的反向传播]]></title>
    <url>%2F2019%2F09%2F23%2F%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[符号约定首先看下图： 我们假设n[l] 表示第 l 层的神经元的数量，那么这个单隐层的神经网络的输入层、隐含层和输出层的维度分别是：n[0]= n_x =3、n[1]=4、n[2]=1。 那么根据浅层神经网络的正向传播)的分析，如果一次性输入带有 m 个样本的矩阵 X 我们可以得到： W^[1] = (4, 3), Z^[1] = (4, m), A^[1] = (4, m) W^[2] = (1, 4), Z^[2] = (1, m), A^[2] = (1, m) 可以总结出如下规律，对第 l 层的单元而言： W^[l] = (n^[l], n^[l-1]), Z^[l] = (n^[l], m), A^[l] = (n^[l], m). 另外，还有一个规律： 对任意层而言： dW 的维度和 W 的维度相同 dZ 的维度和 Z 的维度相同 dA 的维度和 A 的维度相同 注意：以上出现的符号都是将m个样本向量化后的表达。 单个样本的梯度计算讲反向传播前，我们先回顾一下单个样本的正向传播的过程： 其中计算z^[1]时用到的 x 可以看做是 a^[0]，σ( ) 是激活函数中一种，一般情况下，我们更习惯用用 g( ) 来表示激活函数。于是将上述式子一般化可以得到第 l 层的公式为： z^[l] = W^[l]a^[l-1] + b^[l] ————- ① a^[l] = g^[l] ( z^[l] ) —————————- ② 由于我们输入的样本只有一个，所以各个向量的维度如下： W^[l] = (n^[l], n^[l-1]), z^[l] = (n^[l], 1), a^[1] = (n^[l], 1). 我们现在根据式子 ① 和 ② 来讨论反向传播的过程。 因为正向传播的时候我们依次计算了z^[1], a^[1], z^[2], a^[2]最终得到了损失函数L。所以反向传播的时候，我们要从L向前计算梯度。 第一步计算dz^[2]和da^[2]进而算出 dW^[2] 和 db^[2]： da^[2]=dL/da^[2]= -y/a^[2]+(1-y)/(1-a^[2]) dz^[2]= dL/dz^[2] = dL/da^[2]*da^[2]/dz^[2]=-y(1-a^[2])+(1-y)a^[2] = a^[2] - y 说明1： dz^[2] 的维度和z^[2]相同，da^[2] 的维度和a^[2]相同，为(1, 1) g’( z^[2] ) 的维度与 z^[2]维度相同，为(1, 1) 在第二层中，a^[2] 与 z^[2]的维度也相同，为(1, 1) 实际上，dz^[2] 应该等于da^[2] 与 g’( z^[2] ) 的内积的结果，理由我们我们先向下看 说明2 ： 上图中我们可以得到 dW^[2] = dz^[2]乘上 a^[1] 的转置，这里是因为 : dW^[2] 和 W^[2] 的形状是一样的(n^[2], n^[1])也就是(1, 4)， dz^[2] 和 z^[2] 的形状是一样的(n^[2], 1)也就是(1, 1)， a^[1] 的形状是 (n^[1], 1) 也就是(4, 1)，可以得到 a^[1] 的转置 a^( [1]T ) 的形状是 (1, 4)， 这样 dz^[2] 和 a^[1] 的转置 的乘积的形状才能是 dW^[2] 的形状 (1, 4)； 因此 dW^[2] = dz^[2]乘上 a^[1] 的转置。 这里给我们的启发是：向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行转置操作。 第二步计算 da^[1] 和 dz^[1]进而算出 dW^[1] 和 db^[1] 检查矩阵形状是否匹配：（4，1）* （1，1），匹配 说明3： dz^[1] 的维度和z^[1]相同，da^[1] 的维度和a^[1]相同，均为(4, 1) W^[2]T 的维度是 (4, 1)，dz^[2] 的维度是 (1, 1) ，二者的乘积与da^[1] 维度相同。 g’( z^[1] ) 的维度与 z^[1]维度相同，也与da^[1] 维度相同，为(4, 1)。 所以想得到维度为 (4, 1) 的dz^[1] ，da^[1] 与 g’( z^[1] ) 直接的关系为内积 这里给我们的启发是：向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行内积操作。这里就解释了说明（1）留下的问题。 检查维度：（4，3）=（4，1）*（1，3），正确 公式： 多个样本的梯度运算用m个样本作为输入，并进行向量化后正向传播的公式： Z^[1] = W^[1]X + b^[1] A^[1] = g^[1] ( Z^[1] ) Z^[2] = W^[2]A^[1] + b^[2] A^[2] = g^[2] ( Z^[2] ) 由于引入了m个样本，我们也要有个成本函数来衡量整体的表现情况，我们的目的是让J达到最小值 下图中左边列举了我们上面所推导的各种式子，其中图中所用的激活函数 g^[2] () 为sigmoid函数，因而dz^[2] = a^[2] - y。这些左边的式子都是针对单个样本而言的，而右边则是将m个样本作为输入并向量化后的公式的表达形式。 上图的说明： 1、由于成本函数 J 是所有的 损失函数 L 的值求和后的平均值， 那么 J 对 W^[1], b^[1], W^[2], b^[2] 的导数 等价于 各个 L 对 W^[1], b^[1], W^[2], b^[2] 的导数求和后的平均值。所以dW^[1], db^[1], dW^[2], db^[2]的式子中需要乘上 1 / m。 2、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。 3、keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。 更新参数最后，不要忘了要更新参数（式子中的&alpha;是学习率）]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅层神经网络的正向传播]]></title>
    <url>%2F2019%2F09%2F23%2F%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[神经网络概览在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。 如果把上面的图片抽象化，可以得到以下模型： 再进一步简化得到： 上图里，小圆圈是sigmoid单元，它要完成的工作就是先计算 z 然后计算 a 最终作为 y帽 输出。 进而，我们看看下图，这是较为简单的神经网络的样子，它无非就是将多个 sigmoid 单元组合在一起，形成了更复杂的结构。图中每个单元都需要接收数据的输入，并完成数据的输出，其每个单元的计算过程与 logistic回归 的正向传播类似。可以看到，图片里给神经网络分了层次，最左边的是输入层，也就是第0层；中间的是隐藏层，也就是第一层；最右边的是输出层，是第二层。通常，我们不将输入层看做神经网络的一层，因而下图是一个2层的神经网络。 另外要清楚的是，本图中隐藏层只有一个，但实际上，隐藏层可以有多个。由于对用户而言，隐藏层计算得到的数据用户不可预见，也没有太大必要知道，所以称之为隐藏层。也正是因为如此，神经网络的解释器很差。 再进一步认识下这张图片里的标记，隐藏层的每个单元都需要接收输入层的数据，并且各个单元都需要计算 z , 并经过 sigmoid 函数得到各自的 a ，为了便于区分不同层的不同单元的 a，我们做如下约定： a 的右上角有个角标[i]，表示这是第i层的单元；a 的右下角有个角标 j 用于区分这是该层自上向下的第 j 个单元。例如我们用 a^[1]_3 表示这是第一层的第三个单元。输入层的 x1, x2, … xn 可以看做是 a^[0]_1, a^[0]_2 … a^[0]_n. 前一层a[ i ] 的输出，便是后一层 a[ i+1 ] 的所有单元输入。除了输入层外的其它层，也就是隐藏层和输出层的单元都有各自的参数 w 和 b 用于计算 z ，同样是用w^[i]_j, b^[i]_j, z^[i]_j 来区分他们；得到 z^[i]_j 后用sigmoid函数计算 a^[i]_j ，再将本层n个单元计算得到的n个 a 作为下一层的输入，最终在输出层得到预测值 y帽。其计算过程大致如下： 计算一个样本的神经网络输出下图是输入单个样本(该样本含有特征x1, x2, x3)的神经网络图，隐藏层有4个单元： 根据上面的说明，要计算该神经网络的输出，我们首先要计算隐藏层4个单元的输出 第一步就是计算第一层各个单元的 z^[1]_j ，第二步是计算出各个单元的 a^[i]_j，不难想到可以用向量化计算来化简上述操作，我们将所有 w^[1]_j 的转置纵向叠加得到下图的内容，我们将这个大的矩阵记为W^[1]，得出下图： 由于输入的 x 是三维的列向量，所以每个分量x1, x2, x3 都需要一个 w1, w2, w3 对应，因此 w^[1]_j 的转置是一个(1, 3) 的矩阵，又因为隐藏层有4个单元，即 j 的取值为1, 2, 3, 4，故 W^[1] 是 (4, 3) 的矩阵。 同理，第二层，也就是输出层的 W^[2] 由于有4个输入的特征，1个单元，所以 W^[2] 是 (1, 4)的矩阵。 对于只有一个样本的情况，我们不难得到如下式子： z^[1] 是个 (4, 1) 的矩阵。可以再进一步通过 sigmoid 函数得到 至此，第一层神经网络得任务已完成 第二层也就是输出层的工作，无非就是把第一层的 a^[1] 作为输入，继续用 W^[2] 与 a^[1] 相乘 再加上 b^[2] 得到 z^[2]，再通过 sigmoid 函数得到 a^[2] 也就是最终的预测结果 y帽。 至此，我们就完成了一个样本的输出。接下来看看如何用矩阵实现多样本的正向输出。 计算多样本的输出假设我们有m个样本，每个样本有3个特征，并且隐藏层也是4个单元。 那么，通常我们需要使用一个 for 循环将 从 1 遍历至 m 完成以下操作： 角标 ^(i) 表示第 i 个样本。我们可以构造这样一个矩阵 x： 它将所有样本的特征，按列叠加在一起，构成 (3, m) 的矩阵。 如果我们替换上面计算 z^[1] 的过程中使用的 单个样本 x^(1) 为(3, m) 的矩阵 x (也就是下图的绿色框)： 就可以得到下面的式子（为了方便表达，下面的公式中假设 b 等于0）： 到这里，我们求出了 Z^[1] ，并且由于 Z^[1] 是(4, 3)的矩阵W^[1]乘以(3, m)的矩阵x 再加上b，所以它是 (4, m) 的矩阵。再经过 sigmoid 函数即可得到同样是 (4, m) 的矩阵 A^[1]，到此隐藏层的工作完成了。 输出数据矩阵 A^[1]，作为下一层（也就是输出层）的输入参数，经过类似的计算也可以得到 Z^[2] = W^[2] × A^[1] + b^[2]，上面我们分析到 W^[2] 是 (1, 4)的矩阵，所以得到的Z^[2]是 (1, m) 的矩阵，同样经过sigmoid函数处理得到的 A^[2] 也是 (1, m) 的矩阵，A^[2]的每个元素，代表一个样本输出的预测值。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系数据库]]></title>
    <url>%2F2019%2F09%2F19%2F%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[域域是一组具有相同数据类型的值的集合。基数是域中数据的个数 笛卡儿积笛卡尔积直观意义是诸集合各元素间一切可能的组合，可表示为一个二维表。 关系关系是笛卡尔积的有限集合 关系可以有三种类型： ​ 基本表：实际存储数据的逻辑表示 ​ 查询表：查询结果对应的标 ​ 视图表：是虚表，由基本表或其他试图导出，不对应实际存储的数据 关系的基本性质： ​ 列是同质的，每一列中的分量是同一类型的数据，来自同一个域。 ​ 不同的列可出自同一个域，其中的每一列称为一个属性，不同的属性要给予不同的属性名。 ​ 列的顺序无所谓，列的次序可以任意交换。 ​ 行的顺序无所谓，行的次序可以任意交换 ​ 任意两个元祖不能完全相同 ​ 分量必须取原子值，每一个分量都必须是不可分的数据项。这是规范条件中最基本的一条 几个术语：若关系中的某一属性组的值能唯一识别一个元祖，则称该属性为候选码。 若一个关系有多个候选码，则选定其中一个作为主码。 候选码的诸属性称为非码属性 不包含在任何候选码中的属性称为非码属性 若关系模式的所有属性组是这个关系模式的候选码，则称为全码 关系模式关系模式是型，关系是值。关系模式是对关系的描述。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib画图总结]]></title>
    <url>%2F2019%2F09%2F18%2Fmatplotlib%E7%94%BB%E5%9B%BE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧 没有这个库的，pip安装一下。 首先将matplotlib中的pyplot导入，如下： import matplotlib.pyplot as plt，所以以下的plt都是pyplot。 plt.scatter画散点图 123456789101112131415import matplotlib.pyplot as pltx = list(range(1,1001))y = [x**2 for x in x]#s代表点的面积#c代表颜色plt.scatter(x,y,c="green",s=20)#设置标题并加上轴标签plt.title("Squares Numbers",fontsize=24)plt.xlabel("Value",fontsize=14)plt.xlabel("Square of Value",fontsize=14)#设置刻度标记的大小plt.tick_params(axis='both',which='major',labelsize=14)#设置每个坐标的取值范围plt.axis([0,1100,0,1100000])plt.show() 上面的代码运行后，是一条线。那是因为点太多了。绘制很多点的时候，轮廓会连在一起。要删除数据点的轮廓可用scatter，传递参数edgecolor=”none” 模块pyplot内置了一组颜色映射，要使用这些颜色映射，你需要告诉pyplot该如何设置数据集中每个点的颜色。 123456789101112131415import matplotlib.pyplot as pltx = list(range(1,1001))y = [x**2 for x in x]#s代表点的面积#cmap设置颜色映射plt.scatter(x,y,s=20,c=y,cmap=plt.cm.Greens)#设置标题并加上轴标签plt.title("Squares Numbers",fontsize=24)plt.xlabel("Value",fontsize=14)plt.xlabel("Square of Value",fontsize=14)#设置刻度标记的大小plt.tick_params(axis='both',which='major',labelsize=14)#设置每个坐标的取值范围plt.axis([0,1100,0,1100000])plt.show() 保存散点图：savefig 保存图片之前，一定要把plt.show()注释掉，否则会保存一张空白的图片 12#plt.show()plt.savefig("save.jpg",bbox_inches="tight") plt.bar画柱状图，默认是竖直条形图。 1234567import matplotlib.pyplot as pltimport numpy as npy = [20, 10, 30, 25, 15]x = np.arange(5)p1 = plt.bar(x, height=y, width=0.5, )# 展示图形plt.show() 水平条形图：需要把orientation改为horizontal，然后x与y的数据交换 123456789import matplotlib.pyplot as pltimport numpy as npx = [20, 10, 30, 25, 15]y = np.arange(5)# x= 起始位置，bottom= 水平条的底部(左侧), y轴， height：水平条的宽度#width：水平条的长度p1 = plt.bar(x=0, bottom=y, height=0.5, width=x, orientation="horizontal")# 展示图形plt.show() plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘) 画线，也可以用来画折线图。x是横坐标的值。y是纵坐标的值。color参数设置曲线颜色，linewidth设置曲线宽度，linestyle设置曲线风格。 linestyle的可选参数： ‘-‘ solid line style ‘—‘ dashed line style ‘-.’ dash-dot line style ‘:’ dotted line style plt.figure()自定义画布大小，画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小。 figure语法说明 figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True) num:图像编号或名称，数字为编号 ，字符串为名称 figsize:指定figure的宽和高，单位为英寸； dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80 facecolor:背景颜色 edgecolor:边框颜色 frameon:是否显示边框 plt.xticks()设置x轴刻度的表现方式 plt.yticks()设置y轴刻度的表现方式 plt.xlim()设置x轴刻度的取值范围 plt.ylim()设置y轴刻度的取值范围 plt.text(1, 2, “I’m a text”)前两个参数表示文本坐标, 第三个参数为要添加的文本 plt.legend()函数实现了图例功能, 他有两个参数, 第一个为样式对象, 第二个为描述字符,都可以为空 plt.xlabel()添加x轴名字 plt.ylabel()添加y轴名字 plt.tight_layout()tight_layout自动调整subplot(s)参数，以便subplot(s)适应图形区域 plt.subplot()设置画布划分以及图像在画布上输出的位置，将figure设置的画布大小分成几个部分，参数‘221’表示2(row)x2(colu),即将画布分成2x2，两行两列的4块区域，1表示选择图形输出的区域在第一块，图形输出区域参数必须在“行x列”范围 plt.subplots()subplots(nrows,ncols,sharex,sharey,squeeze,subplot_kw,gridspec_kw,**fig_kw) :创建画布和子图 1、nrows,ncols表示将画布分割成几行几列。shares，sharey表示坐标轴的属性是否相同，可选的参数：True、False、row、col。默认值为False。 2、 squeeze bool a.默认参数为True：额外的维度从返回的Axes(轴)对象中挤出，对于N1或1N个子图，返回一个1维数组，对于N*M，N&gt;1和M&gt;1返回一个2维数组。 b.为False，不进行挤压操作：返回一个元素为Axes实例的2维数组，即使它最终是1x1。 3、subplot_kw:字典类型，可选参数。把字典的关键字传递给add_subplot()来创建每个子图。 4、gridspec_kw:字典类型，可选参数。把字典的关键字传递给GridSpec构造函数创建子图放在网格里(grid)。 5、**fig_kw：把所有详细的关键字参数传给figure()函数。 plt.grid()是否开启方格，True为开，False为不显示。默认为False plt.gca()获取当前的子图 plt.xcale()改变x轴的比例。pyplot不仅支持线性轴刻度，还支持对数和logit刻度。如果数据跨越许多数量级，则通常使用此方法 plt.xcale(“logit”)，还有log、symmlog等选择。还可以添加自己的比例。 plt.yscale()改变y轴的比例。用法同plt.xcale一样 随便写了个例子，其他具体的用法还是要自己去练习。 123456789101112import numpy as npimport matplotlib.pyplot as pltplt.scatter([1,3,9],[5,8,2], label="Example one",color="red",s=25,marker="o")plt.plot([2,4,6,8,10],[8,6,2,5,6], label="Example two", color='g')plt.legend()plt.xticks([1,10])plt.yticks([1,15])plt.xlabel('bar number')plt.ylabel('bar height')plt.title('test')plt.grid(True)plt.show()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(4)]]></title>
    <url>%2F2019%2F09%2F17%2FSVM%E8%A7%A3%E8%AF%BB(4)%2F</url>
    <content type="text"><![CDATA[使用SVC时的其他考虑SVC处理多分类问题之前的所有的SVM的(1)-(3)内容，全部是基于二分类的情况来说明的。因为支持向量机是天生二分类的模型。但是，它也可以做多分类。但是SVC在多分类情况的推广是很难的。因为要研究透彻多分类状况下的SVC，就必须研究透彻多分类时所需要的决策边界个数，每个决策边界所需要的支持向量个数，以及这些支持向量如何组合起来计算拉格朗日乘数。本小节只说一小部分。支持向量机是天在生二分类的模型，所以支持向量机在处理多分类问题的时候，是把多分类问题转换成了二分类问题来解决。这种转换有两种模式，一种叫做“一对一”模式（one vs one），一种叫做“一对多”模式(one vs rest)。 在ovo模式下(一对一模式)上，标签中的所有类别都会被两两组合，每两个类别建立一个SVC模型，每个模型生成一个决策边界，分别进行二分类，这种模式下，对于n_class个标签类别的数据来说，SVC会生成总共$C^2_{n_class}$个模型，即会生成总共$C^2_{n_class}$个超平面，其中： ovo模式下，二维空间的三分类状况： 首先让提出紫色点和红色点作为一组，然后求解出两个类之间的SVC和绿色决策边界。然后让绿色点和红色点作为一组，求解出两个类之间的SVC和灰色边界。最后让绿色和紫色组成一组，组成两个类之间的SVC和黄色边界。然后基于三个边界，分别对三个类别进行分类。 ovr模式下，标签中所有的类别会分别于其他类别进行组合，建立n_class个模型，每个模型生成一个决策边界。分别进行二分类。ovr模式下，则会生成以下的决策边界： 紫色类 vs 剩下的类，生成绿色的决策边界。红色类 vs 剩下的类，生成黄色的决策边界。绿色类 vs 剩下的类，生成灰色的决策边界，当类别更多的时候，如此类推下去，我们永远需要n_class个模型。 当类别更多的时候，无论是ovr还是ovo模式需要的决策边界都会越来越多，模型也会越来越复杂，不过ovo模式下的模型计算会更加复杂，因为ovo模式中的决策边界数量增加更快，但相对的，ovo模型也会更加精确。ovr模型计算更快，但是效果往往不是很好。在硬件可以支持的情况下，还是建议选择ovo模式。 模型和超平面的数量变化了，SVC的很多计算、接口、属性都会发生变化，而参数decision_function_shape决定我们究竟使用哪一种分类模式。 decision_function_shape可输入“ovo”，”ovr”，默认”ovr”，对所有分类器，选择使用ovo或者ovr模式。选择ovr模式，则返回的decision_function结构为(n_samples，n_class)。但是当二分类时，尽管选用ovr模式，却会返回(n_samples，)的结构。 选择ovo模式，则使用libsvm中原始的，结构为(n_samples,n_class(n_class-1)/2)的decision_function接口。在ovo模式并且核函数为线性核的情况下，属性coef_和intercepe_会分别返回(n_class\(n_class-1)/2,n_features) 和(n_class*(n_class-1)/2,)的结构，每行对应一个生成的二元分类器。ovo模式只在多分类的状况下使用。 SVC的其它参数、属性和接口的列表在SVM解读(2)/#more](https://brickexperts.github.io/2019/09/15/SVM解读(2)/#more) 线性支持向量机类LinearSVC 线性支持向量机其实与SVC类中选择”linear”作为核函数的功能类似，但是其背后的实现库是liblinear而不是libsvm，这使得在线性数据上，linearSVC的运行速度比SVC中的“linear”核函数要快，不过两者的运行结果相似。在现实中，许多数据都是线性的，因此我们可以依赖计算得更快得LinearSVC类。除此之外，线性支持向量可以很容易地推广到大样本上，还可以支持稀疏矩阵，多分类中也支持ovr方案。 和SVC一样，LinearSVC也有C这个惩罚参数，但LinearSVC在C变大时对C不太敏感，并且在某个阈值之后就不能再改善结果了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(3)]]></title>
    <url>%2F2019%2F09%2F16%2FSVM%E8%A7%A3%E8%AF%BB(3)%2F</url>
    <content type="text"><![CDATA[参数C的理解进阶在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，即无法让训练误差为0，这样的数据被我们称为“存在软间隔的数据”。此时此刻，我们需要让我们决策边界能够忍受一小部分训练误差，我们就不能单纯地寻求最大边际了。 因为对于软间隔地数据来说，边际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡。因此，我们引入松弛系数ζ和松弛系数的系数C作为一个惩罚项，来惩罚我们对最大边际的追求。 那我们的参数C如何影响我们的决策边界呢？在硬间隔的时候，我们的决策边界完全由两个支持向量和最小化损失函数（最大化边际）来决定，而我们的支持向量是两个标签类别不一致的点，即分别是正样本和负样本。然而在软间隔情况下我们的边际依然由支持向量决定，但此时此刻的支持向量可能就不再是来自两种标签类别的点了，而是分布在决策边界两边的，同类别的点。回忆一下我们的图像： 此时我们的虚线超平面&omega;*x+b=1-ζi是由混杂在红色点中间的紫色点来决定的，所以此时此刻，这个蓝色点就是我们的支持向量了。所以软间隔让决定两条虚线超平面的支持向量可能是来自于同一个类别的样本点，而硬间隔的时候两条虚线超平面必须是由来自两个不同类别的支持向量决定的。而C值会决定我们究竟是依赖红色点作为支持向量（只追求最大边界），还是我们要依赖软间隔中，混杂在红色点中的紫色点来作为支持向量（追求最大边界和判断正确的平衡）。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，尽量将掉落在决策边界另一方的样本点预测正确，决策功能会更简单，但代价是训练的准确度，因为此时会有更多红色的点被分类错误。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import svmfrom sklearn.datasets import make_circles, make_moons, make_blobs,make_classificationn_samples = 100datasets = [make_moons(n_samples=n_samples, noise=0.2, random_state=0),make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),make_blobs(n_samples=n_samples, centers=2, random_state=5),make_classification(n_samples=n_samples,n_features =2,n_informative=2,n_redundant=0, random_state=5)]Kernel = ["linear"]#四个数据集分别是什么样子呢？for X,Y in datasets: plt.figure(figsize=(5,4)) plt.scatter(X[:,0],X[:,1],c=Y,s=50,cmap="rainbow")nrows=len(datasets)ncols=len(Kernel) + 1fig, axes = plt.subplots(nrows, ncols,figsize=(10,16)) #第一层循环：在不同的数据集中循环for ds_cnt, (X,Y) in enumerate(datasets): ax = axes[ds_cnt, 0] if ds_cnt == 0: ax.set_title("Input data") ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,edgecolors='k') ax.set_xticks(()) ax.set_yticks(()) for est_idx, kernel in enumerate(Kernel): ax = axes[ds_cnt, est_idx + 1] clf = svm.SVC(kernel=kernel, gamma=2).fit(X, Y) score = clf.score(X, Y) ax.scatter(X[:, 0], X[:, 1], c=Y , zorder=10 , cmap=plt.cm.Paired, edgecolors='k') ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', zorder=10, edgecolors='white') x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape) ax.pcolormesh(XX, YY, Z &gt; 0, cmap=plt.cm.Paired) ax.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1]) ax.set_xticks(()) ax.set_yticks(()) if ds_cnt == 0: ax.set_title(kernel) ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0') , size=15 , bbox=dict(boxstyle='round', alpha=0.8, facecolor='white') # 为分数添加一个白色的格子作为底色 , transform=ax.transAxes # 确定文字所对应的坐标轴，就是ax子图的坐标轴本身 , horizontalalignment='right' # 位于坐标轴的什么方向 )plt.tight_layout()plt.show() 白色圈圈出的就是我们的支持向量，大家可以看到，所有在两条虚线超平面之间的点，和虚线超平面外，但属于另一个类别的点，都被我们认为是支持向量。并不是因为这些点都在我们的超平面上，而是因为我们的超平面由所有的这些点来决定，我们可以通过调节C来移动我们的超平面，让超平面过任何一个白色圈圈出的点。参数C就是这样影响了我们的决策，可以说是彻底改变了SVM的决策过程。 样本不均衡问题分类问题永远逃不过的痛点是样本不均衡问题。样本不均衡是指在一组数据集中，标签的一类天生占有很大的比例，但我们有着捕捉出某种特定的分类的需求的状况。 分类问题天生会倾向于多数的类，让多数类更容易被判断准确，少数类被牺牲掉。因此对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。如果我们希望捕获少数类，模型就会失败。其次，模型评估指标会失去意义。 class_weight可输入字典或者”balanced”，可不填，默认None 对SVC，将类i的参数C设置为class_weight [i] C。如果没有给出具体的class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如{“标签的值1”：权重1，”标签的值2”：权重2}的字典，则参数C将会自动被设为：标签的值1的C：权重1 C，标签的值2的C：权重2C或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为n_samples/(n_classes np.bincount(y)) 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)#其中红色点是少数类，紫色点是多数类clf = svm.SVC(kernel='linear', C=1.0)clf.fit(X, y)#设定class_weightwclf = svm.SVC(kernel='linear', class_weight=&#123;1: 10&#125;)wclf.fit(X, y)#给两个模型分别打分看看，这个分数是accuracy准确度print("没设定class_weight：",clf.score(X,y))print("设定class_weight：",wclf.score(X,y))plt.figure(figsize=(6,5))plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)ax = plt.gca() #获取当前的子图，如果不存在，则创建新的子图xlim = ax.get_xlim()ylim = ax.get_ylim()xx = np.linspace(xlim[0], xlim[1], 30)yy = np.linspace(ylim[0], ylim[1], 30)YY, XX = np.meshgrid(yy, xx)xy = np.vstack([XX.ravel(), YY.ravel()]).T#第二步：找出我们的样本点到决策边界的距离Z_clf = clf.decision_function(xy).reshape(XX.shape)a = ax.contour(XX, YY, Z_clf, colors='black', levels=[0], alpha=0.5, linestyles=['-'])Z_wclf = wclf.decision_function(xy).reshape(XX.shape)b = ax.contour(XX, YY, Z_wclf, colors='red', levels=[0], alpha=0.5, linestyles=['-'])#第三步：画图例 a.collections调用这个等高线对象中画的所有线，返回一个惰性对象plt.legend([a.collections[0], b.collections[0]], ["non weighted", "weighted"],loc="upper right")plt.show() 从图像上可以看出，灰色是我们做样本平衡之前的决策边界。灰色线上方的点被分为一类，下方的点被分为另一类。可以看到，大约有一半少数类（红色）被分错，多数类（紫色点）几乎都被分类正确了。红色是我们做样本平衡之后的决策边界，同样是红色线上方一类，红色线下方一类。可以看到，做了样本平衡后，少数类几乎全部都被分类正确了，但是多数类有许多被分错了。 从准确率的角度来看，不做样本平衡的时候准确率反而更高，做了样本平衡准确率反而变低了，这是因为做了样本平衡后，为了要更有效地捕捉出少数类，模型误伤了许多多数类样本，而多数类被分错的样本数量 &gt; 少数类被分类正确的样本数量，使得模型整体的精确性下降。现在，如果我们的目的是模型整体的准确率，那我们就要拒绝样本平衡，使用class_weight被设置之前的模型。而在现实情况中，我们往往都在捕捉少数类。因为有些情况是宁愿误伤，也要尽量的捕捉少数类。 SVC的模型评估指标上面说了，我们往往都在捕捉少数类的。但是单纯地追求捕捉出少数类，就会成本太高，而不顾及少数类，又会无法达成模型的效果。所以在现实中，我们往往在寻找捕获少数类的能力和将多数类判错后需要付出的成本的平衡。如果一个模型在能够尽量捕获少数类的情况下，还能够尽量对多数类判断正确，则这个模型就非常优秀了。为了评估这样的能力，我们将引入新的模型评估指标：混淆矩阵和ROC曲线来帮助我们 混淆矩阵(Confusion Matrix)精确率和召回率精确度Precision，又叫查准率，表示所有被我们预测为是少数类的样本中，真正的少数类所占的比例 召回率Recall，又被称为敏感度(sensitivity)，真正率，查全率，表示所有真实为1的样本中，被我们预测正确的样本所占的比例。 如果我们希望不计一切代价，找出少数类，那我们就会追求高召回率，相反如果我们的目标不是尽量捕获少数类，那我们就不需要在意召回率。 注意召回率和精确度的分子是相同的，只是分母不同。而召回率和精确度是此消彼长的，两者之间的平衡代表了捕捉少数类的需求和尽量不要误伤多数类的需求的平衡。究竟要偏向于哪一方，取决于我们的业务需求：究竟是误伤多数类的成本更高，还是无法捕捉少数类的代价更高。 为了同时兼顾精确度和召回率，我们创造了两者的调和平均数作为考量两者平衡的综合性指标，称之为F1measure。两个数之间的调和平均倾向于靠近两个数中比较小的那一个数，因此我们追求尽量高的F1 measure，能够保证我们的精确度和召回率都比较高。F1 measure在[0,1]之间分布，越接近1越好： 假负率、特异度(真负率)、假正率从Recall延申出来的另一个评估指标叫做假负率（False Negative Rate），它等于 1 - Recall，用于衡量所有真实为1的样本中，被我们错误判断为0的，通常用得不多。 特异度(Specificity)，也叫做真负率。表示所有真实为0的样本中，被正确预测为0的样本所占的比例。在支持向量机中，可以形象地表示为，决策边界下方的点占所有蓝色点的比例。特异度衡量了一个模型将多数类判断正确的能力，而1 - specificity就是一个模型将多数类判断错误的能力，叫做假正率。 sklearn当中提供了大量的类来帮助我们了解和使用混淆矩阵： ROC曲线ROC曲线，全称The Receiver Operating Characteristic Curve，译为受试者操作特性曲线。这是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。让我们先从概率和阈值开始看起。 阈值(threshold)1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [1.0, 1.0]] #设定两个类别的中心clusters_std = [0.5, 1] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=30)plt.show()from sklearn.linear_model import LogisticRegression as LogiRclf_lo=LogiR().fit(X,y)prob=clf_lo.predict_proba(X)import pandas as pdprob=pd.DataFrame(prob)prob.columns=["0","1"]for i in range(prob.shape[0]): #将0.5设立为阈值 if prob.loc[i,"1"] &gt; 0.5: #创建了一个新的列名为pred的列 prob.loc[i,"pred"] = 1 else: prob.loc[i,"pred"] = 0#创建了名为y_true的列，且将y赋值为该列prob["y_true"] = y#通过“1”列拍序，ascending表示正序or逆序prob = prob.sort_values(by="1",ascending=False)from sklearn.metrics import confusion_matrix as CM, precision_score as P, recall_score as Rfrom sklearn.metrics import accuracy_score as Aprint("混淆矩阵：",CM(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0]))print("精确度：",P(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0]))print("召回率：",R(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0]))print("准确率：",A(prob.loc[:,"y_true"],prob.loc[:,"pred"])) 我们可以设置不同阈值来实验模型的结果，在不同阈值下，我们的模型评估指标会发生变化。我们正利用这一点来观察Recall和FPR之间如何互相影响。但是注意，并不是升高阈值，就一定能够增加或者减少Recall，一切要根据数据的实际分布来进行判断。 概率(probability)我们在画等高线，也就是决策边界的时候曾经使用SVC的接口decision_function，它返回我们输入的特征矩阵中每个样本到划分数据集的超平面的距离。我们在SVM中利用超平面来判断我们的样本，本质上来说，当两个点的距离是相同的符号的时候，越远离超平面的样本点归属于某个标签类的概率就很大。比如说，一个距离超平面0.1的点，和一个距离超平面100的点，明显是距离为0.1的点更有可能是负类别的点混入了边界。同理，一个距离超平面距离为-0.1的点，和一个离超平面距离为-100的点，明显是-100的点的标签更有可能是负类接口decision_function返回的值也因此被我们认为是SVM中的置信度(confidence)。 不过，置信度始终不是概率，它没有边界，可以无限大。大部分时候不是也小数的形式呈现，而SVC的判断过程又不像决策树一样求解出一个比例。为了解决这个矛盾，SVC有重要参数probability。 probability：是否启用概率估计，布尔值，可不填，默认时False。必须在调用fit之前使用它，启用此功能会减慢SVM的计算速度。设置为True会启动，SVC的接口predict_proba和predict_log_proba将生效。在二分类情况下，SVC将使用Platt缩放生成概率，即在decision_function生成的距离上进行Sigmoid压缩，并附加训练数据的交叉验证拟合，来生成类逻辑回归的SVM分数。 1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)plt.show()#其中红色点是少数类，紫色点是多数类clf_proba = svm.SVC(kernel="linear",C=1.0,probability=True).fit(X,y)print("返回预测某标签的概率：",clf_proba.predict_proba(X))print("行数和列数：",clf_proba.predict_proba(X).shape)print("每个样本到划分数据集的超平面的距离：",clf_proba.decision_function(X)) 绘制SVM的ROC曲线现在，我们理解了什么是阈值（threshold），了解了不同阈值会让混淆矩阵产生变化，也了解了如何从我们的分类算法中获取概率。现在，我们就可以开始画我们的ROC曲线了。ROC是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。简单地来说，只要我们有数据和模型，我们就可以在python中绘制出我们的ROC曲线。思考一下，我们要绘制ROC曲线，就必须在我们的数据中去不断调节阈值，不断求解混淆矩阵，然后不断获得我们的横坐标和纵坐标，最后才能够将曲线绘制出来。接下来，我们就来执行这个过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)plt.show()from sklearn.metrics import confusion_matrix as CMfrom sklearn.linear_model import LogisticRegression as LogiRclf_lo=LogiR().fit(X,y)prob=clf_lo.predict_proba(X)import pandas as pdprob=pd.DataFrame(prob)prob.columns=["0","1"]for i in range(prob.shape[0]): #将0.5设立为阈值 if prob.loc[i,"1"] &gt; 0.5: #创建了一个新的列名为pred的列 prob.loc[i,"pred"] = 1 else: prob.loc[i,"pred"] = 0#创建了名为y_true的列，且将y赋值为该列prob["y_true"] = yprob = prob.sort_values(by="1",ascending=False)cm = CM(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0])#开始绘图recall = []FPR = []clf_proba = svm.SVC(kernel="linear",C=1.0,probability=True).fit(X,y)probrange = np.linspace(clf_proba.predict_proba(X)[:,1].min(),clf_proba.predict_proba(X)[:,1].max(),num=50,endpoint=False)from sklearn.metrics import confusion_matrix as CM, recall_score as Rimport matplotlib.pyplot as plotfor i in probrange: y_predict = [] for j in range(X.shape[0]): if clf_proba.predict_proba(X)[j,1] &gt; i: y_predict.append(1) else: y_predict.append(0) cm = CM(y,y_predict,labels=[1,0]) recall.append(cm[0,0]/cm[0,:].sum()) FPR.append(cm[1,0]/cm[1,:].sum())#排序recall.sort()FPR.sort()plt.plot(FPR,recall,c="red")plt.plot(probrange+0.05,probrange+0.05,c="black",linestyle="--")plt.show() 运行后，得出下图： 我们建立ROC曲线的根本目的是找寻Recall和FPR之间的平衡，让我们能够衡量模型在尽量捕捉少数类的时候，误伤多数类的情况会如何变化。横坐标是FPR，代表着模型将多数类判断错误的能力，纵坐标Recall，代表着模型捕捉少数类的能力，所以ROC曲线代表着，随着Recall的不断增加，FPR如何增加。我们希望随着Recall的不断提升，FPR增加得越慢越好，这说明我们可以尽量高效地捕捉出少数类，而不会将很多地多数类判断错误。所以，我们希望看到的图像是，纵坐标急速上升，横坐标缓慢增长，也就是在整个图像左上方的一条弧线。这代表模型的效果很不错，拥有较好的捕获少数类的能力。 中间的虚线代表着，当recall增加1%，我们的FPR也增加1%，也就是说，我们每捕捉出一个少数类，就会有一个多数类被判错，这种情况下，模型的效果就不好，这种模型捕获少数类的结果，会让许多多数类被误伤，从而增加我们的成本。ROC曲线通常都是凸型的。对于一条凸型ROC曲线来说，曲线越靠近左上角越好，越往下越糟糕，曲线如果在虚线的下方，则证明模型完全无法使用。但是它也有可能是一条凹形的ROC曲线。对于一条凹型ROC曲线来说，应该越靠近右下角越好，凹形曲线代表模型的预测结果与真实情况完全相反，那也不算非常糟糕，只要我们手动将模型的结果逆转，就可以得到一条左上方的弧线了。最糟糕的就是，无论曲线是凹形还是凸型，曲线位于图像中间，和虚线非常靠近，那我们拿它无能为力。 现在，我们虽然拥有了这条曲线，但是还是没有具体的数字帮助我们理解ROC曲线和模型效果。接下来将AUC面加，它代表了ROC曲线下方的面积，这个面积越大，代表ROC曲线越接近左上角，模型就越好。 AUC面积sklearn中，我们有帮助我们计算ROC曲线的横坐标假正率FPR，纵坐标Recall和对应的阈值的类sklearn.metrics.roc_curve。同时，我们还有帮助我们计算AUC面积的类sklearn.metrics.roc_auc_score。 AUC面积的分数使用以上类来进行计算，输入的参数也比较简单，就是真实标签，和与roc_curve中一致的置信度分数或者概率值。 1234567891011121314151617from sklearn.metrics import roc_curveFPR, recall, thresholds = roc_curve(y,clf_proba.decision_function(X), pos_label=1)#阈值可以也为负print(FPR,recall,thresholds)from sklearn.metrics import roc_auc_score as AUCarea = AUC(y,clf_proba.decision_function(X))plt.figure()plt.plot(FPR, recall, color='red',label='ROC curve (area = %0.2f)' % area)plt.plot([0, 1], [0, 1], color='black', linestyle='--')plt.xlim([-0.05, 1.05])plt.ylim([-0.05, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('Recall')plt.title('Receiver operating characteristic example')plt.legend(loc="lower right")plt.show() 利用ROC曲线找出最佳阈值对ROC曲线的理解来：ROC曲线反应的是recall增加的时候FPR如何变化，也就是当模型捕获少数类的能力变强的时候，会误伤多数类的情况是否严重。我们的希望是，模型在捕获少数类的能力变强的时候，尽量不误伤多数类，也就是说，随着recall的变大，FPR的大小越小越好。所以我们希望找到的最有点，其实是Recall和FPR差距最大的点。这个点，又叫做约登指数。 12345678910111213141516maxindex=(recall-FPR).tolist().index(max(recall-FPR))#得出最佳阈值print("最佳阈值：",thresholds[maxindex])plt.scatter(FPR[maxindex],recall[maxindex],c="black",s=30)plt.figure()plt.plot(FPR, recall, color='red',label='ROC curve (area = %0.2f)' % area)plt.plot([0, 1], [0, 1], color='black', linestyle='--')plt.scatter(FPR[maxindex],recall[maxindex],c="black",s=30)plt.xlim([-0.05, 1.05])plt.ylim([-0.05, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('Recall')plt.title('Receiver operating characteristic example')plt.legend(loc="lower right")plt.show() 这样，最佳阈值和最好的点都找了出来。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(2)]]></title>
    <url>%2F2019%2F09%2F15%2FSVM%E8%A7%A3%E8%AF%BB(2)%2F</url>
    <content type="text"><![CDATA[非线性SVM与核函数SVC在非线性数据上的推广为了能够找出非线性数据的线性决策边界，我们需要将数据从原始的空间x投射到新空间Φ(x)中，Φ是一个映射函数，它代表了某种非线性的变换，如同我们之前所做过的使用r来升维一样，这种非线性变换看起来是一种非常有效的方式。使用这种变换，线性SVM的原理可以被很容易推广到非线性情况下，其推到过程核逻辑都与线性SVM一摸一样，只不过在第一决策边界之前，我们必须先对数据进行升维，即将原始的x转换成Φ。 重要参数kernel这种变换非常巧妙，但也带有一些实现问题。首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。其次，已知适当的映射函数，我们想要计算类似于Φ(x)i* Φ(xtest)这样的点积，计算量无法巨大。要找出超平面所付出的代价是非常昂贵的。 关键概念：核函数 而解决上面这些问题的数学方式，叫做”核技巧”。是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。具体体现为，K(u,v)= Φ(u)*Φ(v)。而这个原始空间中的点积函数K(u,v)，就被叫做“核函数”。 核函数能够帮助我们解决三个问题： 第一，有了核函数之后，我们无需去担心$\Phi$究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数(positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量的点积一定可以被低维空间中的这两个向量的某种计算来表示。 第二，使用核函数计算低维度中的向量关系比计算原来的Φ(xi) * Φ(xtest)要简单太多了。 第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。 选用不同的核函数，就可以解决不同数据分布下的寻找超平面问题。在SVC中，这个功能由参数“kernel”和一系列与核函数相关的参数来进行控制。之前的代码中我们一直使用这个参数并输入”linear”，但却没有给大家详细讲解，也是因为如果不先理解核函数本身，很难说明这个参数到底在做什么。参数“kernel”在sklearn中可选以下几种选项： 可以看出，除了选项”linear”之外，其他核函数都可以处理非线性问题。多项式核函数有次数d，当d为1的时候它就是再处理线性问题，当d为更高次项的时候它就是在处理非线性问题。我们之前画图时使用的是选项“linear”，自然不能处理环形数据这样非线性的状况。而刚才我们使用的计算r的方法，其实是高斯径向基核函数所对应的功能，在参数”kernel“中输入”rbf“就可以使用这种核函数。 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npfrom sklearn.datasets import make_circlesX,y = make_circles(100, factor=0.1, noise=.1)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plt.show()def plot_svc_decision_function(model,ax=None): if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() x = np.linspace(xlim[0],xlim[1],30) y = np.linspace(ylim[0],ylim[1],30) Y,X = np.meshgrid(y,x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) ax.contour(X, Y, P,colors="k",levels=[-1,0,1],alpha=0.5,linestyles=["--","-","--"]) ax.set_xlim(xlim) ax.set_ylim(ylim) plt.show()clf = SVC(kernel = "linear").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")r = np.exp(-(X**2).sum(1))rlim = np.linspace(min(r),max(r),0.2)from mpl_toolkits import mplot3ddef plot_3D(elev=30,azim=30,X=X,y=y): ax = plt.subplot(projection="3d") ax.scatter3D(X[:,0],X[:,1],r,c=y,s=50,cmap='rainbow') ax.view_init(elev=elev,azim=azim) ax.set_xlabel("x") ax.set_ylabel("y") ax.set_zlabel("r") plt.show()plot_3D()clf = SVC(kernel = "rbf").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plot_svc_decision_function(clf) 运行后，从效果图可以看到，决策边界被完美的找了出来。 探索核函数在不同数据集上的表现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import svmfrom sklearn.datasets import make_circles, make_moons, make_blobs,make_classificationn_samples = 100datasets = [make_moons(n_samples=n_samples, noise=0.2, random_state=0),make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),make_blobs(n_samples=n_samples, centers=2, random_state=5),make_classification(n_samples=n_samples,n_features =2,n_informative=2,n_redundant=0, random_state=5)]Kernel = ["linear","poly","rbf","sigmoid"]#四个数据集分别是什么样子呢？for X,Y in datasets: plt.figure(figsize=(5,4)) plt.scatter(X[:,0],X[:,1],c=Y,s=50,cmap="rainbow")nrows=len(datasets)ncols=len(Kernel) + 1fig, axes = plt.subplots(nrows, ncols,figsize=(20,16))for ds_cnt, (X,Y) in enumerate(datasets):#在图像中的第一列，放置原数据的分布 ax = axes[ds_cnt, 0] if ds_cnt == 0: ax.set_title("Input data") ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,edgecolors='k') ax.set_xticks(()) ax.set_yticks(()) #第二层循环：在不同的核函数中循环 #从图像的第二列开始，一个个填充分类结果 for est_idx, kernel in enumerate(Kernel): #定义子图位置 ax = axes[ds_cnt, est_idx + 1] #建模 clf = svm.SVC(kernel=kernel, gamma=2).fit(X, Y) score = clf.score(X, Y) #绘制图像本身分布的散点图 ax.scatter(X[:, 0], X[:, 1], c=Y ,zorder=10 ,cmap=plt.cm.Paired,edgecolors='k') #绘制支持向量 ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=50, facecolors='none', zorder=10, edgecolors='k') #绘制决策边界 x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 #np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法 #一次性使用最大值和最小值来生成网格 #表示为[起始值：结束值：步长] #如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内 XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] #np.c_，类似于np.vstack的功能 Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape) #填充等高线不同区域的颜色 ax.pcolormesh(XX, YY, Z &gt; 0, cmap=plt.cm.Paired) #绘制等高线 ax.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1]) #设定坐标轴为不显示 ax.set_xticks(()) ax.set_yticks(()) #将标题放在第一行的顶上 if ds_cnt == 0: ax.set_title(kernel) #为每张图添加分类的分数 ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0') , size=15 , bbox=dict(boxstyle='round', alpha=0.8, facecolor='white') #为分数添加一个白色的格子作为底色 , transform=ax.transAxes #确定文字所对应的坐标轴，就是ax子图的坐标轴本身 , horizontalalignment='right' #位于坐标轴的什么方向 )plt.tight_layout()plt.show() 可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。 选取与核函数相关的参数 在知道如何选取核函数后，我们还要观察一下除了kernel之外的核函数相关的参数。对于线性核函数，”kernel”是唯一能够影响它的参数，但是对于其他三种非线性核函数，他们还受到参数gamma，degree以及coef0的影响。参数gamma就是表达式中的&gamma;，degree就是多项式核函数的次数d，参数coef0就是常数项&gamma;。其中，高斯径向基核函数受到gamma的影响，而多项式核函数受到全部三个参数的影响。 12345678910111213141516171819#使用交叉验证得出最好的参数和准确率from sklearn.model_selection import StratifiedShuffleSplitfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_breast_cancerfrom sklearn import svmimport numpy as npdata = load_breast_cancer()X = data.datay = data.targetgamma_range = np.logspace(-10,1,20)coef0_range = np.linspace(0,5,10)param_grid = dict(gamma = gamma_range,coef0 = coef0_range)cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=420)grid = GridSearchCV(svm.SVC(kernel = "poly",degree=1,cache_size=5000),param_grid=param_grid, cv=cv)grid.fit(X, y)print("The best parameters are %s with a score of %0.5f" % (grid.best_params_,grid.best_score_)) 重要参数C关键概念：硬件隔与软件隔 当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在“硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。 看上图，原来的决策边界&omega;x+b=0，原本的平行于决策边界的两个虚线超平面&omega;\x+b=1和&omega;*x+b=-1都依然有效。我们的原始判别函数是: 不过，这些超平面现在无法让数据上的训练误差等于0了，因为此时存在了一个混杂在红色点中的紫色点。于是，我们需要放松我们原始判别函数中的不等条件，来让决策边界能够适用于我们的异常点。于是我们引入松弛系数Φ来帮助我们优化原始的判别函数： 其中ζi&gt;0。可以看的出，这其实是将原来的虚线超平面向图像的上方和下方平移。松弛系数其实蛮好理解的，看上图，在红色点附近的蓝色点在原本的判别函数中必定会被分为红色，所有一定会判断错。我们现在做一条与决策边界平行，且过蓝色点的直线&omega;xi+b=1- ζi(图中的蓝色虚线)。这条直线是由&omega;\xi+b=1平移得到，所以两条直线在纵坐标上的差异就是ζi。而点&omega;xi+b=1的距离就可以表示为 ζi*&omega;/||&omega;||，即ζi在&omega;方向上的投影。由于单位向量是固定的，所以ζi可以作为蓝色点在原始的决策边界上的分类错误的程度的表示。隔得越远，分的越错。注意：ζi并不是点到决策超平面的距离本身。 不难注意到，我们让&omega;*xi+b&gt;=1-ζi作为我们的新决策超平面，是有一定的问题的。虽然我们把异常的蓝色点分类正确了，但我们同时也分错了一系列红色的点。所以我们必须在我们求解最大边际的损失函数中加上一个惩罚项，用来惩罚我们具有巨大松弛系数的决策超平面。我们的拉格朗日函数，拉格朗日对偶函数，也因此都被松弛系数改变。现在，我们的损失函数为 C是用来控制惩罚力度的系数 我们的拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数) 需要满足的KKT条件为 拉格朗日对偶函数为 sklearn类SVC背后使用的最终公式。公式中现在唯一的新变量，松弛系数的惩罚力度C，由我们的参数C来进行控制。 在实际使用中，C和核函数的相关参数（gamma，degree等等）们搭配，往往是SVM调参的重点。与gamma不同，C没有在对偶函数中出现，并且是明确了调参目标的，所以我们可以明确我们究竟是否需要训练集上的高精确度来调整C的方向。默认情况下C为1，通常来说这都是一个合理的参数。 如果我们的数据很嘈杂，那我们往往减小C。当然，我们也可以使用网格搜索或者学习曲线来调整C的值。 123456789101112131415161718192021222324252627282930313233from sklearn.model_selection import StratifiedShuffleSplitfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_breast_cancerimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn import svmimport numpy as npdata = load_breast_cancer()X = data.datay = data.targetC1_range = np.linspace(0.01,30,50)C2_range=np.linspace(0.01,30,50)C3_range=np.linspace(5,7,50)param_grid1 = dict(C=C1_range)cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=420)grid1 = GridSearchCV(svm.SVC(kernel = "linear",gamma= 0.012742749857031322,cache_size=5000),param_grid=param_grid1, cv=cv)grid1.fit(X, y)print("linear The best parameters are %s with a score of %0.5f" % (grid1.best_params_,grid1.best_score_))param_grid2=dict(C=C2_range)grid2 = GridSearchCV(svm.SVC(kernel = "rbf",gamma= 0.012742749857031322,cache_size=5000),param_grid=param_grid2, cv=cv)grid2.fit(X, y)print("rbf(0.01,30,50) The best parameters are %s with a score of %0.5f" % (grid2.best_params_,grid2.best_score_))param_grid3=dict(C=C3_range)grid3=GridSearchCV(svm.SVC(kernel="rbf",gamma=0.012742749857031322,cache_size=5000),param_grid=param_grid3)grid3.fit(X,y)print("rbf(5,7,50) ",grid3.best_params_)print("rbf(5,7,50) ",grid3.best_score_) SVC的参数、属性和接口参数 属性 接口]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2019%2F09%2F12%2FXGBoost%2F</url>
    <content type="text"><![CDATA[前沿在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中和MAC使用pip来安装xgboost的代码： windows： pip install xgboost #安装xgboost库pip install —upgrade xgboost #更新xgboost库 我在这步遇到超时报错，查了以下，改成如下安装： 后面看到一些帖子，发现下面这个方法才是真的好用，在C:\Users\湛蓝星空 这个路径下创建一个pip文件夹，在文件夹创建一个txt，将下面内容加入文件里面，再将文件后缀名改为 .ini。 [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple 这其实就是将源改为清华的源，防止被墙。非常管用 MAC： brew install gcc@7pip3 install xgboost 安装好XGBoost库后，我们有两种方式来使用我们的XGBoost库。第一种方式。是直接使用XGBoost库自己的建模流程。 其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。params可能的取值以及xgboost.train的列表： 我们也可以选择第二种方法，使用xgboost库中的sklearn的API： 有人发现，这两种方法的参数是不同的。其实只是写法不同，功能是相同的。使用xgboost中设定的建模流程来建模，和使用sklearnAPI中的类来建模，模型效果是比较相似的，但是xgboost库本身的运算速度（尤其是交叉验证）以及调参手段比sklearn要简单。 XGBoostXGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的弱评估器，以及应用中的其他过程。 梯度提升树 Boosting过程XGBoost的基础是梯度提升算法，因此我们必须先从了解梯度提升算法开始。梯度提升（Gradient boosting）是构建预测模型的最强大技术之一，它是集成算法中提升法（Boosting）的代表算法。集成算法通过在数据上构建多个弱评估器，汇总所有弱评估器的建模结果，以获取比单个模型更好的回归或分类表现。弱评估器被定义为是表现至少比随机猜测更好的模型，即预测准确率不低于50%的任意模型。 集成不同弱评估器的方法有很多种。有像我们曾经在随机森林的课中介绍的，一次性建立多个平行独立的弱评估器的装袋法。也有像我们今天要介绍的提升法这样，逐一构建弱评估器，经过多次迭代逐渐累积多个弱评估器的方法。提升法的中最著名的算法包括Adaboost和梯度提升树，XGBoost就是由梯度提升树发展而来的。梯度提升树中可以有回归树也可以有分类树，两者都以CART树算法作为主流，XGBoost背后也是CART树，这意味着XGBoost中所有的树都是二叉的。 接下来，我们来了解一些Boosting算法是上面工作：首先，梯度提升回归树是专注于回归的树模型的提升集成模型，其建模过程大致如下：最开始先建立一棵树，然后逐渐迭代，每次迭代过程中都增加一棵树，逐渐形成众多树模型集成的强评估器。 参数 123456789101112131415161718from xgboost import XGBRegressor as XGBRfrom sklearn.ensemble import RandomForestRegressor as RFRfrom sklearn.linear_model import LinearRegression as LinearRfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTSfrom sklearn.metrics import mean_squared_error as MSEimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltdata = load_boston()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)reg = XGBR(n_estimators=100).fit(Xtrain,Ytrain)print("预测：",reg.predict(Xtest)) #传统接口predictprint("准确率：",reg.score(Xtest,Ytest))print("均方误差：",MSE(Ytest,reg.predict(Xtest)))print("模型的重要性分数：",reg.feature_importances_) 使用参数学习曲线观察n_estimators对模型的影响 1234567891011axisx = range(10,1010,50)cv = KFold(n_splits=5, shuffle = True, random_state=42)rs = []for i in axisx: reg = XGBR(n_estimators=i,random_state=420) rs.append(CVS(reg,Xtrain,Ytrain,cv=cv).mean())print(axisx[rs.index(min(rs))],max(rs))plt.figure(figsize=(20,5))plt.plot(axisx,rs,c="red",label="XGB")plt.legend()plt.show() 方差与泛化误差机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定。其中偏差就是训练集上的拟合程度决定，方差是模型的稳定性决定，噪音是不可控的。而泛化误差越小，模型就越理想。 12345678910111213141516171819202122232425262728cv=KFold(n_splits=5,shuffle=True,random_state=42)axisx = range(100,300,10)rs = []var = []ge = []for i in axisx: reg = XGBR(n_estimators=i,random_state=420) cvresult = CVS(reg,Xtrain,Ytrain,cv=cv) rs.append(cvresult.mean()) var.append(cvresult.var()) ge.append((1 - cvresult.mean())**2+cvresult.var())#得出最好的n_estimatorsprint(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))rs = np.array(rs)var = np.array(var)*0.01plt.figure(figsize=(20,5))plt.plot(axisx,rs,c="black",label="XGB")#添加方差线plt.plot(axisx,rs+var,c="red",linestyle='-.')plt.plot(axisx,rs-var,c="red",linestyle='-.')plt.legend()plt.show()plt.figure(figsize=(20,5))#泛化误差的可控部分plt.plot(axisx,ge,c="gray",linestyle='-.')plt.show() 从这个过程中观察n_estimators参数对模型的影响，我们可以得出以下结论： 首先，XGB中的树的数量决定了模型的学习能力，树的数量越多，模型的学习能力越强。只要XGB中树的数量足够 了，即便只有很少的数据， 模型也能够学到训练数据100%的信息，所以XGB也是天生过拟合的模型。但在这种情况 下，模型会变得非常不稳定。 第二，XGB中树的数量很少的时候，对模型的影响较大，当树的数量已经很多的时候，对模型的影响比较小，只能有 微弱的变化。当数据本身就处于过拟合的时候，再使用过多的树能达到的效果甚微，反而浪费计算资源。当唯一指标 或者准确率给出的n_estimators看起来不太可靠的时候，我们可以改造学习曲线来帮助我们。 第三，树的数量提升对模型的影响有极限，开始，模型的表现会随着XGB的树的数量一起提升，但到达某个点之 后，树的数量越多，模型的效果会逐步下降，这也说明了暴力增加n_estimators不一定有效果。这些都和随机森林中的参数n_estimators表现出一致的状态。在随机森林中我们总是先调整n_estimators，当 n_estimators的极限已达到，我们才考虑其他参数，但XGB中的状况明显更加复杂，当数据集不太寻常的时候会更加 复杂。这是我们要给出的第一个超参数，因此还是建议优先调整n_estimators，一般都不会建议一个太大的数目， 300以下为佳。 subsample我们训练模型之前，必然会有一个巨大的数据集。我们都知道树模型是天生过拟合的模型，并且如果数据量太过巨 大，树模型的计算会非常缓慢，因此，我们要对我们的原始数据集进行有放回抽样（bootstrap）。有放回的抽样每 次只能抽取一个样本，若我们需要总共N个样本，就需要抽取N次。每次抽取一个样本的过程是独立的，这一次被抽 到的样本会被放回数据集中，下一次还可能被抽到，因此抽出的数据集中，可能有一些重复的数据。在无论是装袋还是提升的集成算法中，有放回抽样都是我们防止过拟合。 在sklearn中，我们使用参数subsample来控制我们的随机抽样。在xgb和sklearn中，这个参数都默认为1且不能取到0，所以取值范围是(0,1]。这说明我们无法控制模型是否进行随机有放回抽样，只能控制抽样抽出来的样本量大概是多少。 eta or learning_rate在逻辑回归中，我们自定义步长&alpha;来干涉我们的迭代速率，在XGB中看起来却没有这样的设置，但其实不然。在XGB中，我们完整的迭代决策树的公式应该写作： 其中&eta;读作”eta”，是迭代决策树时的步长（shrinkage），又叫做学习率（learning rate）。和逻辑回归中的&alpha;类似，&eta;越大，迭代的速度越快，算法的极限很快被达到，有可能无法收敛到真正的最佳。&eta;越小，越有可能找到更精确的最佳值，更多的空间被留给了后面建立的树，但迭代速度会比较缓慢。 在sklearn中，我们使用参数learning_rate来干涉我们的学习速率： 梯度提升树是XGB的基础，本节中已经介绍了XGB中与梯度提升树的过程相关的四个参数：n_estimators，learning_rate ，silent，subsample。这四个参数的主要目的，其实并不是提升模型表现，更多是了解梯度提升树的原理。现在来看，我们的梯度提升树可是说是由三个重要的部分组成： 一个能够衡量集成算法效果的，能够被最优化的损失函数 一个能够实现预测的弱评估器 一种能够让弱评估器集成的手段，包括我们讲解的迭代方法，抽样手段，样本加权等等过程 booster梯度提升算法中不只有梯度提升树，XGB作为梯度提升算法的进化，自然也不只有树模型一种弱评估器。在XGB中，除了树模型，我们还可以选用线性模型，比如线性回归，来进行集成。虽然主流的XGB依然是树模型，但我们也可以使用其他的模型。基于XGB的这种性质，我们有参数“booster”来控制我们究竟使用怎样的弱评估器。 两个参数都默认为”gbtree”，如果不想使用树模型，则可以自行调整。当XGB使用线性模型的时候，它的许多数学过程就与使用普通的Boosting集成非常相似。 1234for booster in ["gbtree","gblinear","dart"]:reg = XGBR(n_estimators=180,learning_rate=0.1,random_state=420,booster=booster).fit(Xtrain,Ytrain) print(booster) print(reg.score(Xtest,Ytest)) objective在众多机器学习算法中，损失函数的核心是衡量模型的泛化能力，即模型在未知数据上的预测的准确与否，我们训练模型的核心目标也是希望模型能够预测准确。在XGB中，预测准确自然是非常重要的因素，但我们之前提到过，XGB的是实现了模型表现和运算速度的平衡的算法。普通的损失函数，比如错误率，均方误差等，都只能够衡量模型的表现，无法衡量模型的运算速度。回忆一下，我们曾在许多模型中使用空间复杂度和时间复杂度来衡量模型的运算效率。XGB因此引入了模型复杂度来衡量算法的运算效率。因此XGB的目标函数被写作：传统损失函数 + 模型复杂度。 其中i代表数据集中的第i个样本，m表示导入第K棵树的数据总量，K代表建立的所有树(n_estimators)。 第二项代表模型的复杂度，使用树模型的某种变换$\Omega$表示，这个变化代表了一个从树的结构来衡量树模型的复杂度的式子,可以有多种定义。我们在迭代每一颗的过程中，都最小化Obj来求最优的yi。 在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定，而泛化误差越小，模型就越理想。从下面的图可以看出来，方差和偏差是此消彼长的，并且模型的复杂度越高，方差越大，偏差越小。 方差可以被简单地解释为模型在不同数据集上表现出来地稳定性，而偏差是模型预测的准确度。那方差-偏差困境就可以对应到我们的Obj中了： 第一项是衡量我们的偏差，模型越不准确，第一项就会越大。第二项是衡量我们的方差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。所以我们求解Obj的最小值，其实是在求解方差与偏差的平衡点，以求模型的泛化误差最小，运行速度最快。 在应用中，我们使用参数“objective”来确定我们目标函数的第一部分，也就是衡量损失的部分。 xgb自身的调用方式： 1234567891011121314151617181920212223242526from xgboost import XGBRegressor as XGBRfrom sklearn.metrics import mean_squared_error as MSEfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_split as TTSdata = load_boston()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#默认reg:linearreg = XGBR(n_estimators=180,random_state=420).fit(Xtrain,Ytrain)print("sklearn中的XGboost准确率：",reg.score(Xtest,Ytest))print("sklearn中的xgb均方误差：",MSE(Ytest,reg.predict(Xtest)))#xgb实现法import xgboost as xgb#使用类Dmatrix读取数据dtrain = xgb.DMatrix(Xtrain,Ytrain)dtest = xgb.DMatrix(Xtest,Ytest)#写明参数，silent默认为False，通常需要手动将它关闭param = &#123;'silent':False,'objective':'reg:linear',"eta":0.1&#125;num_round = 180#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入bst = xgb.train(param, dtrain, num_round)#接口predictfrom sklearn.metrics import r2_scoreprint("xgb中的准确率：",r2_score(Ytest,bst.predict(dtest)))print("xgb中的均方误差：",MSE(Ytest,bst.predict(dtest))) 看得出来，无论是从R2还是从MSE的角度来看，都是xgb库本身表现更优秀。 (alpha or reg_alpha) &amp; (lambda or reg_lambda)对于XGB来说，每个叶子节点上会有一个预测分数（prediction score），也被称为叶子权重。这个叶子权重就是所有在这个叶子节点上的样本在这一棵树上的回归取值,用fk(xi)或者&omega;来表示。 当有多棵树的时候，集成模型的回归结果就是所有树的预测分数之和，假设这个集成模型中总共有K棵决策树，则整个模型在这个样本i上给出的预测结果为 如下图： 设一棵树上总共包含了T个叶子节点，其中每个叶子节点的索引为j，则这个叶子节点上的样本权重是wj。依据这个，我们定义模型的复杂度$\Omega$(f)为（注意这不是唯一可能的定义，我们当然还可以使用其他的定义，只要满足叶子越多/深度越大，复杂度越大的理论): 使用L2正则项： 使用L1正则项： 还可以两个一起用： 这个结构中有两部分内容，一部分是控制树结构的&gamma;，另一部分则是我们的正则项。叶子数量T可以代表整个树结构，这是因为在XGBoost中所有的树都是CART树（二叉树），所以我们可以根据叶子的数量T判断出树的深度，而&gamma;是我们自定的控制叶子数量的参数。 至于第二部分正则项，类比一下我们岭回归和Lasso的结构，参数&alpha;和&lambda;的作用其实非常容易理解，他们都是控制正则化强度的参数，我们可以二选一使用，也可以一起使用加大正则化的力度。当 和 都为0的时候，目标函数就是普通的梯度提升树的目标函数。 来看正则化系数分别对应的参数： gamma 回忆一下决策树中我们是如何进行计算：我们使用基尼系数或信息熵来衡量分枝之后叶子节点的不纯度，分枝前的信息熵与分治后的信息熵之差叫做信息增益，信息增益最大的特征上的分枝就被我们选中，当信息增益低于某个阈值时，就让树停止生长。在XGB中，我们使用的方式是类似的：我们首先使用目标函数来衡量树的结构的优劣，然后让树从深度0开始生长，每进行一次分枝，我们就计算目标函数减少了多少，当目标函数的降低低于我们设定的某个阈值时，就让树停止生长。 原理还不是很明白，先贴最后的Gain函数 从上面的Gain函数，从上面的目标函数和结构分数之差Gain的式子来看，&gamma;使我们每增加一片叶子就会被剪去的惩罚项。增加的叶子越多，结构分数之差Gain会被惩罚越重，所以&gamma;也被称为”复杂性控制“。所以&gamma;是我们用来防止过拟合的重要参数。&gamma;是对梯度提升树影响最大的参数之一，其效果不逊色与n_estimators和放过拟合神器max_depth。同时&gamma;还是我们让树停止生长的重要参数。 在XGB中，规定只要结构分数之差Gain大于0，即只要目标函数还能减小，我们就允许继续进行分枝。也就是说，我们对于目标函数减小量的要求是： 因此，我们可以直接通过设定&gamma;的大小让XGB的树停止生长。&gamma;因此被定义为，在树的叶节点上进行进一步分枝所需的最小目标函数减少量，在决策树和随机森林中也有类似的参数(min_split_loss，min_samples_split)。 设定越大，算法就越保守，树的叶子数量就越少，模型的复杂度就越低。 12345678910111213141516171819202122232425262728293031from xgboost import XGBRegressor as XGBRfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_split as TTSimport numpy as npfrom matplotlib import pyplot as pltfrom sklearn.model_selection import cross_val_score as CVSdata = load_boston()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)axisx = np.arange(0,5,0.05)rs = []var = []ge = []for i in axisx: reg = XGBR(n_estimators=180,random_state=420,gamma=i) result = CVS(reg,Xtrain,Ytrain,cv=20) rs.append(result.mean()) var.append(result.var()) ge.append((1 - result.mean())**2+result.var())print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))rs = np.array(rs)var = np.array(var)*0.1plt.figure(figsize=(20,5))plt.plot(axisx,rs,c="black",label="XGB")plt.plot(axisx,rs+var,c="red",linestyle='-.')plt.plot(axisx,rs-var,c="red",linestyle='-.')plt.legend()plt.show() 为了调整&gamma;，我们需要引入新的工具，xgboost库中的类xgboost.cv 为了使用xgboost.cv，我们必须要熟悉xgboost自带的模型评估指标。xgboost在建库的时候本着大而全的目标，和sklearn类似，包括了大约20个模型评估指标，然而用于回归和分类的其实只有几个，大部分是用于一些更加高级的功能比如ranking。来看用于回归和分类的评估指标都有哪些： 1234567891011121314151617181920from matplotlib import pyplot as pltimport xgboost as xgbfrom sklearn.datasets import load_breast_cancerdata2 = load_breast_cancer()x2 = data2.datay2 = data2.targetdfull2 = xgb.DMatrix(x2,y2)param1 = &#123;'silent':True,'obj':'binary:logistic',"gamma":0,"nfold":5&#125;param2 = &#123;'silent':True,'obj':'binary:logistic',"gamma":2,"nfold":5&#125;num_round = 100cvresult1 = xgb.cv(param1, dfull2, num_round,metrics=("error"))cvresult2 = xgb.cv(param2, dfull2, num_round,metrics=("error"))plt.figure(figsize=(20,5))plt.grid()plt.plot(range(1,101),cvresult1.iloc[:,0],c="red",label="train,gamma=0")plt.plot(range(1,101),cvresult1.iloc[:,2],c="orange",label="test,gamma=0")plt.plot(range(1,101),cvresult2.iloc[:,0],c="green",label="train,gamma=2")plt.plot(range(1,101),cvresult2.iloc[:,2],c="blue",label="test,gamma=2")plt.legend()plt.show() scale_pos_weightXGB中存在着调节样本不平衡的参数scale_pos_weight,这个参数非常类似于之前随机森林和支持向量机中我们都使用到过的class_weight参数。 其它参数XGBoost应用的核心之一就是减轻过拟合带来的影响。作为树模型，减轻过拟合的方式主要是靠对决策树剪枝来降低模型的复杂度，以求降低方差。在之前的讲解中，我们已经学习了好几个可以用来防止过拟合的参数，包括上一节提到的复杂度控制&lambda;，正则化的两个参数&lambda;和&alpha;，控制迭代速度的参数 以及管理每次迭代前进行的随机有放回抽样的参数subsample。所有的这些参数都可以用来减轻过拟合。但除此之外，我们还有几个影响重大的，专用于剪枝的参数： 使用Pickle保存和调用模型pickle是python编程中比较标准的一个保存和调用模型的库，我们可以使用pickle和open函数的连用，来讲我们的模型保存到本地。 1234567891011121314151617181920212223242526#保存模型的codingimport pickleimport xgboost as xgbfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_breast_cancerdata2 = load_breast_cancer()x2 = data2.datay2 = data2.targetXtrain,Xtest,Ytrain,Ytest=train_test_split(x2,y2,test_size=0.3,random_state=420)dtrain = xgb.DMatrix(Xtrain,Ytrain)#设定参数，对模型进行训练param = &#123;'silent':True,'obj':'reg:linear',"subsample":1,"eta":0.05,"gamma":20,"lambda":3.5,"alpha":0.2,"max_depth":4,"colsample_bytree":0.4,"colsample_bylevel":0.6,"colsample_bynode":1&#125;num_round = 180bst = xgb.train(param, dtrain, num_round)#保存模型pickle.dump(bst, open("xgboostonboston.dat","wb")) 1234567891011121314151617181920#调用模型的codingfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split as TTSfrom sklearn.metrics import mean_squared_error as MSEimport pickleimport xgboost as xgbdata = load_breast_cancer()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#注意，如果我们保存的模型是xgboost库中建立的模型，则导入的数据类型也必须是xgboost库中的数据类型dtest = xgb.DMatrix(Xtest,Ytest)#导入模型loaded_model = pickle.load(open("xgboostonboston.dat", "rb"))print("Loaded model from: xgboostonboston.dat")#做预测ypreds = loaded_model.predict(dtest)from sklearn.metrics import mean_squared_error as MSE, r2_scoreprint("均方误差：",MSE(Ytest,ypreds))print(r2_score(Ytest,ypreds)) 使用Joblib保存和调用模型Joblib是SciPy生态系统中的一部分，它为Python提供保存和调用管道和对象的功能，处理NumPy结构的数据尤其高效，对于很大的数据集和巨大的模型非常有用。Joblib与pickle API非常相似 123456789from xgboost import XGBRegressor as XGBRbst = XGBR(n_estimators=200,eta=0.05,gamma=20,reg_lambda=3.5,reg_alpha=0.2,max_depth=4,colsample_bytree=0.4,colsample_bylevel=0.6).fit(Xtrain,Ytrain)#保存模型joblib.dump(bst,"xgboost-boston.dat")#调用模型loaded_model = joblib.load("xgboost-boston.dat")#这里可以直接导入Xtestypreds = loaded_model.predict(Xtest)print（MSE(Ytest, ypreds)）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn中的神经网络]]></title>
    <url>%2F2019%2F09%2F12%2Fsklearn%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[神经网络概述人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学习的基础。神经网络算法试图模拟生物神经系统的学习过程，以此实现强大的预测性能。不过由于是模仿人类大脑，所以神经网络的模型复杂度很高也是众所周知。在现实应用中，神经网络可以说是解释性最差的模型之一 。 神经网络原理最开始是基于感知机提出——感知机是最古老的机器学习分类算法之一 。和今天的大部分模型比较起来，感知机的泛化能力比较弱。但支持向量机和神经网络都基于它的原理来建立。感知机的原理在支持向量机中介绍过，使用一条线性决策边界z=&omega;*x+b来划分数据集，决策边界的上方是一类数据(z&gt;=0)，决策边界的下方是另一类数据(z&lt;0)的决策过程。使用神经元表示，如下图： 不同的特征数据被输入后，我们通过神经键将它输入我们的神经元。每条神经键上对应不同的参数&omega;，b。因此特征数据会经由神经键被匹配到一个参数向量&omega;，b。基于参数向量&omega;算法可以求解出决策边界z=&omega;*x+b，然后用决策函数sign(z)进行判断，最终预测出标签y并且将结果输出，其中函数sign(z)被称为”激活函数“。这是模拟人类的大脑激活神经元的过程所命名的，其实本质就是决定了预测标签的输出会是什么内容的预测函数。 注意：在这个过程中，有三个非常重要的点： ​ 1、每个输入的特征都会匹配到一个参数&omega;，我们都知道参数向量&omega;中含有的参数数量与我们的特征数目是一致的，在感知机中也是如此。也就是说，任何基于感知机的算法，必须至少要有参数向量&omega;可求。 ​ 2、一个线性关系z，z是由参数和输入的数据共同决定的。这个线性关系，往往就是我们的决策边界，或者它也可以是多元线性回归，逻辑回归等算法的线性表达式 ​ 3、激活函数的结果，是基于激活函数的本身，参数向量&omega;和输入的数据一同计算出来的。也就是说，任何基于感知机的算法。必须要存在一个激活函数。 神经网络就相当于众多感知机的集成，因此，确定激活函数，并找出参数向量&omega;也是神经网络的计算核心。我们来看看神经网络的基本结构： 首先，神经网络有三层。第一层叫做输入层（Input layer），输入特征矩阵用，因此每个神经元上都是一个特征向量。极端情况下，如果一个神经网络只训练一个样本，则每个输入层的神经元上都是这个样本的一个特征取值。 最后一层叫做输出层（output layer），输出预测标签用。如果是回归类，一般输出层只有一个神经元，回归的是所有输入的样本的标签向量。如果是分类，可能会有多个神经元。二分类有两个神经元，多分类有多个神经元，分别输出所有输入的样本对应的每个标签分类下的概率。但无论如何，输出层只有一层，是用于输出预测结果用。 输入层和输出层中间的所有层，叫做隐藏层（Hidden layers），最少一层。也就是说整个神经网络是最少三层。隐藏层是我们用来让算法学习的网络层级，从更靠近输入层的地方开始叫做”上层”，相对而言，更靠近输出层的一层，叫做”下层”。在隐藏层上，每个神经元中都存在一个激活函数，我们的数据被逐层传递，每个下层的神经元都必须处理上层的神经元中的激活函数处理完毕的数据，本质是一个感知器嵌套的过程。隐藏层中上层的每个神经元，都与下层中的每个神经元相连，因此隐藏层的结构随着神经元的变多可以变得非常非常复杂。神经网络的两个要点：参数&omega;和激活函数，也都在这一层发挥作用，因此理解隐藏层是神经网络原理中最难的部分。 神经网络的每一层的结果之间的关系是嵌套，不是迭代。由于我们执行的是嵌套，不是迭代。所以我们的每一个系数之间是相互独立的，每一层的系数之间也是相互独立的，我们不是在执行使用上一层或者上一个神经元的参数来求解下一层或者下一个神经元的参数的过程。我们不断求解，是激活函数的结果a，不是参数&omega;。在一次神经网络计算中，我们没有迭代参数&omega; 上面这张图还不算真实数据中特别复杂的情况，但已经含有总共8*9*9*9*4=23328个&omega;。神经网络可能是我们遇到的最难调参的算法。接下来看看sklearn中的神经网络。 sklearn中的神经网络sklearn是专注于机器学习的库，它在神经网络的模块中特地标注：sklearn不是用于深度学习的平台，因此这个神经网络不具备做深度学习的功能，也不具备处理大型数据的能力。 neural_network.MLPClasifier 重要参数hidden_layer_sizes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import numpy as npfrom sklearn.neural_network import MLPClassifier as DNNfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_score as cvimport matplotlib.pyplot as pltfrom sklearn.datasets import load_breast_cancerfrom sklearn.tree import DecisionTreeClassifier as DTCfrom sklearn.model_selection import train_test_split as TTSfrom time import timeimport datetimedata = load_breast_cancer()X = data.datay = data.targetXtrain, Xtest, Ytrain, Ytest = TTS(X,y,test_size=0.3,random_state=420)times = time()dnn = DNN(hidden_layer_sizes=(100,),random_state=420)print("dnn的交叉验证：",cv(dnn,X,y,cv=5).mean())print("dnn所用的时间：",time() - times)#使用决策树进行一个对比times = time()clf = DTC(random_state=420)print("决策树的交叉验证：",cv(clf,X,y,cv=5).mean())print("决策树所用的时间：",time() - times)dnn = DNN(hidden_layer_sizes=(100,),random_state=420).fit(Xtrain,Ytrain)print("准确率为：",dnn.score(Xtest,Ytest))#使用重要参数n_layers_，得出层数print("n_layers_：",dnn.n_layers_)#可见，默认层数是三层，由于必须要有输入和输出层，所以默认其实就只有一层隐藏层#增加一个隐藏层上的神经元个数dnn = DNN(hidden_layer_sizes=(200,),random_state=420)dnn = dnn.fit(Xtrain,Ytrain)print("增加一个隐藏层的准确率是：",dnn.score(Xtest,Ytest))s = []for i in range(100,2000,100): dnn = DNN(hidden_layer_sizes=(int(i),),random_state=420).fit(Xtrain,Ytrain) s.append(dnn.score(Xtest,Ytest)) print(i,max(s))plt.figure(figsize=(20,5))plt.plot(range(200,2100,100),s)plt.show()#增加隐藏层，控制神经元个数s = []layers = [(100,),(100,100),(100,100,100),(100,100,100,100),(100,100,100,100,100),(100,100,100,100,100,100)]for i in layers: dnn = DNN(hidden_layer_sizes=(i),random_state=420).fit(Xtrain,Ytrain) s.append(dnn.score(Xtest,Ytest)) print(i,max(s))plt.figure(figsize=(20,5))plt.plot(range(3,9),s)plt.xticks([3,4,5,6,7,8])plt.xlabel("Total number of layers")plt.show()#增加隐藏层，控制神经元个数s = []layers = [(100,),(150,150),(200,200,200),(300,300,300,300)]for i in layers: dnn = DNN(hidden_layer_sizes=(i),random_state=420).fit(Xtrain,Ytrain) s.append(dnn.score(Xtest,Ytest)) print(i,max(s))plt.figure(figsize=(20,5))plt.plot(range(3,7),s)plt.xticks([3,4,5,6])plt.xlabel("Total number of layers")plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维算法]]></title>
    <url>%2F2019%2F09%2F12%2F%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更快，效果更好。对图像来说，维度就是图像中特征向量的数量。 sklearn中的降维算法sklearn中降维算法都被包括在模块decomposition中 PCA在降维过程中，我们会减少特征的数量，这意味着删除数据，数据量变少则表示模型可以获取的信息会变少，模型的表现可能会因此受影响。同时，在高维数据中，必然有一些特征是不带有有效的信息的（比如噪音），或者有一些特征带有的信息和其他一些特征是重复的（比如一些特征可能会线性相关）。我们希望能够找出一种办法来帮助我们衡量特征上所带的信息量，让我们在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵。 在特征过程中，我们有说过一种特别的特征选择方法：方差过滤。如果一个特征的方差很小，则意味着这个特征上很可能有大量取值都相同（比如90%都是1，只有10%是0，甚至100%是1），那这一个特征的取值对样本而言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说明这个特征上带有大量的信息。因此，在降维中，PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。 Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。 PCA和特征选择技术都是特征工程的一部分，他们有什么不同？ 特征工程中有三种方式：特征提取，特征创造和特征选择。仔细观察上面的降维例子和上周我们讲解过的特征选择，你发现有什么不同了吗?特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。而PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。以PCA为代表的降维算法因此是特征创造（feature creation，或feature construction）的一种。 重要参数n_compontents：n_components是我们降维后需要的维度，即降维后需要保留的特征数量。 1234567891011121314151617181920212223242526import matplotlib.pyplot as pltfrom sklearn.datasets import load_irisfrom sklearn.decomposition import PCAiris = load_iris()y = iris.targetX = iris.datapca = PCA(n_components=2) #实例化#也可以一步到位，x_dr=pca.fit_transform(X)pca = pca.fit(X) #拟合模型X_dr = pca.transform(X) #获取新矩阵"""可以写三行代码，也可以写成for循环plt.figure()plt.scatter(X_dr[y==0, 0], X_dr[y==0, 1], c="red", label=iris.target_names[0])plt.scatter(X_dr[y==1, 0], X_dr[y==1, 1], c="black", label=iris.target_names[1])plt.scatter(X_dr[y==2, 0], X_dr[y==2, 1], c="orange", label=iris.target_names[2])plt.legend()plt.title('PCA of IRIS dataset')plt.show()"""colors = ['red', 'black', 'orange']plt.figure()for i in [0, 1, 2]: plt.scatter(X_dr[y == i, 0],X_dr[y == i, 1],alpha=.7,c=colors[i],label=iris.target_names[i])plt.legend()plt.title('PCA of IRIS dataset')plt.show() 选择最好的n_components累积可解释方差贡献率曲线当参数n_components中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于转换了新特征空间，但没有减少特征的个数。一般来说，不会使用这种输入方式。但我们却可以使用这种输入方式来画出累计可解释方差贡献率曲线，以此选择最好的n_components的整数取值。累积可解释方差贡献率曲线是一条以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标的曲线，能够帮助我们决定n_components最好的取值。 12345678910#接上面的代码import numpy as nppca_line = PCA().fit(X)#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比#又叫做可解释方差贡献率plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))plt.xticks([1,2,3,4]) #这是为了限制坐标轴显示为整数plt.xlabel("number of components after dimension reduction")plt.ylabel("cumulative explained variance ratio")plt.show() 最大似然估计自选超参数除了输入整数，n_components还有哪些选择呢？让PCA用最大似然估计(maximum likelihood estimation)自选超参数的方法，输入“mle”作为n_components的参数输入，就可以调用这种方法。 12345pca_mle = PCA(n_components="mle")pca_mle = pca_mle.fit(X)X_mle = pca_mle.transform(X)#从这里看出，mle自动为我们选了几个特征print(X_mle.shape) 按信息占比选超参数输入[0,1]之间的浮点数，并且让参数svd_solver ==’full’，表示希望降维后的总解释性方差占比大于n_components指定的百分比，即是说，希望保留百分之多少的信息量。 12345pca_f = PCA(n_components=0.97,svd_solver="full")pca_f = pca_f.fit(X)X_f = pca_f.transform(X)#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比print(pca_f.explained_variance_ratio_) PCA中的SVD其实上面的svd_solver是奇异值分解器。其实SVD可以跳过数学，不计算协方差矩阵，直接找出一个新特征向量组成的n维向量。也就是说，奇异值分解可以不计算协方差矩阵等等计算冗长的矩阵，就直接求出新特征空间和降维后的特征矩阵。简而言之，SVD在矩阵分解中的过程比PCA简单快速。 重要参数svd_solver参数svd_solver是在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选：”auto”, “full”, “arpack”,”randomized”，默认”auto”。 “auto”：基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80％，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生 “full”：从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时间充足的情况。 “arpack”：从scipy.sparse.linalg.svds调用ARPACK分解器来运行截断奇异值分解(SVD truncated)，分解时就将特征数量降到n_components中输入的数值k，可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况，此过程包含一定的随机性。 “randomized”，通过Halko等人的随机方法进行随机SVD。在”full”方法中，分解器会根据原始数据和输入的n_components值去计算和寻找符合需求的新特征向量，但是在”randomized”方法中，分解器会先生成多个随机向量，然后一一去检测这些随机向量中是否有任何一个符合我们的分解需求，如果符合，就保留这个随机向量，并基于这个随机向量来构建后续的向量空间。这个方法已经被Halko等人证明，比”full”模式下计算快很多，并且还能够保证模型运行效果。适合特征矩阵巨大，计算量庞大的情况。 random_state参数random_state在参数svd_solver的值为”arpack” or “randomized”的时候生效，可以控制这两种SVD模式中的随机模式。通常我们就选用”auto“，不必对这个参数纠结太多。 PCA参数、属性和接口参数列表 属性列表 接口列表 PCA对手写数字数据集的降维123456789101112131415from sklearn.decomposition import PCAfrom sklearn.ensemble import RandomForestClassifier as RFCfrom sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata = pd.read_csv(r"digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]pca_line = PCA().fit(X)plt.figure(figsize=[20,5])plt.plot(np.cumsum(pca_line.explained_variance_ratio_))plt.xlabel("number of components after dimension reduction")plt.ylabel("cumulative explained variance ratio")plt.show() 接着降维后维度的学习曲线，继续缩小最佳维度的范围 123456789score = []for i in range(1,101,10): X_dr = PCA(i).fit_transform(X) once = cross_val_score(RFC(n_estimators=10,random_state=0) ,X_dr,y,cv=5).mean() score.append(once)plt.figure(figsize=[20,5])plt.plot(range(1,101,10),score)plt.show() 细化学习曲线，找出降维后的最佳维度 12345678score = []for i in range(10,25): X_dr = PCA(i).fit_transform(X) once = cross_val_score(RFC(n_estimators=10,random_state=0),X_dr,y,cv=5).mean() score.append(once)plt.figure(figsize=[20,5])plt.plot(range(10,25),score)plt.show() 导入找出的最佳维度进行降维，查看模型效果 12x_dr=PCA(23).fit_transform(X)print(cross_val_score(RFC(n_estimators=100,random_state=0),x_dr,y,cv=5).mean())]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2019%2F09%2F08%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[集成算法概述集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型建模结果。基本上所有的机器学习领域都可以看到集成学习的身影。 集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。 多个模型集成称为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器(base estimator)。通常来说，有三类集成算法：装袋法(Bagging)、提升法(Boosting)、stacking 装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。装袋法的代表模型就是随机森林。提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树 sklearn中的集成算法sklearn集成算法模块ensemble 分类树参数、属性和接口 参数n_estimators这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。 random_state随机森林的本质是一种装袋集成算法(bagging)，装袋集成算法是对基评估器的预测结果进行平均或用多数表决元则来决定集成评估器的结果。决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个功能由参数random_state控制。随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树。 bootstrap&amp;oob_score要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。bootstrap参数默认True，代表采用这种有放回的随机抽样技术。通常，这个参数不会被我们设置为False 如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果。 参数的详细解释和其它控制基评估器的参数请参考决策树) 属性随机森林中有三个非常重要的属性：.estimators_，.oob_score_以及.feature_importances_。.estimators_是用来查看随机森林中所有树的列表的。.oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度。而.feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性 接口随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。 回归树 所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致。 criterion：回归树衡量分枝质量的指标，支持的标准有三种：1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。 2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ： 其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 随机森林coding：1234567891011121314151617181920212223242526272829303132333435363738from sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import load_winewine = load_wine()from sklearn.model_selection import train_test_splitXtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)clf = DecisionTreeClassifier(random_state=0)rfc = RandomForestClassifier(random_state=0)clf = clf.fit(Xtrain,Ytrain)rfc = rfc.fit(Xtrain,Ytrain)score_c = clf.score(Xtest,Ytest)score_r = rfc.score(Xtest,Ytest)print("Single Tree:&#123;&#125;".format(score_c),"Random Forest:&#123;&#125;".format(score_r))#画出随机森林和决策树一组交叉验证的对比from sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as pltrfc = RandomForestClassifier(n_estimators=25)rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10)clf = DecisionTreeClassifier()clf_s = cross_val_score(clf,wine.data,wine.target,cv=10)plt.plot(range(1,11),rfc_s,label = "RandomForest")plt.plot(range(1,11),clf_s,label = "Decision Tree")plt.legend()plt.show()#画出随机森林和决策树十组交叉验证的对比rfc_l = []clf_l = []for i in range(10): rfc = RandomForestClassifier(n_estimators=25) rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean() rfc_l.append(rfc_s) clf = DecisionTreeClassifier() clf_s = cross_val_score(clf,wine.data,wine.target,cv=10).mean() clf_l.append(clf_s)plt.plot(range(1,11),rfc_l,label = "Random Forest")plt.plot(range(1,11),clf_l,label = "Decision Tree")plt.legend()plt.show() 机器学习中调参的基本思想泛化误差在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error） 当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。 1）模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点2）模型太复杂就会过拟合，模型太简单就会欠拟合3）对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂4）树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动 偏差和方差观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。 方差和偏差对模型的影响： 然而，方差和偏差是此消彼长的，不可能同时达到最小值 从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。 我们调参的目标是，达到方差和偏差的完美平衡 ！ 随机森林的调参12345678910111213141516171819from sklearn.datasets import load_breast_cancerfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCV,cross_val_scoreimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata=load_breast_cancer()rfc=RandomForestClassifier(n_estimators=100,random_state=90)score_pre=cross_val_score(rfc,data.data,data.target,cv=10).mean()scorel = []for i in range(0,200,10): rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1,random_state=90) score = cross_val_score(rfc,data.data,data.target,cv=10).mean() scorel.append(score)plt.figure(figsize=[20,5])plt.plot(range(1,201,10),scorel)plt.show() 调n_estimators1234567891011121314#从曲线看，n_estimators较平稳且准确率高的范围在35-45之间scorel = []#在划分好的范围内，继续细化学习曲线for i in range(35,45): rfc = RandomForestClassifier(n_estimators=i, n_jobs=-1, random_state=90) score = cross_val_score(rfc,data.data,data.target,cv=10).mean() scorel.append(score)#得出最高准确率的n_estimators，为39print(max(scorel),([*range(35,45)][scorel.index(max(scorel))]))plt.figure(figsize=[20,5])plt.plot(range(35,45),scorel)plt.show() 调整max_depth12345678rfc = RandomForestClassifier(n_estimators=39,random_state=90)param_grid=&#123;"max_depth":np.arange(1,20,1)&#125;GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)#得出最好的深度print("参数为：",GS.best_params_)print(GS.best_score_) 调整max_feature1234567rfc=RandomForestClassifier(n_estimators=39,max_depth=11,random_state=90)param_grid=&#123;"max_features":np.arange(1,20,1)&#125;GS=GridSearchCV(rfc,param_grid=param_grid,cv=10)GS.fit(data.data,data.target)#得出最好的max_featureprint("参数为：",GS.best_params_)print(GS.best_score_) 注意：在这步的max_features升高之后，模型的准确率却没有变化。说明模型本身已经处于泛化误差最低点，已经达到了模型的预测上限，没有参数可以左右的部分了。剩下的那些误差，是噪声决定的，已经没有方差和偏差的舞台了。如果是现实案例，我们到这一步其实就可以停下了，因为复杂度和泛化误差的关系已经告诉我们，模型不能再进步了。调参和训练模型都需要很长的时间，明知道模型不能进步了还继续调整，不是一个有效率的做法。如果我们希望模型更进一步，我们会选择更换算法，或者更换做数据预处理的方式 。但我让我们的探究继续。ps：我不要你觉得，我要我觉得 1234567param_grid=&#123;"min_samples_leaf":np.arange(1,10,1)&#125;rfc = RandomForestClassifier(n_estimators=39,max_depth=11,random_state=90)GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)print(GS.best_params_)print(GS.best_score_) 这步后的准确率还是没有变化，不要改变参数，让它默认就好 调整min_samples_split12345678param_grid=&#123;'min_samples_split':np.arange(2, 2+20, 1)&#125;rfc = RandomForestClassifier(n_estimators=39,random_state=90,max_depth=11)GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)print(GS.best_params_)print(GS.best_score_) 还是没有变化 调整criterion1234567param_grid = &#123;'criterion':['gini', 'entropy']&#125;rfc = RandomForestClassifier(n_estimators=39,random_state=90,max_depth=11)GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)print(GS.best_params_)print(GS.best_score_) 在整个调参过程之中，我们首先调整了n_estimators（无论如何都请先走这一步），然后调整max_depth，通max_depth产生的结果，来判断模型位于复杂度-泛化误差图像的哪一边，从而选择我们应该调整的参数和调参的方向。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库的数据模型]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。 由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求： 1、比较真实地描述现实世界 2、易为用户所理解 3、易于在计算机上实现 为什么需要数据模型？ 由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。 数据模型含有哪些内容？ 数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。 ​ 1、数据结构 ​ 用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面 ​ 2、数据操作 ​ 用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查 ​ 3、数据的约束条件 ​ 是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。 实体联系数据模型的地位与作用： 实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。 数据模型是用来描述数据的一组概念和定义，是描述数据的手段。 ​ 概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。 ​ 逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模 ​ 物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。 逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。 数据模式是对数据结构、联系和约束的描述。数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。 信息世界中的基本概念： ​ (1) 实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。 ​ (2) 属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画 ​ (3) 键(Key)或称为码：唯一标识实体的属性集称为码 ​ (4) 域(Domain)：属性的取值范围称为该属性的域 ​ (5) 实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型 ​ (6) 实体集(Entity Set)：同一类型实体的集合称为实体集 ​ (7) 联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。 概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。 ​ 实体型：用矩形表示，矩形框内写明实体名 ​ 属性：用椭圆表示，并用无向边将其与相应的实体连接起来。 ​ 联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n） ​ 键：用下划线表示。 最常用的数据模型 ​ 非关系模型 ​ 层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。 ​ 满足以下两个条件称为层次模型： ​ (1) 有且仅有一个结点无双亲。这个结点称为“根节点” ​ (2) 其它节点有且仅有一个双亲，但可以有多个后继 ​ 网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。 ​ 网状结构特点： ​ (1) 允许一个以上的结点无双亲； ​ (2) 一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。 ​ 关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。 ​ 关系数据模型的数据结构： ​ 关系(Relation)：一个关系对应通常说的一张表 ​ 元祖(Tuple)：表中的一行即为一个元祖 ​ 属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。 ​ 主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。 ​ 域(Domain)：属性的取值范围 ​ 分量：元祖中的一个属性值 ​ 关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n) 关系数据模型的操作主要包括：查询、插入、删除、更新 关系的完整性约束条件：实体完整性、参照完整性、用户定义完整性]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的评判和调优]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[精确率和召回率 分类模型评估API：F1-score,反应了模型的稳健性 模型选择和调优交叉验证交叉验证为了让被评估的模型更加准确可信 网格搜索网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。 sklearn.model_Selection.GridSearchCV]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means]]></title>
    <url>%2F2019%2F09%2F07%2FK-means%2F</url>
    <content type="text"><![CDATA[无监督学习与聚类算法有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。而聚类算法，就是无监督学习的代表算法。 聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。 聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要实例化，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。 KMeans是如何工作的关键概念：簇与质心 ​ 簇：KMeans算法将一组N个样本的特征矩阵x划分为k个无交集的簇，直观上来看是簇是一组组聚集在一起的数据，在一个簇中的数据被认为是同一类。簇就是聚类的结果表现。 ​ 质心：簇中所有数据的均值通常被称为这个簇的质心。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点纵坐标的均值。同理可推广到高维空间。 在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下 ： ​ 1、随机抽取k个样本作为最初的质心 ​ 2、开始循环 ​ 2.1、将每个样本点分配到离他们最近的质心，生成k个簇 ​ 2.2、对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心 ​ 3、当质心的位置不在发生变化，迭代停止，聚类完成 那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。 对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距离的衡量方法有多种，令x表示簇中的一个样本点，ui表示该簇中的质心，n表示每个样本点中的特征数目，i表示组成点x的每个特征，则该样本点到质心的距离可以由以下距离来度量： 如我们采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为: 其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum ofSquare），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。因此,KMeans追求的是，求解能够让Inertia最小化的质心。实际上，在质心不断变化不断迭代的过程中，总体平方和是越来越小的。我们可以使用数学来证明，当整体平方和最小的时候，质心就不再发生变化了。如此，K-Means的求解过程，就变成了一个最优化问题 损失函数本质是用来衡量模型的拟合效果的，只有有着求解参数需求的算法，才会有损失函数。Kmeans不求解什么参数，它的模型本质也没有在拟合数据，而是在对数据进行一种探索。所以，K-Means不存在什么损失函数。Inertia更像是Kmeans的模型评估指标，而非损失函数。 重要参数和属性 参数n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果。通常，在开始聚类之前，我们并不知道n_clusters究竟是多少，因此我们要对它进行探索。 在之前描述K-Means的基本流程时我们提到过，当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前， 我们也可以使用max_iter，大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下 来。有时候，当我们的n_clusters选择不符合数据的自然分布，或者我们为了业务需求，必须要填入与数据的自然 分布不合的n_clusters，提前让迭代停下来反而能够提升模型的表现。 max_iter：整数，默认300，单次运行的k-means算法的大迭代次数tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下 属性labels_：查看聚好的类别，每个样本所对应的类 cluster_centers_：查看质心 inertia_：查看总距离平方和 n_iter_：实际的迭代次数 coding： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.datasets import make_blobsimport matplotlib.pyplot as plt#这是自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数#random_state代表随机性X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)fig, ax1 = plt.subplots(1)ax1.scatter(X[:, 0], X[:, 1]#画点图,marker='o'#代表点的形状,s=8)#代表点的大小#最开始数据集的形状plt.show()color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(4): ax1.scatter(X[y==i, 0], X[y==i, 1] ,marker='o' ,s=8 ,c=color[i])plt.show()from sklearn.cluster import KMeans#簇为3n_clusters = 3cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)#查看聚好的列表y_pred = cluster.labels_pre = cluster.fit_predict(X)#查看质心centroid = cluster.cluster_centers_#查看总距离平方和inertia = cluster.inertia_print(inertia)color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(n_clusters): ax1.scatter(X[y_pred==i, 0], X[y_pred==i, 1] ,marker='o' ,s=8 ,c=color[i])#画出质心ax1.scatter(centroid[:,0],centroid[:,1],marker="x",s=15,c="black")plt.show()#簇为4n_clusters=4cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)#查看聚好的列表y_pred = cluster.labels_pre = cluster.fit_predict(X)#查看质心centroid = cluster.cluster_centers_#查看总距离平方和inertia = cluster.inertia_print(inertia)color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(n_clusters): ax1.scatter(X[y_pred==i, 0], X[y_pred==i, 1] ,marker='o' ,s=8 ,c=color[i])#画出质心ax1.scatter(centroid[:,0],centroid[:,1],marker="x",s=15,c="black")plt.show() 聚类算法的模型评估上面的算法的簇随着数字的增大，总的距离和在不断的变小。难道我们我们就这样实验得出簇的个数吗？难道簇越小越好吗？看看算法的评估。聚类模型的结果不是某种标签输出，并且聚类的结果是不确定的，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？ KMeans的目标是确保“簇内差异小，簇外差异大”，我们就可以通过衡量簇内差异来衡量聚类的效果。我们刚才说过，Inertia是用距离来衡量簇内差异的指标，因此，我们是否可以使用Inertia来作为聚类的衡量指标呢？Inertia越小模型越好嘛。可以，但是这个指标的缺点和极限太大。 当真实标签未知的时候使用轮廓系数这样的聚类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中轮廓系数是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。单个样本的轮廓系数计算为 这个公式可以看作： 很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。 在sklearn中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 12345#接上面的代码from sklearn.metrics import silhouette_samplesfrom sklearn.metrics import silhouette_scoreprint("4个簇的时候的轮廓系数：",silhouette_score(X,y_pred))print("四个簇的每个样本的轮廓系数：",silhouette_samples(X,y_pred)) 当真实标签未知的时候用CHI除了轮廓系数是常用的，我们还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用。 在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言，Calinski-Harabaz指数s(k)写作如下公式： 其中N为数据集中的样本量，k为簇的个数(即类别的个数)，Bk是组间离散矩阵，即不同簇之间的协方差矩阵， Wk是簇内离散矩阵，即一个簇内数据的协方差矩阵，而tr表示矩阵的迹。在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A) 。数据之间的离散程度越高，协方差矩阵的迹就会越大。组内离散程度低，协方差的迹就会越小，Tr(Wk)也就越小，同时，组间离散程度大，协方差的的迹也会越大， Tr(Bk)就越大，这正是我们希望的，因此Calinski-harabaz指数越高越好。 123#接上面的代码from sklearn.metrics import calinski_harabaz_scoreprint(calinski_harabaz_score(X, y_pred)) 基于轮廓系数选择簇的个数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from sklearn.datasets import make_blobsimport matplotlib.pyplot as pltfrom sklearn.metrics import silhouette_samplesfrom sklearn.metrics import silhouette_scorefrom sklearn.cluster import KMeansimport matplotlib.cm as cmimport numpy as np#自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数#random_state代表随机性X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)n_clusters = 4fig, (ax1, ax2) = plt.subplots(1, 2)fig.set_size_inches(18, 7)ax1.set_xlim([-0.1, 1])ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10])clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X)cluster_labels = clusterer.labels_silhouette_avg = silhouette_score(X, cluster_labels)print("For n_clusters =", n_clusters,"The average silhouette_score is :", silhouette_avg)sample_silhouette_values = silhouette_samples(X, cluster_labels)y_lower = 10for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10ax1.set_title("The silhouette plot for the various clusters.")ax1.set_xlabel("The silhouette coefficient values")ax1.set_ylabel("Cluster label")ax1.axvline(x=silhouette_avg, color="red", linestyle="--")ax1.set_yticks([])ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1],marker='o' #点的形状,s=8 #点的大小,c=colors)centers = clusterer.cluster_centers_ax2.scatter(centers[:, 0], centers[:, 1], marker='x',c="red", alpha=1, s=200)ax2.set_title("The visualization of the clustered data.")ax2.set_xlabel("Feature space for the 1st feature")ax2.set_ylabel("Feature space for the 2nd feature")plt.suptitle(("Silhouette analysis for KMeans clustering on sample data ""with n_clusters = %d" % n_clusters),fontsize=14, fontweight='bold')plt.show()for n_clusters in [2,3,4,5,6,7]: n_clusters = n_clusters fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) ax1.set_xlim([-0.1, 1]) ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10]) clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X) cluster_labels = clusterer.labels_ silhouette_avg = silhouette_score(X, cluster_labels) print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg) sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7 ) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10ax1.set_title("The silhouette plot for the various clusters.")ax1.set_xlabel("The silhouette coefficient values")ax1.set_ylabel("Cluster label")ax1.axvline(x=silhouette_avg, color="red", linestyle="--")ax1.set_yticks([])ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1],marker='o' #点的形状,s=8 #点的大小,c=colors)centers = clusterer.cluster_centers_ax2.scatter(centers[:, 0], centers[:, 1], marker='x', c="red", alpha=1, s=200)ax2.set_title("The visualization of the clustered data.")ax2.set_xlabel("Feature space for the 1st feature")ax2.set_ylabel("Feature space for the 2nd feature")plt.suptitle(("Silhouette analysis for KMeans clustering on sample data ""with n_clusters = %d" % n_clusters),fontsize=14, fontweight='bold')plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2019%2F09%2F07%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归推导过程逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以下线性回归。线性回归写作一个人人熟悉的方程： &theta;被统称为模型的参数，其中&theta;被称为截距。&theta;1—&theta;n被称为系数。我们可以用矩阵表示这个方程，其中x和&theta;都可以被看作是一个列矩阵： 线性回归的任务，就是构造一个预测函数z来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心就是找出模型的参数：&theta;的转置矩阵和&theta;0，著名的最小二乘法就是用来求解线性回归中参数的数学方法。 通过函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务（比如预测产品销量，预测股价等等）。那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型变量，我们要怎么办呢？我们可以通过引入联系函数(link function)，将线性回归方程z变换为g(z)，并且令g(z)的值分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数： Sigmoid函数的公式和性质：Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1。 将z带入Sigmoid，得到二元逻辑回归的一般形式： 而g(z)就是我们逻辑回归返回的标签值。此时,y(x)的取值都在[0,1]之间，因此 y(x)和1-y(x)相加必然为1。令y(x)除以1-y(x)可以得到形似几率得y(x)/(1-y(x))，在此基础上取对数，得： 不难发现，g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。 逻辑回归有着基于训练数据求解参数&theta;的需求，并且希望训练出来的模型能够尽可能地拟合训练数据，即模型在训练集上的预测准确率越靠近100%越好。因此，我们使用”损失函数“这个评估指标，来衡量参数&theta;的优劣，即这一组参数能否使模型在训练集上表现优异。 ​ 关键概念：损失函数，衡量参数&theta;的优劣的评估指标，用来求解最优参数的工具损失函数小，模型在训练集上表现优异，拟合充分，参数优秀损失函数大，模型在训练集上表现差劲，拟合不足，参数糟糕我们追求，能够让损失函数最小化的参数组合。注意：没有”求解参数“需求的模型没有损失函数，比如KNN，决策树。 损失函数只适用于单个训练样本，所以引入了成本函数来。损失函数是衡量单一训练样例的效果。成本函数用于衡量参数w和b的效果。 我们要找到一组w和b是成本函数J最小。 推导过程： 既然是最大似然，我们的目标当然是要最大化似然概率： 对于二分类问题有： 用一个式子表示上面这个分段的函数(写成相乘的形式)： 如果用hθ(xi)表示p0，1 - hθ(xi)表示p1，将max函数换成min，则得到最终形式： 由于我们追求成本函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题：如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。虽然逻辑回归和线性回归是天生欠拟合的模型，但我们还是需要控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。 逻辑回归的类 linear_model.LogisticRegression 正则化重要参数正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。 其中J(&theta;)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数。j代表每个参数。在这里，J要大于等于1，因为我们的参数向量&theta;中，第一个参数是&theta;0，使我们的截距。它通常不参与正则化。上面的式子还有另一种写法，本质是一样的。 L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），参数 &theta;的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。 multi_class输入”ovr”, “multinomial”, “auto”来告知模型，我们要处理的分类问题的类型。默认是”ovr”。 ‘ovr’(one-vs-rest)：表示分类问题是二分类，或让模型使用”一对多”的形式来处理多分类问题。 ‘multinomial’：表示处理多分类问题，这种输入在参数solver是’liblinear’时不可用。 “auto”：表示会根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型。比如说，如果数据是二分类，或者solver的取值为”liblinear”，”auto”会默认选择”ovr”。反之，则会选择”nultinomial”。注意：默认值将在0.22版本中从”ovr”更改为”auto”。 12345678from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressioniris = load_iris()for multi_class in ['multinomial', 'ovr']: clf = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=multi_class).fit(iris.data, iris.target)#打印两种multi_class模式下的训练分数 print("training score : %.3f (%s)" % (clf.score(iris.data, iris.target),multi_class)) solver对于小数据集，‘liblinear’是一个不错的选择，而’sag’和’saga’对于大数据集来说更快。对于多类问题，只有’newton-cg’，‘sag’，’saga’和’lbfgs’处理多项损失。‘newton-cg’，’lbfgs’和’sag’只处理L2 penalty，而’liblinear’和’saga’处理L1 penalty。 参数列表 属性 接口 逻辑回归的特征工程高效的embedded嵌入法我们已经说明了，由于L1正则化会使得部分特征对应的参数为0，因此L1正则化可以用来做特征选择，结合嵌入法的模块SelectFromModel，我们可以很容易就筛选出让模型十分高效的特征。注意，此时我们的目的是，尽量保留原数据上的信息，让模型在降维后的数据上的拟合效果保持优秀，因此我们不考虑训练集测试集的问题，把所有的数据都放入模型进行降维。 12345678910111213from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import cross_val_scorefrom sklearn.feature_selection import SelectFromModeldata = load_breast_cancer()print(data.data.shape)LR_ = LR(solver="liblinear",C=0.9,random_state=420)print(cross_val_score(LR_,data.data,data.target,cv=10).mean())X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)print(X_embedded.shape)print(cross_val_score(LR_,X_embedded,data.target,cv=10).mean()) 看看结果，特征数量被减小到个位数，并且模型的效果却没有下降太多，如果我们要求不高，在这里其实就可以停下了。但是，能否让模型的拟合效果更好呢？在这里，我们有两种调整方式：1）调节SelectFromModel这个类中的参数threshold，这是嵌入法的阈值，表示删除所有参数的绝对值低于这个阈值的特征。现在threshold默认为None，所以SelectFromModel只根据L1正则化的结果来选择了特征，即选择了所有L1正则化后参数不为0的特征。我们此时，只要调整threshold的值（画出threshold的学习曲线），就可以观察不同的threshold下模型的效果如何变化。一旦调整threshold，就不是在使用L1正则化选择特征，而是使用模型的属性.coef_中生成的各个特征的系数来选择。coef_虽然返回的是特征的系数，但是系数的大小和决策树中的feature_ importances_以及降维算法中的可解释性方差explained_vairance_概念相似，其实都是衡量特征的重要程度和贡献度的，因此SelectFromModel中的参数threshold可以设置为coef_的阈值，即可以剔除系数小于threshold中输入的数字的所有特征。 12345678910111213141516fullx = []fsx = []threshold = np.linspace(0,abs((LR_.fit(data.data,data.target).coef_)).max(),20)k=0for i in threshold: X_embedded = SelectFromModel(LR_,threshold=i).fit_transform(data.data,data.target) fullx.append(cross_val_score(LR_,data.data,data.target,cv=5).mean()) fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=5).mean()) print((threshold[k],X_embedded.shape[1])) k+=1plt.figure(figsize=(20,5))plt.plot(threshold,fullx,label="full")plt.plot(threshold,fsx,label="feature selection")plt.xticks(threshold)plt.legend()plt.show() 这种方法其实是比较无效的，大家可以用学习曲线来跑一跑：当threshold越来越大，被删除的特征越来越多，模型的效果也越来越差。2）第二种调整方法，是调逻辑回归的类LR_，通过画C的学习曲线来实现： 123456789101112131415161718192021222324252627282930313233343536373839fullx = []fsx = []C=np.arange(0.01,10.01,0.5)for i in C: LR_ = LR(solver="liblinear",C=i,random_state=420) fullx.append(cross_val_score(LR_,data.data,data.target,cv=10).mean()) X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target) fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())print("最好的准确率和对应的index：",max(fsx),C[fsx.index(max(fsx))])plt.figure(figsize=(20,5))plt.plot(C,fullx,label="full")plt.plot(C,fsx,label="feature selection")plt.xticks(C)plt.legend()plt.show()#继续细化了学习曲线fullx = []fsx = []C=np.arange(6.05,7.05,0.005)for i in C: LR_ = LR(solver="liblinear",C=i,random_state=420) fullx.append(cross_val_score(LR_,data.data,data.target,cv=10).mean()) X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target) fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())print(max(fsx),C[fsx.index(max(fsx))])plt.figure(figsize=(40,5))plt.plot(C,fullx,label="full")plt.plot(C,fsx,label="feature selection")plt.xticks(C)plt.legend()plt.show()#验证模型效果：降维之前LR_ = LR(solver="liblinear",C=6.069999999999999,random_state=420)print("降维之前的模型效果：",cross_val_score(LR_,data.data,data.target,cv=10).mean())#验证模型效果：降维之后LR_ = LR(solver="liblinear",C=6.069999999999999,random_state=420)X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)print("降维之后的模型效果：",cross_val_score(LR_,X_embedded,data.target,cv=10).mean())print(X_embedded.shape) 系数累加法系数累加法的原理非常简单。在PCA中，我们通过绘制累积可解释方差贡献率曲线来选择超参数，在逻辑回归中我们可以使用系数coef_来这样做，并且我们选择特征个数的逻辑也是类似的：找出曲线由锐利变平滑的转折点，转折点之前被累加的特征都是我们需要的，转折点之后的我们都不需要。不过这种方法相对比较麻烦，因为我们要先对特征系数进行从大到小的排序，还要确保我们知道排序后的每个系数对应的原始特征的位置，才能够正确找出那些重要的特征。如果要使用这样的方法，不如直接使用嵌入法来得方便。 包装法相对的，包装法可以直接设定我们需要的特征个数。 梯度下降之前提到过，逻辑回归的数学目的是求解能够让模型最优化的参数&theta;的值，即求解能够让损失函数最小化的的值，而这个求解过程，对于二元逻辑回归来说，有多种方法可以选择，最常见的有梯度下降法(Gradient Descent)，坐标轴下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。 成本函数就是1/m的损失函数之和。损失函数可以衡量算法的结果，成本函数可以看出参数w和b在训练集上的效果。 单个样本的梯度下降 其中，&sigma;就是sigmoid函数： 假设某个样本的特征只有 x1 和 x2 ，即(2,1)的列向量： x = [x1, x2]^T, w 是一个(2,1)的列向量， 那么 根据 z = w^T*X + b 可以得到下面的图，其中w1, w2, b 是未知的参数，之后我们会通过训练获得一组最佳的取值。 首先，让我们计算损失函数的导数 dL(a,y)/da 和 da/dz 不难得到： da / dz = a * (1 - a)； ① ​ da/dz=e^(-z)/(1+e^(-z))^2=(1+e^(-z)-1)/((1+e^(-z))^2)=1/(1+e^(-z))-1/(1+e^(-z))^2=a-a^2=a(1-a) dL / da = - y / a + (1 - y) / (1 - a)； ② 根据链式求导法则，从图的右边向左推，得 dz = dL / dz = ② * ① = a - y； 再进一步： dw1 = dL / dw1 = ( dL / dz ) ( dz / dw1) = dz x1 = ( a - y ) * x1 dw2 = dL / dw2 = ( dL / dz ) ( dz / dw2) = dz x2 = ( a - y ) * x2 db = dL / db = ( dL / dz ) ( dz / db) = dz 1 = ( a - y ) 然后更新w1, w2, b 的值即可： w1 = w1 - dw1 * α （α 是学习率）， w2 = w2 - dw2 * α， b = b - α * db； 这样我们就完成了单个样本的参数更新 多个样本的梯度下降先回顾一下成本函数 J(w, b) 的含义, 它是m个样本损失函数求和后的平均值. 注意到公式里有上角标 i ,表示这是第 i 个样本的数据. 既然 成本函数 J 是 所有样本的 L 累加后的平均值, 那么 J 对 w1 的导数 等价于 各个 L 对 w1 的导数求和后的平均值, 同理, 那么 J 对 w2, b 的导数 也是 各个L 对 w2, b 的导数求和后的平均值. 于是得出各个参数更新过程的伪代码： 1234567891011121314151617181920212223242526# 初始化变量J=0,dw_1=0,dw_2=0,db=0# 遍历m个数据集for i = 1 to m z(i) = w^T*x(i)+b a(i) = sigmoid(z(i)) J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i)) dz(i) = a(i)-y(i) # dw1 dw2 db 用作累加器, 循环结束后除以m即可得到平均值 dw1 += x1(i)dz(i) dw2 += x2(i)dz(i) # 假设每个样本只有两个维度, 因此只要累加 dw1, dw2 即可 # 如果x有n个维度 ,就需要使用 for循环 遍历x所有的特征,累加 dw3, dw4 .... dwn # dw3 += x3(i)dz(i) # ....... # dwn += xn(i)dz(i) db += dz(i) J /= m # 取平均值 dw1 /= m dw2 /= m db /= m # 更新dw1 dw2 db， alpha 是学习率 w1 = w1 - alpha*dw1 w2 = w2 - alpha*dw2 b = b - alpha*db 这样, m 个样本的各个参数也就能得到调整了. 但是这部分代码, 仅仅更新了这些参数一次, 需要多次执行, 才能让 J 沿着梯度下降到最低点。这部分代码使用的 for 循环会造成执行效率不高, 如果我们可以把部分数据向量化, 利用矩阵乘法代替 for 循环可以大幅度提高程序执行效率。 在梯度下降过程中，应该有两个for循环。第一个大的for循环代表特征数量，第二个for嵌在第一个for循环里，代表样本数量。两个显式for循环会大大减慢运行速度。为了加快速度，可以通过向量化。初始化的时候用np.zeros初始化成零向量，利用np.dot实现&omega;*x。 向量化我们对代码的改动： ① 使用 numpy 生成 (n_x, 1) 的列矩阵存放w1, w2, w3..wn , 并初始化为0 ② 利用矩阵的加法，将第 i 个样本 x^(i) 的每个特征全部乘以 dz^(i) 并与 列向量 dw 相加 ③ 利用 numpy 的特性， dw /= m 可以使得 dw 中每个元素都除以 m 去掉内层for循环后的伪代码： 1234567Z = np.dot(w^T, X) + bA = σ(Z)dZ = A - Ydw = 1 / m * np.dot(X, dZ^T)db = 1 / m * np.sum( dZ )w = w - dw * alphab = b - db * alpha 这样就完成了m个样本的梯度下降的一次迭代。然而，我们需要多次迭代才能使得成本函数 J 到达最低点，因此 我们仍然需要使用 for 循环，外层for循环暂时还没有办法将其去掉。 步长的概念与解惑： 许多博客和教材在描述步长的时候，声称它是”梯度下降中每一步沿梯度的反方向前进的长度“，”沿着最陡峭最易下山的位置走的那一步的长度“或者”梯度下降中每一步损失函数减小的量“，甚至有说，步长是二维平面著名的求导三角形中的”斜边“或者“对边”的。这些说法都是错误的 请看下图，A(&theta;a,J(&theta;a))就是小球最初的位置，B(&theta;b,J(&theta;b))就是一次滚动后小球移动到的位置。从A到B的方向就是梯度向量的反方向，指向损失函数在A点下降最快的方向。而梯度向量的大小是点A在图像上对&theta;求导后的结果，也是点A切线方向的斜率，橙色角的tan结果，记作d。 梯度下降每走一步，损失函数减小的量，是损失函数在&theta;变化之后的取值的变化，写作J(&theta;b)-J(&theta;a)。梯度下降每走一步，参数变量的变化，写作&theta;a-&theta;b，根据我们参数向量的迭代公式，就是我们的步长梯度向量的大小。记作&alpha;\d，这是三角形的邻边。梯度下降中每走一步，也就是三角形中的斜边。 所以，步长不是任何物理距离，它甚至不是梯度下降过程中任何距离的直接变化，它是梯度向量的大小d上的一个比例，影响着参数向量&theta;每次迭代后改变的部分。 不难发现，既然参数迭代是靠梯度向量的大小d * 步长&alpha;来实现的，而J(&theta;)的降低又是靠调节&theta;来实现的，所以步长可以调节损失函数下降的速率。在损失函数降低的方向上，步长越长,&theta;的变动就越大。相对的，步长如果很短，&theta;的每次变动就很小。具体地说，如果步长太大，损失函数下降得就非常快，需要的迭代次数就很少，但梯度下降过程可能跳过损失函数的最低点，无法获取最优值。而步长太小，虽然函数会逐渐逼近我们需要的最低点，但迭代的速度却很缓慢，迭代次数就需要很多。 在彩色图中，小球在进入深蓝色区域后，并没有直接找到某个点，而是在深蓝色区域中来回震荡了数次才停下，这种”震荡“其实就是因为我们设置的步长太大的缘故。但是在我们开始梯度下降之前，我们并不知道什么样的步长才合适，但梯度下降一定要在某个时候停止才可以，否则模型可能会无限地迭代下去。因此，在sklearn当中，我们设置参数max_iter最大迭代次数来代替步长，帮助我们控制模型的迭代速度并适时地让模型停下。max_iter越大，代表步长越小，模型迭代时间越长，反之，则代表步长设置很大，模型迭代时间很短。 迭代结束，获取到J(&theta;)的最小值后，我们就可以找出这个最小值&theta;对应的参数向量 ，逻辑回归的预测函数也就可以根据这个参数向量&theta;来建立了。 接下来的是max_iter的学习曲线coding： 1234567891011121314151617181920212223242526272829from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoredata = load_breast_cancer()X = data.datay = data.targetl2 = []l2test = []Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)for i in np.arange(1,201,10): lrl2 = LR(penalty="l2",solver="liblinear",C=0.9,max_iter=i) lrl2 = lrl2.fit(Xtrain,Ytrain) l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain)) l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))graph = [l2,l2test]color = ["black","gray"]label = ["L2","L2test"]plt.figure(figsize=(20,5))for i in range(len(graph)): plt.plot(np.arange(1,201,10),graph[i],color[i],label=label[i])plt.legend(loc=4)plt.xticks(np.arange(1, 201, 10))plt.show()lr = LR(penalty="l2",solver="liblinear",C=0.9,max_iter=300).fit(Xtrain,Ytrain)#.n_iter_来调用本次调用本次求解中真正实现的迭代次数print(lr.n_iter_) 运行上面的代码。会弹出红色警告。因为max_iter中限制的步数已经走完了，逻辑回归却还没找到损失函数的最小值。参数&theta;还没有收敛，sklearn就会弹出警告： 当参数solver=”liblinear”： 当参数solver=“sag”： 虽然写法看起来略有不同，但其实都是一个含义，这是在提醒我们：参数没有收敛，请增大max_iter中输入的数字。但我们不一定要听sklearn的。max_iter很大，意味着步长小，模型运行得会更加缓慢。虽然我们在梯度下降中追求的是损失函数的最小值，但这也可能意味着我们的模型会过拟合（在训练集上表现得太好，在测试集上却不一定），因此，如果在max_iter报红条的情况下，模型的训练和预测效果都已经不错了，那我们就不需要再增大max_iter中的数目了，毕竟一切都以模型的预测效果为基准——只要最终的预测效果好，运行又快，那就一切都好，无所谓是否报红色警告了 coding1234567891011121314151617181920212223242526272829303132333435363738394041#建立两个逻辑回归，L1正则化和L2正则化。效果一目了然from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoredata = load_breast_cancer()X = data.datay = data.targetdata.data.shapelrl1 = LR(penalty="l1",solver="liblinear",C=0.5,max_iter=1000)lrl2 = LR(penalty="l2",solver="liblinear",C=0.5,max_iter=1000)#逻辑回归的重要属性coef_，查看每个特征所对应的参数lrl1 = lrl1.fit(X,y)lrl1.coef_(lrl1.coef_ != 0).sum(axis=1)lrl2 = lrl2.fit(X,y)lrl2.coef_l1 = []l2 = []l1test = []l2test = []Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)for i in np.linspace(0.05,1,19): lrl1 = LR(penalty="l1",solver="liblinear",C=i,max_iter=1000) lrl2 = LR(penalty="l2",solver="liblinear",C=i,max_iter=1000) lrl1 = lrl1.fit(Xtrain,Ytrain) l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain)) l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest)) lrl2 = lrl2.fit(Xtrain,Ytrain) l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain)) l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))graph = [l1,l2,l1test,l2test]color = ["green","black","lightgreen","gray"]label = ["L1","L2","L1test","L2test"]plt.figure(figsize=(6,6))for i in range(len(graph)): plt.plot(np.linspace(0.05,1,19),graph[i],color[i],label=label[i])plt.legend(loc=4) #图例的位置在哪里? 4表示，右下角plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归]]></title>
    <url>%2F2019%2F09%2F07%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归原理回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目标根本不是求解出标签，注意加以区别。 线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。 线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个有n个特征的样本i而言，它的回归结果可以写作一个几乎人人熟悉的方程 ： &omega;被统称为模型的参数。&omega;0被称为截距&omega;1~&omega;n被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 Xi1-Xin是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： 我们可以使用矩阵来表示这个方程： y=X&omega;，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中&omega;可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量&omega;。 在多元线性回归中，我们在损失函数如下定义： 在这个平方结果下，我们的真实标签和预测值分别如上图表示，也就是说，这个损失函数是在计算我们的真实标签和预测值之间的距离。因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ： 第一次看不明白上面红字，后面看了书，才明白。这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。 现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对&omega;求导。 我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得： 线性回归APIsklearn中的线性模型模块是linear_model。 coding： 模型评估指标回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。第一，我们是否预测到了正确的数值。第二，我们是否拟合到了足够的信息。 sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 ​ 注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格 12345678910111213141516171819from sklearn.linear_model import LinearRegression as LRfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import cross_val_scorefrom sklearn.metrics import mean_squared_errorfrom sklearn.datasets import fetch_california_housing as fchhousevalue=fch()xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=0.3,random_state=420)std_x=StandardScaler()xtrain=std_x.fit_transform(xtrain)xtest=std_x.transform(xtest)std_y=StandardScaler()ytrain=std_y.fit_transform(ytrain.reshape(-1,1))ytest=std_y.transform(ytest.reshape(-1,1))reg=LR().fit(xtrain,ytrain)ypredict=reg.predict(xtest)print(std_y.inverse_transform(ypredict))print("均方误差：",mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="mean_squared_error")) 这里如果我们运行上面的代码，会在第十九行报错： 我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。 1print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="neg_mean_squared_error")) 为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助: 在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。 R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。 12print("R2",r2_score(ypredict,ytest))print(reg.score(xtest,ytest)) ????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是预测值在分子，真实值在分母。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数： 1234#直接用r2_scoreprint("R2",r2_score(y_true=ytest,y_pred=ypredict))#利用接口scoreprint(reg.score(xtest,ytest)) 123456#EVS的两种调用方法from sklearn.metrics import explained_variance_score as evs#第一种print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="explained_variance"))#第二种print("evs",evs(ytest,ypredict)) 多重共线性 矩阵A中第一行和第三行的关系，被称为“精确相关关系”，即完全相关，一行可使另一行为0。在这种精确相关关系下，矩阵A的行列式为0，则矩阵A的逆不可能存在。在我们的最小二乘法中，如果矩阵中存在这种精确相关关系，则逆不存在，最小二乘法完全无法使用，线性回归会无法求出结果。 矩阵B中第一行和第三行的关系不太一样，他们之间非常接近于”精确相关关系“，但又不是完全相关，一行不能使另一行为0，这种关系被称为”高度相关关系“。在这种高度相关关系下，矩阵的行列式不为0，但是一个非常接近0数，矩阵A的逆存在，不过接近于无限大。在这种情况下，最小二乘法可以使用，不过得到的逆会很大，直接影响我们对参数向量w的求解： 这样求解出来的参数向量w会很大，因此会影响建模的结果，造成模型有偏差或者不可用。精确相关关系和高度相关关系并称为”多重共线性”。在多重共线性下，模型无法建立，或者模型不可用。 相对的，矩阵C的行之间结果相互独立，梯形矩阵看起来非常正常，它的对角线上没有任何元素特别接近于0，因此其行列式也就不会接近0或者为0，因此矩阵C得出的参数向量w就不会有太大偏差，对于我们拟合而言是比较理想的。 从上面的所有过程我们可以看得出来，一个矩阵如果要满秩，则要求矩阵中每个向量之间不能存在多重共线性。这也构成了线性回归算法对于特征矩阵的要求。 多重共线性与相关性多重共线性如果存在，则线性回归就无法使用最小二乘法来进行求解，或者求解就会出现偏差。幸运的是，不能存在多重共线性，不代表不能存在相关性——机器学习不要求特征之间必须独立，必须不相关，只要不是高度相关或者精确相关就好。 关键概念：多重共线性与相关性 多重共线性是一种统计现象，是指线性模型中的特征（解释变量）之间由于存在精确相关关系或高度相关关系，多重共线性的存在会使模型无法建立，或者估计失真。多重共线性使用指标方差膨胀因子（variance inflation factor，VIF）来进行衡量（from statsmodels.stats.outliers_influence import variance_inflation_factor），通常当我们提到“共线性”，都特指多重共线性。 相关性是衡量两个或多个变量一起波动的程度的指标，它可以是正的，负的或者0。当我们说变量之间具有相关性，通常是指线性相关性，线性相关一般由皮尔逊相关系数进行衡量，非线性相关可以使用斯皮尔曼相关系数或者互信息法进行衡量。 处理多重共线性的方法 这三种手段中，第一种相对耗时耗力，需要较多的人工操作，并且会需要混合各种统计学中的知识和检验来进行使用。第二种手段在现实中应用较多，不过由于理论复杂，效果也不是非常高效。我们的核心是使用第三种方法：改进线性回归来处理多重共线性。为此，岭回归、Lasso、弹性网就被研究出来了。 岭回归在线性模型之中，除了线性回归之外，最知名的就是岭回归与Lasso了。这两个算法非常神秘，他们的原理和应用都不像其他算法那样高调，学习资料也很少。这可能是因为这两个算法不是为了提升模型表现，而是为了修复漏洞而设计的。 岭回归在多元线性回归的损失函数上加上了正则项，表达为系数w的L2范式（即系数w的平方项）乘以正则化系数&alpha;。岭回归的损失函数的完整表达式写作： linear_model.Ridge在sklearn中，岭回归由线性模型库中的Ridge类来调用 和线性回归相比，岭回归的参数稍微多了那么一点点，但是真正核心的参数就是我们的正则项的系数&alpha; ，其他的参数是当我们希望使用最小二乘法之外的求解方法求解岭回归的时候才需要的，通常我们完全不会去触碰这些参数。所以,大家只需要了解&alpha;的用法就可以了。 123456789101112131415161718192021222324252627282930313233343536import numpy as npimport pandas as pdfrom sklearn.linear_model import Ridge, LinearRegression, Lassofrom sklearn.model_selection import train_test_split as TTSfrom sklearn.datasets import fetch_california_housing as fchfrom sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as plthousevalue = fch()X = pd.DataFrame(housevalue.data)#print(housevalue.data)print(X.head())y = housevalue.targetX.columns = ["住户收入中位数","房屋使用年代中位数","平均房间数目","平均卧室数目","街区人口","平均入住率","街区的纬度","街区的经度"]Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#数据集索引恢复print(Xtest.shape[0])for i in [Xtrain,Xtest]: i.index = range(i.shape[0])#使用岭回归来进行建模reg = Ridge(alpha=1).fit(Xtrain,Ytrain)reg.score(Xtest,Ytest)alpharange = np.arange(1,1001,100)ridge, lr = [], []for alpha in alpharange: reg = Ridge(alpha=alpha) linear = LinearRegression() regs = cross_val_score(reg,X,y,cv=5,scoring = "r2").mean() linears = cross_val_score(linear,X,y,cv=5,scoring = "r2").mean() ridge.append(regs) lr.append(linears)plt.plot(alpharange,ridge,color="red",label="Ridge")plt.plot(alpharange,lr,color="orange",label="LR")plt.title("Mean")plt.legend()plt.show() 运行上面代码，可以看出，加利佛尼亚数据集上，岭回归的结果轻微上升，随后骤降。可以说，加利佛尼亚房屋价值数据集带有很轻微的一部分共线性，这种共线性被正则化参数 消除后，模型的效果提升了一点点，但是对于整个模型而言是杯水车薪。在过了控制多重共线性的点后，模型的效果飞速下降，显然是正则化的程度太重，挤占了参数 本来的估计空间。从这个结果可以看出，加利佛尼亚数据集的核心问题不在于多重共线性，岭回归不能够提升模型表现。 123456789101112131415#观察模型方差alpharange = np.arange(1,1001,100)ridge, lr = [], []for alpha in alpharange: reg = Ridge(alpha=alpha) linear = LinearRegression() varR = cross_val_score(reg,X,y,cv=5,scoring="r2").var() varLR = cross_val_score(linear,X,y,cv=5,scoring="r2").var() ridge.append(varR) lr.append(varLR)plt.plot(alpharange,ridge,color="red",label="Ridge")plt.plot(alpharange,lr,color="orange",label="LR")plt.title("Variance")plt.legend()plt.show() 可以发现，模型的方差上升快速。虽然岭回归和Lasso不是设计来提升模型表现，而是专注于解决多重共线性问题的，但当&alpha;在一定范围内变动的时候，消除多重共线性也许能够一定程度上提高模型的泛化能力。 不是很明白这句话，后面补 选取最佳的正则化参数既然要选择&alpha;的范围，我们就不可避免的进行最优参数的选择。在各种机器学习教材中，总是教导使用岭迹图来判断正则项参数的最佳取值。传统的岭迹图长成下图： 这一个以正则化参数为横坐标，线性模型求解的系数&alpha;为纵坐标的图像，其中每一条彩色的线都是一个系数&alpha;。其目标是建立正则化参数与系数&alpha;之间的直接关系，以此来观察正则化参数的变化如何影响了系数w的拟合。岭迹图认为，线条交叉越多，则说明特征之间的多重共线性越高。我们应该选择系数较为平稳的喇叭口所对应的&alpha;取值作为最佳的正则化参数的取值。绘制岭迹图的方法非常简单，代码如下: 123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model#创造10*10的希尔伯特矩阵X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])y = np.ones(10)#计算横坐标n_alphas = 200alphas = np.logspace(-10, -2, n_alphas)coefs = []for a in alphas: ridge = linear_model.Ridge(alpha=a, fit_intercept=False) ridge.fit(X, y) coefs.append(ridge.coef_)#绘图展示结果ax = plt.gca()ax.plot(alphas, coefs)ax.set_xscale('log')ax.set_xlim(ax.get_xlim()[::-1]) #将横坐标逆转plt.xlabel('正则化参数alpha')plt.ylabel('系数w')plt.title('岭回归下的岭迹图')plt.axis('tight')plt.show() 我非常不建议大家使用岭迹图来作为寻找最佳参数的标准。有这样的两个理由： 1、岭迹图的很多细节，很难以解释。比如为什么多重共线性存在会使得线与线之间有很多交点？当&alpha;很大了之后看上去所有的系数都很接近于0，难道不是那时候线之间的交点最多吗？ 2、岭迹图的评判标准，非常模糊。哪里才是最佳的喇叭口？哪里才是所谓的系数开始变得”平稳“的时候？一千个读者一千个哈姆雷特的画像？未免也太不严谨了 我们应该使用交叉验证来选择最佳的正则化系数。在sklearn中，我们有带交叉验证的岭回归可以使用： RidgeCV的重要参数、属性和接口： 123456789101112131415161718import numpy as npimport pandas as pdfrom sklearn.linear_model import RidgeCV, LinearRegressionfrom sklearn.datasets import fetch_california_housing as fchhousevalue = fch()X = pd.DataFrame(housevalue.data)y = housevalue.targetX.columns = ["住户收入中位数","房屋使用年代中位数","平均房间数目","平均卧室数目","街区人口","平均入住率","街区的纬度","街区的经度"]Ridge_ = RidgeCV(alphas=np.arange(1,1001,100),store_cv_values=True).fit(X, y)#无关交叉验证的岭回归结果print("没有交叉验证的岭回归：",Ridge_.score(X,y))#调用所有交叉验证的结果print("调用所有交叉验证：",Ridge_.cv_values_.shape)#进行平均后可以查看每个正则化系数取值下的交叉验证结果print("平均后的每个正则化系数交叉验证结果：",max(Ridge_.cv_values_.mean(axis=0)))#查看被选择出来的最佳正则化系数print("最佳正则化系数：",Ridge_.alpha_) Lasso除了岭回归之外，最常被人们提到还有模型Lasso。Lasso全称最小绝对收缩和选择算子（least absolute shrinkage and selection operator），由于这个名字过于复杂所以简称为Lasso。和岭回归一样，Lasso是被创造来作用于多重共线性问题的算法，不过Lasso使用的是系数w的L1范式（L1范式则是系数w的绝对值）乘以正则化系数&alpha; ，所以,Lasso的损失函数表达式为： 岭回归VSLasso：岭回归可以解决特征间的精确相关关系导致的最小二乘法无法使用的问题，而Lasso不行。 Lasso不是从根本上解决多重共线性问题，而是限制多重共线性带来的影响。 Lasso的核心作用：特征选择sklearn中我们使用类Lasso来调用lasso回归，众多参数中我们需要比较在意的就是参数&alpha; ，正则化系数。另外需要注意的就是参数positive。当这个参数为”True”的时候，是我们要求Lasso回归出的系数必须为正数，以此来保证我们的&alpha;一定以增大来控制正则化的程度。 需要注意的是，在sklearn中我们的Lasso使用的损失函数是： 红色框框住的只是作为系数存在。用来消除我们对损失函数求导后多出来的那个2的（求解w时所带的1/2），然后对整体的RSS求了一个平均而已，无论时从损失函数的意义来看还是从Lasso的性质和功能来看，这个变化没有造成任何影响，只不过计算上会更加简便一些。 1234567891011121314151617181920212223import pandas as pdfrom sklearn.linear_model import Ridge, LinearRegression, Lassofrom sklearn.model_selection import train_test_split as TTSfrom sklearn.datasets import fetch_california_housing as fchimport matplotlib.pyplot as plthousevalue = fch()X = pd.DataFrame(housevalue.data)y = housevalue.targetX.columns = ["住户收入中位数","房屋使用年代中位数","平均房间数目","平均卧室数目","街区人口","平均入住率","街区的纬度","街区的经度"]Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#恢复索引for i in [Xtrain,Xtest]: i.index = range(i.shape[0])#线性回归进行拟合reg = LinearRegression().fit(Xtrain,Ytrain)print("线性回归：",(reg.coef_*100).tolist())#岭回归进行拟合Ridge_ = Ridge(alpha=0).fit(Xtrain,Ytrain)print("岭回归：",(Ridge_.coef_*100).tolist())#Lasso进行拟合lasso_ = Lasso(alpha=0).fit(Xtrain,Ytrain)print("Lasso",(lasso_.coef_*100).tolist()) 可以看到，岭回归没有报出错误，但Lasso就不一样了，虽然依然对系数进行了计算，但是报出了整整三个警告： 这三条分别是这样的内容： ​ 1、正则化系数为0，这样算法不可收敛！如果你想让正则化系数为0，请使用线性回归吧 ​ 2、没有正则项的坐标下降法可能会导致意外的结果，不鼓励这样做！ ​ 3、目标函数没有收敛，你也许想要增加迭代次数，使用一个非常小的alpha来拟合模型可能会造成精确度问题！ sklearn不推荐我们使用0这样的正则化系数。如果我们的确希望取到0，那我们可以使用一个比较很小的数，比如0.01这样的值： 123456#岭回归拟合Ridge_ = Ridge(alpha=0.01).fit(Xtrain,Ytrain)print((Ridge_.coef_*100).tolist())#Lasso进行拟合lasso_ = Lasso(alpha=0.01).fit(Xtrain,Ytrain)print((lasso_.coef_*100).tolist()) 这样就不会报任何警告了。 选取最佳的正则化 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.linear_model import LassoCVfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.datasets import fetch_california_housing as fchimport numpy as nphousevalue=fch()Xtrain,Xtest,Ytrain,Ytest=train_test_split(housevalue.data,housevalue.target,test_size=0.3,random_state=420)#自己建立Lasso进行alpha选择的范围alpharange = np.logspace(-10, -2, 200,base=10)lasso_ = LassoCV(alphas=alpharange #自行输入的alpha的取值范围,cv=5 #交叉验证的折数).fit(Xtrain, Ytrain)#查看被选择出来的最佳正则化系数print("最佳的正则化系数：",lasso_.alpha_)#调用所有交叉验证的结果print("所有交叉验证的结果：",lasso_.mse_path_)print(lasso_.mse_path_.shape) #返回每个alpha下的五折交叉验证结果print(lasso_.mse_path_.mean(axis=1)) #有注意到在岭回归中我们的轴向是axis=0吗？#在岭回归当中我们的交叉验证结果返回的是，每一个样本在每个alpha下的交叉验证结果#因此我们要求每个alpha下的交叉验证均值，就是axis=0，跨行求均值#而在这里，我们返回的是，每一个alpha取值下，每一折交叉验证的结果#因此我们要求每个alpha下的交叉验证均值，就是axis=1，跨列求均值#最佳正则化系数下获得的模型的系数结果print("最佳正则化系数获得的模型结果：",lasso_.coef_)print("最佳正则化系数的准确率：",lasso_.score(Xtest,Ytest))#与线性回归相比如何？reg = LinearRegression().fit(Xtrain,Ytrain)print("线性回归的准确率：",reg.score(Xtest,Ytest))#使用lassoCV自带的正则化路径长度和路径中的alpha个数来自动建立alpha选择的范围ls_ = LassoCV(eps=0.00001,n_alphas=300,cv=5).fit(Xtrain, Ytrain)print(ls_.alpha_)print(ls_.alphas_) print(ls_.alphas_.shape)print(ls_.score(Xtest,Ytest))print(ls_.coef_) 模型效果上表现和普通的Lasso没有太大的区别，不过他们都在各个方面对原有的Lasso做了一些相应的改进（比如说提升了本来就已经很快的计算速度，增加了模型选择的维度，因为均方误差作为损失函数只考虑了偏差，不考虑方差的在。除了解决多重共线性这个核心问题之外，线性模型还有更重要的事情要做：提升模型表现。这才是机器学习最核心的需求，而Lasso和岭回归不是为此而设计的。为了提升模型表现而做出的改进：多项式回归。 多项式回归首先，“线性”这个词用于描述不同事物时有着不同的含义，我们最常使用的线性是指“变量之间的线性关系(linear relationship)”。它表示两个变量之间的关系可以展示为一条直线，即可以使用方程y=ax+b来进行拟合。要探索两个变量之间的关系是否线性，最简单的方式就是绘制散点图，如果散点图能够相对均匀地分布在一条直线的两端，则说明这两个变量之间的关系是线性的。从线性关系这个概念出发，我们有了一种说法叫做“线性数据”。通常来说，一组数据由多个特征和标签组成。当这些特征分别与标签存在线性关系的时候，我们就说这一组数据是线性数据。 但当我们在进行分类的时候，我们的数据分布往往是这样的： 这些数据都不能由一条直线来进行拟合，他们也没有均匀分布在某一条线的周围，那我们怎么判断，这些数据是线性数据还是非线性数据呢？在这里就要注意了，当我们在回归中绘制图像时，绘制的是特征与标签的关系图，横坐标是特征，纵坐标是标签，我们的标签是连续型的，所以我们可以通过是否能够使用一条直线来拟合图像判断数据究竟属于线性还是非线性。然而在分类中，我们绘制的是数据分布图，横坐标是其中一个特征，纵坐标是另一个特征，标签则是数据点的颜色。因此在分类数据中，我们使用“是否线性可分”（linearly separable）这个概念来划分分类数据集。当分类数据的分布上可以使用一条直线来将两类数据分开时，我们就说数据是线性可分的。反之，数据不是线性可分的。 ps：上面那张图我也不知道是不是线性可分的，大家知道以下这个概念就好。 12345678910111213141516171819202122232425262728293031323334import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressionfrom sklearn.tree import DecisionTreeRegressorrnd = np.random.RandomState(42) #设置随机数种子X = rnd.uniform(-3, 3, size=100) #random.uniform，从输入的任意两个整数中取出size个随机数#生成y的思路：先使用NumPy中的函数生成一个sin函数图像，然后再人为添加噪音y = np.sin(X) + rnd.normal(size=len(X)) / 3 #random.normal，生成size个服从正态分布的随机数#使用散点图观察建立的数据集是什么样子plt.scatter(X, y,marker='o',c='k',s=20)plt.show()#为后续建模做准备：sklearn只接受二维以上数组作为特征矩阵的输入X = X.reshape(-1, 1)#使用原始数据进行建模LinearR = LinearRegression().fit(X, y)TreeR = DecisionTreeRegressor(random_state=0).fit(X, y)#放置画布fig, ax1 = plt.subplots(1)#创建测试数据：一系列分布在横坐标上的点line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)#将测试数据带入predict接口，获得模型的拟合效果并进行绘制ax1.plot(line, LinearR.predict(line), linewidth=2, color='green',label="linear regression")ax1.plot(line, TreeR.predict(line), linewidth=2, color='red',label="decision tree")#将原数据上的拟合绘制在图像上ax1.plot(X[:, 0], y, 'o', c='k')#其他图形选项ax1.legend(loc="best")ax1.set_ylabel("Regression output")ax1.set_xlabel("Input feature")ax1.set_title("Result before discretization")plt.tight_layout()plt.show() 从图像上可以看出，线性回归无法拟合出这条带噪音的正弦曲线的真实面貌，只能够模拟出大概的趋势，而决策树却通过建立复杂的模型将几乎每个点都拟合出来了。可见，使用线性回归模型来拟合非线性数据的效果并不好，而决策树这样的模型却拟合得太细致，相比之下，还是决策树的拟合效果更好一些。线性模型可以用来拟合非线性数据，而非线性模型也可以用来拟合线性数据，更神奇的是，有的算法没有模型也可以处理各类数据，而有的模型可以既可以是线性，也可以是非线性模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的保存和加载]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[模型的保存和加载 joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。 joblib.load()：读取模型。参数是模型的目录]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树推导首先看看下面这组数据集： 得出下面这颗决策树： 关键概念： ​ 信息熵公式： ​ 信息增益公式：就是熵和特征条件熵的差 ​ 随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好 决策树算法的需要解决的核心问题： ​ 1、如何从数据表中找出最佳节点和最佳分支？ ​ 2、如何让决策树停止生长，防止过拟合？ 决策树的基本过程： 直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。 决策树五大模块sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类： 分类树参数、属性和接口 参数criterion为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 criterion这个参数正是用来决定不纯度的计算方法的： ​ 1、输入”entropy“，使用信息熵 ​ 2、输入”gini“，使用基尼系数 其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。 random_state&amp;splitterrandom_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 max_depth限制树的最大深度，超过设定深度的树枝全部剪掉这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效果再决定是否增加设定深度。 min_samples_leaf&amp;min_samples_splitmin_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。 max_features&amp;min_impurity_decreasemax_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的功能，在0.19版本之前时使用min_impurity_split。 class_weight&amp;min_weight_fraction_leaf完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 属性分类树的七个参数，一个属性，四个接口，以及绘图所用的代码。七个参数：Criterion，两个随机性相关的参数（random_state，splitter），四个剪枝参数（max_depth， min_sample_leaf，max_feature，min_impurity_decrease）一个属性：feature_importances_ ，属性是在模型训练之后，能够调用查看的模型的各种性质。 接口四个接口：ﬁt，score，apply，predicd apply：返回每个测试样本所在叶子节点的索引 predict：返回每个测试样本的分类/回归结果 coding： 回归树参数解读 几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。 criterion：回归树衡量分枝质量的指标，支持的标准有三种：1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。 2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ： 其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 决策树的本地保存：Graphvizwindows版本下载地址：https://graphviz.gitlab.io/_pages/Download/Download_windows.html 双击msi文件，一直next就完事了。 找到bin文件夹 在下面这张图片的位置加入环境变量 用dot -version检查是否安装成功 将dot文件转为png文件的命令：dot -Tpng .dot -o .png 123456789101112131415161718#分类树from sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier,export_graphvizimport pandas as pdtitan=pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")x=titan[["pclass","age","sex"]]y=titan["survived"]x["age"].fillna(x["age"].mean(),inplace=True)x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)dict=DictVectorizer(sparse=False)x_train=dict.fit_transform(x_train.to_dict(orient="records"))x_test=dict.transform(x_test.to_dict(orient="records"))dec=DecisionTreeClassifier()dec.fit(x_train,y_train)print("准确率为：",dec.score(x_test,y_test))#图形化export_graphviz(dec,out_file="./tree.dot",feature_names=["age","pclass=1st","pclass=2nd","pclass=3rd","female","male"]) 123456789101112131415161718192021222324#回归树import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltrng = np.random.RandomState(1)X = np.sort(5 * rng.rand(80,1), axis=0)y = np.sin(X).ravel()y[::5] += 3 * (0.5 - rng.rand(16))regr_1 = DecisionTreeRegressor(max_depth=2)regr_2 = DecisionTreeRegressor(max_depth=5)regr_1.fit(X, y)regr_2.fit(X, y)X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)plt.figure()plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[贝叶斯的性质关键概念 ​ 联合概率：X取值为x和Y取值为y两个事件同时发生的概率，表示为P(X=x,Y=y) ​ 条件概率：在X取值为x的前提下，Y取值为y的概率。表示为P(Y=y|X=x) 这里的C代表类别，W代表特征。 我们学习的其它分类算法总是有一个特点：这些算法先从训练集中学习，获取某种信息来建立模型，然后用模型去对测试集进行预测。比如逻辑回归，我们要先从训练集中获取让损失函数最小的参数，然后用参数建立模型，再对测试集进行预测。在比如支持向量机，我们要先从训练集中获取让边际最大的决策边界，然后 用决策边界对测试集进行预测。相同的流程在决策树，随机森林中也出现，我们在ﬁt的时候必然已经构造好了能够让对测试集进行判断的模型。而朴素贝叶斯，似乎没有这个过程。这说明，朴素贝叶斯是一个不建模的算法。以往我们学的不建模算法，比如KMeans，比如PCA，都是无监督学习，而朴素贝叶斯是第一个有监督的，不建模的分类算法。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类。 sklearn中的朴素贝叶斯sklearn中，基于高斯分布、伯努利分布、多项式分布为我们提供了四个朴素贝叶斯的分类器 高斯朴素贝叶斯sklearnnaive_bayes.GaussianNB(priors=None,var_smoothing=1e-09) 高斯朴素贝叶斯，通过假设P(Xi|Y)是服从高斯分布(正态分布)，估计每个特征下每个类别上的条件概率。对于每个特征下的取值，有以下公式： 1234567891011121314from sklearn.naive_bayes import GaussianNBfrom sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitdigits = load_digits()X, y = digits.data, digits.targetXtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420)gnb = GaussianNB().fit(Xtrain,Ytrain)#查看分数acc_score = gnb.score(Xtest,Ytest)#查看预测结果Y_pred = gnb.predict(Xtest)#查看预测的概率结果prob = gnb.predict_proba(Xtest)print(prob) 概率类模型的评估指标布里尔分数Brier Score 概率预测的准确程度被称为“校准程度”，是衡量算法预测出的概率和真实结果的差异的一种方式。一种比较常用的指标叫做布里尔分数，它被计算为是概率预测相对于测试样本的均方误差，表示为： 其中N是样本数量，Pi为朴素贝叶斯预测出的概率，Oi是样本所对应的真实结果，只能取到0或者1，如果事件发生则为1，如果不发生则为0。这个指标衡量了我们的概率距离真实标签结果的差异，其实看起来非常像是均方误差。布里尔分数的范围是从0到1，分数越高则预测结果越差劲，校准程度越差，因此布里尔分数越接近0越好。由于它的本质也是在衡量一种损失，所以在sklearn当中，布里尔得分被命名为brier_score_loss。我们可以从模块metrics中导入这个分数来衡量我们的模型评估结果： 12345#接上面的代码from sklearn.metrics import brier_score_loss#注意，第一个参数是真实标签，第二个参数是预测出的概率值#我们的pos_label与prob中的索引一致，就可以查看这个类别下的布里尔分数是多少print(brier_score_loss(Ytest, prob[:,1], pos_label=1)) 对数似然函数另一种常用的概率损失衡量是对数损失（log_loss），又叫做对数似然，逻辑损失或者交叉熵损失。由于是损失，因此对数似然函数的取值越小，则证明概率估计越准确，模型越理想。值得注意得是，对数损失只能用于评估分类型模型 在sklearn，我们可以从metrics模块中导入我们的对数似然函数： 12from sklearn.metrics import log_lossprint(log_loss(Ytest,prob)) 第一个参数是真实标签，第二个参数是我们预测的概率。真正的概率必须要以接口predict_proba来调用，千万避免混淆。 那什么时候使用对数似然，什么时候使用布里尔分数？ 可靠性曲线可靠性曲线（reliability curve），又叫做概率校准曲线（probability calibration curve），可靠性图（reliabilitydiagrams），这是一条以预测概率为横坐标，真实标签为纵坐标的曲线。我们希望预测概率和真实值越接近越好，最好两者相等，因此一个模型/算法的概率校准曲线越靠近对角线越好。 12345678910111213141516171819202122232425262728293031323334import matplotlib.pyplot as pltimport pandas as pdfrom sklearn.datasets import make_classification as mcfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import brier_score_lossfrom sklearn.model_selection import train_test_splitX, y = mc(n_samples=100000,n_features=20 #总共20个特征,n_classes=2 #标签为2分类,n_informative=2 #其中两个代表较多信息,n_redundant=10 #10个都是冗余特征,random_state=42)#样本量足够大，因此使用1%的样本作为训练集Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,test_size=0.99,random_state=42)gnb = GaussianNB()gnb.fit(Xtrain,Ytrain)y_pred = gnb.predict(Xtest)prob_pos = gnb.predict_proba(Xtest)[:,1] #我们的预测概率 - 横坐标clf_score=brier_score_loss(Ytest,prob_pos,pos_label=1)#Ytest - 我们的真实标签 - 横坐标#在我们的横纵表坐标上，概率是由顺序的（由小到大），为了让图形规整一些，我们要先对预测概率和真实标签按照预测#概率进行一个排序，这一点我们通过DataFrame来实现df = pd.DataFrame(&#123;"ytrue":Ytest[:500],"probability":prob_pos[:500]&#125;)df = df.sort_values(by="probability")df.index = range(df.shape[0])#紧接着我们就可以画图了fig = plt.figure()ax1 = plt.subplot()ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated") #得做一条对角线来对比呀ax1.plot(df["probability"],df["ytrue"],"s-",label="%s (%1.3f)" % ("Bayes", clf_score))ax1.set_ylabel("True label")ax1.set_xlabel("predcited probability")ax1.set_ylim([-0.05, 1.05])ax1.legend()plt.show() 为什么存在这么多上下穿梭的直线？因为我们是按照预测概率的顺序进行排序的，而预测概率从0开始到1的过程中，真实取值不断在0和1之间变化，而我们是绘制折线图，因此无数个纵坐标分布在0和1的被链接起来了，所以看起来如此混乱。那我们换成散点图来试试看呢？ 12345678910#接上面的代码fig = plt.figure()ax1 = plt.subplot()ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")ax1.scatter(df["probability"],df["ytrue"],s=10)ax1.set_ylabel("True label")ax1.set_xlabel("predcited probability")ax1.set_ylim([-0.05, 1.05])ax1.legend()plt.show() 可以看到，由于真实标签是0和1，所以所有的点都在y=1和y=0这两条直线上分布，这完全不是我们希望看到的图像。回想一下我们的可靠性曲线的横纵坐标：横坐标是预测概率，而纵坐标是真实值，我们希望预测概率很靠近真实值，那我们的真实取值必然也需要是一个概率才可以，如果使用真实标签，那我们绘制出来的图像完全是没有意义的。但是，我们去哪里寻找真实值的概率呢？这是不可能找到的——如果我们能够找到真实的概率，那我们何必还用算法来估计概率呢，直接去获取真实的概率不就好了么？所以真实概率在现实中是不可获得的。但是，我们可以获得类概率的指标来帮助我们进行校准。一个简单的做法是，将数据进行分箱，然后规定每个箱子中真实的少数类所占的比例为这个箱上的真实概率trueproba，这个箱子中预测概率的均值为这个箱子的预测概率predproba，然后以trueproba为纵坐标，predproba为横坐标，来绘制我们的可靠性曲线。 在sklearn中，这样的做法可以通过绘制可靠性曲线的类calibration_curve来实现。和ROC曲线类似，类calibration_curve可以帮助我们获取我们的横纵坐标，然后使用matplotlib来绘制图像。该类有如下参数： 123456789101112131415#接上面的代码from sklearn.calibration import calibration_curve#从类calibiration_curve中获取横坐标和纵坐标trueproba, predproba = calibration_curve(Ytest, prob_pos,n_bins=10 #输入希望分箱的个数)fig = plt.figure()ax1 = plt.subplot()ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")ax1.plot(predproba, trueproba,"s-",label="%s (%1.3f)" % ("Bayes", clf_score))ax1.set_ylabel("True probability for class 1")ax1.set_xlabel("Mean predcited probability")ax1.set_ylim([-0.05, 1.05])ax1.legend()plt.show() 根据不同的n_bins取值得出不同的曲线 123456789101112#接上面的代码fig, axes = plt.subplots(1,3,figsize=(18,4))for ind,i in enumerate([3,10,100]): ax = axes[ind] ax.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated") trueproba, predproba = calibration_curve(Ytest, prob_pos,n_bins=i) ax.plot(predproba, trueproba,"s-",label="n_bins = &#123;&#125;".format(i)) ax.set_ylabel("True probability for class 1") ax.set_xlabel("Mean predcited probability") ax.set_ylim([-0.05, 1.05]) ax.legend()plt.show() 很明显可以看出，n_bins越大，箱子越多，概率校准曲线就越精确，但是太过精确的曲线不够平滑，无法和我们希望的完美概率密度曲线相比较。n_bins越小，箱子越少，概率校准曲线就越粗糙，虽然靠近完美概率密度曲线，但是无法真实地展现模型概率预测地结果。因此我们需要取一个既不是太大，也不是太小的箱子个数，让概率校准曲线既不是太精确，也不是太粗糙，而是一条相对平滑，又可以反应出模型对概率预测的趋势的曲线。通常来说，建议先试试看箱子数等于10的情况。箱子的数目越大，所需要的样本量也越多，否则曲线就会太过精确。 校准可靠性曲线sklearn中的概率校正类CalibratedClassifierCV来对二分类情况下的数据集进行概率校正 base_estimator需要校准其输出决策功能的分类器，必须存在predict_proba或decision_function接口。 如果参数cv = prefit，分类器必须已经拟合数据完毕。 cv整数，确定交叉验证的策略。可能输入是：None，表示使用默认的3折交叉验证。在版本0.20中更改：在0.22版本中输入“None”，将由使用3折交叉验证改为5折交叉验证 任意整数，指定折数对于输入整数和None的情况下来说，如果时二分类，则自动使用类sklearn.model_selection.StratifiedKFold进行折数分割。如果y是连续型变量，则使用sklearn.model_selection.KFold进行分割。 已经使用其他类建好的交叉验证模式或生成器cv。 可迭代的，已经分割完毕的测试集和训练集索引数组。 输入”prefit”，则假设已经在分类器上拟合完毕数据。在这种模式下，使用者必须手动确定用来拟合分类器的数据与即将倍校准的数据没有交集 method进行概率校准的方法，可输入”sigmoid”或者”isotonic” 输入’sigmoid’，使用基于Platt的Sigmoid模型来进行校准 输入’isotonic’，使用等渗回归来进行校准当校准的样本量太少（比如，小于等于1000个测试样本）的时候，不建议使用等渗回归，因为它倾向于过拟合。样本量过少时请使用sigmoids，即Platt校准。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import matplotlib.pyplot as pltimport pandas as pdfrom sklearn.datasets import make_classification as mcfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import brier_score_lossfrom sklearn.calibration import calibration_curvefrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.model_selection import train_test_splitX, y = mc(n_samples=100000,n_features=20 #总共20个特征,n_classes=2 #标签为2分类,n_informative=2 #其中两个代表较多信息,n_redundant=10 #10个都是冗余特征,random_state=42)#样本量足够大，因此使用1%的样本作为训练集Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,test_size=0.99,random_state=42)def plot_calib(models,name,Xtrain,Xtest,Ytrain,Ytest,n_bins=10): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6)) ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated") for clf, name_ in zip(models, name): clf.fit(Xtrain, Ytrain) y_pred = clf.predict(Xtest) # hasattr(obj,name)：查看一个类obj中是否存在名字为name的接口，存在则返回True if hasattr(clf, "predict_proba"): prob_pos = clf.predict_proba(Xtest)[:, 1] else: # use decision function prob_pos = clf.decision_function(Xtest) prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min()) # 返回布里尔分数 clf_score = brier_score_loss(Ytest, prob_pos, pos_label=y.max()) trueproba, predproba = calibration_curve(Ytest, prob_pos, n_bins=n_bins) ax1.plot(predproba, trueproba, "s-", label="%s (%1.3f)" % (name_, clf_score)) ax2.hist(prob_pos, range=(0, 1), bins=n_bins, label=name_, histtype="step", lw=2) ax2.set_ylabel("Distribution of probability") ax2.set_xlabel("Mean predicted probability") ax2.set_xlim([-0.05, 1.05]) ax2.legend(loc=9) ax2.set_title("Distribution of probablity") ax1.set_ylabel("True probability for class 1") ax1.set_xlabel("Mean predcited probability") ax1.set_ylim([-0.05, 1.05]) ax1.legend() ax1.set_title('Calibration plots(reliability curve)') plt.show()from sklearn.calibration import CalibratedClassifierCVname = ["GaussianBayes","Logistic","Bayes+isotonic","Bayes+sigmoid"]gnb = GaussianNB()models = [gnb ,LR(C=1., solver='lbfgs',max_iter=3000,multi_class="auto")#定义两种校准方式,CalibratedClassifierCV(gnb, cv=2, method='isotonic'),CalibratedClassifierCV(gnb, cv=2, method='sigmoid')]if __name__=="__main__": plot_calib(models,name,Xtrain,Xtest,Ytrain,Ytest) 多项式朴素贝叶斯在sklearn中，用来执行多项式朴素贝叶斯的类MultinomialNB包含如下的参数和属性： 1234567891011121314151617181920212223242526272829from sklearn.preprocessing import MinMaxScalerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_blobsfrom sklearn.metrics import brier_score_lossclass_1 = 500class_2 = 500 #两个类别分别设定500个样本centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [0.5, 0.5] #设定两个类别的方差X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)mnb = MultinomialNB().fit(Xtrain_, Ytrain)mnb.predict(Xtest_)mnb.predict_proba(Xtest_)mnb.score(Xtest_,Ytest)print(brier_score_loss(Ytest,mnb.predict_proba(Xtest_)[:,1],pos_label=1)) 其实效果不是很理想，来试试看把Xtrain转换成分类型数据吧。注意我们的Xtrain没有经过归一化，因为做哑变量之后自然所有的数据就不会又负数了。看下面的代码： 1234567from sklearn.preprocessing import KBinsDiscretizerkbs = KBinsDiscretizer(n_bins=10, encode='onehot').fit(Xtrain)Xtrain_ = kbs.transform(Xtrain)Xtest_ = kbs.transform(Xtest)mnb = MultinomialNB().fit(Xtrain_, Ytrain)mnb.score(Xtest_,Ytest)print(brier_score_loss(Ytest,mnb.predict_proba(Xtest_)[:,1],pos_label=1)) 伯努利朴素贝叶斯 12345678910111213141516171819202122232425262728293031323334from sklearn.preprocessing import MinMaxScalerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_blobsfrom sklearn.metrics import brier_score_lossclass_1 = 500class_2 = 500 #两个类别分别设定500个样本centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [0.5, 0.5] #设定两个类别的方差X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)from sklearn.naive_bayes import BernoulliNB#普通来说我们应该使用二值化的类sklearn.preprocessing.Binarizer来将特征一个个二值化#然而这样效率过低，因此我们选择归一化之后直接设置一个阈值mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)#不设置二值化bnl_ = BernoulliNB().fit(Xtrain_, Ytrain)bnl_.score(Xtest_,Ytest)brier_score_loss(Ytest,bnl_.predict_proba(Xtest_)[:,1],pos_label=1)#设置二值化阈值为0.5bnl = BernoulliNB(binarize=0.5).fit(Xtrain_, Ytrain)bnl.score(Xtest_,Ytest)print(brier_score_loss(Ytest,bnl.predict_proba(Xtest_)[:,1],pos_label=1)) 补集朴素贝叶斯在sklearn中，补集朴素贝叶斯由类ComplementNB完成，它包含的参数和多项式贝叶斯也非常相似：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2019%2F09%2F07%2Fk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[分类算法-k近邻算法(KNN)：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 如何求距离： k值取很小，容易受异常点影响 k值取很大,容易受k值数量的波动]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(1)]]></title>
    <url>%2F2019%2F09%2F05%2FSVM%E8%A7%A3%E8%AF%BB(1)%2F</url>
    <content type="text"><![CDATA[​ 支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。 sklearn中的支持向量机 注意，除了特别表明是线性的两个类LinearSVC和LinearSVR之外，其他的所有类都是同时支持线性和非线性的。NuSVC和NuSVC可以手动调节支持向量的数目，其他参数都与最常用的SVC和SVR一致。注意OneClassSVM是无监督的类。除了本身所带的类之外，sklearn还提供了直接调用libsvm库的几个函数。libsvm是一个简单、易于使用和快速有效的英文的SVM库。 SVM原理解读支持向量机原理的三层理解： 第一层理解支持向量机所做的事情其实非常容易理解，看下图： 上图是一组两种标签的数据，两种标签分别用圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在位置数据集上的分类误差(泛化误差)尽量小。 关键概念： ​ 超平面：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。 ​ 决策边界：在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就 说这个超平面是数据的“决策边界”。 决策边界一侧的所有点在分类为属于一个类，而另一个类的所有店分类属于另一个类。比如上面的数据集，我们很容易的在方块和圆的中间画一条线并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。 所以，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。 但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有B1和B2两条可能的决策边界。我们可以把决策边界B1向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到b11和b12的距离相等。在b11和b12中间的距离，叫做 这条决策边界的边际(margin)，通常记作d。对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。 接着我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误认成了圆，有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，拥有更大边际的决策边界在分类中的泛化误差更小，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。 结论：支持向量机，就是通过找出边际最多的决策边界，来对数据进行分类的分类器。 我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决 策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为： 我们将此表达式变换一下： 其中[a, -1]就是我们的参数向量 ， 就是我们的特征向量， 是我们的截距。 我们要求解参数向量w和截距b 。如果在决策边界上任意取两个点Xa，Xb，并带入决策边界的表达式，则有： 将两式相减，得： Xa和Xb是一条直线上的两个点，相减后的得到的向量方向是由Xb指向Xa，所以Xa-Xb的方向是平行于他们所在的直线——我们的决策边界的。而w与Xa-Xb相互垂直，所以参数向量w方向必然是垂直于我们的决策边界。 此时，我们有了我们得决策边界，任意一个紫色得点Xp就可以表示为： 由于紫色的点所代表的标签y是1，所以我们规定，p&gt;0。 同样的，对于任意一个红色的点Xr而言，我们可以将它表示为： 由于红色点所表示的标签y是-1，所以我们规定，r&lt;0 由上面得知：如果我们有新的测试数据Xt，则Xt的标签就可以根据以下式子来判定： 注意：p和r的符号是我们人为规定的 两类数据中距离我们的决策边界 最近的点，这些点就被称为支持向量。 紫色类的点为Xp ，红色类的点为Xr，则我们可以得到： 两个式子相减，得： 如下图所示： (Xp-Xr)可表示为两点之间的连线，而我们的边际d是平行于w的，所以我们现在，相当于是得到 了三角型中的斜边，并且知道一条直角边的方向。在线性代数中，向量有这样的性质：向量a除以向量b的模长||b|| ，可以得到向量a在向量b的方向上的投影的长度。所以，我们另上述式子两边同时除以||w||，则可以得到 最大边界所对应的决策边界，那问题就简单了，要最大化d，就求解w的最小值。极值问题可以相互转化，我们可以把求解w的最小值转化为，求解以下函数的最小值： 之所以要在模长上加上平方，是因为模长的本质是一个距离，所以它是一个带根号的存在，我们对它取平方，是为了消除根号。 我们得到我们SVM的损失函数： 至此，SVM的第一层理解就完成了。 关键概念：函数间隔与几何间隔 对于给定的数据集T和超平面(&omega;,b),定义超平面(&omega;,b)关于样本点(xi,yi)的函数间隔为： 这其实是我们的虚线超平面的表达式整理过后得到的式子。函数间隔可以表示分类预测的正确性以及确信度。再在这个函数间隔的基础上除以&omega;的模长||&omega;||来得到几何间隔： 几何间隔的本质其实是点xi到超平面(&omega;,b)，即到我们的决策边界的带符号的距离(signed distance)。 第二层理解用拉格朗日对偶函数求解线性SVM有了我们的损失函数以后，我们就需要对损失函数进行求解。我们之前得到了线性SVM损失函数的最初形态。 这个损失函数分为两部分：需要最小化的函数，以及参数求解后必须满足的约束条件。这是一个最优化问题。 为什么要进行转换？我们的目标是求解让损失函数最小化的&omega;,但其实很容易看的出来，如果||&omega;||为0，f(&omega;)必然最小了。但是，||&omega;||=0其实是一个无效的值，原因很简单：首先，我们的决策边界是&omega;x+b=0，如果&omega;为0，则这个向量的所有元素都为0，那就有b=0这个唯一值。如果b和&omega;都为0，决策边界就不在是一条直线了，函数间隔yi(&omega;\xi+b)就会为0，条件中的y(&omega;*xi+b)&gt;=1就不可能实现，所有&omega;不可以是一个0向量。可见，单纯让f(&omega;)为0，是不能求解出合理的&omega;的。我们希望能够能够找出一种方式，能够让我们的条件yi(&omega;*xi+b)&gt;=1在计算中也被纳入考虑，其中一种方法就是使用拉格朗日乘数法。 为什么可以进行转换？我们的损失函数是二次的(quadratic)，并且我们损失函数中的约束条件在参数&omega;和b下是线性的，求解这样的损失函数被称为“凸优化问题”(convex optimization problem)。拉格朗日乘数法正好可以用来解决凸优化问题，这种方法也是业界常用的，用来解决带约束条件，尤其是带有不等式的约束条件的函数的数学方法。首先第一步，我们需要使用拉格朗日乘数来将损失函数改写为考虑了约束条件的形式： 上面被称为拉格朗日函数。其中&alpha;i就叫做拉格朗日乘数。此时此刻，我们要求解的就不只有参数向量&omega;和截距b了，我们也要求拉格朗日乘数&alpha;，而我们的xi和yi都是我们已知的特征矩阵和标签。 怎么进行转换？ 拉格朗日函数也分为两部分。第一部分和我们原始的损失函数一样，第二部分呈现了我们带有不等式的约束条件。我们希望，L(&omega;,b,&alpha;)不仅能够代表我们原有的损失函数f(&omega;)和约束条件。还能够表示我们想要最小化损失函数来求解&omega;和b的意图，所以我们要先以&alpha;为参数，L(&omega;,b,&alpha;)的最大值，再以&omega;和b为参数，求解L(&omega;,b,&alpha;)的最小值。因此，我们的目标可以写作： 首先，我们先执行max，即最大化L(&omega;,b,&alpha;)，那就有两种情况： 若把函数第二部分当作一个惩罚项来看待，则yi(&omega;*xi+b)大于1时函数没有收到惩罚。而yi(&omega;*xi+b)小于1时函数受到极致的惩罚，即加上一个正无穷项，函数整体永远不可能渠道最小值。所以第二部，我们执行min的命令，求解函数整体的最小值，我们就永远不能让&alpha;必须取到正无穷的状况出现，即是说永远不让yi(&omega;*xi+b)&lt;1的状况出现。从而实现了求解最小值的同时让约束条件满足。现在，L(&omega;,b,&alpha;)就是我们新的损失函数。我们的目标时通过先最大化，再最小化它来求解参数向量&omega;和截距b的值。 拉格朗日函数转换为拉格朗日对偶函数为什么要进行转换？要求极值，最简单的方法还是对参数求导后让一阶导数等于0。我们先来试试看对拉格朗日函数求极值，在这里我们对参数向量&omega;和截距b分别求偏导并且让他们等于0。 求导过程如下： 由于两个求偏导结果中都带有未知的拉格朗日乘数&alpha;i，因此我们还是无法求解出&omega;和b，我们必须想出一种方法来求解拉格朗日乘数&alpha;i。幸运地是，拉格朗日函数可以被转换成一种只带有&alpha;i，而不带有&omega;和b的形式，这种形式被称为拉格朗日对偶函数。在对偶函数下，我们就可以求解出拉格朗日乘数&alpha;i，然后带入到上面推导出的(1)和(2)式中来求解&omega;和b。 为什么能够进行转换？对于任何一个拉格朗日函数，都存在一个与它对于的对偶函数g(&alpha;)，只带有拉格朗日乘数&alpha;作为唯一的参数。如果L(x,a)的最优解存在并可以表示为min L(x,a)，并且对偶函数的最优解也存在并可以表示为max g(&alpha;)，则可以定义对偶差异。即拉格朗日函数的最优解与其对偶函数的最优解之间的差值： 如果$\Delta$=0，则称L(x,&alpha;)与其对偶函数之间存在强对偶关系(strong duality property)，此时我们就可以通过求解其对偶函数的最优解来替代求解原始函数的最优解。 那强对偶关系上面时候存在呢？那就是这个拉格朗日函数必须满足KTT(Karush-Kuhn-Tucker)条件： 这里的条件其实都比较好理解。首先是所有参数的一阶导数必须为0，然后约束条件中的函数本身需要小于等于0，拉格朗日乘数需要大于等于0，以及约束条件乘以拉格朗日乘数必须等于0，即不同i的取值下，两者之中至少有一个为0。当所有限制都被满足，则拉格朗日函数L(x,&alpha;)的最优解与其对偶函数的最优解相等，我们就可以将原始的最优化问题转换成为对偶函数的最优化问题。而不难注意到，对于我们的损失函数L(&alpha;,b,&alpha;)而言，KTT条件都是可以操作的。如果我们能够人为让KTT条件全部成立，我们就可以求解出L(&omega;,b,&alpha;)的对偶函数来解出&alpha;。 之前我们已经让拉格朗日函数上对参数&omega;和b的求导为0，得到了式子： 并且在我们的函数中，我们通过先求解最大值再求解最小值的方法使得函数天然满足： 接下来，我们只需要再满足一个条件： 这个条件其实很容易满足，能够让yi(&omega;*xi+b)-1=0的就是落在虚线的超平面上的样本点，即我们的支持向量。所有不是支持向量的样本点则必须满足&alpha;i=0。满足这个式子说明了，我们求解的参数&omega;和b以及求解的超平面的存在，只与支持向量相关，与其他样本点无关。五个条件满足后，就可以使用L(&omega;,b,&alpha;)的对偶函数来求解&alpha;了。 怎么进行转换？首先让拉格朗日函数对参数&omega;和b求导后的结果为0，本质时再探索拉格朗日函数的最小值。然后： 这个Ld就是我们的对偶函数。对所有存在对偶函数的拉格朗日函数我们有对偶差异如下： 则对于我们的L(&omega;,b,&alpha;)和Ld，我们则有： 我们推到Ld的第一步是对L(&omega;,b,&alpha;)求偏导并让偏导数都为0。所有我们求解对偶函数的过程其实是在求解L(&omega;,b,&alpha;)的最小值。所以我们又可以把公式写成： 如此，我们只需要求解对偶函数的最大值，就可以求出&alpha;了。最终，我们的目标函数变化为： 求解拉格朗日对偶函数到了这一步，我们就需要使用梯度下降，smo或者二次规划(QP，quadratic programming)来求解&alpha;。由于数学太难且我的数学也是太差，我不看了也就不讲了，头都要秃了。大家只需要知道，一但我们求得了&alpha;值，我们就可以使用求导后得到的(1)式求解&omega;，并可以使用(1)式子和决策边界的表达式结合，得到下面的式子来求解b： 当求得特征向量&omega;和b，我们就得到了我们的决策边界的表达式。也就可以利用决策边界和其有关的超平面来进行分类，我们的决策函数就可以写作： 其中xtest是任意测试样本，sign(h)是h&gt;0时返回-1的符号函数。到这里，我们也就完成了对SVM的第二层理解的大部分内容，我们了解了线性SVM的四种相关函数：损失函数的初始形态、拉格朗日函数、拉格朗日对偶函数以及最后的决策函数。 第三层理解熟练以上的推导过程，就是我们的第三层理解。 线性SVM决策过程的可视化我们可以使用sklearn的式子来可视化我们的决策边界、支持向量、以及决策边界平行的两个超平面。 画决策边界的函数：contour contour是专门用来画等高线的函数。等高线，本质上是在二维图像表现三维图像的一种形式，其中两维X和Y是两条坐标轴上的取值，而Z表示高度。Contour就是将由X和Y构成平面上的所有点中，高度一致的点连接成线段的函数，在同一条等高线上的点一定具有相同的Z值。我们可以利用这个性质来绘制我们的决策边界。 回忆一下，我们的决策边界是&omega;*x+b=0，并在决策边界的两边找出两个超平面，使得超平面到决策边界的相对距离为1。那其实，我们只需要在我们的样本构成的平面上，把所有到决策边界的距离为0的点相连，就是我们的决策边界，而把所有到决策边界的相对距离为1的点相连，就是我们的两个平行于决策边界的超平面了。此时，我们的Z就是平面上的任意点到达超平面的距离。那首先，我们需要获取样本构成的平面，作为一个对象。 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.datasets import make_blobsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npX,y = make_blobs(n_samples=50, centers=2, random_state=0,cluster_std=0.6)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plt.xticks([])plt.yticks([])#制作数据的散点图plt.show()def plot_svc_decision_function(model,ax=None):#获取当前的子图，如果没有，就创建 if ax is None: ax = plt.gca()#获取x轴的最小最大值 xlim = ax.get_xlim()#获取y轴的最小最大值 ylim = ax.get_ylim()#在两个最小最大值间生成30个值 x = np.linspace(xlim[0],xlim[1],30) y = np.linspace(ylim[0],ylim[1],30)#使用meshgrid函数将两个一维向量转换为特征矩阵 Y,X = np.meshgrid(y,x)#列合并numpy xy = np.vstack([X.ravel(), Y.ravel()]).T#decision_function是一个重要接口，返回每个输入的样本所对应到决策边界的距离，#P的本质是输入的样本到决策边界的距离，而contour函数中的level其实是输入了这个距离 P = model.decision_function(xy).reshape(X.shape) ax.contour(X, Y, P,colors="k",levels=[-1,0,1],alpha=0.5,linestyles=["--","-","--"]) ax.set_xlim(xlim) ax.set_ylim(ylim) plt.show()clf = SVC(kernel = "linear").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plot_svc_decision_function(clf)print("预测值：",clf.predict(X))print("准确度：",clf.score(X,y))print("返回支持向量：",clf.support_vectors_)print("返回每个类中支持向量的个数：",clf.n_support_) 那如果推广到非线性数据上呢，比如环形数据 1234567from sklearn.datasets import make_circlesX,y = make_circles(100, factor=0.1, noise=.1)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plt.show()clf = SVC(kernel = "linear").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plot_svc_decision_function(clf) 运行代码后看效果图，很明显，现在线性SVM已经不适合于我们的状况了，我们无法找出一条直线来划分我们的数据集，让直线的两边分别是两种类别。这个时候，如果我们能够在原本的X和y的基础上，添加一个维度r，变成三维，我们可视化这个数据，来看看添加维度让我们的数据如何变化。 1234567891011121314#即使下面的代码没有看出来用了这个模块，但是必须要引入from mpl_toolkits import mplot3d#定义一个绘制三维图像的函数#elev表示上下旋转的角度#azim表示平行旋转的角度def plot_3D(elev=30,azim=30,X=X,y=y): ax = plt.subplot(projection="3d") ax.scatter3D(X[:,0],X[:,1],r,c=y,s=50,cmap='rainbow') ax.view_init(elev=elev,azim=azim) ax.set_xlabel("x") ax.set_ylabel("y") ax.set_zlabel("r") plt.show()plot_3D() 可以看见，此时此刻我们的数据明显是线性可分的了：我们可以使用一个平面来将数据完全分开，并使平面的上方的所有数据点为一类，平面下方的所有数据点为另一类。此时我们的数据在三维空间中，我们的超平面就是一个二维平面。明显我们可以用一个平面将两类数据隔开，这个平面就是我们的决策边界了。我们刚才做的，计算r，并将r作为数据的第三维度来将数据升维的过程，被称为“核变换”，即是将数据投影到高维空间中，以寻找能够将数据完美分割的超平面，即是说寻找能够让数据线性可分的高维空间。为了详细解释这个过程，我们需要引入SVM中的核心概念：核函数 后续内容请看SVM解读(2)](https://brickexperts.github.io/2019/09/15/SVM%E8%A7%A3%E8%AF%BB(2)/#more/#more)]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库概述]]></title>
    <url>%2F2019%2F09%2F04%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[​ 大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。 数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。 数据库数据具有永久存储、有组织、可共享三个基本特点。 数据库系统(DBS)由计算机硬件、数据库、数据库管理系统、应用软件和数据库管理系统组成。 数据库设计时面向数据模型对象、数据库系统的数据冗余度小、数据共享度高、数据库系统的数据和程序之间具有较高的独立性、数据库中数据的最小存取单位是数据项、数据库系统通过DBMS进行数据安全性、完整性、并发控制和数据恢复控制。 数据库数据物理独立性高是指当数据的物理结构（存储结构）发生变化时，应用程序不需要修改也可以正常工作 数据库数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行 DBMS的功能结构 ​ 数据定义功能：能够提供数据定义语言(DDL)和相应的建库机制。用户利用DDL可以方便建立数据库。 ​ 数据操纵功能：实现数据的插入、修改、删除、查询、统计等数据存取操作的功能称为数据操纵功能。数据库管理系统通过提供数据操纵语言(DML)实现其数据操纵功能。 ​ 运行管理功能：包括并发控制、数据的存取控制、数据完整性条件的检查和执行、数据库内部的维护等 ​ 建立和维护功能：指数据的载入、转储、重组织功能及数据库的恢复功能，指数据库结构的修改、变更及扩充功能。 从数据库管理系统的角度查看，数据库系统通常采用外模式、模式、内模式三级模式结构。从数据库最终用户的角度看，数据库系统的就够分为单用户结构、主从结构、分布式结构、客户/服务器结构。 外模式：也称为子模式或用户模式。它是数据库用户看见和使用的、对局部数据的逻辑结构和特征的描述，是与某一应用有关的数据的逻辑表示。 模式：模式是数据库中数据的逻辑结构和特征的描述。它仅仅涉及到型的描述，不涉及到具体的值。模式的具体值被称为模式的一个实例。同一模式可以有很多实例。模式是相对稳定的，实例是相对变动的。模式反映的是数据的结构及其关系，而实例反映的是数据某一时刻的状态。 内模式：也称存储模式，它是对数据的物理结构和存储结构的描述，是数据在数据库内部的表示方式。 为了能够在内部实现这三个抽象层次的联系和转换，数据库系统在这三级模式之间提供了两层映像：外模式/概念模式映像、概念模式/内模式映像。这两层映像保证了数据库系统中的数据能够具有较高的逻辑独立性和物理独立性。 外模式/模式映像保证了数据与程序间的逻辑独立性，模式/内模式映像保证了数据的物理独立性。 逻辑独立性：当数据库的整体逻辑结构发生变化时，通过调整外模式和模式之间的映像，使得外模式中的局部数据及其结构不变，程序不在修改。 物理独立性：当数据库的存储结构发生变化时，通过调整模式和内模式之间的映像，使得整体模式不变，当然外模式及应用程序不用改变。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数以及作用]]></title>
    <url>%2F2019%2F09%2F03%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[本片博客介绍激活函数以及激活函数的作用。 首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。 激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。 激活函数三问问题一：为什么我们要使用激活函数呢？ 如果不使用激活函数，我们的每一层输出只是承接了上一层输入函数的线性变换，无论神经网络有多少层，输出都是输入的线性组合。如果使用的话，激活函数给神经元引入了非线性的因素，使得神经网络可以逼近任何非线性函数，这样神经网络就可以应用到非线性模型中。 问题二：那么为什么我们需要非线性函数？ 非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。 问题三：如何选择激活函数？ 1、sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。 2、tanh 激活函数： tanh 是非常优秀的， 几乎适合所有场合。 3、ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。 常见激活函数接下来介绍一下常用的激活函数： sigmoid该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： 缺点：当 z 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z)将接近 00 。这会导致权重 W 的梯度将接近 00 ，使得梯度更新十分缓慢，即梯度消失 tanh该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到(−1,1) 之间，其公式与图形为： tanh函数的缺点同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z)接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。因此再介绍一个机器学习里特别受欢迎的激活函数 Relu函数。 Relu函数 只要z是正值的情况下，导数恒等于 1，当z是负值的时候，导数恒等于 0。z 等于0的时候没有导数，但是我们不需要担心这个点，假设其导数是0或者1即可。 激活函数选择的经验：如果输出是0和1的二分类问题，则输出层选择 sigmoid 函数，其他层选择 Relu 函数 Leaky Relu这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： 其中a取值在（0，1）之间 总结一下Relu 激活函数的优点： Relu 函数在 z&gt;0 的部分的导数值都大于0，并且不趋近于0，因而梯度下降速度较快。 Relu 函数在 z0。 激活函数的导数在进行神经网络反向传播的时候，需要计算激活函数的斜率或者导数 sigmoid的求导 或者让 a=g(z)，得 tanh的求导 我们同样让a=g(z)，得 Relu的求导]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸检测综述]]></title>
    <url>%2F2019%2F08%2F28%2F%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[人脸标注方法：矩形标注和椭圆形标注 矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、椭圆圆心x坐标、椭圆圆心y坐标。 判断算法性能好坏： 每一个标记只允许有一个检测与之相对应，也就是说，我们检测出来的每一个人脸图像只能同我们在标注中人脸图像中数据中的一个相对应。如果有多个，就会视为重复检测。重复检测会被视为错误检测。接着我们就可以得出正确率和错误率，可以画出ROC曲线和PR曲线。可以根据曲线看出算法的好坏。 人脸采集常用方法： 活体检测 判断用户是否为正常操作，通过指定用户做随机动作，一搬有张嘴、摇头、点头、凝视、眨眼等等，防止照片攻击。判断用户是否真实在操作，指定用户上下移动手机，防止视频攻击和非正常动作的攻击。 3D检测 验证采集到的是否为立体人像，能够防止平面照片、不同弯曲程度的照片等。 连续检测 通过连续的检测，验证运动轨迹是否正常，防止跳过活体检测直接替换采集的照片，也能防止中途换人。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>人脸检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测常用名词]]></title>
    <url>%2F2019%2F08%2F22%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[图像分类：一张图像中是否包含某种物体 物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。 语义分割：按对象得内容进行图像得分割，分割的依据是内容，即对象类别。 实例分割：按对象个体进行分割，分割的依据是单个目标。 滑动窗口——为什么要有候选区域？既然目标是在图像中的某一区域，那么最直接的方法就是滑窗法（sliding window approach），就是遍历图像的所有区域，用不同大小的窗口在整个图像上滑动，那么就会产生所有的矩形区域，然后再后续排查，思路很简单，但是开销巨大。 region proposal（RP）：候选区域 IOU：region proposal与Ground Truth的窗口的交集比并集的比值，相当于准确率。‘ SPP：Spatial Pyramid Pooling 空间金字塔采样 在pooing的过程中计算pooling后的结果对应的两个像素点映射到feature map上所占的范围，然后在那个范围中进行max或者average。 ROI Pooling：就是将一个个大小不同的box矩形框，都映射到大小为w*h的矩形框。 GT box:Ground Truth box 如上图所示，绿色的框为飞机的Ground Truth，红色的框是提取的Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU&lt;0.5)，那么这张图相当于没有正确的检测出飞机。如果我们能对红色的框进行微调，使得经过微调后的窗口跟Ground Truth更接近，这样岂不是定位会更准确。 带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。 如下图所示，(a)是普通的3×33×3卷积，其视野就是3×33×3，(b)是扩张率为2，此时视野变成7×77×7，(c)扩张率为4时，视野扩大为15×1515×15，但是视野的特征更稀疏了。 后面遇见会继续完善。。。。。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD系列算法]]></title>
    <url>%2F2019%2F08%2F21%2FSSD%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[主干网络介绍 主干网络原始作者采用的VGG16，我们也可以将其他神经网络作为主干网络。例如：ResNet、MobileNets等 输入300300的image，将VGG16网络FC6、FC7换成conv6和conv7，同时将池化层变为stride=1，pool_size=3\3，这样做的目的是为了不减少feature map size，为了配合这种变化，conv6会使用扩张率为6的带孔卷积。 ​ 带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。具体看目标检测常用名词) 接着然后移除dropout层和fc8层，并新增conv7，conv8，conv9，conv10，conv11，在检测数据集上做finetuing。其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是38×38，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet），以保证和后面的检测层差异不是很大，这个和Batch Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma。 多尺度feature map预测多尺度feature map预测，也就是在预测的时候，在接下来预测的时候，针对接下来六个不同的尺寸进行预测。如下图的六条连线，分别是38*38、19*19、10*10、5*5、3*3、1*1。将这六个不同尺度的feature map分别作为检测、预测层的输入，最后通过NMS进行筛选和合并。 对于六种不同尺度的网络，我们通常使用pooling来降采样。对于每一层的feature map，我们输入到相应的预测网络中。而预测网络中，我们会包括Prior box的提取过程。Prior box对应Fast R-CNN中Anchor的概念，也就是说，在Prior box中，feature map上的每一个点都作为一个cell（相当于Anchor）。以这个cell为中心，按照等比的放缩，找到它在原始图片的位置。接着以这个点为中心，提取不同尺度bounding box。而这些不同尺度的bounding box就是Prior box。然后对于每一个Prior box，我们通过和真值比较，就能够拿到它的label。对于每一个Prior box，我们都会分别预测它的类别概率和坐标（x，y，w，h）。也就是说，对于每一个cell，我们会将它对应到不同的Prior box，分别来预测当前这个Prior box所对应当前这个类别的概率分布和坐标。 对Prior Box的具体定义： 这里我们假设Prior Box的输入是m*n维的feature map。 如果每一个点都作为cell，那就会有mn个cell。接着每个cell上生成固定尺寸和不同长宽比例的box。每个cell对应k个bounging box，每个bounding box预测c个类别分数和4个偏移坐标。其中c个类别分数实际上是当前bounding box所对应的不同类别的概率分布。如果输入大小为m\n，那就会输出(c+4)*k*m*n。其中尺寸(scale)和比例(ratio)是超参数。 接下来我们看看Prior box是怎么生成的： 每个feature map上的点定义了六种长宽比的default box。也就是说，最后对于每一个anchor都会获得六个不同尺寸和长宽比的default box。对于3838层，每个feature map上的点，我们都会提取4个default box作为prior box。对于19\19层、10*10层、5*5，提取6个default box也就是全部都是prior box。而3*3、1*1提取4个default作为prior box。所以最后得到8732个prior box(38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4)。prior box就是选择的default box。尺寸和比例都是可以通过SSD的配置文件进行配置，后面实战详解。 对default box进行筛选成为prior box：每一个feature map cell不是k个default box都取，prior box与GT box(Ground Truth box)做匹配，IOU&gt;阈值为正样本。IOU&lt;阈值 为负样本。介于正样本和负样本中间阈值的default box去掉。 SSD系列算法优化及扩展SSD算法对小目标不够鲁棒，原因最主要是浅层feature map的表征能力不够强。 DSSD： DSSD相当原来的SSD模型主要作了两大更新。一是替换掉VGG，而改用了Resnet-101作为特征提取网络并在对不同尺度feature maps特征进行default boxes检测时使用了更新的检测单元；二则在网络的后端使用了多个deconvolution layers以有效地扩展低维度信息的contextual information，从而有效地提高了小尺度目标的检测。 下图为DSSD模型与SSD模型的整体网络结构对比： DSOD： SSD+DenseNet=DSOD DSOD可以从0开始训练数据，不需要预训练模型。 FSSD： 借鉴了FPN的思想，重构了一组pyramid feature map（金字塔特征），使得算法的精度有了明显特征，速度也没有下降很多。具体是把网络中某些feature调整为同一size再contact（连接），得到一个像素层，以此层为base layer来生成pyramid feature map，作者称之为Feature Fusion Module。 Feature Fusion 对上面图的解读： ​ (a) image pyramid ​ (b) rcnn系列，只在最后一层feature预测 ​ (c) FPN，语义信息一层传递回去，而且有很多相加的计算 ​ (d) SSD，在各个level的feature上直接预测，每个level之间没联系 ​ (e) FSSD的做法，把各个level的feature concat，然后从fusion feature上生成feature pyramid FSSD网络结构： RSSD： rainbow concatenation方式(pooling加deconvolution)融合不同层的特征，再增加不同层之间feature map关系的同时也增加了不同层的feature map个数。这种融合方式不仅解决了传统SSD算法存在的重复框问题，同时一定程度上解决了small object的检测问题。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One-stage基本介绍]]></title>
    <url>%2F2019%2F08%2F20%2FOne-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ One-stage是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。 One-stage常见算法： One-stage和Two-stage的区别在于是否存在RPN网络。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
      </categories>
      <tags>
        <tag>One-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Two-stage基本介绍]]></title>
    <url>%2F2019%2F08%2F18%2FTwo-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。 Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。]]></content>
      <categories>
        <category>目标检测</category>
        <category>Two-stage</category>
      </categories>
      <tags>
        <tag>Two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMS算法]]></title>
    <url>%2F2019%2F08%2F18%2FNMS%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ NMS全称是非极大值抑制算法。 目的：为了消除多余的框，找到最佳的物体检测的位置。 思想:选取那些邻域里分数最高的窗口，同时抑制那些分数低的窗口。 其实NMS的处理不太合理。所以有人提出了Soft-NMS。 NMS和Soft-NMS的区别： 相邻区域内的检测框的分数进行调整而非彻底抑制，从而提高了高检索率情况下的准确率。再低检索率时仍能对物体检测性能有明显提升。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DPM算法]]></title>
    <url>%2F2019%2F08%2F18%2FDPM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ DPM算法是传统目标检测方法的巅峰。 步骤： 1、计算DPM特征图 2、计算响应图 3、Latent SVM分类器训练 4、检测识别]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
      </categories>
      <tags>
        <tag>传统目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HOG+SVM算法]]></title>
    <url>%2F2019%2F08%2F18%2FHOG-SVM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[HOG+SVM算法主要用于行人检测。 步骤： 1、提取HOG特征。如果彩色图需要用HOG特征，则需要先转化为灰度图。 2、训练SVM分类器 ３、利用滑动窗口提取目标区域，进行分类判断 4、NMS 5、输出检测结果]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
      </categories>
      <tags>
        <tag>传统目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VJ算法]]></title>
    <url>%2F2019%2F08%2F18%2FVJ%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ VJ算法全称为Viola—Jones，多用于人脸检测。 步骤： 1、Haar特征抽取 Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。 2、训练人脸分类器（Adaboost算法等） 3、滑动窗口]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
      </categories>
      <tags>
        <tag>传统目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql基础命令]]></title>
    <url>%2F2019%2F08%2F13%2Fmysql%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[sql语句最后需要以；号结尾，sql语句最后需要以；号结尾，sql语句最后需要以；号结尾。重要的事说三遍。 数据库操作select version()：显示数据库版本 select now()：显示时间 show databases：查看所有数据库 show create database 数据库名：查看创建数据库的语句 create database 数据库名字：创建数据库 create database 数据库名字 charset=utf8：创建指定编码格式的数据库 drop database 数据库名：删除数据库 use 数据库名：使用数据库 select database ()：查看当前使用的数据库 数据表操作show tables:查看当前数据库中所有表 drop table 表名：删除表 show create table 表名：查看创建表的语句 create table 表名（字段 类型 约束[，字段 类型 约束]）：创建表 参数：auto_increment表示自动增长、not null表示不能为空、primary key表示主键、default 默认值 desc 表名：查看表的状态 insert into 表名 values()：向表插入数据，按照参数类型写参数 select * from 表名：查看表中所有的数据 alter table 表名 add 列名 类型：向表中添加字段 alter table 表名 modify 列名 类型 约束：不重命列名版 alter table 表名 change 列的原名 列的新名 类型 约束 ：重命列名版 alter table 表名 drop 列名：删除字段 数据的增删改查增加全列插入：insert into 表名 values(数据) 主键字段：可以用0 null default 来占位。因为auto_increment是自动增加的 部分插入：insert into 表名(列名1) values(值1) 没有的值取默认值 多行插入：insert into 表名 values (数据1)，(数据2) update 表名 set 列名 ：整列都改 update 表名 set 列名 where 条件：根据条件改 通过其他表来更新一个表： update 其他表 as 新名 inner join 被更新的表 as 新名 on 条件 set 需要改的数据 查询：select from 表名：查询整个表，号代表全部 select * from 表名 where ：根据条件查询 select 查询的列名 from 表名：根据列名查询。查询多列时，列名间用,隔开 select 查询的列名 as 列的新名字 from 表名：将列查询后以新的名字显示出来 select 表名.列名 from 表名 select 表的新名字.列名 from 表名 as 表的新名字 select distinct 列名 from 表名：可以去重，只显示相同数据第一次数据出现的位置 条件查询：and、or、not都可以用，类似python的语法。判断是否为空，is NULL。 模糊查询：like ：%代表一个或多个，_代表一个。 select name from 表名 where name like “%小%”：查询名字中有”小”字的名字 select name from 表名 where name like “__”：查询两个字的名字 select name from 表名 where name like “__%”：查询两个字以上的名字 rlike：利用正则查询 范围查询：in表示在一个非连续的范围内 between 数字 and 数字表示在一个连续的范围内 排序：order by 默认从小到大排序asc：从小到大排 desc：从大到小排 如果排序字段相同，我们可以设置多个排序字段。若不设置，默认按照主键大小排。 聚合函数(count，max，min，avg，sum，round)：不能跟其他字段一起用count：计算个数，其他类似 max：计算最大的 min：计算最小的 avg：计算平均值 sum：求和 round：四舍五入 ，round(123.23,1)保留一位小数=123.2 分组：group by按照性别分组： 配合聚合函数使用计算每种性别的人数： 计算男性的人数： group_concat():查询同一组的其他字段 还可以用字符串分割 having：将达到条件的组输出，可以配合聚合函数一起使用 having和where的区别：having是在分组后进行筛选，where是在原表的基础上进行筛选 分页：limit start，count 限制查询出来的数据个数。start代表从哪开始，count 代表查询的数据个数。start默认为0 链接查询：内链接、左链接、右链接内链接：inner join ……on 在两个表中取交集，如果存在则将两表数据合并。不存在则跳过。 可以利用as化简语句： 可以根据需求修改需要显示的数据： 还可以修改数据显示的位置： 通过某个表的字段排序： 左链接：left join……on。谁在左边，谁就是左表。查询的结果为两个表匹配到的数据，左表特有的数据，对于右表不存在的数据使用null填充 将左表特有的数据提取出来： 右链接：right join……on：类似左表。可以直接将右链接的表中左表和右表的显示交换即可得右链接。自关联：补。。。。 子查询：查询里嵌套一个查询 删除删除分为物理删除和逻辑删除： 物理删除： delete from 表名:删除整个表 delete from 表名 where 条件：删除符合条件的数据 逻辑删除：（用一个字段表示，这条信息是否还能用） alter table 表名 add 字段 类型 default 默认值]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络知识散记]]></title>
    <url>%2F2019%2F08%2F13%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[端口端口是英文port的意译，可以认为是设备与外界通讯交流的出口。端口可分为虚拟端口和物理端口，其中虚拟端口指计算机内部或交换机路由器内的端口，不可见。例如计算机中的80端口、21端口、23端口等。物理端口又称为接口，是可见端口，计算机背板的RJ45网口，交换机路由器集线器等RJ45端口。电话使用RJ11插口也属于物理端口的范畴。（用来区分哪个进程） 同一台电脑用pid区分进程，不同电脑用端口区分进程。 端口范围是0到65535 知名端口是众所周知的端口，范围是0到1023 动态端口的范围是1024到65535 查看端口状态：用netstat -an查看 TCP/IP协议TCP/IP:这不是两个协议，这是一个协议族，包含很多协议。主要是TCP/IP协议。 四层：物理层、网络层、传输层、应用层 七层：物理层、链路层、网络层、传输层、 会话层、表示层、应用层 IP地址用来标记唯一一台电脑。每一个IP地址都包括网络地址和主机地址 网络地址相同，则处于同一个网段，主机地址用来标记网里的电脑 A类网络的IP地址范围为：1.0.0.1－126.255.255.254； B类网络的IP地址范围为：128.1.0.1－191.255.255.254； C类网络的IP地址范围为：192.0.1.1－223.255.255.254 1．A类IP地址 一个A类IP地址由1字节（每个字节是8位）的网络地址和3个字节主机地址组成,即第一段数字范围为1～126。每个A类地址可连接16387064台主机(不能用0(产生冲突)和255(广播地址)，Internet有126个A类地址。 2．B类IP地址 一个B类IP地址由2个字节的网络地址和2个字节的主机地址组成，第一段数字范围为128～191。每个B类地址可连接64516(254*254)台主机(不能用0(产生冲突)和255(广播地址))，Internet有16256个B类地址。 3．C类IP地址 一个C类地址是由3个字节的网络地址和1个字节的主机地址组成，第一段数字范围为192～223。每个C类地址可连接254台主机(不能用0(产生冲突)和255(广播地址)，Internet有2054512个C类地址。 4．D类地址用于多点播送。 第一个字节的数字范围为224～239，是多点播送地址，用于多目的地信息的传输，和作为备用。全零（“0.0.0.0”）地址对应于当前主机，全“1”的IP地址（“255.255.255.255”）是当前子网的广播地址。多播和广播的区别：广播在同一个局域网都能收到，多播是指定那些人可以收得到，其他人收不到，常用于视频会议。 5.E类地址 第一段数字范围为240～254。E类地址保留，仅作实验和开发用。 全零（“0．0．0．0”）地址对应于当前主机。全“1”的IP地址（“255．255．255．255”）是当前子网的广播地址。 在IP地址3种主要类型里，各保留了3个区域作为私有地址，常见于局域网中。 常用术语网络号：网络号等于ip地址和网络掩码按位与操作 网络掩码（子网掩码）的作用：取网络号、主机号 两台电脑能通信的前提是处于同一个网络号 集线器（hub）的作用：实现多台电脑连接在一起，组成一个小型局域网，交换机也是。 集线器和交换机的区别：集线器是广播发数据，交换机不是每次都是广播，效率高。 实际地址：代表网卡地址（MAC）。由六个字节组成，前三个字节代表厂商，后三个字节代表厂商生产 arp：根据ip找mac地址 rarp：根据mac地址找ip icmp：ping的时候用 arp -a即是查看本地局域网内所有用户ip和mac地址绑定关系的一个命令。 ARP -d 就是清除缓存中的数据。也是删除ip和mac绑定的项目。 路由器：连接不同的网络，使他们之间能够通信 rip：路由解析协议 mac：标记实际转发数据时的地址 ip：标记逻辑上的地址 natmask：和ip地址一起确定网络号 默认网关：发送的ip不在同一个网段内，那么会把这个数据转发给默认网关。 为什么TCP比UDP稳定？在TCP中，如果有一方接收到对方的数据，一定会发送ack确认包给对方。而在UDP中，没有这个过程。 TCP三次握手：确定一定发送数据到对方 四次挥手：调用close时使用 TCP长连接、短连接： TTL：表示经过的路由器数目。每经过一个路由器，TTL-1。 MSL：表示一个数据包存在的最多时间 CDN：内容分发 查看域名解析的IP地址： nslookup 域名 例子：nslookup baidu.com 常见的网络攻击： DDOS攻击：拒绝服务器攻击。 DNS攻击：1.DNS服务器被劫持：篡改IP ​ 2.DNS欺骗： ARP攻击：中间人攻击]]></content>
      <categories>
        <category>网络知识</category>
      </categories>
      <tags>
        <tag>网络知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测概述]]></title>
    <url>%2F2019%2F08%2F12%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[​ 目标检测方法分为传统目标检测方法和深度学习目标检测方法。 传统目标检测方法：Viola-Jones、HOG+SVM、DPM等 Viola-Jones：采用积分图特征，进行人脸检测 HOG+SVM：行人检测。通过HOG特征结合SVM分类器进行检测。 DPM：同样通过HOG特征，并加入许多其他额外的策略进行检测。传统目标检测最好的方法。 深度学习目标检测方法：One-state、Two-stage One-stage：YOLO和SSD系列，直接回归目标位置。 Two-stage：Faster RCNN系列，利用网络对候选区进行推荐。 目标检测问题基本流程： Viola-Jones（人脸检测）步骤 1、Haar特征抽取 2、训练人脸分类器（Adaboost算法） 3、滑动窗口 HOG+SVM（行人检测）步骤 DPM（物体检测）步骤 NMS（非极大值抑制算法） 目的：为了消除多余的框，找到最佳的物体检测的位置 Soft-NMS是对NMS算法的改进]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv颜色识别]]></title>
    <url>%2F2019%2F08%2F12%2Fopencv%E9%A2%9C%E8%89%B2%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[数字图像处理中常用的采用模型是RGB（红，绿，蓝）模型和HSV（色调，饱和度，亮度），RGB广泛应用于彩色监视器和彩色视频摄像机，我们平时的图片一般都是RGB模型。而HSV模型更符合人描述和解释颜色的方式，HSV的彩色描述对人来说是自然且非常直观的。 这里的颜色识别是指根据人们的意愿提取图片中对应的颜色区域。 颜色识别步骤： 1、读取一张图片或视频. 2、用cvtcolor将它从RGB转为HSV。 3、通过inrange得出掩膜。 4、用 图像的”与”操作(bitwise_and)得出对应区域的图像。 coding： 读者可以加一些其他的操作提高效果，例如什么开运算、滤波之类的。这里就不赘述了，接下来，贴一张HSV的颜色阈值表，可以参考：]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>颜色识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魔法方法]]></title>
    <url>%2F2019%2F08%2F10%2F%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ 魔法方法是指Python内部已经包含的，被双下划线所包围的方法，这些方法在进行特定的操作时会自动被调用。使用Python的魔法方法可以是Python的自由度变得更高，当不需要重写魔法方法也可以在规定的默认情况下生效。在需要重写时也可以让使用者根据自己的需求来重写部分方法来达到自己的期待。 常用的魔法方法：__doc__：表示类的描述信息 __module__:表示当前操作的对象在哪个模块 __class__:表示当前操作的对象的类是什么 __call__：让对象直接调用call方法 __dict__：类或对象的所有属性 __getitem__、__setitem__、__delitem__： 魔法方法集合]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>魔法方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow（2）]]></title>
    <url>%2F2019%2F08%2F09%2Ftensorflow%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[序列比较与索引提取 常量tf.constant(value, dtype=None, shape=None, name=’Const’, verify_shape=False) value：是一个必须的值，可以是一个数值，也可以是一个列表；可以是一维的，也可以是多维的。 dtype：数据类型，一般是tf.float32，tf.float64等 shape：表示张量的形状，即维数以及每一维的大小 name：可以是任何内容，只要是字符串就行 verify_shape：默认为False，如果修改为True的话表示检查value的形状与shape是否相符，如果不符会报错。 12345678#未指定shape参数import tensorflow as tfa=tf.constant(1.0)b=tf.constant([1,2])sess=tf.Session()with sess.as_default(): print("a的结果是：",sess.run(a)) print("b的结果是：",b.eval()) 12345678910111213141516171819202122232425262728#如果指定shape参数。当第一个参数value是数字时，张量的所有元素都会用该数字填充import tensorflow as tfa=tf.constant(1.0,shape=[2,3])sess=tf.Session()with sess.as_default(): print("a的结果是：",sess.run(a))a的结果是： [[1. 1. 1.] [1. 1. 1.]]b的结果是： [1 2]#而当第一个参数value是一个列表时，注意列表的长度必须小于等于第三个参数shape的大小（即各维大小的乘积），否则会报错，这是因为函数会生成一个shape大小的张量，然后用value这个列表中的值一一填充shape中的元素。这里列表大小为7，而shape大小为2*3=6，无法正确填充，所以发生了错误import tensorflow as tfa=tf.constant([1,2,3,4,5,6,7],shape=[2,3])sess=tf.Session()with sess.as_default(): print("a的结果是：",sess.run(a))#报错ValueError: Too many elements provided. Needed at most 6, but received 7#而如果列表大小小于shape的大小，则会用列表的最后一项元素填充剩余的张量元素import tensorflow as tfa=tf.constant([1,2,3,4,5,6,7],shape=[2,2,3])sess=tf.Session()with sess.as_default(): print("a的结果是：",sess.run(a))a的结果是： [[[1 2 3] [4 5 6]] [[7 7 7] [7 7 7]]] 变量在构建模型时，需要使用tf.Variable来创建一个变量。 tf.Variable(initial_value,trainable=True,collections=None,validate_shape=True,name=None) 1biases=tf.Variable(tf.zeros([2]),name="biases") 但在某种情况下， 一个模型需要使用其他模型创建的变量， 两个模型一起训练。 比如， 对抗网络中的生成器模型与判别器模型 。 如果使用tf.Variable， 将会生成一个新的变量， 而我们需要的是原来的那个biases变量。 这时怎么办呢 ？ 这是就要通过get_variable方法，实现共享变量来解决这个问题。这个方法可以使用多套网络模型来训练一套权重。 使用get_variable获取变量get_variable一般会配合variable_scope一起使用，以实现共享变量。variable_scope的意思时变量作用域。在某一作用域中的变量可以被设置成共享的方式，被其它网络模型使用。 get_variable函数的定义如下: tf.get_variable(name,shape,initializer) 在Tensorflow里，使用get_variable生成的变量是以指定的name属性为唯一标识，并不是定义的变量名称。使用时一般通过name属性定位到具体变量，并将其共享到其它模型中。 首先我们来看看variable和get_variable的用法： variable的用法演示： 123456789101112131415161718import tensorflow as tfvar1 = tf.Variable(1.0 , name='firstvar')print ("var1:",var1.name)var1 = tf.Variable(2.0 , name='firstvar')print ("var1:",var1.name)var2 = tf.Variable(3.0 )print ("var2:",var2.name)var2 = tf.Variable(4.0 )print ("var1:",var2.name)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("var1=",var1.eval()) print("var2=",var2.eval()) 上面代码运行后的结果： 上面代码中定义了两次var1，可以看到在内存中生成了两个var1（因为name不一样），但是对于图来讲后面的var1是生效的，也就是var1=2.0。 var2表明了Variable定义时没有指定名字，系统会自动给加上一个名字Variable：0。 get_variable用法演示： 接着上面的代码，使用get_variable添加get_var1变量。 12345get_var1 = tf.get_variable("firstvar",[1], initializer=tf.constant_initializer(0.3))print ("get_var1:",get_var1.name)get_var1 = tf.get_variable("firstvar",[1], initializer=tf.constant_initializer(0.4))print ("get_var1:",get_var1.name) 加上上面的代码后，就会在执行第四行语句时报错。这表明，使用get_variable只能定义一次指定名称的变量。同时由于变量firstvar在前面使用Variable函数生成过一次，所以系统自动变成了firstvarvar_2：0。 把上面的代码换成下面再加到variable的用法代码里： 12345678get_var1 = tf.get_variable("firstvar",[1], initializer=tf.constant_initializer(0.3))print ("get_var1:",get_var1.name)get_var1 = tf.get_variable("firstvar1",[1], initializer=tf.constant_initializer(0.4))print ("get_var1:",get_var1.name)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("get_var1=",get_var1.eval()) 从运行结果知道，这次仍然是又定义了一个get_var1，不同的是改变了它的名字firstvar1。新的get_var1会在图中生效，所以它的输出值是0.4而不是0.3。 变量作用域：再前面的例子中，都已经知道使用get_variable创建两个同样名字的变量是行不通的。像以下代码就是行不通的： 12var1=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)var2=tf.get_variable("firstvar",shape=[2],dtype=tf.float32) 如果真想这么做，可以使用variable_scope将它们隔开， 1234567import tensorflow as tfwith tf.variable_scope("test1"): var1=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)with tf.variable_scope("test2"): var2=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)print("var1",var1.name)print("var2",var2.name) 可以看出，var1和var2都使用firstvar的名字来定义。通过输出可以看出，其实生成的两个变量var1和var2是不同的，它们作用在不同的scope下，这就是scope的作用。 另外，scope还支持嵌套，将上面的代码中的第二个scope缩进一下， 1234567import tensorflow as tfwith tf.variable_scope("test1"): var1=tf.get_variable("firstvar",shape=[2],dtype=tf.float32) with tf.variable_scope("test2"): var2=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)print("var1",var1.name)print("var2",var2.name) variable_scope还可以让模型更直观的显示。 实现共享变量费劲使用get_variable，目的就是为了通过它实现共享变量的功能。 variable_scope里面有个reuse=True属性，表示使用已经定义过的变量。这是get_variable将不再创建新的变量，而是去图中get_variable所创建过的变量中找与name相同的变量。 1234567891011121314import tensorflow as tfwith tf.variable_scope("test1"): var1=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)with tf.variable_scope("test2"): var2=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)with tf.variable_scope("test1",reuse=True): var3=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)with tf.variable_scope("test2",reuse=True): var4=tf.get_variable("firstvar",shape=[2],dtype=tf.float32) print("var1",var1.name)print("var2",var2.name)print("var3",var3.name)print("var4",var4.name) 输出如下： var1和var3的输出名字是一样的，var2和var4的输出名字也是一样的。这表明var1和var3共用一个变量，var2和var4共用一个变量。这就实现了共享变量。在实际应用过程中，可以把var1和var2放到一个网络模型里去训练，把var3和var4放到另一个网络模型里去训练，而两个模型的训练结果都会作用于一个模型的学习参数上。 初始化共享变量的作用域variable_scope和get_variable都有初始化的功能。 在初始化时， 如果没有对当前变量初始化，则TensorFlow会默认使用作用域的初始化方法对其初始化， 并且作用域的初始化方法也有继承功能。 12345678910111213141516import tensorflow as tf#将test1作用域进行初始化为0.4with tf.variable_scope("test1",initializer=tf.constant_initializer(0.4)):#var1没有初始化 var1=tf.get_variable("firstvar",shape=[2],dtype=tf.float32) with tf.variable_scope("test2"):#嵌套的test2作用域也没有初始化 var2=tf.get_variable("firstvar",shape=[2],dtype=tf.float32)#test2下的var3进行了初始化 var3=tf.get_variable("var3",shape=[2],initializer=tf.constant_initializer(0.3))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("var1=",var1.eval()) print("var2=",var2.eval()) print("var3=",var3.eval()) 运行代码结果如下： var1的数组值为0.4，表明继承了test1的值。var2数组值为0.4，表明其所在的作用域test2也继承了test1的初始化。变量var3在创建时同步指定了初始化操作，所以数组值为0.3。 如果去掉test2的缩进，结果如下： 作用域和操作符的受限范围variable_scope还可以使用with variable_scope(“name”) as xxxscope的方式定义作用域，当使用这种方式时，所定义的作用域变量xxxscope将不在受到外围的scope所限制。 123456789101112131415import tensorflow as tfwith tf.variable_scope("scope1") as sp: var1=tf.get_variable("v",[1])#输出作用域名称print("sp：",sp.name)print("var1：",var1.name)with tf.variable_scope("scope2"): var2=tf.get_variable("v",[1]) with tf.variable_scope(sp) as sp1: var3=tf.get_variable("v3",[1])print("sp1：",sp1.name)print("var2：",var2.name)print("var3：",var3.name) 上面这个代码定义了作用域scope1 as sp，然后将sp放在作用域scope2中，并as 成sp1。运行代码结果如下： sp1在scope2下， 但是输出仍是scope1， 没有改变。 在它下面定义的var3的名字是scope1/v3： 0， 表明也在scope1下， 再次说明sp没有受到外层（也就是scope2）的限制。 另外再介绍一个操作符的作用域tf.name_scope。操作符不仅受到tf.name_scope作用域的限制，还受到tf.variable_scope作用域的限制。 1234567import tensorflow as tfwith tf.variable_scope("scope"): with tf.name_scope("bar"): v=tf.get_variable("v",[1]) x=1.0+vprint("v:",v.name)print("x.op",x.op.name) 从结果可以看出，虽然v和x都在scope的bar下面，但是v的命名只受到scope的限制，tf.name_scope只能限制op。不能限制变量的命名。 在tf.name_scope函数中，还可以使用空字符将作用域返回到顶层。 123456789101112131415161718import tensorflow as tfwith tf.variable_scope("scope1") as sp: var1=tf.get_variable("v",[1])with tf.variable_scope("scope2"): var2=tf.get_variable("v",[1]) with tf.variable_scope(sp) as sp1: var3=tf.get_variable("v3",[1])#空字符作为作用域名 with tf.variable_scope(""): var4=tf.get_variable("v4",[1])with tf.variable_scope("scope"): with tf.name_scope("bar"): v=tf.get_variable("v",[1]) x=1.0+v with tf.name_scope(""): y=1.0+vprint("var4：",var4.name)print("y.op：",y.op.name) 从结果可以看出，y变成顶层了，而var4多了一个空层 添加权重参数、损失值等的变化 首先收集变量： 而后在会话中运行：]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow（1）]]></title>
    <url>%2F2019%2F08%2F09%2Ftensorflow%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[张量张量可以说时Tensorflow的标志。因为整个框架的名称Tensorflow就是张量流的意思。 在tensorflow中把数据称为张量（tensor）。每个tensor包含了类型（type）、阶（rank）和形状（shape）。 tensor类型下面把tensor类型和python的类型放在一起比较。 tensor阶张量的阶：相当于数组的维度。 但张量的阶和矩阵的阶并不是同一个概念，主要是看有几层中括号。例如：对于一个传统的三阶矩阵a=[[1,2,3],[4,5,6],[7,8,9]]。在张量中的阶数表示为2阶（因为它有两层中括号） tensor形状shape用于描述张量内部的组织关系。“形状”可以通过Python中的整数列表或元祖来表示，也可以Tensorflow中的相关形状函数来表示。 举例：一个二阶张量a=[[1,2,3],[4,5,6]]形状是两行三列，描述为(2,3)。 tensorflow中张量形状分为动态形状和静态形状，其在于有没有生成一个新的张量数据。静态形状的修改不能跨维度修改 相关操作张量的相关操作包括类型转换、数值操作、形状变换、数据操作。 类型转换 数值操作 tf.ones_like和tf.zeros_like的描述错了，应该分别生成1和0 形状变换 数据操作 算术运算函数 矩阵相关函数 规约计算规约计算的操作都会有降维的功能，在所有reduce_xxx系列操作函数中，都是以xxx的手段降维，每个函数都有axis这个参数，即沿某个方向，使用xxx方法对输入的Tensor进行降维。 axis的默认值是None，即把input_tensor降到0维。即一个数。对于二维input_tensor而言，axis=0，则按列计算。axis=1，则按行计算。 tf.reduce_mean()函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。 如果想设置为原来向量的维度，keep_dims=True。 分割分割操作是tensorflow不常用的操作，在复杂的网络模型里偶尔才会用到。 图整个程序的结构称为图（graph），可以把图看作一个计算任务。 一个Tensorflow程序默认是建立一个图的，除了系统自动建图外，还可以自主建图。 建立图 12345678910111213141516import tensorflow as tfimport numpy as npc=tf.constant(0.0)g=tf.Graph()with g.as_default(): c1=tf.constant(0.0) print(c1.graph) print(g) print(c.graph) g2=tf.get_default_graph()print(g2)tf.reset_default_graph()g3=tf.get_default_graph()print(g3) 运行上面代码，得出结果如下： c是刚开始的默认图中建立的，所以图的打印值就是原始的默认图的打印值。然后使用tf.Graph函数建立了一个图g，并且在新建的图里添加变量，可以通过变量的“.graph”获得所在的图。在新图g的作用域外，使用tf.get_default_graph又获得了原始的默认图，接着又使用tf.reset_default_graph函数，相当于重新建立一张图来代替原来的默认图。 注意：在使用tf.reset_default_graph函数时必须保证当前图的资源已经全部释放，否则会报错。 获取张量在图里可以通过名字得到其对应的元素，例如get_tensor_by_name可以获得图里面的张量。在上面的代码的基础上加上下面这段代码： 12345678print(c1.name)t=g.get_tensor_by_name(name="Const":0)print(t)#运行结果#Const:0#Tensor("Const:0",shape=(),dtype=float32) 常量c1是在一个子图g中建立的。with tf.Graph()、as_default()代码表示使用tf.Graph函数来创建一个图，并在其上面定义op。 获取节点操作获取节点操作op的方法是get_operation_by_name。 123456789101112import tensorflow as tfa=tf.constant([[1.0,2.0]])b=tf.constant([[1.0],[3.0]])tensor1=tf.matmul(a,b,name="exampleop")print(tensor1.name,tensor1)g3=tf.get_default_graph()test=g3.get_tensor_by_name("exampleop:0")print("test",test)print(tensor1.op.name)testop=g3.get_operation_by_name("exampleop")print("testop",testop) 打印了一堆信息，看不懂。。。 获取元素列表如果想看图中的全部元素，可以使用get_operations函数来实现。 12tt2=g.get_operations()print("所有的元素",tt2) 有多少个常量，就打印多少条信息。 获取对象前面是根据名字来获取信息，还可以根据对象来获取对象。使用tf.Graph.as_graph_element(obj,allow_tensor=True,allow_operation=True)函数，即传入的是一个对象，返回一个张量或是一个op。 12345tt3=g.as_graph_element(c1)print(tt3)#输出内容#Tensor("Const:0", shape=(), dtype=float32) 看上述代码对tt3的打印来看，变量tt3所指的张量名字为Const0。而在获取张量那小节可以看到量名c1所指向的真实张量名字也为Const0。这表明：函数as_graph_element获得了c1的真实张量对象，并赋给了变量tt3。 动态图（Eager）动态图是相对于静态图而言的。所谓的动态图是指在Python中代码被调用后，其操作立即被执行的计算。其与静态图最大的区别是不需要使用session来建立会话了。即：在静态图中，需要在会话中调用run方法才可以获得某个张量的具体值，而在动态图中，直接运行就可以得到具体值。 动态图是在Tensorflow1.3版本之后出现的。启动动态图只需要在程序的最开始处加上两行代码 12import tensorflow.contrib.eager as tfetfe.enable_eager_execution() 上面这两行代码的作用就是开启动态图计算功能。例如：调用tf.matmul时，将会立即计算两个数相乘的值，而不是op。 在创建动态图的过程中， 默认也建立了一个session。 所有的代码都在该session中进行， 而且该session具有进程相同的生命周期。 这表明一旦使用动态图就无法实现静态图中关闭session的功能。 这便是动态图的不足之处： 无法实现多session操作。 如果当前代码只需要一个session来完成的话， 建议优先选择动态图Eager来实现 会话运算程序的图称为会话（Session）。一次只能运行一个图 会话的作用：1、运行图的结构 2、分配资源运算 3、掌握资源 会话需要进行资源释放，需要run后进行close。否则可以使用with作为上下文管理器 可以在会话当中指定图去运行 sess.run(fetches，feed_dict=None,graph=None)启动整个图。 用来运行op和计算tensor feed_dict常与placeholder(占位符)一起使用 变量：tensorflow中的变量也是一种op，是一种特殊的张量能够进行存储持久化，它的值就是张量，默认被训练。其中有个trainable参数默认为True，如果改为False，变量将不再变化。 图的可视化（tensorboard）tensorflow提供了一个可视化工具TensorBoard，它可以把训练过程中的各种绘制数据展示出来，包括标量(Scalers)、图片(Images)、音频(Audio)、计算图(Graph)、数据分布、直方图(Histograms)和嵌入式向量。可以通过网页来观察模型的结构和训练过程中各个参数的变化。 当然，TensorBoard不会自动把代码展示出来，其实它是一个日志展示系统，需要在session中运算图时，将各种类型的数据汇总并输出到日志文件中。然后启动TensorBoard服务，TensorBoard读取这些日志文件，并开启6006端口提供web服务，让用户可以在浏览器中查看数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltplotdata = &#123; "batchsize":[], "loss":[] &#125;def moving_average(a, w=10): if len(a) &lt; w: return a[:] return [val if idx &lt; w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]#生成模拟数据train_X = np.linspace(-1, 1, 100)train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.3 # y=2x，但是加入了噪声#图形显示plt.plot(train_X, train_Y, 'ro', label='Original data')plt.legend()plt.show()tf.reset_default_graph()# 创建模型# 占位符X = tf.placeholder("float")Y = tf.placeholder("float")# 模型参数W = tf.Variable(tf.random_normal([1]), name="weight")b = tf.Variable(tf.zeros([1]), name="bias")# 前向结构z = tf.multiply(X, W)+ btf.summary.histogram('z',z)#将预测值以直方图显示#反向优化cost =tf.reduce_mean( tf.square(Y - z))tf.summary.scalar('loss_function', cost)#将损失以标量显示learning_rate = 0.01optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) #Gradient descent# 初始化变量init = tf.global_variables_initializer()#参数设置training_epochs = 20display_step = 2# 启动sessionwith tf.Session() as sess: sess.run(init) merged_summary_op = tf.summary.merge_all()#合并所有summary #创建summary_writer，用于写文件 summary_writer = tf.summary.FileWriter('log/mnist_with_summaries',sess.graph) # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;) #生成summary summary_str = sess.run(merged_summary_op,feed_dict=&#123;X: x, Y: y&#125;); summary_writer.add_summary(summary_str, epoch);#将summary 写入文件 #显示训练中的详细信息 if epoch % display_step == 0: loss = sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;) print ("Epoch:", epoch+1, "cost=", loss,"W=", sess.run(W), "b=", sess.run(b)) if not (loss == "NA" ): plotdata["batchsize"].append(epoch) plotdata["loss"].append(loss) print (" Finished!") print ("cost=", sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), "W=", sess.run(W), "b=", sess.run(b)) #print ("cost:",cost.eval(&#123;X: train_X, Y: train_Y&#125;)) #图形显示 plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() plotdata["avgloss"] = moving_average(plotdata["loss"]) plt.figure(1) plt.subplot(211) plt.plot(plotdata["batchsize"], plotdata["avgloss"], 'b--') plt.xlabel('Minibatch number') plt.ylabel('Loss') plt.title('Minibatch run vs. Training loss') plt.show() print ("x=0.2，z=", sess.run(z, feed_dict=&#123;X: 0.2&#125;)) 运行上面代码显示的内容和以前一样没什么变化，在代码的同级目录下多了一个文件夹，里面还有个文件夹。文件夹内有个文件。首先在命令行下pip安装tensorboard，接着在这个文件的上级路径下，输入下面命令 双引号中间填绝对路径，注意不要出现中文和空格。 打开浏览器，输入 http://127.0.0.6006。会跳到以下界面，点击SCALARS，会看到之前创建的loss\_function。这个loss\_function也是可以点开的，点击loss\_function可以看到损失值随迭代次数的变化情况。还可以调节平滑数来改变右边标量的曲线。类似的还可以点击GRAPHS看看神经网络的内部结构，点击HISTOGRAMS看另一个显示值z。 注意：1、浏览器最好使用谷歌。2、在命令行里启动TensorBoard时，一定要先进入到日志所在的上级目录下，否则打开的页面找不到创建好的信息。 深度学习中的线性回归： 123456789101112131415161718192021222324252627282930313233import tensorflow as tfimport matplotlib.pyplot as pltimport numpy as nptrain_x=np.linspace(-1,1,100)train_y=2*train_x+np.random.randn(*train_x.shape)*0.3#plt.plot(train_x,train_y,label="original data")#plt.legend()#plt.show()X=tf.placeholder("float")Y=tf.placeholder("float")W=tf.Variable(tf.random_normal([1]),name="weight")b=tf.Variable(tf.zeros([1]),name="bias")z=tf.multiply(X,W)+bcost=tf.reduce_mean(tf.square(Y-z))learn_rate=0.005optimizer=tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)init=tf.global_variables_initializer()training_epochs=20display_step=2with tf.Session() as sess: sess.run(init) plotdata=&#123;"batchsize":[],"loss":[]&#125; for epoch in range(training_epochs): for (x,y) in zip(train_x,train_y): sess.run(optimizer,feed_dict=&#123;X:x,Y:y&#125;) if epoch%display_step==0: loss=sess.run(cost,feed_dict=&#123;X:train_x,Y:train_y&#125;) print("Epoch:",epoch+1,"cost",loss,"W=",sess.run(W),"b=",sess.run(b)) if not (loss=="NA"): plotdata["batchsize"].append(epoch) plotdata["loss"].append(loss) print("finish") print("cost",sess.run(cost,feed_dict=&#123;X:train_x,Y:train_y&#125;),"W=",sess.run(W),"b=",sess.run(b)) 如果后面的几个数据的迭代得出的结果是一样的，那么证明该模型已经梯度下降到最低点。此时应该把学习率调小。 保存和载入模型保存模型首先需要建立一个saver，然后再session中通过saver的save即可将模型保存起来。 1234567#生成saversaver=tf.train.Saver()with tf.Session() as sess:#对模型初始化 sess.run(tf.global_variables_initializer())#训练完后，使用saver.save进行保存 saver.save(sess,"save_path/file_name") 运行以上代码后，在代码的同级目录下，生成几个文件。如图： 载入模型123456saver=tf.train.Saver()with tf.Session() as sess:#参数可以进行初始化，也可以不进行初始化。即使初始化了，初始化的值也会被restore函数值覆盖。 sess.run(tf.global_variable_initializer())#会将已经保存的变量值restore到变量中 saver.restore(sess,"save_path/file_name") 保存模型的其它方法Saver还可以指定存储变量名字与变量的对应关系 1saver=tf.train.Saver(&#123;"weight":W,"bias":b&#125;) 上面这种写法代表将W变量的值放到weight名字中，类似的写法还有以下两种： 1234#放到一个list里saver=tf.train.Saver([W,b])#将op的名字当作keysaver=tf.train.Saver(&#123;v.op.name:v for v in [W,b]&#125;) 模型内容虽然模型已经保存了，但是仍然对我们不透明。下面代码将模型内容打印出来，看看保存了那些东西 12from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_fileprint_tensors_in_checkpoint_file("./save_path/model") 添加保存检查点保存模型并不限于在训练之后，在训练之中也需要保存，因为tensorflow训练模型时难免会出现中断的情况。我们希望能够将幸苦得到的中间参数保留下来，否则下次又要重新开始。这种，在训练中保存模型，习惯上称为保存检查点。 ​ PS：我感觉预训练模型和这个一样 此时用到saver的另一个参数，max_to_keep。表明最多只保存检查点文件的个数。 saver=tf.train.Saver(max_to_keep=1)，代表在迭代过程中只保存一个文件。这样，在循环训练模型中，新生成的模型就会覆盖以前的模型。 下面两种方法可以快速获取到检查点文件： 12345678#第一种ckpt=tf.train.get_checkpoint_state(ckpt_dir)if ckpt and ckpt.model_checkpoint_path: saver.restore(sess,ckpt.model_checkpoint_path)#第二种kpt=tf.train.latest_checkpoint(savedir)if kpt!=None: saver.restore(sess,kpt)]]></content>
      <categories>
        <category>深度学习</category>
        <category>框架</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法分类和数据分割]]></title>
    <url>%2F2019%2F08%2F09%2FML%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[机器学习算法分类 区分监督学习和非监督学习的方法：看是否有标准答案。无监督学习无标准答案，只有特征值。 区分分类问题和回归问题的方法:目标值是否是离散型。目标值是离散型，则是分类问题。目标值是连续型，则是回归问题。 数据分割模型=算法+数据 数据分为训练集和测试集 fit_transform=fit+transform：fit做的是计算平均值和标准差，transform做的是转化]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理与特征工程]]></title>
    <url>%2F2019%2F08%2F09%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 前些天我把python的处理图像的库——opencv总结了一下，但这终究是传统方法处理图像。现在都是用深度学习网络处理图像。所以，在学深度学习之前，我看了些机器学习的知识。但在看机器学习的算法前，我们先来看看特征工程。 sklearn中的数据堪称完美，各大教材的数据也是一样。但是到我们现实应用中时，发现模型调用效果差，这是因为现实中的数据离完美的数据集差十万八千里。所以数据预处理和特征工程是必要的。 sklearn中六大板块有两大板块是关于数据预处理和特征工程的：也就是黄色标识的两个模块 特征抽取在机器学习中，大多数算法都只能处理数值型数据，不能处理文字。在sklearn当中，除了专用来处理文字的算法，其它的算法在fit的时候全部要求输入数组和矩阵，也不能导入文字型数据。然而在现实生活中，许多标签和特征在数据收集完毕后，都不是以数字来表现的。为了让数据适应算法，我们必须将数据进行编码，即将文字型数据转换为数值型。 preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值。这个没运行成功，好像是我的数据集有问题。我自己设了个数据 123456789101112131415from pandas import DataFrame,Seriesfrom sklearn.preprocessing import LabelEncoderimport pandas as pddata=DataFrame(data=[[-1,2],[-0.5,6],[0,18],[1,18]])print(data)y=data.iloc[:,-1]#要输入的是标签，所以容许一维数组print(y)le=LabelEncoder()le.fit(y) #导入数据，调取结果label=le.transform(y)#print(le.fit_transform(y))print("调取结果是：",label)print(le.classes_) #查看标签中有多少类型data.iloc[:,-1]=labelprint(data) preprocessing.OrdinalEncoder：特征专用，能够将分类特征转换为分类数值 字典特征抽取，把数据中的以字符串标记的数据转化为one-hot编码。 由上面代码可得下面的稀疏（sparse）矩阵： 将稀疏矩阵转为矩阵： 将矩阵转化为字典： 文本特征抽取： 和字典特征抽取不一样的是CountVectorizer没有sparse参数，只能通过矩阵的toarray转化为数组 可得： 注意：单个字母不统计。因为单个英文字母没有依据 如果文本是中文，根据需要的词频给文本添加空格。同样，单个字无法统计 利用jieba分词对数据进行one-hot编码： TF-IDF特征抽取：用以评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度 数据预处理数据归一化处理通过对原始数据进行交换把数据映射到（默认为[0,1]）之间 归一化缺点：注意在特定场景最大值最小值是变化的。另外，最大值与最小值非常容易受异常点的影响，所以这种方法的鲁棒性差，只适合传统精确小数据场景。 数据标准化处理通过对原始数据进行变换把数据变换到均值为0，方差为1的范围。 标准化总结：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 除了StandardScaler和MinMaxScaler之外，sklearn也提供了各种其他的缩放处理： 缺失值处理方法老版本用Imputer，新版本用SimpleImputer 删除：如果每列或者行数据缺失值达到一定比例时，建议放弃整行或者整列 插补：可以通过缺失值每行或者每列的平均值、中位数来填充 Imputer： SimpleImputer： 1234567891011import pandas as pdfrom sklearn.preprocessing import LabelEncoderfrom sklearn.impute import SimpleImputerdata=pd.read_csv("train.csv",index_col=0)#打印行数，列数，列索引，列非空值个数，列类型，内存占用print(data.info())Age = data.loc[:,"Age"].values.reshape(-1,1) #sklearn当中特征矩阵必须是二维imp_mean = SimpleImputer() #实例化，默认均值填补imp_mean = imp_mean.fit_transform(Age) #fit_transform一步完成调取结果data.loc[:,"Age"]=imp_mean #使用中位数填补Ageprint(data["Age"]) 处理连续型特征二值化和分段 sklearn.preprocessing.Binarizer 根据阈值将数据二值化(将特征值设置为0或1)，用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射为1。 1234from sklearn.preprocessing import BinarizerAge=data.loc[:,"Age"].values.reshape(-1,1)Bin=Binarizer(threshold=30).fit_transform(Age)print(Bin) sklearn.preprocessing.KBinsDiscretizer 这是将连续变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。三个重要参数： 12345678from sklearn.preprocessing import KBinsDiscretizerx=data.loc[:,"Age"].values.reshape(-1,1)est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')est.fit_transform(x)#查看转换后分的箱:变成了一列中的三箱 set(est.fit_transform(X).ravel()) ravel降维est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform') #查看转换后分的箱变成了哑变量result=est.fit_transform(x).toarray()print(result) 特征工程当数据预处理完成后，我们就要开始进行特征工程了 我们有四种方法可以用来选择特征：过滤法，嵌入法，包装法，和降维算法。 过滤法方差过滤这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征。VarianceThreshold有重要参数threshold，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。 12345678910from sklearn.feature_selection import VarianceThresholdimport pandas as pddata = pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]print(X.shape)selector = VarianceThreshold() #实例化，不填参数默认方差为0X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵#也可以直接写成 X = VairanceThreshold().fit_transform(X)print(X_var0.shape) 可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了： 123456789from sklearn.feature_selection import VarianceThresholdimport pandas as pddata = pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]import numpy as npX_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)np.median(X.var().values)print(X_fsvar.shape) 当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为： 其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率。 12345678from sklearn.feature_selection import VarianceThresholdimport pandas as pddata = pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征X_bvar = VarianceThreshold(.8 * (1 - .8)).fit_transform(X)print(X_bvar.shape) 如果你用knn和随机森林对没过滤的数据和过滤后的数据进行交叉验证，会发现，随机森林的准确率略低于knn，但随机森林运行的速度比knn快很多。为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？这是由于两种算法的原理中涉及到的计算量不同。最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。因此，过滤法的主要对象是：需要遍历特征或升维的算法们，而过滤法的主要目的是：在维持算法表现的前提下，帮助算法们降低计算成本。 对受影响的算法来说，我们可以将方差过滤的影响总结如下： 我们怎样知道，方差过滤掉的到底时噪音还是有效特征呢？过滤后模型到底会变好还是会变坏呢？答案是：每个数据集不一样，只能自己去尝试。这里的方差阈值，其实相当于是一个超参数，要选定最优的超参数，我们可以画学习曲线，找模型效果最好的点。 相关性过滤方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息。 卡方过滤卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。 123456789101112131415from sklearn.ensemble import RandomForestClassifier as RFCfrom sklearn.model_selection import cross_val_scorefrom sklearn.feature_selection import VarianceThresholdfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2import pandas as pddata=pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]import numpy as npX_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)#假设在这里我一直我需要300个特征X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)print(X_fschi.shape)print(cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()) 可以看出，模型的效果降低了，这说明我们在设定k=300的时候删除了与模型相关且有效的特征，我们的K值设置得太小，要么我们需要调整K值，要么我们必须放弃相关性过滤。当然，如果模型的表现提升，则说明我们的相关性过滤是有效的，是过滤掉了模型的噪音的，这时候我们就保留相关性过滤的结果。 123456789#接上面的代码import matplotlib.pyplot as pltscore = []for i in range(200,390,10): X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y) once = cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean() score.append(once)plt.plot(range(200,390,10),score)plt.show() 通过这条曲线，我们可以观察到，随着K值的不断增加，模型的表现不断上升，这说明，K越大越好，数据中所有的特征都是与标签相关的。但是运行这条曲线的时间同样也是非常地长。还有一种更好的k的方法：看p值选择k 卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和P值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平，即p值判断的边界，具体我们可以这样来看： 调用SelectKBest之前，我们可以直接从chi2实例化后的模型中获得各个特征所对应的卡方值和P值。 123456789#接上面的代码chivalue, pvalues_chi = chi2(X_fsvar,y)#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征#pvalues_chi就是p值print(pvalues_chi)#从此看出所有特征的p值为0k = chivalue.shape[0] - (pvalues_chi &gt; 0.05).sum()print(k)X_fschi = SelectKBest(chi2, k=k).fit_transform(X_fsvar, y)print((cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean())) 可以观察到，所有特征的p值都是0，这说明对于digit recognizor这个数据集来说，方差验证已经把所有和标签无关的特征都剔除了，或者这个数据集本身就不含与标签无关的特征。在这种情况下，舍弃任何一个特征，都会舍弃对模型有用的信息，而使模型表现下降，因此在我们对计算速度感到满意时，我们不需要使用相关性过滤来过滤我们的数据。 F检验F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式。 F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，我们希望选取p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。 1234567#接上面的代码from sklearn.feature_selection import f_classifF, pvalues_f = f_classif(X_fsvar,y)k = F.shape[0] - (pvalues_f &gt; 0.05).sum()print(k)X_fsF = SelectKBest(f_classif, k=k).fit_transform(X_fsvar, y)print(cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()) 得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。 互信息法互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。 所有特征的互信息量估计都大于0，因此所有特征都与标签相关。 Embedder嵌入法嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性。我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。 SelectFromModel是一个元变换器，可以与任何在拟合后具有coef_，feature_importances_属性或参数中可选惩罚项的评估器一起使用（比如随机森林和树模型就具有属性feature_importances_，逻辑回归就带有l1和l2惩罚项，线性支持向量机也支持l2惩罚项） 12345678910111213141516171819from sklearn.model_selection import cross_val_scoreimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltdata=pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import RandomForestClassifier as RFCRFC_ = RFC(n_estimators =10,random_state=0)X_embedded = SelectFromModel(RFC_,threshold=0.005).fit_transform(X,y)threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20)score = []for i in threshold: X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y) once = cross_val_score(RFC_,X_embedded,y,cv=5).mean() score.append(once)plt.plot(threshold,score)plt.show() 从图像上来看，随着阈值越来越高，模型的效果逐渐变差，被删除的特征越来越多，信息损失也逐渐变大。 12345678910#细化学习曲线score2 = []for i in np.linspace(0,0.002,20): X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y) once = cross_val_score(RFC_,X_embedded,y,cv=5).mean() score2.append(once)plt.figure(figsize=[20,5])plt.plot(np.linspace(0,0.002,20),score2)plt.xticks(np.linspace(0,0.002,20))plt.show() wrapper包装法包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。 区别于过滤法和嵌入法的一次是过滤法和嵌入法训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。 参数estimator是需要填写的实例化后的评估器，n_features_to_select是想要选择的特征个数，step表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，.support_：返回所有的特征的是否最后被选中的布尔矩阵，以及.ranking_返回特征的按数次迭代中综合重要性的排名。类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。 1234567891011121314151617181920212223from sklearn.model_selection import cross_val_scoreimport pandas as pdimport matplotlib.pyplot as pltdata=pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]from sklearn.ensemble import RandomForestClassifier as RFCfrom sklearn.feature_selection import RFERFC_ = RFC(n_estimators =10,random_state=0)selector = RFE(RFC_, n_features_to_select=340, step=50).fit(X, y)print("返回所有的特征的最后被选中的布尔矩阵个数：",selector.support_.sum())print("返回特征的按数次迭代中综合重要性的排名：",selector.ranking_)X_wrapper = selector.transform(X)print(cross_val_score(RFC_,X_wrapper,y,cv=5).mean())score = []for i in range(1,751,50): X_wrapper = RFE(RFC_,n_features_to_select=i, step=50).fit_transform(X,y) once = cross_val_score(RFC_,X_wrapper,y,cv=5).mean() score.append(once)plt.figure(figsize=[20,5])plt.plot(range(1,751,50),score)plt.xticks(range(1,751,50))plt.show() 特征选择总结至此，我们讲完了降维之外的所有特征选择的方法。这些方法的代码都不难，但是每种方法的原理都不同，并且都涉及到不同调整方法的超参数。经验来说，过滤法更快速，但更粗糙。包装法和嵌入法更精确，比较适合具体到算法去调整，但计算量比较大，运行时间长。当数据量很大的时候，优先使用方差过滤和互信息法调整，再上其他特征选择方法。使用逻辑回归时，优先使用嵌入法。使用支持向量机时，优先使用包装法。迷茫的时候，从过滤法走起，看具体数据具体分析。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(7)]]></title>
    <url>%2F2019%2F08%2F09%2Fopencv-7%2F</url>
    <content type="text"><![CDATA[直方图横坐标：图像中各个像素的灰度级 纵坐标：具有该灰度级的像素个数 归一化直方图： 横坐标：图像中各个像素的灰度级 纵坐标：出现这个灰度级的概率 掩膜：通过掩膜可以将一张图的某块区域的直方图画出来 直方图均值化： 注意:用cv2.equalizeHist进行直方图均衡化处理时，对彩色图像进行处理时，分通道进行。灰度图像直接均衡化 利用CLAHE有限对比适应性直方图均衡化，对彩色图像也是分通道进行。灰度图像直接均衡化。 背景建模：以高斯混合模型为基础的背景/前景分割算法 apply可以得到前景的掩膜 傅里叶变换补 。。。。。]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(6)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-6%2F</url>
    <content type="text"><![CDATA[特征提取图像特征：可以表达图像中对象的主要信息、并且以此为依据可以从其他未知图像中检测出相似或者相同图像。 常见的图像特征：边缘、角点、纹理 角点检测：cv2.cornerHarris() img:输入图像 blockSize：角点检测中要考虑的领域大小 ksize：Sobel求导中使用的窗口大小 k：方程参数，参数为[0.04，0.06] 亚像素级的角点检测：红色是原先的角点，绿色像素是修正后的像素 适合于跟踪的角点检测：cv2.goodFeaturesToTrack(): 输入灰度图像、检测的角点数目、设置角点的质量水平，0-1之间，低于这个数的都会被忽略、设置两个角点间的最短欧式距离。 SIFT尺度不变特征变换匹配算法： cv2.xfeatures2d.SIFT_create()：创建sift特征器 sift.detect()可以在图像找到关键点。如果想在图像的特定区域搜索，可以创建一个掩膜图像作为参数。属性如下： pt：表示图像中关键点的x坐标和y坐标 size：表示特征的直径 angle：表示特征的方向 response：表示关键点强度 octave：表示特征所在金字塔的层级 class_id：表示关键点的ID sift.compute():计算关键点的描述符，在sift.detect后使用。 sift.detectAndCompute()：直接找到关键点并计算出描述符 cv2.drawKeypoints(): image:原始图像 keypoints：特征点向量 outimage：特征点绘制的画布图像，可以是原图像 color：绘制的特征点颜色，可以是原图像。 flags：五种绘制模式： DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS:就会绘制代表关键点大小的圆圈甚至可以绘制除关键点的方向。 DEFAULT：只绘制特征点的坐标点,显示在图像上就是一个个小圆点,每个小圆点的圆心坐标都是特征点的坐标。 DRAW_OVER_OUTIMG：函数不创建输出的图像,而是直接在输出图像变量空间绘制,要求本身输出图像变量就是一个初始化好了的,size与type都是已经初始化好的变量 NOT_DRAW_SINGLE_POINTS：单点的特征点不被绘制 DRAW_RICH_KEYPOINTS：绘制特征点的时候绘制的是一个个带有方向的圆,这种方法同时显示图像的坐标,size，和方向,是最能显示特征的一种绘制方式 SURF：加速稳健特征算法，加快版的SIFT。与SIFT类似 BRIEF:这是一种特征描述符，他不提供查找特征的方法。所以我们需要使用其他的特征检测器。例如：SIFT或SURF。推荐使用STAR。 orb检测： bf暴力匹配： cv2.BFMatcher()：创建一个BFMatcher对象。参数： ​ normType：指定使用的距离测试类型。默认值为cv2.NORM_L2。cv2.NORM_L1也行。这两种适用于SIFT和SURF。对于ORB,BRIEF，应使用cv2.NORM_HAMMING。如果VTA_K==3或4，normType应设置为cv2.NORM_HAMMING2。 ​ crossCheck：默认为False。如果设置为True，匹配条件就会更加严格。 BFMatcher对象具有两个方法：match()和knnMatch()。第一种方法会返回最佳匹配，第二个方法为每个关键点返回k个最佳匹配（降序排列后取前k个）。 cv2.drawMatches cv2.drawMatchsKnn：就像使用 cv2.drawKeypoints() 绘制关点一样我们可以使用 cv2.drawMatches()来绘制匹配的点。它会将两幅图像先水平排列然后在最佳匹配的点之间绘制直线从原图像到目标图像。如果前面使用的是BFMatcher.knnMatch()现在我们可以使用函数 cv2.drawMatchsKnn 为每个关键点和它的 k 个最佳匹配点绘制匹配线。如果 k 等于 2就会为每个关键点绘制两条最佳匹配直线。如果我们选择性绘制就给函数传入一个掩模。 对ORB描述符进行暴力匹配： match=bf.match(des1，des2)：返回值是一个DMatch对象列表。具有以下属性： DMatch.distance - 描述符之间的距离。越小越好。 • DMatch.trainIdx - 目标图像中描述符的索引。 • DMatch.queryIdx - 查询图像中描述符的索引。 • DMatch.imgIdx - 目标图像的索引 对SIFT描述符进行暴力匹配： FLANN匹配器： cv2.FlannBasedMatcher( [, indexParams[, searchParams]] ) indexParams： searchParams：]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(5)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-5%2F</url>
    <content type="text"><![CDATA[视频处理Videocapture：VideoCapture(args)：如果args=0，则打开摄像头。如果args=路径， 则打开视频源 摄像头设置和查询： 调用摄像头读取图像数据，以及使用 cap.set( propId ， value ) cap.get( propId ) 图片合成视频： 视频分解图片： normalize：归一化函数为了消除指标之间的影响，需要对数据进行标准化处理，以解决数据指标之间的可比性 cv2.normalize(src[, dst[, alpha[, beta[, norm_type[, dtype[, mask]]]]]]) src:输入数组 dst:与src大小相同的输出数组 alpha:下限边界 beta:上限边界 norm_type:NORM_MINMAX NORM_INF NORM_L1 NORM_L2 dType:当输出为负时，输出数组具有与src相同的类型。否则，具有与src相同的信道数和深度 mask:掩膜 画图cv2.rectangle（）：若将五改为-1，则填充整个矩形 cv2.circle：要画圆的话，只需要指定圆形的中心点坐标和半径大小，颜色和粗细 cv2.ellispe:一个参数是中心点的位置坐标。下一个参数是长轴和短轴的长度。椭圆沿逆时针方向旋转的角度。椭圆弧沿顺时针方向起始的角度和结束角度，如果是 0 到 360，就是整个椭圆。 图像金字塔我们对同一图像的不同分辨率的子图像处理。比如我们在一幅图像中查找某个目标比如脸，我们不知目标在图像中的尺寸大小。这种情况下我们创建一组图，这些图像是具有不同分率的原始图像。我们把组图像叫做图像字塔简单来就是同一图像的不同分率的子图集合。如果我们把大的图像放在底部最小的放在顶部。看来像一座金字塔，故而得名图像金字塔。 图像金字塔：高斯金字塔和拉普拉斯金字塔 向下采样：将图像缩小，图像信息丢失 向上采样：将图像放大，图像会变模糊 向下采样和向上采样不是可逆的，无法将图像变为原始图像。 向下采样函数：cv2.pyrDown(原始图像) 向上采样函数：cv2.pyrUP 拉普拉斯金字塔： opencv删除窗口：删除所有窗口 ：cv2.destroyAllWindow() 删除指定窗口：cv2.destroyWindow(“original”) waitKey(x):参数为等待键盘触发的时间。单位为毫秒。如不输入参数，则输入任意键退出。 setMouseCallback():event为事件 x，y代表鼠标位于窗口的（x，y）位置 flags：代表鼠标的拖曳事件以及鼠标和键盘联合的事件。共有32种 param：函数指针，标识所相应的事件函数。 namewindow WINDOW_NORMAL 用户可以改变窗口的大小； WINDOW_AUTOSIZE 窗口大小会根据显示图像自动调整，用户必能手动改变窗口大小； WINDOW_OPENGL 支持OpenGL。]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(4)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-4%2F</url>
    <content type="text"><![CDATA[图像类型转换 cvtColor:颜色空间转换函数 cv2.cvtColor()支持多种颜色空间之间的转换，其 需要注意的是cvtColor()函数不能直接将RGB图像转换为二值图像(Binary Image)，需要借助threshold()函数 HSV中：H为色彩，取值范围是[0,179]，S为饱和度，取值范围是[0,255],V是亮度，取值范围是[0,255]。 支持的转换类型和转换码如下： 1、RGB和BGR（opencv默认的彩色图像的颜色空间是BGR）颜色空间的转换 COLOR_BGR2RGB COLOR_RGB2BGR COLOR_RGBA2BGRA COLOR_BGRA2RGBA 2、向RGB和BGR图像中增添alpha通道 COLOR_RGB2RGBA COLOR_BGR2BGRA 3、从RGB和BGR图像中去除alpha通道 COLOR_RGBA2RGB COLOR_BGRA2BGR 4、从RBG和BGR颜色空间转换到灰度空间 COLOR_RGB2GRAY COLOR_BGR2GRAY COLOR_RGBA2GRAY COLOR_BGRA2GRAY 5、从灰度空间转换到RGB和BGR颜色空间 COLOR_GRAY2RGB COLOR_GRAY2BGR COLOR_GRAY2RGBA COLOR_GRAY2BGRA 6、RGB和BGR颜色空间与BGR565颜色空间之间的转换 COLOR_RGB2BGR565 COLOR_BGR2BGR565 COLOR_BGR5652RGB COLOR_BGR5652BGR COLOR_RGBA2BGR565 COLOR_BGRA2BGR565 COLOR_BGR5652RGBA COLOR_BGR5652BGRA 7、灰度空间域BGR565之间的转换 COLOR_GRAY2BGR555 COLOR_BGR5552GRAY 8、RGB和BGR颜色空间与CIE XYZ之间的转换 COLOR_RGB2XYZ COLOR_BGR2XYZ COLOR_XYZ2RGB COLOR_XYZ2BGR 9、RGB和BGR颜色空间与uma色度（YCrCb空间）之间的转换 COLOR_RGB2YCrCb COLOR_BGR2YCrCb COLOR_YCrCb2RGB COLOR_YCrCb2BGR 10、RGB和BGR颜色空间与HSV颜色空间之间的相互转换 COLOR_RGB2HSV COLOR_BGR2HSV COLOR_HSV2RGB COLOR_HSV2BGR 11、RGB和BGR颜色空间与HLS颜色空间之间的相互转换 COLOR_RGB2HLS COLOR_BGR2HLS COLOR_HLS2RGB COLOR_HLS2BGR 12、RGB和BGR颜色空间与CIE Lab颜色空间之间的相互转换 COLOR_RGB2Lab COLOR_BGR2Lab COLOR_Lab2RGB COLOR_Lab2BGR 13、RGB和BGR颜色空间与CIE Luv颜色空间之间的相互转换 COLOR_RGB2Luv COLOR_BGR2Luv COLOR_Luv2RGB COLOR_Luv2BGR 14、Bayer格式（raw data）向RGB或BGR颜色空间的转换 COLOR_BayerBG2RGB COLOR_BayerGB2RGB COLOR_BayerRG2RGB COLOR_BayerGR2RGB COLOR_BayerBG2BGR COLOR_BayerGB2BGR COLOR_BayerRG2BGR COLOR_BayerGR2BGR inrange：实现二值化功能 image=cv2.inrance(hsv,lower_red,upper_red) 第一个参数：hsv指的是原图 第二个参数：lower_red指的是图像中低于这个lower_red的值，图像值变为0 第三个参数：upper_red指的是图像中高于这个upper_red的值，图像值变为0 而在lower_red～upper_red之间的值变成255 图像中的与、或、异或、非操作opencv中的bitwise_not，bitwise_xor，bitwise_or，bitwise_and的使用方法与效果。 bitwise_and是对二进制数据进行“与”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“与”操作，1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0。 bitwise_or是对二进制数据进行“或”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“或”操作，1|1=1，1|0=0，0|1=0，0|0=0。 bitwise_xor是对二进制数据进行“异或”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“异或”操作，1^1=0,1^0=1,0^1=1,0^0=0。 bitwise_not是对二进制数据进行“非”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“非”操作，~1=0，~0=1。 Hough直线变换cv2.HoughLines()，返回值就是距离和角度。这个函数的第一个参数是一个二值化图像。第二个和第三个值代表距离和角度的精确度。第四个参数是阈值，只有累加其中的值高于阈值时才被认为是一条直线。 cv2.HoughLinesP():比上面多两个参数，minLineLength和MaxLineGap。比较简单 minLineLength：线的最短长度，比这个短的线都忽略 MaxLineGap：两条线段的最大间断，如果小于此值，这两条直线被看成一条直线 Hough圆环变换：HoughCircles(image, method, dp, minDist[, circles[, param1[, param2[, minRadius[, maxRadius]]]]]) image参数表示8位单通道灰度输入图像矩阵。 method参数表示圆检测方法，目前唯一实现的方法是HOUGH_GRADIENT。 dp参数表示累加器与原始图像相比的分辨率的反比参数。例如，如果dp = 1，则累加器具有与输入图像相同的分辨率。如果dp=2，累加器分辨率是元素图像的一半，宽度和高度也缩减为原来的一半。 minDist参数表示检测到的两个圆心之间的最小距离。如果参数太小，除了真实的一个圆圈之外，可能错误地检测到多个相邻的圆圈。如果太大，可能会遗漏一些圆圈。 circles参数表示检测到的圆的输出向量，向量内第一个元素是圆的横坐标，第二个是纵坐标，第三个是半径大小。 param1参数表示Canny边缘检测的高阈值，低阈值会被自动置为高阈值的一半。 param2参数表示圆心检测的累加阈值，参数值越小，可以检测越多的假圆圈，但返回的是与较大累加器值对应的圆圈。 minRadius参数表示检测到的圆的最小半径。 maxRadius参数表示检测到的圆的最大半径]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(3)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-3%2F</url>
    <content type="text"><![CDATA[图像轮廓和图像边缘不一样，边缘不连续。将边缘连接成一个整体构成轮廓。 提取图像轮廓的方法：先调用cv2.findContours()，后调用cv2.drawCont() cv2.findCount函数使用方法: 参数mode的几种方式： 参数method的几种方法： 轮廓特征：cv2.moments()会将计算得到的矩以一个字典的形式返回。 轮廓面积可以使用函数 cv2.contourArea() 计算得到 轮廓周长：cv2.arcLength(cnt，True) 函数的第二个参数用来指定对象的形状是闭合的(True)，还是打开的。 轮廓近似：cv2.approxPolyDP(cnt，epsilon，True) 凸包：函数cv2.convexHull() 可以用来检测一个曲线是否具有凸性缺陷并能纠正凸性缺陷。凸性曲线总是凸出来的，至少是平的。如果有地方凹去了就叫做凸性缺陷。 凸性检测：cv2.isContourConvex()可以用来检测一个曲线是不是凸的，返回True和False。 边界矩形：直边界矩形和旋转的边界矩形。 直边界矩形：面积不是最小的。x,y,w,h=cv2.boundingRect()。x,y,为矩形左上角的坐标，w,h为矩形的宽和高 旋转的边界矩形：面积最小。考虑了对象的旋转。cv2.minAreaRect())。返回的是一个 Box2D 结构，其中包含矩形左上点的坐标x，y矩形的宽和高w，h以及旋度。但是绘制个矩形矩形的 4 个点可以函数cv2.boxPoints() 获 得。 最小外接圆：cv2.minEnlosingCircle() 椭圆拟合：旋转边界矩形的内切圆 cv2.fitEllipse(cnt) 形状匹配：cv2.matchShape()可以帮我们比较两个形状或轮廓的相似度。返回值越小，匹配越好。 getStructuringElement构建一个核。前面腐蚀膨胀的numpy构建的结构化元素是正方形的 getStructuringElement 与 Numpy 定义的元素结构是完全一样的这个函数的第一个参数表示内核的形状，有三种形状可以选择。 矩形：MORPH_RECT; 交叉形：MORPH_CROSS; 椭圆形：MORPH_ELLIPSE; 第二和第三个参数分别是内核的尺寸以及锚点的位置。对于锚点的位置，有默认值（-1,-1），表示锚点位于中心点。element形状唯一依赖锚点位置，其他情况下，锚点只是影响了形态学运算结果的偏移。 透视变换在不同的视觉拍摄同一个物体，会有不同的图像。透视变换就是类似于改变拍摄物体的角度 仿射变换：由平移、错切、缩放、反转、旋转复合而成，是透视变换的特殊形式 OpenCV提供了两个变换函数cv2.warpAﬃne(仿射变换)和cv2.warpPerspective(透视变换) ，cv2.warpAﬃne 接收的参数是 2×3 的变换矩阵，而cv2.warpPerspective 接收的参数是 3×3 的变换矩阵。 函数cv2.warpAﬃne() 的第三个参数的是输出图像的大小。它的格式应是图像的(宽,高)。注意的是图像的宽对应的是列数，高对应的是行数。可以实现图片平移。可以和cv2.getAffineTransform配合使用 其中两个参数是变换前后的位置关系 函数cv2.warpPerspective配合cv2.getPerspectiveTransform()使用。同时可以用findHomography返回的单应性矩阵。 getPersonspectiveTransform得出变换矩阵： 得出变换矩阵以后用warpPerspective()：第一个参数是输入图像，M是变换矩阵，第三个参数是输出图像的大小 findHomography：提供正确估计的好的匹配被叫做inliers，而其他的叫做outliers。cv2.findHomography()返回一个掩图来指定inline和outline。第一个和第二个参数分别是原图像和目的图像，第三个参数可选为cv2.RANSAC、cv2.LMEDS.第二个参数取值范围在1到10。 getPerspectiveTransform和findHomography的区别： 旋转：cv2.getRotationMatrix2D()]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(2)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-2%2F</url>
    <content type="text"><![CDATA[图像滤波：即在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像预处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。 图像滤波图像滤波的目的 a、消除图像中混入的噪声 b、为图像识别抽取出图像特征 均值滤波，用blur函数 方框滤波：进行归一化处理后，和均值滤波相同。当normalize为零时，不进行归一化处理。当normalize为一时，进行归一化处理（默认）。目标图像深度通常于原始图像一样，值为-1。 高斯滤波： 注意：ksize：核大小必须为单数 sigmaX、sigmaY：控制权重 中值滤波： 两种边缘保留滤波： biateraFilter：能在保持边界清晰的情况下有效的去除噪音 filter2D:对2D图像实施低通滤波。去除噪音，模糊图像 膨胀和腐蚀膨胀、腐蚀时用到的kernel的形状一般有下面三种： 矩形: MORPH_RECT 交叉形: MORPH_CROSS 椭圆形: MORPH_ELLIPSE 图像腐蚀：调用erode函数 图像膨胀：调用dialate 开运算：先通过图像腐蚀，后经过图像膨胀可以图像去噪。iteration表示次数 闭运算：先是先通过图像膨胀，后经过图像腐蚀 梯度运算：图像膨胀-图像腐蚀 图像礼帽（图像顶帽）：原始图像-开运算图像，得到噪声图像 图像黑帽：闭运算-原始图像 求梯度梯度简单来说就是求导。Sobel，Scharr是求一阶导数或二阶导数。Scharr是对Sobel的优化。Laplacian是求二阶导数。 Sobel算子：当一个像素右边的值减去左边的值不为零，该像素为边界。 计算梯度的函数： ddepth通常取cv2.CV_64F。 dx=0,dy=1计算y轴的边界,dx=1,dy=0计算x轴的边界。满足条件dx&gt;=0&amp;&amp;dy&gt;=0&amp;&amp;dx+dy=1 将图像中的负值转为正： 将一个图像的边缘提取出来：如果没有converScaleAbs，所有的负值都会被截断为0.换句话就是把边界丢掉。 Scharr算子：比sobel算子精准，使用方法基本一样 计算梯度调用函数：cv2.Scharr(src,ddpetch,dx,dy) des=cv2.Scharr(src,ddpetch,dx,dy) 等价于des=cv2.Sobel(src,ddpetch,dx,dy,-1) 为图像扩边：cv2.copyMakeBorder()src:原图图像 top,bottom,left,right分别表示在原图四周扩充边缘的大小 borderType：扩充边缘的类型，就是外插的类型，OpenCV中给出以下几种方式 * BORDER_REPLICATE * BORDER_REFLECT * BORDER_REFLECT_101 * BORDER_WRAP * BORDER_CONSTANT canny函数提取图片边缘cv2.canny(img,threshold1,threshold2) img代表原始图像，threshold1、threshold2为阈值。两个阈值越小，得出图像边缘越详细。反之，边框越边缘。 提取原理步骤：1、高斯模糊 - GaussianBlur 2、灰度转换 - cvtColor 3、计算梯度 – Sobel/Scharr 4、非最大信号抑制5、高低阈值输出二值图像]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(1)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-1%2F</url>
    <content type="text"><![CDATA[​ 因为我是对图像处理的方面比较感兴趣的。所以我也是对python的视觉处理模块opencv进行了学习。这个模块虽然是传统方法，但还是蛮有用的。而且它有上千个API，我总结了以下我所学到的。开始吧。 cv2是opencv的扩展模块。 imread第二个参数类型: cv2.imwrite:保存图片 下文中：绝对路径代表绝对路径也行 改变图片某行某列的像素： 批量改变图片像素： 获取图像属性： 拆分通道： 合并通道： 图片移位： 将两张图片加在一起：1、取模加法 2、饱和运算 注意：两张图片的大小和类型相等 减法(subtract)、乘法(multiply)和除法(divide)和加法(add)类似 图像融合：将两张或两张以上的图片融合到一张图片上 图像缩放（参数必须为整数）： 图像翻转：调用cv2.flip(原始图像，flipcode) 三种情况： filpcode=0：以x轴为对称轴的上下翻转 flipcode&gt;0:以Y轴为对称轴的左右翻转 flipcode&lt;0:X、Y轴各翻转一次 图像颜色反转： 图片打上马赛克： 图片上写字： 图片修补： 图片亮度增强： 图像阈值分割：调用了threshold函数 五种分割方法： 二进制阈值化（cv2.THRESH_BINARY）：选定一个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素阈点值设为最大，若小于该阈值，则将该像素点阈值设为零。 反二进制阈值化（cv.THRESH_BINARY_INV）：选定一个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素点的阈值设为零，若小于该阈值，则将该像素点阈值设为最大。 截断阈值化（cv.THRESH_TRUNC）：选定个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素点的阈值设为该阈值，小于该阈值的像素点的阈值不变。 阈值化为0（THRESH_TOZERO）：先选定一个阈值，像素点的灰度值大于该阈值的不进行任何改变；像素点的灰度值小于该阈值的，其灰度值全部变为0。 反阈值化为0(THRESH_TOZERO_INV)：先选定一个阈值，像素点的灰度值小于该阈值的不进行任何改变；像素点的灰度值大于该阈值的，其灰度值全部变为0。 还有另一种阈值分割函数：自适应阈值函数 cv2.adaptiveThreshold()]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间模块]]></title>
    <url>%2F2019%2F08%2F08%2F%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python里面的时间模块。time 模块提供各种时间相关的功能。在 Python 中，与时间处理有关的模块包括：time，datetime 以及 calendar。 首先先来一波术语解释： 时间戳（timestamp）的方式：通常来说，时间戳表示的是从 1970 年 1 月 1 日 00:00:00 开始按秒计算的偏移量（time.gmtime(0)）此模块中的函数无法处理 1970 纪元年以前的日期和时间或太遥远的未来（处理极限取决于 C 函数库，对于 32 位系统来说，是 2038 年） UTC（Coordinated Universal Time，世界协调时）也叫格林威治天文时间，是世界标准时间。在中国为 UTC+8 DST（Daylight Saving Time）即夏令时的意思。一些实时函数的计算精度可能低于它们建议的值或参数，例如在大部分 Unix 系统，时钟一秒钟“滴答”50~100 次 gmtime()，localtime()和 strptime() 以时间元祖（struct_time）的形式返回。 注一：范围真的是0-61.这是基于历史原因。 time time.altzone：返回格林威治西部的夏令时地区的偏移秒数；如果该地区在格林威治东部会返回负值（如西欧，包括英国）；对夏令时启用地区才能使用。 time.asctime([t])：接受时间元组并返回一个可读的形式为”Tue Dec 11 18:07:14 2015”（2015年12月11日 周二 18时07分14秒）的 24 个字符的字符串。 time.clock()：用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） time.ctime([secs]) ：作用相当于 asctime(localtime(secs))，未给参数相当于 asctime() time.gmtime([secs])：接收时间辍（1970 纪元年后经过的浮点秒数）并返回格林威治天文时间下的时间元组 t（注：t.tm_isdst 始终为 0） time.daylight：如果夏令时被定义，则该值为非零。 time.localtime([secs])：接收时间辍（1970 纪元年后经过的浮点秒数）并返回当地时间下的时间元组 t（t.tm_isdst 可取 0 或 1，取决于当地当时是不是夏令时） time.mktime(t)：接受时间元组并返回时间辍（1970纪元后经过的浮点秒数） time.perf_counter()：返回计时器的精准时间（系统的运行时间），包含整个系统的睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 time.process_time() ：返回当前进程执行 CPU 的时间总和，不包含睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 time.sleep(secs)：推迟调用线程的运行，secs 的单位是秒。 time.clock()：用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。 Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） time.time()：返回当前时间的时间戳（1970 纪元年后经过的浮点秒数） time.timezone：time.timezone 属性是当地时区（未启动夏令时）距离格林威治的偏移秒数（美洲 &gt;0；大部分欧洲，亚洲，非洲 &lt;= 0） time.tzname：time.tzname 属性是包含两个字符串的元组：第一是当地非夏令时区的名称，第二个是当地的 DST 时区的名称。 time.strftime(format[, t]) ：把一个代表时间的元组或者 struct_time（如由 time.localtime() 和 time.gmtime() 返回）转化为格式化的时间字符串。如果 t 未指定，将传入 time.localtime()。如果元组中任何一个元素越界，将会抛出 ValueError 异常。 time.strptime(string[, format])：把一个格式化时间字符串转化为 struct_time。实际上它和 strftime() 是逆操作 format 格式如下： 注1：“%p”只有与“%I”配合使用才有效果。 注2：范围真的是*0 ~ 61（你没有看错哦）；60代表闰秒，61是基于历史原因保留。 注3：当使用 strptime() 函数时，只有当在这年中的周数和天数被确定的时候%U 和 %W 才会被计算。 datetimedatetime.today()：返回一个表示当前本地时间的 datetime 对象，等同于 datetime.fromtimestamp(time.time()) datetime.now(tz=None)：返回一个表示当前本地时间的 datetime 对象；如果提供了参数 tz，则获取 tz 参数所指时区的本地时间 datetime.utcnow():返回一个当前 UTC 时间的 datetime 对象 datetime.fromtimestamp(timestamp, tz=None): 根据时间戮创建一个 datetime 对象，参数 tz 指定时区信息 datetime.utcfromtimestamp(timestamp): 根据时间戮创建一个 UTC 时间的 datetime 对象 datetime.fromordinal(ordinal): 返回对应 Gregorian 日历时间对应的 datetime 对象 datetime.combine(date, time): 根据参数 date 和 time，创建一个 datetime 对象 datetime.strptime(date_string, format): 将格式化字符串转换为 datetime 对象 datetime.timedelta对象代表两个时间之间的时间差，两个date或datetime对象相减就可以返回一个timedelta对象。 datetime.timedelta([days[, seconds[, microseconds[, milliseconds[, minutes[, hours[, weeks]]]]]]]) 往前算： 往后算： datetime.date()：返回一个 date 对象datetime.time() - 返回一个 time 对象（tzinfo 属性为 None） datetime.timetz()： 返回一个 time() 对象（带有 tzinfo 属性） datetime.replace([year[, month[, day[, hour[, minute[, second[, microsecond[, tzinfo]]]]]]]])： 生成一个新的日期对象，用参数指定日期和时间代替原有对象中的属性 datetime.astimezone(tz=None)： 传入一个新的 tzinfo 属性，返回根据新时区调整好的 datetime 对象 datetime.utcoffset()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.utcoffset(self) datetime.dst()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.dst(self) datetime.tzname()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.tzname(self) datetime.timetuple()：返回日期对应的 time.struct_time 对象（类似于 time模块的 time.localtime()） datetime.utctimetuple()：返回 UTC 日期对应的 time.struct_time 对象 datetime.toordinal()：返回日期对应的 Gregorian Calendar 日期（类似于 self.date().toordinal()） datetime.timestamp()：返回当前时间的时间戳（类似于 time 模块的 time.time()） datetime.weekday()：返回 0 ~ 6 表示星期几（星期一是 0，依此类推） datetime.isoweekday()： 返回 1 ~ 7 表示星期几（星期一是1， 依此类推） datetime.isocalendar() ：返回一个三元组格式 (year, month, day) datetime.isoformat(sep=’T’)：返回一个 ISO 8601 格式的日期字符串，如 “YYYY-MM-DD” 的字符串 datetime.__str__()：对于 date 对象 d 来说，str(d) 相当于 d.isoformat() datetime.ctime()：返回一个表示日期的字符串，相当于 time模块的 time.ctime(time.mktime(d.timetuple())) datetime.strftime(format):返回自定义格式化字符串表示日期。 datetime.__format__(format)跟 datetime.strftime(format) 一样，这使得调用 str.format() 时可以指定 data 对象的字符串 calendarcalendar.calendar(year,w,l,c)(year为年份，w是每日宽度，c为间隔距离，l为每星期行数) 日历 calendar.isleap(year) ：如果是闰年就返回True，否则返回False calendar.leapdays(y1,y2)：返回y1与y2两年间的闰年总数 calendar.month(year,month,w,l) ：year代表年份，month代表月日历，每行宽度间隔为w，l是每星期的行数 calendar.prcal ：相当于print(calendar.calendar(year,w,l,c)) calendar.prmonth(year,month,w,l)：相当于print(calendar.calendar(year,w,l,c)) calendar.weekday(year,month,day)： 返回指定日期的日期码，0-6（星期），1-12（月份） calendar.firstweekday()返回当前起始日期的设置 calendar.setfirstweekday()：设置每周的起始日期码 calendar.timegm(时间元祖) 和time.gmtime相反 calendar.monthrange(year,month)：返回两个整数。第一个是该月第一天是星期几的日期码，第二个是该月的日期码。日从0（星期一）到6（星期日）;月从1到12。 timeit测试一个函数的执行时间：timeit.timeit timeit.repeat:返回一个包含了每次实验的执行时间的列表 三个时间模块的函数特别多，我也没有一一去试。用到的时候才会深究，各位看官根据自己的情况来试着使用起来吧。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python时间管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（3）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[textwrap用来重新格式化文本的输出 fill()调整换行符,每行显示给定宽度 dedent()去除缩进 indent()给定前缀 首行缩进： 多余的省略号shorter() wrap():wrap(s,width) 以单词为单位(包括字符)最大长度不超过width个字符 itertoolsPython的内建模块itertools提供了非常有用的用于操作迭代对象的函数 count()count()会创建一个无限的迭代器，所以下述代码会打印出自然数序列，根本停不下来，只能按Ctrl+C退出。 cycle()cycle()会把传入的一个序列无限重复下去： repeat()repeat()负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数： chainchain()可以把一组迭代对象串联起来，形成一个更大的迭代器： groupby()groupby()把迭代器中相邻的重复元素挑出来放在一起： 实际上挑选规则是通过函数完成的，只要作用于函数的两个元素返回的值相等，这两个元素就被认为是在一组的，而函数返回值作为组的key。如果我们要忽略大小写分组，就可以让元素’A’和’a’都返回相同的key： permutations()输出输入序列的全排列,考虑顺序 combinations()同上，不过不考虑顺序 product()输出输入序列的笛卡儿积 compress可以对一个序列的筛选结果施加到另一个相关的序列上 dropwhile()筛选满足条件的元素 islice() zip_longeszip可产生元祖。当其中摸个输入序列中没有元素可以继续迭代时，迭代过程结束。所以整个迭代的长度和最短的输入序列相同。 如果不想这样就用zip_longest：]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（2）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[collection：collections是Python内建的一个集合模块，提供了许多有用的集合类。 nametuplenamedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。 利用namedtuple定义和使用具名元祖：第一个参数为类名，第二个参数为类的各个字段的名字 具名元祖有些专用的属性：类属性_fields,类方法_make(),实例方法_asdict() 如果需要修改任何属性，可以通过使用nametupled实例的_replace方法来实现。该方法会创建按一个新的命名元祖，并对相应的值进行替换。 dequedeque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈。deque(maxlen=N)创建一个固定长度的队列，当有新记录加入而队列已满时会自动移除最老的那条记录。 defaultdictdefaultdict:使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict。defaultdict的一个特点就是会自动初始化第一个值。 用defaultdict的效率比不用高: OrderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用OrderedDict。 OrderDict的大小是普通字典的2倍多，这是由于它额外创建的链表所导致。 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key Counter一个简单的计数器 计数器的更新包括增加和减少两种，增加使用update，减少用subtract。 most_common(x):根据x返回频率前x的项。 itemgetteritemgetter可以通过公共键对字典列表排序。 ChainMapChainMap可接受多个映射然后再逻辑上是它们表现为一个单独的映射结构。如果有重复的键，那么会采用第一个映射中所对应的值。修改映射的操作总是会作用在列出的第一个映射结构上。 randomrandom.random() 产生0-1的随机浮点数 random.uniform(a, b) 产生指定范围内的随机浮点数 random.randint(a, b) 产生指定范围内的随机整数 random.randrange([start], stop[, step]) 从一个指定步长的集合中产生随机数 random.choice(sequence) 从序列中产生一个随机数 random.shuffle(x[, random]) 将一个列表中的元素打乱 random.sample(sequence, k) 从序列中随机获取指定长度的片断 functoolsreduce(function, sequence, value)对sequence中的item顺序迭代调用function，如果有value，还可以作为初始值调用。function接收的参数个数只能为2，先把sequence中第一个值和第二个值当参数传给function，再把function的返回值和第三个值当参数传给function，然后只返回一个结果。 partial基于一个函数创建一个新的可调用对象，把原函数的某些参数固定。 偏函数：只需要传一次值，后面想传就传]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（1）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 这几年，python大火。其中的一个原因是python的库特别多，而且封装非常好。接下来我来总结以下我用过的一些库，虽然都不是很大的库，但还是有用的。 jsonjson主要执行序列化和反序列化的功能，通过Python的json模块，可以将字符串形式的json数据转化为字典，也可以将Python中的字典数据转化为字符串形式的json数据。 通过json字符串转为字典 json.loads 字典转换为json：json.dumps json.loads()、dumps解码python json格式 json.load、dump加载json格式文件 pickle使用方法与json一样 区别： json是可以在不同语言之间交换数据的，而pickle只在python之间使用。 json只能序列化最基本的数据类型，而pickle可以序列化所有的数据类型，包括类，函数都可以序列化。 hashlib该模块提供了常见的摘要算法。如MD5，SHA1……摘要算法又称哈希算法、散列算法，通过一个函数，把任意长度的数据转换为一个长度固定的数据串。 摘要算法是一个单向函数，通过摘要函数f()对任意长度的数据data计算出固定长度的摘要digest，目的是发现原始数据是否被改动过。计算f(data)很容易，但通过digest反推data却非常困难，对原始数据做一个字节的修改，都会导致计算出来的摘要不同。 stringstring.digits:包含数字0-9的字符串 string.letters:包含所有字母（大写或小写）的字符串 string.lowercase:包含所有小写字母的字符串 string.printable:包含所有可打印字符的字符串 string.punctuation:包含所有标点的字符串 string.uppercase:包含所有大写字母的字符串 string.ascii_letters和string.digits方法，其中ascii_letters是生成所有字母，从a-z和A-Z,digits是生成所有数字0-9. 词云 decimal如果期望获得更高的精度（并且愿意牺牲掉一些性能），可以使用decimal模块。 fractions模块可以用来处理涉及分数的数学计算问题 copy完成深拷贝和浅拷贝 is和==的区别：==是看值，is看是否指向同一个 浅拷贝：拷贝内容的地址 深拷贝：开发另一片空间存放要拷贝的内容 copy会判断数据类型是否为可变类型，如元祖为不可变类型，则只会完成浅拷贝。 fileinput可以快速对一个或多个文件进行循环遍历 fileinput.input([files[, inplace[, backup[, mode[, openhook]]]]]])功能:生成FileInput模块类的实例。能够返回用于for循环遍历的对象。注意:文件名可以提供多个 inplace：是否返回输出结果到源文件中，默认为零不返回。设置为1时返回。 backup：备份文件的扩展名 mode：读写模式。只能时读、写、读写、二进制四种模式。默认是读 openhook：必须是一个函数，有两个参数，文件名和模式。返回相应的打开文件对象 fileinput.filename()：返回当前正在读取的文件的名称。在读取第一行之前，返回None。 fileinput.fileno()：返回当前文件的整数“文件描述符”。如果没有打开文件（在第一行之前和文件之间），则返回-1。 fileinput.lineno()：返回刚读过的行的累计行号。在读取第一行之前，返回0。读取完最后一个文件的最后一行后，返回该行的行号。 fileinput.filelineno()：返回当前文件中的行号。在读取第一行之前，返回0。读取完最后一个文件的最后一行后，返回该文件中该行的行号。 fileinput.isfirstline()：如果刚刚读取的行是其文件的第一行，则返回true，否则返回false。 fileinput.isstdin()：如果读取了最后一行sys.stdin，则返回true，否则返回false。 fileinput.nextfile()：关闭当前文件，以便下一次迭代将读取下一个文件的第一行（如果有的话）; 未从文件中读取的行将不计入累计行数。直到读取下一个文件的第一行之后才会更改文件名。在读取第一行之前，此功能无效; 它不能用于跳过第一个文件。读取完最后一个文件的最后一行后，此功能无效。 fileinput.close()关闭序列 subprocess：subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用subprocess.PIPE将多个子进程的输入和输出连接在一起，构成管道(pipe) subprocess.call():父进程等待子进程完成，返回退出信息(returncode，相当于Linux exit code) subprocess.check_call():父进程等待子进程完成，返回0，检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查 subprocess.check_output():父进程等待子进程完成，返回子进程向标准输出的输出结果，检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try…except…来检查。 subprocess.Popen()，以下为参数： args：shell命令，可以是字符串，或者序列类型，如list,tuple。 bufsize：缓冲区大小，可不用关心 stdin,stdout,stderr：分别表示程序的标准输入，标准输出及标准错误 shell：与上面方法中用法相同 cwd：用于设置子进程的当前目录 env：用于指定子进程的环境变量。如果env=None，则默认从父进程继承环境变量 universal_newlines：不同系统的的换行符不同，当该参数设定为true时，则表示使用\n作为换行符 常用方法： poll() ： 检查子进程状态 kill() ： 终止子进程 send_signal() :向子进程发送信号 terminate() ： 终止子进程 communicate:从PIPE中读取PIPE的文本，该方法会阻塞父进程，直到子进程完成 常用属性：pid：子进程的pid，returncode：子进程的退出码。]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[元类]]></title>
    <url>%2F2019%2F08%2F07%2F%E5%85%83%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[​ 今天来讲一下python里面较难的一个东西。我学到的python内容里面有两个东西是比较难的，一个是描述符，另一个就是今天讲的元类。 通过python知识散记我们知道global的功能是将局部变量转为全局变量，接下来要说的是globals()。这个globals()函数会以字典类型返回当前位置的全部全局变量。 __builtins__模块是默认加载的，在ipython中打开。 python通过类创建对象，通过元类创建类。（类也是对象） 动态创建类： 函数choose_class根据name的不同而动态的创建不同的类，但这种做法效率相当low。因为如果类多了就会要很多if-else来判断创建哪个类。 通过type动态创建类。命名规则： type(类名，由父类名称构成的元祖（针对继承的情况，可以为空），包含属性的字典（名称和值）) 继承： 添加实例方法： 添加类方法： 添加静态方法： 元类应用： metaclass用来指定按照upper_attr来创建，如果不指定，则默认使用type创建。Foo传到class_name，父类(object)传到class_name，新的字典传到class_attr。 用类完成以上代码：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的元类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas用法总结]]></title>
    <url>%2F2019%2F08%2F07%2Fpandas%2F</url>
    <content type="text"><![CDATA[​ pandas也是一个非常强大的库，所以我也只是总结了我用到的方法。 pandas常用的数据类型：1、Series 一维 带标签的数组（标签就是索引）2、DataFrame 二维 Series的容器 Series：通过列表创建Series： 索引可以指定，默认从0开始： 通过字典创建Series：可以通过astype修改类型 取值： 可以将条件和value、index配合使用： pandas读取外部数据： read_csv:读取CSV文件 read_excel:读取excel文件 其他文件类似 DataFrame通过列表创建： 通过数组创建： 通过字典创建： DataFrame的基础属性： shape ：行数 列数 dtypes：列数据类型 ndim：数据维度 index：行索引 columns：列索引 values：对象值 DataFrame的方法： head(n):显示头n行。默认是前5行 tail(n)：显示尾n行。 info()：行数，列数，列索引，列非空值个数，列类型，内存占用 describe()：计数 均值 标准差 最大值 四分位数 最小值 DataFrame排序：sort_values()。通过设置by来确定排序的key。设置ascending确定升序or降序。 DataFrame的取值： 方括号写数字，表示取行。对行进行操作。根据实际情况写 对列进行操作： 配合使用： loc：DataFrame通过标签索引获取行数据 根据多个索引取多个对应的值： iloc：DataFrame通过位置获取列数据。与ioc类似，只是将索引换成数字。 数组合并： 1、join 按行索引合并 2、merge按列索引进行合并 on指定按哪一列合并 how：合并方式 inner(交集，默认) outer(并集) left(左边为准，NaN补全) right(右边为准，NaN补全) 如果列索引不同。可以left_on和right_on指定左边、右边DataFrame的合并列。 另一种写法： 分组:groupby(by) by:通过什么分组，可以设置多个条件分组 聚合：count 计算数量 sum 求和 mean 求平均值 median 求中位数 std、var 求标准差和方差 min、max 求最大和最小值 DataFrame的索引和复合索引：简单的索引操作： 获取index：df.index 指定index：df.index=[“”,””] 同理可得指定columns：df.columns=[“ “,” “] 重新设置index：df.reindex() 指定某一列成为index：df.set_index()。drop决定是否保留设定的列 可以设定多个列成为index： 返回index的唯一值：df.set_index().index.unique() 时间序列：date_range(start,end,period,freq) 生成一段时间范围。start和end表示范围，periods表示个数，freq表示频率(年、月、天) 频率类型： 时间段：PeriodIndex 重采样resample：指的是将时间序列从一个频率转化为另一个频率进行处理的过程。将高频率数据转化为低频率数据为降采样。低频率数据转化为高频率为升采样。 判断数据是否为NaN：pd.isnull() pd.notfull() 在DataFrame中对缺失数据（NaN）的处理： 方式1：删除NaN所在的行列dropna(axis,how,inplace):how=”any”时一行(列)里有一个为nan就删。how=”all”时，一行全部为nan时才删。inplace为True，原地修改。False为False，不修改。 方式2：填充数据，fillna() 处理为0的数据：t[t==0]=np.nan 计算平均值时：nan不参与计算，0参与]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（3）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 今天的知识散记讲三个器，哪三个器呢？装饰器、迭代器、生成器。还有列表生成式、字典生成式。 装饰器：python装饰器就是用于拓展原来函数功能的一种函数，这个函数的特殊之处在于它的返回值也是一个函数，使用python装饰器的好处就是在不用更改原函数的代码前提下给函数增加新的功能。 使用闭包完成的装饰器原理： 当执行test(test)时，func已经指向test()函数。所以func()相当于test()。这就是装饰器的原理。但是实际不是这样写的，请看下图： 使用装饰器对有参数的函数装饰： 对不定长参数的装饰： 装饰器对有返回值的函数装饰： 带参数的装饰器 用类做装饰器： 装饰器的一个关键特性：函数装饰器在导入模块时立即执行，而被装饰的函数只在明确调用时运行。 装饰器完了，自己慢慢悟吧。 迭代器迭代器只能前进不能后退。使用迭代器不要求事先准备好整个迭代过程中的所有元素。迭代器仅仅在迭代到某个元素时才计算该元素，而在这之前或之后元素可以不存在或者被销毁。因此迭代器适合遍历一些数量巨大甚至无限的序列。 Python中迭代器的本质上每次调用__next__()方法都返回下一个元素或抛出StopIteration的容器对象 由于Python中没有“迭代器”这个类，因此具有以下两个特性的类都可以称为“迭代器”类： 1、有__next__()方法，返回容器的下一个元素或抛出StopIteration异常 2、有__iter__()方法，返回迭代器本身 all(iterable)：如果迭代器里面的所有元素都为True时,返回True;否则返回False any(iterable）：如果迭代器里面的所有元素为False,返回False;否则返回True. 生成器一个个的生成数据，但占用内存更少，生成器是特殊的迭代器。 当yield存在函数时，函数就变成一个生成器。yield不像return那样返回值，而是每次产生多个值。每次使用yield产生一个值，函数就会被冻结。 用aim.__next__()得出结果： aim.send()得出结果： 列表生成式和字典推导式：]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（2）]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 接着上次的知识散记，我们接着看。 闭包在函数内部定义一个函数，并且这个函数用到了外面的变量。将这个函数以及用到的一些变量称之为闭包。 当某个函数被当成对象返回时，夹带了外部 变量，就形成了一个闭包。 global：一般多用在函数内，声明变量的作用域为全局作用域。箭头变化的时候加，箭头不变的时候可以不加。（箭头类似指针） nonlocal：nonlocal关键字用来在函数或其他作用域中使用外层(非全局)变量 lambda匿名函数： 不要看匿名函数蛮简单的，其实还是有点注意事项的。对默认参数的赋值只会在函数定义的时候绑定一次。x是一个自由变量。在运行的时候绑定，而不是在定义的时候绑定。执行时，x的值是多少就是多少。如果希望匿名函数可以在定义的时候绑定，并保持值不变，则采用下面方法。 assert:断言用来直接让程序崩溃，在程序中置入检查点 条件后可以添加字符串，用来解释断言 format: 对*和**的解释：星号(*)和(**)作为形参的时候是起到“打包”的作用，相反，作为实参的时候是起到“解包”的作用。 星号(*)或(**)作为形参，表示调用可变参数函数：通过在形参前加一个星号(*)或两个星号(**)来指定函数可以接收任意数量的实参。 从两个示例的输出可以看出：当参数形如 *args 时，传递给函数的任意个实参会按位置打包成一个元 组（tuple）；当参数形如 **args 时，传递给函数的任意个 key = value 实参会被包装进一个字典（dict）。 星号(**)和(*)作为实参时，表示通过解包参数调用函数： 常用内置函数补充：复数可以通过complex（real，imag）来指定。conjugete提取共轭复数 hasattr：hasattr() 函数用于判断对象是否包含对应的属性。如果对象有该属性返回 True，否则返回 False。 上下文管理器（context manager）：任何实现了__enter__和__exit__方法的对象都可称为上下文管理器 __enter__():方法返回资源对象，这里就是你将要打开的那个文件对象，__exit__()处理一些清除工作。因为File类实现上下文管理器，现在就可以使用with语句了。 实现上下文管理器的其他方法：使用contextmanager装饰器 python的三种修饰符：staticmethod、classmethod 和 property，作用分别是把类中定义的实例方法变成静态方法、类方法和类属性。staticmethod、classmethod具体看python的类和对象。 注意： 函数先定义，再修饰它；反之会编译器不认识； 修饰符“@”后面必须是之前定义的某一个函数； 每个函数只能有一个修饰符，大于等于两个则不可以。 property用法:它的作用把方法当作属性来访问（注意getnum和setter的顺序，一定getnum在第一个）]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（1）]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 今天来讲一下python的一些散装知识，还是蛮多的。所以分了几个部分。今天的部分是最简单的，废话少说。开始吧 导入模块方法：1、最常见的方式，直接将要导入的模块名称写在后面导入。import xxxx 2、from .. import .. 与import类似，只是更明确的要导入的方法或变量。 3、from modname import *，导入所有的类和公有方法。 if __name__==”__main__“:让你写的脚本模块既可以导入到别的模块中用，另外该模块自己也可执行。 常用的一些内置函数callable(object)：检查对象object是否可调用。如果返回True，object仍然可能调用失败；但如果返回False，调用对象ojbect绝对不会成功。 divmod(a,b)：以元祖的方式放回a//b以及a%b。 ord(str)：把对应的字符转成整数. chr(integer)：把整数转化成对应的字母. bool(x)：把一个值转化为布尔值,如果该值为假或者省略返回False,否则返回True abs(x) ：返回一个数的绝对值.该参数可以是整数或浮点数.如果参数是一个复数,则返回其大小 round(number[, ndigits])：返回浮点数number保留ndigits位小数后四舍五入的值。 dir([object]): 没有参数,返回当前局部范围的名单列表。有参数，试图返回该对象的有效的属性列表 issubclass(class, classinfo):返回True如果参数class是classinfo的一个子类，否则返回False。 isinstance(object, classinfo):返回True如果参数object是classinfo的一个实例，否则返回False(适用于继承)。 zip(*iterables):生成一个迭代器，迭代器聚合了从每个可迭代数集里的元素。它的内容只能被消费一次 map：第一个参数 function 以参数序列中的每一个元素调用 function函数，返回包含每次 function 函数返回值的新列表。 filter：filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。应该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 enumerate是一个非常有用的函数，直接看效果。 eval(str [,globals [,locals ]])：用来计算存储在字符串中的有效python表达式。 exec(object[, globals[, locals]])， 用来执行存储在字符串或文件中的python语句 格式化输出格式： python语句中一些基本规则和特殊字符： python调试：python调试两种方法都有用到pdb模块 第一种：在代码的目录下，打开cmd，输入python -m 文件名 h：帮助命令 第二种：可以在交互界面进行调试]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一些协程。协程是python中独有的，在其他语言中是没有这个概念的。协程是利用线程在等待的时候做事情。 使用yield完成协程： 使用greenlet完成协程： 使用gevent完成协程： 要想用时间模块延迟，则必须打补丁： 我还没看完，后面补。未完待续……]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的进程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多进程：multiprocessing模块 os.getpid()获取当前进程的id os.getppid()获取父进程的id 大量启动子进程，可以用进程池pool批量创建子进程。可以通过processes改变创建的进程数目。 apply_async(func[, args=()[, kwds={}[, callback=None]]])该函数用于传递不定参数，非阻塞且支持结果返回进行回调。 将函数添加到进程池： map(func, iterable[, chunksize=None])：Pool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到返回结果。 注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。 close()：关闭进程池（pool），使其不在接受新的任务。 terminate()：结束工作进程，不在处理未处理的任务。 join([timeout])：主进程阻塞等待子进程的退出，join方法必须在close或terminate之后使用。timeout表示等待最多时间。若超出，则会直接执行下列代码 Value、Array是通过共享内存的方式共享数据 Value：将一个值存放在内存中， Array：将多个数据存放在内存中，但要求数据类型一致 Value: Array:两种情况 若为数字，表示开辟的共享内存中的空间大小，（Value表示为该空间绑定一个数值） 若为数组，表示在共享内存中存入数组 说明：三个0表示开辟的共享内存容量为3，当再超过3时就会报错。 Manager（Value、Array、dict、list、Lock、Semaphore等）是通过共享进程的方式共享数据。 进程间通信：Queue ，Pipe 使用方法和队列差不多 q.get_nowait()：和get()差不多，不用等。 Pipe:Pipe可以是单向(half-duplex)，也可以是双向(duplex)。我们通过mutiprocessing.Pipe(duplex=False)创建单向管道 (默认为双向)。一个进程从PIPE一端输入对象，然后被PIPE另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。 这里的Pipe是双向的。 Pipe对象建立的时候，返回一个含有两个元素的表，每个元素代表Pipe的一端(Connection对象)。我们对Pipe的某一端调用send()方法来传送对象，在另一端使用recv()来接收。 生产者与消费者模式：在两者中找一个缓冲的东西（队列，缓冲池），解决数据生产方和数据处理方数据不分配的问题。 耦合：谁和谁的关系越强，耦合性就越强。耦合性越强，程序维护越难。 解耦的好处：哪块不合适，就改那块。 接上面：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的线程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[重要的话写在前面：进程间不共享全局变量，线程间共享全局变量。 同步：按预定的先后次序进行运行 异步：不确定的次序 对于操作系统来说，一个任务就是一个进程。进程内的子任务成为线程 ，一个进程至少有一个线程 多任务执行的方式： 多进程 多线程 多进程+多线程 多线程：Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 传入参数为元祖。也就是即使只有一个参数，也要写逗号。 join():将线程加入到当前线程，并等待其终止 判断线程是否在运行： 守护线程：将daemon属性设为True，则该线程无法被连接 daemon属性可以保证主线程结束时可以同时结束子线程或者使主线程等待子线程结束后在结束。故称为守护线程。daemon默认为False，如需修改，必须在调用start()方法启动线程之前进行设置。不适用与idle的交互模式或脚本模式 当daemon属性为False时，主线程会检测子线程是否结束，如果子线程还在运行，则主线程会等待他完成后在退出。当daemon属性为True时，子线程没执行的不再执行，主线程直接退出。 通过轮询终止线程： threading的常用方法： ​ active_count() 当前活动的 Thread 对象个数 ​ current_thread() 返回当前 Thread 对象 ​ current_thread().name返回当前的Thread对象的名字 ​ get_ident() 返回当前线程 ​ enumerater() 返回当前活动 Thread 对象列表 ​ main_thread() 返回主 Thread 对象 ​ settrace(func) 为所有线程设置一个 trace 函数 ​ setprofile(func) 为所有线程设置一个 profile 函数 ​ stack_size([size]) 返回新创建线程栈大小；或为后续创建的线程设定栈大小为 size ​ TIMEOUT_MAX Lock.acquire(), RLock.acquire(), Condition.wait() 允许的最大值 threading 可用对象列表： ​ Thread 表示执行线程的对象 ​ Lock 锁原语对象 ​ RLock 可重入锁对象，使单一进程再次获得已持有的锁(递归锁) Condition： 条件变量对象，使得一个线程等待另一个线程满足特定条件，比如改变状态或某个值 ​ wait(timeout): 线程挂起，直到收到一个notify通知或者超时（可选的，浮点数，单位是秒s）才会被唤醒继续运行。wait()必须在已获得Lock前提下才能调用，否则会触发RuntimeError。 ​ condition = threading.Condition(lock=None) # 创建Condition对象 参数可以不传 ​ condition.acquire() # 加锁 ​ condition.release() # 解锁 ​ condition.wait(timeout=None) # 阻塞，直到有调用notify(),或者notify_all()时再触发 ​ condition.wait_for(predicate, timeout=None) # 阻塞，等待predicate条件为真时执行 ​ condition.notify(n=1) # 通知n个wait()的线程执行, n默认为1 ​ condition.notify_all() # 通知所有wait着的线程执行 ​ with condition: # 支持with语法，不必每次手动调用acquire()/release() Semaphore 为线程间共享的有限资源提供一个”计数器”，如果没有可用资源会被阻塞 Events：它是由线程设置的信号标志，如果信号标志为真，则其他线程等待直到信号接触。 Event对象实现了简单的线程通信机制，它提供了设置信号，清除信号，等待等用于实现线程间的通信。 event = threading.Event() 创建一个event 1、设置信号 event.set() 使用Event的set（）方法可以设置Event对象内部的信号标志为真。Event对象提供了isSet（）方法来判断其内部信号标志的状态。 当使用event对象的set（）方法后，isSet（）方法返回真 2、清除信号 event.clear() 使用Event对象的clear（）方法可以清除Event对象内部的信号标志，即将其设为假，当使用Event的clear方法后，isSet()方法返回假 3 、等待 event.wait() Event对象wait的方法只有在内部信号为真的时候才会很快的执行并完成返回。当Event对象的内部信号标志为假时，则wait方法一直等待到其为真时才返回。也就是说必须set新号标志为真 主线程在等事件设置后才继续执行 event使用示范： Barrier :创建一个”阻碍”，必须达到指定数量的线程后才可以继续 每个线程中都调用了wait()方法，在所有（此处设置为3）线程调用wait方法之前是阻塞的。也就是说，只有等到3个线程都执行到了wait方法这句时，所有线程才继续执行。 计算处于alive的Thread对象数量： 多线程避免全局变量的改变：上锁。上锁后执行的代码越少越好。 互斥锁：Lock是比较低级的同步原语，当被锁定后不属于特定的线程。一个锁有两个状态:Locked和unLocked.刚创建的的Locked处于unlocked状态。如果锁处于unlocked状态，acquire()方法将其修改为Locked并立即返回。如果锁处于locked状态，则阻塞当前线程并等待其他线程释放锁，然后将其修改为locked并立即返回。release()方法用来将锁的状态从locked修改为unlocked并立即返回。如果锁的状态本来就是unlocked，则会抛出异常 可重入锁Rlock对象也是一种常用的线程同步原语，可被同一个线程acquire()多次。当locked状态时，某现场拥有该锁，当处于unlocked状态时，该锁不属于任何线程。Rlock对象的acquire()/release()调用对可以嵌套，仅当最后一个或者最外层的release执行结束后，锁才会被设置为unlocked状态 死锁：双方都在等待对方的条件满足 避免死锁的方法：1、添加超时事件 2、 尽量避免（银行家算法） Threadlocal：保存当前线程的专有状态，这个状态对其他线程不可见。 全局变量local就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local看成全局变量，但每个属性如local.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 线程池：threadpool模块或concerrent.futures模块 threadpool模块比较老旧，不是主流。 threadpool.ThreadPool(poolsize)：定义一个线程池，创建了poolsize个线程。 threadpool.makeRequest(开启多线程的函数，函数相关参数，[回调函数]) putRequest:将所有要运行多线程的请求扔进线程池。 concerrent.futures：这个模块轻松绕开GIL ThreadPoolExecutor构造实例的时候，传入max_workers参数来设置线程池中最多能同时运行的线程数目。 使用submit函数来提交线程需要执行的任务（函数名和参数）到线程池中，并返回该任务的句柄（类似于文件、画图），注意submit()不是阻塞的，而是立即返回。 通过submit函数返回的任务句柄，能够使用done()方法判断该任务是否结束。上面的例子可以看出，由于任务有2s的延时，在task1提交后立刻判断，task1还未完成，而在延时4s之后判断，task1就完成了。 使用cancel()方法可以取消提交的任务，如果任务已经在线程池中运行了，就取消不了。这个例子中，线程池的大小设置为2，任务已经在运行了，所以取消失败。如果改变线程池的大小为1，那么先提交的是task1，task2还在排队等候，这是时候就可以成功取消。 使用result()方法可以获取任务的返回值。查看内部代码，发现这个方法是阻塞的 as_completed:一次取出所有任务的结果。as_completed()方法是一个生成器，在没有任务完成的时候，会阻塞，在有某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞住，循环到所有的任务结束。从结果也可以看出，先完成的任务会先通知主线程。 map的作用和submit一样，但略有不同。输出顺序和参数列表的顺序相同 wait方法接受三个参数，等待的任务序列，超时时间，以及等待条件。等待条件return_when默认为ALL_COMPLTED，表明要等待所有的任务都结束。还可以设为FIRST_COMPLETED，表示第一个任务完成就结束等待。FITST_EXCEPTION(注意要导入) 通过类创建线程： t.start后一定调用run函数，不定义run函数该线程不执行。对其他函数的调用只能在run函数里执行。多线程可以共享全局变量，但当数据量大时，数据会出错（产生资源竞争）。 线程是真的多，看到最后。迷迷糊糊，有错一定要提醒我。而且很多我还没有用过。后面用到的话，会补充上去的。接下来说最后一个：GIL。何为GIL？ GIL：全局解释器锁 单CPU的系统中运行多个进程那样，内存中可以存放多个程序，但任意时刻，只有一个程序在CPU中运行。同样地，虽然Python解释器中可以“运行”多个线程，但在任意时刻，只有一个线程在解释器中运行。 GIL保证了多线程时只有一个线程被调用。 所以多进程效率比多线程高，但是进程间通信比线程难。 解决方法：用C语言写关键部分。模块（ctypes）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy(1)]]></title>
    <url>%2F2019%2F08%2F06%2Fnumpy(1)%2F</url>
    <content type="text"><![CDATA[最近这几年，机器学习和深度学习大火。而这其中的数据计算是非常多，而这得益于python的numpy模块。多提一句：numpy是没有GIL（多线程解释器锁）的。所以，numpy中的计算是非常快的。那什么是GIL呢？请看python的多线程。话不多说，让我们开启今天的数学之旅。PS：numpy很多API，我还没有学完。我只是总结了一部分，后面会补充的。 通过array生成矩阵： 可以通过dtype设置矩阵数据的类型。astype可以修改数据类型。 通过asarray生成矩阵用法和array一样。array和asarray都可将结构数据转换为ndarray类型。但是主要区别就是当数据源是ndarray时，array仍会copy出一个副本，占用新的内存，但asarray不会。 其他生成矩阵的方式 numpy生成随机数： seed的使用方法： 使用RandomState获得随机数生成器 >&gt;&gt;rdm=np.random.RandomState(2) # 2是随机数种子>&gt;&gt;a=np.random.randint(0,20,(3,4))>&gt;&gt;aarray([[ 8, 11, 1, 5], [ 3, 8, 0, 18], [ 5, 15, 8, 14]]) reshape&amp;resize可以通过reshape修改列表的行数和列数，resize改变数组的尺寸大小。根据reshape传入的参数判断转为哪种数组。 numpy运算加、减、乘、除类似。如果两个矩阵形状相同，两个矩阵对应的元素做操作。若两个矩阵形状不相同，其中一个矩阵的维度与另一个矩阵的维度相同，可以在该维度上做操作。 通过axis求每行（列）的元素和或最大、最小值。0代表列，1代表行。 还可以获得最大、最小值： 其他一些计算:累加、累差、中位数、平均值 对clip函数的解释：小于5的元素都设为5，大于9的元素都设为9 flatten对数组展开为一维数组 numpy的合并： numpy的分割： 转置矩阵： numpy.linspace(start,stop,num,endpoint,retstep,dtype)该方法的作用是子啊start和stop之间产生num个线性向量。每个向量之间的距离是相同的。创建等差数列 start：起始点 stop：终止点 num : int, optional 默认50，生成start和stop之间50个等差间隔的元素 endpoint : bool, optional。If True, stop is the last sample. Otherwise, it is not included. Default is True。生成等差间隔的元素，但是不包含stop，即间隔为 （stop - start）/num retstep : bool, optional。If True, return (samples, step), where step is the spacing between samples.返回一个（array，num）元组，array是结果数组，num是间隔大小 dtype : dtype, optional。The type of the output array. If dtype is not given, infer the data type from the other input arguments。输出数组的类型。如果没有给出dtype，则从其他输入参数推断数据类型。 numpy.newaxi：插入新的维度 >&gt;&gt; import numpy as np >&gt;&gt;b=np.array([1,2,3,4,5,6])>&gt;&gt;b[np.newaxis]>&gt;&gt;array([[1, 2, 3, 4, 5, 6]]) numpy.meshgrid：生成网格点坐标矩阵 由上图，得知meshgrid的作用是：根据传入的两个一维数组参数生成两个数组元素的列表。如果第一个参数是xarray，维度是xdimesion，第二个参数是yarray，维度是ydimesion。那么生成的第一个二维数组是以xarray为行，共ydimesion行的向量；而第二个二维数组是以yarray的转置为列，共xdimesion列的向量 可能你看到这还不明白，看下图： A,B,C,D,E,F是6个网格点，坐标如图，如何用矩阵形式（坐标矩阵）来批量描述这些点的坐标呢？答案如下： 这就是坐标矩阵——横坐标矩阵X中的每个元素，与纵坐标矩阵Y中对应位置元素，共同构成一个点的完整坐标。 numpy.logspace创建等比数列，参数和logspace差不多，第四个参数代表基数。默认为10，如果想要修改其默认基数，输入第四个参数base，改变就可以了。 未完待续……]]></content>
      <categories>
        <category>python</category>
        <category>数学计算</category>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>python的数学计算模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一下网络编程。这里面用到一个库：socket。网络通信其实就是两个进程之间在编程。先说两个重要的协议：TCP协议 和 UDP协议。TCP协议是传输控制协议，UDP协议是数据传输协议。TCP和UDP的区别：TCP慢但是稳定，因为它经过了三次握手和四次挥手，不会丢失数据。UDP快。 socket:注意参数是一个tuple，包含地址和端口号。 在同一个os中，端口不允许相同，即如果某个端口已经被使用了，那么在这个进程释放之前，其他进程都不能使用这个端口。（端口用来区分进程，若相同，不能把数据发送到准确的进程） 创建Socket时，AF_INET指定使用IPv4协议，如果要用更先进的IPv6，就指定为AF_INET6。SOCK_STREAM指定使用面向流的TCP协议，这样，一个Socket对象就创建成功，但是还没有建立连接。 coding： 主机名可以通过调用socket.gethostname()获得 接收数据时，调用recv(max)方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到recv()返回空数据，表示接收完毕，退出循环。当我们接收完数据后，调用close()方法关闭Socket，这样，一次完整的网络通信就结束了 创建TCP连接时，主动发起连接的叫客户端，被动响应连接的叫服务器。 客户端要主动发起TCP连接，必须知道服务器的IP地址和端口号。 TCP服务端建立步骤： 一般是服务器（接受方）绑定端口，客户端（发送方）不绑定 UDP不需要调用listen（）方法。可以直接接收数据 TCP调用listen()方法开始监听端口，将主动套接字（默认）变为被动套接字，传入的参数指定等待连接的最大数量 TCP服务端： TCP客户端： 一般send()和recv()用于TCP，sendto()及recvfrom()用于UDP。sendto和recvfrom一般用于UDP协议中,但是如果在TCP中connect函数调用后也可以用。 服务器编程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立Socket连接，随后的通信就靠这个Socket连接了。 由于服务器会有大量来自客户端的连接，所以，服务器要能够区分一个Socket连接是和哪个客户端绑定的。一个Socket依赖4项：服务器地址、服务器端口、客户端地址、客户端端口来唯一确定一个Socket。但是服务器还需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。 然后，我们要绑定监听的地址和端口()。服务器可能有多块网卡，可以绑定到某一块网卡的IP地址上，也可以用0.0.0.0绑定到所有的网络地址，还可以用127.0.0.1绑定到本机地址。127.0.0.1是一个特殊的IP地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。 端口号需要预先指定。请注意，小于1024的端口号必须要有管理员权限才能绑定： 每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接 利用多线程和socket进行聊天室的创建（UDP）： 下面这段代码是单进程服务器，配合进程或线程也可以建立多任务服务器（TCP）： serverSocket：当这个套接字被关闭时，代表不再接收新的客户端连接 clientSocket：当这个套接字被关闭时，代表不能使用send和recv发收数据。 当利用线程建立多任务服务器时，clientSocket不能关闭。因为子线程共用数据 当利用进程建立时，clientSocket能关闭。子进程和父进程完全”一样“（实时拷贝） 单进程实现多任务： 最后，讲一下单播，多播和广播。 单播：一对一 多播：一对多 广播：一对所有 UDP有广播，TCP没有广播 UDP发送广播数据的条件：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F08%2F05%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式（不能随意添加空格，不然会改变原来含义）： 元字符(不能匹配自身): . $ ^ ( ) { } [ ] $ + \ | *， |：A | B 会匹配 A 或 B 中出现的任何字符。为了能够更加合理的工作，| 的优先级非常低。例如 Fish|C 应该匹配 Fish 或 C，而不是匹配 Fis，然后一个 ‘h’ 或 ‘C’。同样，我们使用 | 来匹配 ‘|’ 字符本身；或者包含在一个字符类中，像这样 [|]。 ^:匹配字符串的起始位置。如果设置了 MULTILINE 标志，就会变成匹配每一行的起始位置。在 MULTILINE 中，每当遇到换行符就会立刻进行匹配。 $:匹配字符串的结束位置，每当遇到换行符也会离开进行匹配。 +：用于指定前一个字符匹配一次或多次 ：匹配的是零次或多次 ？：指定前一个字符匹配零次或者一次。 {m，n}(m和n都是十进制整数)：它的含义是前一个字符必须匹配m次到n（包括n次）次之间 原始字符串来表示正则表达式（就是在字符串前边加上 r） \A:只匹配字符串的起始位置。如果没有设置 MULTILINE 标志的时候，\A 和 ^ 的功能是一样的；但如果设置了 MULTILINE 标志，则会有一些不同：\A 还是匹配字符串的起始位置，但 ^ 会对字符串中的每一行都进行匹配。 \Z:只匹配字符串的结束位置 \B:另一个零宽断言，与 \b 的含义相反，\B 表示非单词边界的位置。 零宽断言： 有些元字符它们不匹配任何字符，只是简单地表示成功或失败，因此这些字符也称之为零宽断言 前向断言： （1）：前向肯定断言：如果当前包含的正则表达式（这里以 … 表示）在当前位置成功匹配，则代表成功，否则失败。一旦该部分正则表达式被匹配引擎尝试过，就不会继续进行匹配了；剩下的模式在此断言开始的地方继续尝试。 （2）：前向否定断言：这跟前向肯定断言相反（不匹配则表示成功，匹配表示失败） 假定我们要处理一段html，我们要替换掉相对url，例如text 这个a标签我们要替换成text，而对于代码 这样的a标签则要保留不做替换。这个应用场景下 就需要判断A标签的href属性如果不是以http://开头则匹配，即要做前向否定的断言. 脱字符：^ ,例如 5 会匹配任何字符 “5”之外的任何字符 [ ]他们指定一个字符类用于存放你需要的字符集合。可以单独列出需要匹配字符，也可以两个字符和一个横杆-指定匹配的范围。元字符在方括号中不会触发“特殊功能”，在字符类中，它们只匹配自身。 反斜杠 \：如果在反斜杠后边紧跟着一个元字符，那么元字符的“特殊功能”也不会被触发。例如你需要匹配符号[ 或 \，你可以在他们前面加上一个反斜杠，以消除他们的特殊功能：\[ ,\\\ 注意用小括号括住要重复的内容： 匹配ip（万能版）： 非捕获组和命名组： 非捕获组的语法是 (?:…)，这个 … 你可以替换为任何正则表达式。 “捕获”就是匹配的意思啦，普通的子组都是捕获组，因为它们能从字符串中匹配到数据 命名组：：(?P)。很明显，&lt; &gt; 里边的 name 就是命名组的名字啦，除了使用名字访问， 命名组仍然可以使用数字序号进行访问 正则表达式使用以下方法修改字符串： split(*string*[, *maxsplit=0*])*：通过正则表达式匹配来分割字符串。如果在 RE 中，你使用了捕获组，那么它们的内容会作为一个列表返回。你可以通过传入一个 maxsplit 参数来设置分割的数量。如果 maxsplit 的值是非 0，表示至多有 maxsplit* 个分割会被处理，剩下的内容作为列表的最后一个元素返回。 .sub(*replacement*, *string*[, *count=0*])*返回一个字符串，这个字符串从最左边开始，所有 RE 匹配的地方都替换成 replacement。如果没有找到任何匹配，那么返回原字符串。可选参数 count* 指定最多替换的次数，必须是一个非负值。默认值是 0，意思是替换所有找到的匹配。下边是使用 sub() 方法的例子，它会将所有的颜色替换成 color： subn:subn() 方法跟 sub() 方法干同样的事情，但区别是返回值为一个包含有两个元素的元组：一个是替换后的字符串，一个是替换的数目。 匹配方法： 匹配的方法和属性： group(N) 返回第N组括号匹配的字符，groups() 返回所有括号匹配的字符，以tuple格式 match匹配的m： findall() 需要在返回前先创建一个列表，而 finditer() 则是将匹配对象作为一个迭代器返回 利用compile来先编译的方法是模式级别的方法（适用于多次使用该正则表达式），可以针对同一种模式做多次匹配，如下图：另一种是模式对象方法 import re import re p=re.compile() re.search(“”,””) p.search() 贪婪模式和非贪婪模式： 贪婪模式是让正则表达式尽可能的匹配符合的内容 在匹配的字符后面加一个问号，启动非贪婪模式 编译标志：编译标志让你可以修改正则表达式的工作方式。在 re模块下，编译标志均有两个名字：完整名和简写 ASCII(re.A) 使得 \w，\W，\b，\B，\s 和 \S 只匹配 ASCII 字符，而不匹配完整的 Unicode 字符。这个标志仅对 Unicode 模式有意义，并忽略字节模式。 DOTALL(re.S) 使得 . 可以匹配任何字符，包括换行符。如果不使用这个标志，. 将匹配除了换行符的所有字符。 IGNORECASE(re.I) 字符类和文本字符串在匹配的时候不区分大小写。举个例子，正则表达式 [A-Z] 也将会匹配对应的小写字母，像 FishC 可以匹配 FishC，fishc 或 FISHC 等。如果你不设置 LOCALE，则不会考虑语言（区域）设置这方面的大小写问题。 LOCALE(re.L) 使得 \w，\W，\b 和 \B 依赖当前的语言（区域）环境，而不是 Unicode 数据库。区域设置是 C 语言的一个功能，主要作用是消除不同语言之间的差异。例如你正在处理的是法文文本，你想使用 \w+ 来匹配单词，但是 \w 只是匹配 [A-Za-z] 中的单词，并不会匹配 ‘é’ 或 ‘&#231;’。如果你的系统正确的设置了法语区域环境，那么 C 语言的函数就会告诉程序 ‘é’ 或 ‘&#231;’ 也应该被认为是一个字符。当编译正则表达式的时候设置了 LOCALE 的标志，\w+ 就可以识别法文了，但速度多少会受到影响。 MULTILINE(re.M) 通常 ^ 只匹配字符串的开头，而 $ 则匹配字符串的结尾。当这个标志被设置的时候，^ 不仅匹配字符串的开头，还匹配每一行的行首；&amp; 不仅匹配字符串的结尾，还匹配每一行的行尾。 VERBOSE(re.X) 这个标志使你的正则表达式可以写得更好看和更有条理，因为使用了这个标志，空格会被忽略（除了出现在字符类中和使用反斜杠转义的空格）；这个标志同时允许你在正则表达式字符串中使用注释， 符号后边的内容是注释，不会递交给匹配引擎（除了出现在字符类中和使用反斜杠转义的 ） 正则表达式特殊符号及用法：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读取、写入excel]]></title>
    <url>%2F2019%2F08%2F05%2F%E8%AF%BB%E5%8F%96%E3%80%81%E5%86%99%E5%85%A5excel%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python如何对excel进行 读取和写入操作。 xlrd只支持对excel文件个是为xls文件的读取。 table = data.sheets()[0] #通过索引顺序获取 table = data.sheet_by_index(sheet_index)) #通过索引顺序获取 table = data.sheet_by_name(sheet_name)#通过名称获取 name=workbook_r.sheet_names() #获取文件的所有工作表名字 对行的操作： nrows = table.nrows #获取该sheet中的有效行数 table.row(rowx) #返回由该行中所有的单元格对象组成的列表 table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表 table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表 table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表 table.row_len(rowx) #返回该列的有效单元格长度 对列的操作： ncols = table.ncols #获取列表的有效列数 table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表 table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表 table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表 table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 获取单元格内容： 单元格 A1= table.cell(0,0).value C4=able.cell(2,3).value 使用行列索引 cell_A1 = table.row(0)[0].value cell_A2 = table.col(1)[0].value xlwt只支持对Excel文件格式为xls文件的写入 add_sheet(sheet_name): 添加sheet get_sheet(Sheet_name): 选择sheet save(file_name): 保存]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>xlrd、xlwt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收GC]]></title>
    <url>%2F2019%2F08%2F05%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6GC%2F</url>
    <content type="text"><![CDATA[​ 每种语言都有自己的垃圾回收机制。接下来我们来讲一下python的垃圾回收机制。 小整数对象池：python对小整数的定义为[-5，257)，这些整数对象是提前建立好的，不会被垃圾回收。单个字母也一样，但是当定义两个相同的字符串（没有空格等特殊符号），触发intern机制，引用计数为零，触发垃圾回收。 引用计数机制的优点：简单、实时性（一旦没有引用，内存就直接释放了）。 缺点：维护引用计数消耗资源、循环引用 python以引用计数为主，隔代回收为辅进行垃圾回收 GC模块（不能重写del方法）： 1、gc.set_debug(flags) 设置gc的debug日志，一般设置为gc.DEBUG_LEAK 2、gc.collect([generation]) 显式进行垃圾回收，可以输入参数，0代表只检查第一代的对象，1代表检查一，二代的对象，2代表检查一，二，三代的对象，如果不传参数，执行一个full collection，也就是等于传2。 返回不可达（unreachable objects）对象的数目 3、gc.get_threshold() 获取的gc模块中自动执行垃圾回收的频率 4、gc.set_threshold(threshold0[,threshold1[, threshold2]) 设置自动执行垃圾回收的频率。 5、gc.get_count() 获取当前自动执行垃圾回收的计数器，返回一个长度为3的列表 6、gc.disable() 把gc关闭,gc.enable()打开gc（默认打开） 7.gc.garbage 存储垃圾 导致引用计数+1的情况： 导致引用计数-1的情况： 查看一个对象的引用计数： 因为调用函数的时候传入a，所以是2.真正的引用计数=sys.getrefcount()-1]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂生产模式]]></title>
    <url>%2F2019%2F08%2F05%2F%E5%B7%A5%E5%8E%82%E7%94%9F%E4%BA%A7%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一下什么是工厂方法模式。工厂方法模式是在基类完成基本框架的搭建，在子类中具体实现方法的实现。工厂模式是一种典型的解耦模式。 函数或者类之间的关系越强，耦合性越强。代码就越难更新。 使用函数进行解耦： 使用类进行解耦：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python生产模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类和对象]]></title>
    <url>%2F2019%2F08%2F05%2F%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[​ 众所周知，面向过程是根据业务逻辑从上到下写代码，面向过程：根据业务逻辑从上到下写代码面向对象：将数据与函数绑定到一起，进行封装，这样能更快速的开发程序，减少了重复代码的重写过程。面向对象语言三个基本要素：封装 继承 多态把函数和全局变量和在一起就是封装。而python就属于面向对象的语言。 类与对象的关系和区别：类是抽象的概念，仅仅代表事物的模板。对象是一个能够看得到，摸得着的具体的实体比如：飞机是对象，飞机图纸是类。 类由三部分构成: 类的名称:类名 类的属性：一组数据 一个特殊的对象：能够知道这个对象的class. 类的方法：允许进行的操作 类的抽象：拥有相同或者类似属性和行为的对象都可以抽象出一个类 python调用__init__方法的作用：初始化对象 python调用__str__方法： 私有方法：外界不能直接调用 私有属性：可以添加，可以添加后取值，不可以取值。（以双下划线开头为私有变量，单下划线开头的为保护变量） 通过内部方法取值： 私有属性无法取值的原因是因为名字重整技术，其实可以通过_类名_属性可以访问： 在这里讲一下私有的原理：这是通过名字重整机制改变的。命名规则：_类名__num（尽量不要用） python可以自己调用__del方法，\del__是该对象被删除后调用的方法，注意：这里的删除是没有引用对象 完全删除后（没有引用对象）： 测量对象的引用对象：用sys模块的sys.getrefcount，得出结果后减一。 __new__方法：创建对象 实例化对象相当于做三件事： 1.通过__init来创建对象，然后找了一个变量来接收_\init__的返回值。这个返回值表示创建出来的对象引用。 2.__init__（刚刚创建出来的对象的引用） 3.返回对象的引用。 __new__方法 和 __init__方法的区别: 1.__init__ 通常用于初始化一个新实例，控制这个初始化的过程，比如添加一些属性， 做一些额外的操作，发生在类实例被创建完以后。它是实例级别的方法。没有返回值。负责初始化 2.__new 通常用于控制生成一个新实例的过程。它是类级别的方法，参数是cls。负责创建。__init__和__new方法合起来相当于C++的构造函数。 下面讲一个新的概念：单例 单例是创建了多少个对象都是指向同一片内存。 只初始化一次对象： 继承：可以少写代码。继承父类的方法和属性 继承可以多类继承。当子类和父类有什么不同时，可以进行重写（在子类中写一个和父类方法名相同的方法进行不同操作） 私有方法，私有属性不能被继承，可以被间接调用，如果在子类中实现了一个公有方法调用的方法或属性，那么这个方法是不能调用继承的父类的私有方法和私有属性 多继承： 当子类中的方法和父类方法名一样时会按照以下顺序去进行执行： 子类调用父类的三种方法： 父类.方法 super().init(不用传self，传参数) super(父类，self).init（不传self，参数）可以根据父类指定调用哪一个父类 多态：在写完方法的时候并不知道调用的是什么方法。真正执行的时候才知道 接下来的内容也很重要哦： 类在程序中也属于一个对象，称之为类对象。同过类创建出来的对象称之为实例对象。 类属性(classmethod)：类对象里的属性 实例属性：实例对象里的属性 实例属性和类属性的区别：实例属性和具体的某个实例对象有关系，且一个实例对象和另外一个实例对象是不共享属性的。类属性属于类对象，并且多个实例对象共享同一个类属性。 实例方法和类方法： 静态方法： 实例方法 类方法和静态方法的区别：实例方法和类方法必须传一个参数（实例方法self用来接收对象，类方法cls用于接收类），静态方法不需要参数（可以有）。 动态添加属性和方法： slots可以限制添加属性：这可以告诉解释器这个类的所有实例属性都在这了。可以节省大量内存。每个子类都要定义slots属性，因为解释器会忽略继承的slots属性。 内建属性： getattribute:属性拦截器 使用类名调用类属性时，不会经过__getattribute__方法，其他均要调用。（可以用来做日志）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的类和对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python异常]]></title>
    <url>%2F2019%2F08%2F05%2Fpython%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python的异常。何为异常？即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。异常时python对象，表示一个错误。当python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。 try/except可以用来捕获异常 一个try捕获多个异常： 一个try对多个except：根据不同的except做不同的操作 try-finally语句无论是否发生异常都将执行最后的代码。异常传递 raise自定义异常： 最后是python的标准异常：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python数据类型]]></title>
    <url>%2F2019%2F08%2F04%2Fpython%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ 每种编程语言都有属于自己的数据类型。今天，我们就来讲一下python的数据类型——列表，元祖，字典，字符串，堆，集合，队列。 列表​ 同时还有一些比较常用的方法，类似cmp(比较大小，python3已经找不到。如要使用，可以利用operator模块)，len(list)计算列表元素个数，max(list)求列表中的最大值，min(list)求最小值。下面对上面列表的排序方法(list.sort)进行讲解： 下张图片是对以上方法的coding： 元祖（tuple）元祖不像列表可以改变。元祖是不可变的。但可以利用切片灵活使用。 字典（dict）注意：字典支持常见的集合操作（&amp;等操作）字典有多个键与其对应的值构成的键-值对组成。键-值对称为项。每个键和它的值之间用:(冒号)隔开，项之间用,(逗号)隔开。 len(d)返回d中项(键-值对)的数量 d[k]返回关联到键k的值 d[k]=v将值关联到键k上 del d[k] 删除键为k的项 k in d 检查d中是否有含有键为k的值 clear()：清除字典中所有的项 fromkeys()：使用给定的键建立新的字典，每个键都对应一个默认的值None，也可以设立默认值 get():访问字典项，访问一个不存在的键时，没有异常。且可以定义默认值为None。 items和iteritems： items将所有的项以列表方式返回，列表中的每一项都表示为（键，值）对的形式。 iteritems方法的作用大致相同，但是会返回一个迭代器对象而不是列表。 keys和iterkeys： keys方法将字典中的键以列表形式返回，而iterkeys则返回针对键的迭代器 values和itervalues： values方法以列表的形式返回字典中的值，itervalues返回值的迭代器。与返回键的列表不同的是，返回值的列表中可以包含重复的元素。 pop：用来获得对应于给定键的值，然后将这个键-值对从字典中移除。 popitem：弹出随机的项 setdefault：获得与给定键相关联的值，能在字典中不含有给定键的情况下设立相应的键值。 update：可以利用一个字典项更新另外一个字典 堆（heapq）使用该数据类型前，我们先导入一个新的模块heapq heappush(heap,x):将x入对 heapop(heap)：将堆中最小的元素弹出 heapify(heap)：将heap属性强制应用到任意一个列表 heapreplace(heap,x)：将堆中最小的元素弹出，同时将x入堆 nlargest(n,iter)：返回iter中前n大的元素 nsmallest(n,iter)：返回iter中前n小的元素 集合（set）集合可取交集、取并集、取差集、对称差集： 利用set做去重操作： 集合分为可变集合和不可变集合： 队列（Queue）在python中，队列时线程间最常用的交换数据形式，Queue模块时提供队列操作的模块。 先进先出（FIFO）： 先进后出（LIFO）： 优先级队列：优先级队列put进去一个元祖，（优先级，数据），优先级数字越小，优先级越高。 注意：如果有两个元素优先级是一样的，那么在出队的时候是按照先进先出的顺序。 双端队列： 队列的方法： 使用put方法往队列中添加元素，需要考虑是否能放下的问题如果放不下了，默认会阻塞(block=True)，阻塞时可以定义超时时间timeout。可以使用block=False设置阻塞时立即报错 使用get()从队列里取数据。如果为空的话，blocking= False 直接报 empty异常。如果blocking = True，就是等一会，timeout必须为 0或正数。None为一直等下去，0为不等，正数n为等待n秒还不能读取，报empty异常。 字符串（string）用+拼接字符串： 将值转换为字符串的机制： 1、通过str函数，把值转换为合理形式的字符串，以便用户可以理解 2、通过repr函数创建一个字符串，以合法的python表达式的形式表示值 join和split： 字符串格式化方式： capitalize() 把字符串的第一个字符改为大写 casefold() 把整个字符串的所有字符改为小写 center(width) 将字符串居中，并使用空格填充至长度 width 的新字符串 count(sub[, start[, end]]) 返回 sub 在字符串里边出现的次数，start 和 end 参数表示范围，可选。 encode(encoding=’utf-8’, errors=’strict’) 以 encoding 指定的编码格式对字符串进行编码。 startswith(prefix[, start[, end]]) 检查字符串是否以 prefix 开头，是则返回 True，否则返回 False。start 和 end 参数可以指定范围检查，可选。 endswith(sub[, start[, end]])检查字符串是否以 sub 子字符串结束，如果是返回 True，否则返回 False。start 和 end 参数表示范围，可选。 startswith和endswith如果需要同时针对多个选项做检查，只需要给startswith和endswith提供包含可能选项的元祖。 expandtabs([tabsize=8]) 把字符串中的 tab 符号（\t）转换为空格，如不指定参数，默认的空格数是 tabsize=8。 find(sub[, start[, end]])检测 sub 是否包含在字符串中，如果有则返回索引值，否则返回 -1，start 和 end 参数表示范围，可选。 maketrans() 方法用于创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。 注：两个字符串的长度必须相同，为一一对应的关系。 translate(table)根据 table 的规则（可以由 str.maketrans(‘a’, ‘b’) 定制）转换字符串中的字符 index(sub[, start[, end]]) 跟 find 方法一样，不过如果 sub 不在 string 中会产生一个异常。 isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返回 True，否则返回 False。 isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True，否则返回 False。 isdecimal() 如果字符串只包含十进制数字则返回 True，否则返回 False。 isdigit() 如果字符串只包含数字则返回 True，否则返回 False。 islower() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是小写，则返回 True，否则返回 False。 isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。 isspace() 如果字符串中只包含空格，则返回 True，否则返回 False。 istitle() 如果字符串是标题化（所有的单词都是以大写开始，其余字母均小写），则返回 True，否则返回 False。 isupper() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是大写，则返回 True，否则返回 False。 join(sub) 以字符串作为分隔符，插入到 sub 中所有的字符之间。 ljust(width) 返回一个左对齐的字符串，并使用空格填充至长度为 width 的新字符串。 rjust(width) 返回一个右对齐的字符串，并使用空格填充至长度为 width 的新字符串。 format也可以完成对齐的任务。“&lt;”：左对齐 “&gt;”：右对齐 “^”：居中对齐 lower() 转换字符串中所有大写字符为小写。 lstrip() 去掉字符串左边的所有空格无法去除中间的字符 rstrip() 删除字符串末尾的空格。无法去除中间的字符 partition(sub) 找到子字符串 sub，把字符串分成一个 3 元组 (pre_sub, sub, fol_sub)，如果字符串中不包含 sub 则返回 (‘原字符串’, ‘’, ‘’) replace(old, new[, count]) 把字符串中的 old 子字符串替换成 new 子字符串，如果 count 指定，则替换不超过 count 次。 rfind(sub[, start[, end]]) 类似于 find() 方法，不过是从右边开始查找。返回值是下标 rindex(sub[, start[, end]]) 类似于 index() 方法，不过是从右边开始。 rpartition(sub) 类似于 partition() 方法，不过是从右边开始查找。 split(sep=None, maxsplit=-1) 不带参数默认是以空格为分隔符切片字符串，如果 maxsplit 参数有设置，则仅分隔 maxsplit 个子字符串，返回切片后的子字符串拼接的列表。 splitlines(([keepends])) 在输出结果里是否去掉换行符，默认为 False，不包含换行符；如果为 True，则保留换行符。。 strip([chars]) 删除字符串前边和后边所有的空格，chars 参数可以定制删除的字符，可选。 swapcase() 翻转字符串中的大小写。 title() 返回标题化（所有的单词都是以大写开始，其余字母均小写）的字符串。 upper() 转换字符串中的所有小写字符为大写。 zfill(width) 返回长度为 width 的字符串，原字符串右对齐，前边用 0 填充。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2019%2F08%2F03%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[​ 你好陌生人，欢迎来到湛蓝星空的博客。这是我的第一篇博客，纠结了很久要不要写，因为我通常都是用一些笔记软件来记录学的一些知识。在这里，希望你能学到你希望学到的东西。 联系方式 qq：1171708687 邮箱：1171708687@qq.com 微信：18269658106 加我的时候备注博客就好 PS：本人实在太菜。如果有错误，请谅解。]]></content>
      <categories>
        <category>你好</category>
      </categories>
      <tags>
        <tag>你好</tag>
      </tags>
  </entry>
</search>
