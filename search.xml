<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2019%2F09%2F08%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[集成算法概述集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型建模结果。基本上所有的机器学习领域都可以看到集成学习的身影。 集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。 多个模型集成称为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器(base estimator)。通常来说，有三类集成算法：装袋法(Bagging)、提升法(Boosting)、stacking 装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。装袋法的代表模型就是随机森林。提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树 sklearn中的集成算法sklearn集成算法模块ensemble n_estimators，这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。 其它控制基评估器的参数参考决策树 随机森林：12345678910111213141516171819202122232425262728from sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import load_winefrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import cross_val_scorefrom matplotlib import pyplot as pltwine=load_wine()Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)clf = DecisionTreeClassifier(random_state=0)rfc = RandomForestClassifier(random_state=0)clf = clf.fit(Xtrain,Ytrain)rfc = rfc.fit(Xtrain,Ytrain)score_c = clf.score(Xtest,Ytest)score_r = rfc.score(Xtest,Ytest)print("Single Tree:&#123;&#125;".format(score_c),"Random Forest:&#123;&#125;".format(score_r))rfc_l = []clf_l = []for i in range(10): rfc = RandomForestClassifier(n_estimators=25) rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean() rfc_l.append(rfc_s) clf = DecisionTreeClassifier() clf_s = cross_val_score(clf,wine.data,wine.target,cv=10).mean() clf_l.append(clf_s)plt.plot(range(1,11),rfc_l,label = "Random Forest")plt.plot(range(1,11),clf_l,label = "Decision Tree")plt.legend()plt.show() 随机森林中有三个非常重要的属性：.estimators_，.oob_score_以及.feature_importances_。.estimators_是用来查看随机森林中所有树的列表的。.oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度。而.feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性 随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。 随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。 泛化误差在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error） 当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。 1）模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点2）模型太复杂就会过拟合，模型太简单就会欠拟合3）对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂4）树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动 偏差和方差观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。 方差和偏差对模型的影响： 然而，方差和偏差是此消彼长的，不可能同时达到最小值 从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。 我们调参的目标是，达到方差和偏差的完美平衡 ！]]></content>
      <categories>
        <category>机器学习</category>
        <category>随机森林</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库的数据模型]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。 由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求： 1、比较真实地描述现实世界 2、易为用户所理解 3、易于在计算机上实现 为什么需要数据模型？ 由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。 数据模型含有哪些内容？ 数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。 ​ 1、数据结构 ​ 用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面 ​ 2、数据操作 ​ 用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查 ​ 3、数据的约束条件 ​ 是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。 实体联系数据模型的地位与作用： 实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。 数据模型是用来描述数据的一组概念和定义，是描述数据的手段。 ​ 概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。 ​ 逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模 ​ 物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。 逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。 数据模式是对数据结构、联系和约束的描述。数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。 信息世界中的基本概念： ​ (1) 实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。 ​ (2) 属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画 ​ (3) 键(Key)或称为码：唯一标识实体的属性集称为码 ​ (4) 域(Domain)：属性的取值范围称为该属性的域 ​ (5) 实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型 ​ (6) 实体集(Entity Set)：同一类型实体的集合称为实体集 ​ (7) 联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。 概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。 ​ 实体型：用矩形表示，矩形框内写明实体名 ​ 属性：用椭圆表示，并用无向边将其与相应的实体连接起来。 ​ 联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n） ​ 键：用下划线表示。 最常用的数据模型 ​ 非关系模型 ​ 层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。 ​ 满足以下两个条件称为层次模型： ​ (1) 有且仅有一个结点无双亲。这个结点称为“根节点” ​ (2) 其它节点有且仅有一个双亲，但可以有多个后继 ​ 网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。 ​ 网状结构特点： ​ (1) 允许一个以上的结点无双亲； ​ (2) 一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。 ​ 关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。 ​ 关系数据模型的数据结构： ​ 关系(Relation)：一个关系对应通常说的一张表 ​ 元祖(Tuple)：表中的一行即为一个元祖 ​ 属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。 ​ 主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。 ​ 域(Domain)：属性的取值范围 ​ 分量：元祖中的一个属性值 ​ 关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n) ​ 面向对象模型： ​ 对象关系模型]]></content>
      <categories>
        <category>数据库</category>
        <category>数据模型</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的评判和调优]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[精确率和召回率 分类模型评估API：F1-score,反应了模型的稳健性 模型选择和调优：1、交叉验证 2、网格搜索 交叉验证为了让被评估的模型更加准确可信 网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。 sklearn.model_Selection.GridSearchCV]]></content>
      <categories>
        <category>机器学习</category>
        <category>模型的评判和调优</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means]]></title>
    <url>%2F2019%2F09%2F07%2FK-means%2F</url>
    <content type="text"><![CDATA[无监督学习与聚类算法有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。而聚类算法，就是无监督学习的代表算法。 聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。 聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要实例化，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。 KMeans是如何工作的关键概念：簇与质心 ​ 簇：KMeans算法将一组N个样本的特征矩阵x划分为k个无交集的簇，直观上来看是簇是一组组聚集在一起的数据，在一个簇中的数据被认为是同一类。簇就是聚类的结果表现。 ​ 质心：簇中所有数据的均值通常被称为这个簇的质心。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点纵坐标的均值。同理可推广到高维空间。 在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下 ： ​ 1、随机抽取k个样本作为最初的质心 ​ 2、开始循环 ​ 2.1、将每个样本点分配到离他们最近的质心，生成k个簇 ​ 2.2、对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心 ​ 3、当质心的位置不在发生变化，迭代停止，聚类完成 那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。 对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距离的衡量方法有多种，令x表示簇中的一个样本点，ui表示该簇中的质心，n表示每个样本点中的特征数目，i表示组成点x的每个特征，则该样本点到质心的距离可以由以下距离来度量： 如我们采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为: 其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum ofSquare），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。因此,KMeans追求的是，求解能够让Inertia最小化的质心。实际上，在质心不断变化不断迭代的过程中，总体平方和是越来越小的。我们可以使用数学来证明，当整体平方和最小的时候，质心就不再发生变化了。如此，K-Means的求解过程，就变成了一个最优化问题 损失函数本质是用来衡量模型的拟合效果的，只有有着求解参数需求的算法，才会有损失函数。Kmeans不求解什么参数，它的模型本质也没有在拟合数据，而是在对数据进行一种探索。所以，K-Means不存在什么损失函数。Inertia更像是Kmeans的模型评估指标，而非损失函数。 重要参数和属性 参数n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果。通常，在开始聚类之前，我们并不知道n_clusters究竟是多少，因此我们要对它进行探索。 在之前描述K-Means的基本流程时我们提到过，当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前， 我们也可以使用max_iter，大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下 来。有时候，当我们的n_clusters选择不符合数据的自然分布，或者我们为了业务需求，必须要填入与数据的自然 分布不合的n_clusters，提前让迭代停下来反而能够提升模型的表现。 max_iter：整数，默认300，单次运行的k-means算法的大迭代次数tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下 属性labels_：查看聚好的类别，每个样本所对应的类 cluster_centers_：查看质心 inertia_：查看总距离平方和 n_iter_：实际的迭代次数 coding： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.datasets import make_blobsimport matplotlib.pyplot as plt#这是自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数#random_state代表随机性X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)fig, ax1 = plt.subplots(1)ax1.scatter(X[:, 0], X[:, 1]#画点图,marker='o'#代表点的形状,s=8)#代表点的大小#最开始数据集的形状plt.show()color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(4): ax1.scatter(X[y==i, 0], X[y==i, 1] ,marker='o' ,s=8 ,c=color[i])plt.show()from sklearn.cluster import KMeans#簇为3n_clusters = 3cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)#查看聚好的列表y_pred = cluster.labels_pre = cluster.fit_predict(X)#查看质心centroid = cluster.cluster_centers_#查看总距离平方和inertia = cluster.inertia_print(inertia)color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(n_clusters): ax1.scatter(X[y_pred==i, 0], X[y_pred==i, 1] ,marker='o' ,s=8 ,c=color[i])#画出质心ax1.scatter(centroid[:,0],centroid[:,1],marker="x",s=15,c="black")plt.show()#簇为4n_clusters=4cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)#查看聚好的列表y_pred = cluster.labels_pre = cluster.fit_predict(X)#查看质心centroid = cluster.cluster_centers_#查看总距离平方和inertia = cluster.inertia_print(inertia)color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(n_clusters): ax1.scatter(X[y_pred==i, 0], X[y_pred==i, 1] ,marker='o' ,s=8 ,c=color[i])#画出质心ax1.scatter(centroid[:,0],centroid[:,1],marker="x",s=15,c="black")plt.show() 聚类算法的模型评估上面的算法的簇随着数字的增大，总的距离和在不断的变小。难道我们我们就这样实验得出簇的个数吗？难道簇越小越好吗？看看算法的评估。聚类模型的结果不是某种标签输出，并且聚类的结果是不确定的，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？ KMeans的目标是确保“簇内差异小，簇外差异大”，我们就可以通过衡量簇内差异来衡量聚类的效果。我们刚才说过，Inertia是用距离来衡量簇内差异的指标，因此，我们是否可以使用Inertia来作为聚类的衡量指标呢？Inertia越小模型越好嘛。可以，但是这个指标的缺点和极限太大。 当真实标签未知的时候使用轮廓系数这样的聚类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中轮廓系数是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。单个样本的轮廓系数计算为 这个公式可以看作： 很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。 在sklearn中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 12345#接上面的代码from sklearn.metrics import silhouette_samplesfrom sklearn.metrics import silhouette_scoreprint("4个簇的时候的轮廓系数：",silhouette_score(X,y_pred))print("四个簇的每个样本的轮廓系数：",silhouette_samples(X,y_pred)) 当真实标签未知的时候用CHI除了轮廓系数是常用的，我们还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用。 在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言，Calinski-Harabaz指数s(k)写作如下公式： 其中N为数据集中的样本量，k为簇的个数(即类别的个数)，Bk是组间离散矩阵，即不同簇之间的协方差矩阵， Wk是簇内离散矩阵，即一个簇内数据的协方差矩阵，而tr表示矩阵的迹。在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A) 。数据之间的离散程度越高，协方差矩阵的迹就会越大。组内离散程度低，协方差的迹就会越小，Tr(Wk)也就越小，同时，组间离散程度大，协方差的的迹也会越大， Tr(Bk)就越大，这正是我们希望的，因此Calinski-harabaz指数越高越好。 123#接上面的代码from sklearn.metrics import calinski_harabaz_scoreprint(calinski_harabaz_score(X, y_pred)) 基于轮廓系数选择簇的个数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from sklearn.datasets import make_blobsimport matplotlib.pyplot as pltfrom sklearn.metrics import silhouette_samplesfrom sklearn.metrics import silhouette_scorefrom sklearn.cluster import KMeansimport matplotlib.cm as cmimport numpy as np#自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数#random_state代表随机性X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)n_clusters = 4fig, (ax1, ax2) = plt.subplots(1, 2)fig.set_size_inches(18, 7)ax1.set_xlim([-0.1, 1])ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10])clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X)cluster_labels = clusterer.labels_silhouette_avg = silhouette_score(X, cluster_labels)print("For n_clusters =", n_clusters,"The average silhouette_score is :", silhouette_avg)sample_silhouette_values = silhouette_samples(X, cluster_labels)y_lower = 10for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10ax1.set_title("The silhouette plot for the various clusters.")ax1.set_xlabel("The silhouette coefficient values")ax1.set_ylabel("Cluster label")ax1.axvline(x=silhouette_avg, color="red", linestyle="--")ax1.set_yticks([])ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1],marker='o' #点的形状,s=8 #点的大小,c=colors)centers = clusterer.cluster_centers_ax2.scatter(centers[:, 0], centers[:, 1], marker='x',c="red", alpha=1, s=200)ax2.set_title("The visualization of the clustered data.")ax2.set_xlabel("Feature space for the 1st feature")ax2.set_ylabel("Feature space for the 2nd feature")plt.suptitle(("Silhouette analysis for KMeans clustering on sample data ""with n_clusters = %d" % n_clusters),fontsize=14, fontweight='bold')plt.show()for n_clusters in [2,3,4,5,6,7]: n_clusters = n_clusters fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) ax1.set_xlim([-0.1, 1]) ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10]) clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X) cluster_labels = clusterer.labels_ silhouette_avg = silhouette_score(X, cluster_labels) print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg) sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7 ) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10ax1.set_title("The silhouette plot for the various clusters.")ax1.set_xlabel("The silhouette coefficient values")ax1.set_ylabel("Cluster label")ax1.axvline(x=silhouette_avg, color="red", linestyle="--")ax1.set_yticks([])ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1],marker='o' #点的形状,s=8 #点的大小,c=colors)centers = clusterer.cluster_centers_ax2.scatter(centers[:, 0], centers[:, 1], marker='x', c="red", alpha=1, s=200)ax2.set_title("The visualization of the clustered data.")ax2.set_xlabel("Feature space for the 1st feature")ax2.set_ylabel("Feature space for the 2nd feature")plt.suptitle(("Silhouette analysis for KMeans clustering on sample data ""with n_clusters = %d" % n_clusters),fontsize=14, fontweight='bold')plt.show()]]></content>
      <categories>
        <category>机器学习</category>
        <category>K-means</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2019%2F09%2F07%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归推导过程逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以下线性回归。线性回归写作一个人人熟悉的方程： &theta;被统称为模型的参数，其中&theta;被称为截距。&theta;1——&theta;n被称为系数。我们可以用矩阵表示这个方程，其中x和&theta;都可以被看作是一个列矩阵： 线性回归的任务，就是构造一个预测函数z来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心就是找出模型的参数：&theta;的转置矩阵和&theta;0，著名的最小二乘法就是用来求解线性回归中参数的数学方法。 通过函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务（比如预测产品销量，预测股价等等）。那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型变量，我们要怎么办呢？我们可以通过引入联系函数(link function)，将线性回归方程z变换为g(z)，并且令g(z)的值分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数： Sigmoid函数的公式和性质：Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1。 将z带入Sigmoid，得到二元逻辑回归的一般形式： 而g(z)就是我们逻辑回归返回的标签值。此时,y(x)的取值都在[0,1]之间，因此 y(x)和1-y(x)相加必然为1。令y(x)除以1-y(x)可以得到形似几率得y(x)/(1-y(x))，在此基础上取对数，得： 不难发现，g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。 逻辑回归的类 逻辑回归有着基于训练数据求解参数&theta;的需求，并且希望训练出来的模型能够尽可能地拟合训练数据，即模型在训练集上的预测准确率越靠近100%越好。因此，我们使用”损失函数“这个评估指标，来衡量参数&theta;的优劣，即这一组参数能否使模型在训练集上表现优异。 ​ 关键概念：损失函数，衡量参数&theta;的优劣的评估指标，用来求解最优参数的工具损失函数小，模型在训练集上表现优异，拟合充分，参数优秀损失函数大，模型在训练集上表现差劲，拟合不足，参数糟糕我们追求，能够让损失函数最小化的参数组合。注意：没有”求解参数“需求的模型没有损失函数，比如KNN，决策树。 逻辑回归的损失函数是由最大似然法来推导出来的，具体结果可以写作 ： &theta;表示求解出来的一组参数，m是样本的个数，yi是样本i上真实的标签。y&theta;(xi)是样本i上，基于参数&theta;计算出来的逻辑回归返回值，xi是样本i的取值。我们的目标，就是求解出使J(&theta;)最小的&theta;的取值。 由于我们追求损失函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题：如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。虽然逻辑回归和线性回归是天生欠拟合的模型，但我们还是需要控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。 正则化 重要参数正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。 其中J(&theta;)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数。j代表每个参数。在这里，J要大于等于1，因为我们的参数向量&theta;中，第一个参数是&theta;0，使我们的截距。它通常不参与正则化。 L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），参数 &theta;的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化 。 梯度下降重要参数之前提到过，逻辑回归的数学目的是求解能够让模型最优化的参数&theta;的值，即求解能够让损失函数最小化的 的值，而这个求解过程，对于二元逻辑回归来说，有多种方法可以选择，最常见的有梯度下降法(Gradient Descent)，坐标轴下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。 以梯度下降法为例，我们来看看求解过程是如何完成的。下面这个华丽的平面就是我们的损失函数在输入了一组特征矩阵和标签之后在三维立体坐标系中的图像。现在，我们寻求的是损失函数的最小值，也就是图像的最低点（看起来像是深蓝色区域的某处），一旦我们获取了图像在最低点的取值J(&theta;)0，在我们的损失函数公式中，唯一未知的就是我们的参数向量&theta;。 12345678910111213141516171819202122232425262728293031323334353637383940from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoredata = load_breast_cancer()X = data.datay = data.targetdata.data.shapelrl1 = LR(penalty="l1",solver="liblinear",C=0.5,max_iter=1000)lrl2 = LR(penalty="l2",solver="liblinear",C=0.5,max_iter=1000)#逻辑回归的重要属性coef_，查看每个特征所对应的参数lrl1 = lrl1.fit(X,y)lrl1.coef_(lrl1.coef_ != 0).sum(axis=1)lrl2 = lrl2.fit(X,y)lrl2.coef_l1 = []l2 = []l1test = []l2test = []Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)for i in np.linspace(0.05,1,19): lrl1 = LR(penalty="l1",solver="liblinear",C=i,max_iter=1000) lrl2 = LR(penalty="l2",solver="liblinear",C=i,max_iter=1000) lrl1 = lrl1.fit(Xtrain,Ytrain) l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain)) l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest)) lrl2 = lrl2.fit(Xtrain,Ytrain) l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain)) l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))graph = [l1,l2,l1test,l2test]color = ["green","black","lightgreen","gray"]label = ["L1","L2","L1test","L2test"]plt.figure(figsize=(6,6))for i in range(len(graph)): plt.plot(np.linspace(0.05,1,19),graph[i],color[i],label=label[i])plt.legend(loc=4) #图例的位置在哪里? 4表示，右下角plt.show()]]></content>
      <categories>
        <category>机器学习</category>
        <category>逻辑回归</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归]]></title>
    <url>%2F2019%2F09%2F07%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归原理回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目标根本不是求解出标签，注意加以区别。 线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。 线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个有 个特征的样本 而言，它的回归结果可以写作一个几乎人人熟悉的方程 ： W被统称为模型的参数。W0被称为截距W1-Wn被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 Xi1-Xin是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： 我们可以使用矩阵来表示这个方程： y=Xw，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中w可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量w 。 在多元线性回归中，我们在损失函数如下定义： 因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ： 第一次看不明白上面红字，这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。 现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对w求导。 我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得： 线性回归APIsklearn中的线性模型模块是linear_model。 coding： 模型评估指标回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。第一，我们是否预测到了正确的数值。第二，我们是否拟合到了足够的信息。 sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 ​ 注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格 12345678910111213141516171819from sklearn.linear_model import LinearRegression as LRfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import cross_val_scorefrom sklearn.metrics import mean_squared_errorfrom sklearn.datasets import fetch_california_housing as fchhousevalue=fch()xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=0.3,random_state=420)std_x=StandardScaler()xtrain=std_x.fit_transform(xtrain)xtest=std_x.transform(xtest)std_y=StandardScaler()ytrain=std_y.fit_transform(ytrain.reshape(-1,1))ytest=std_y.transform(ytest.reshape(-1,1))reg=LR().fit(xtrain,ytrain)ypredict=reg.predict(xtest)print(std_y.inverse_transform(ypredict))print("均方误差：",mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="mean_squared_error")) 这里如果我们运行上面的代码，会在第十九行报错： 我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。 1print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="neg_mean_squared_error")) 为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助: 在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。 R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。 12print("R2",r2_score(ypredict,ytest))print(reg.score(xtest,ytest)) ????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是预测值在分子，真实值在分母。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数： 1234#直接用r2_scoreprint("R2",r2_score(y_true=ytest,y_pred=ypredict))#利用接口scoreprint(reg.score(xtest,ytest)) 123456#EVS的两种调用方法from sklearn.metrics import explained_variance_score as evs#第一种print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="explained_variance"))#第二种print("evs",evs(ytest,ypredict)) 过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂) 解决方法：正则化。用岭回归实现 欠拟合：一个假设在训练数据上不能获得更好的拟合，但是在训练数据外的数据集上也不能很好地拟合数据。此时认为这个假设出现了欠拟合的现象。(模型过于简单) 解决办法是添加数据的特征数量。]]></content>
      <categories>
        <category>机器学习</category>
        <category>线性回归</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的保存和加载]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[模型的保存和加载 joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。 joblib.load()：读取模型。参数是模型的目录]]></content>
      <categories>
        <category>机器学习</category>
        <category>模型的保存和加载</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树推导首先看看下面这组数据集： 得出下面这颗决策树： 关键概念： ​ 信息熵公式： ​ 信息增益公式：就是熵和特征条件熵的差 ​ 随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好 决策树算法的需要解决的核心问题： ​ 1、如何从数据表中找出最佳节点和最佳分支？ ​ 2、如何让决策树停止生长，防止过拟合？ 决策树的基本过程： 直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。 决策树五大模块sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类： 分类树参数、属性和接口 参数criterion为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 criterion这个参数正是用来决定不纯度的计算方法的： ​ 1、输入”entropy“，使用信息熵 ​ 2、输入”gini“，使用基尼系数 其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。 random_state&amp;splitterrandom_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 max_depth限制树的最大深度，超过设定深度的树枝全部剪掉这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效果再决定是否增加设定深度。 min_samples_leaf&amp;min_samples_splitmin_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。 max_features&amp;min_impurity_decreasemax_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的功能，在0.19版本之前时使用min_impurity_split。 class_weight&amp;min_weight_fraction_leaf完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 属性分类树的七个参数，一个属性，四个接口，以及绘图所用的代码。七个参数：Criterion，两个随机性相关的参数（random_state，splitter），四个剪枝参数（max_depth， min_sample_leaf，max_feature，min_impurity_decrease）一个属性：feature_importances_ ，属性是在模型训练之后，能够调用查看的模型的各种性质。 接口四个接口：ﬁt，score，apply，predicd apply：返回每个测试样本所在叶子节点的索引 predict：返回每个测试样本的分类/回归结果 coding： 回归树参数解读 几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。 criterion：回归树衡量分枝质量的指标，支持的标准有三种：1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。 2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ： 其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 决策树的本地保存：Graphvizwindows版本下载地址：https://graphviz.gitlab.io/_pages/Download/Download_windows.html 双击msi文件，一直next就完事了。 找到bin文件夹 在下面这张图片的位置加入环境变量 用dot -version检查是否安装成功 将dot文件转为png文件的命令：dot -Tpng *.dot -o *.png 123456789101112131415161718#分类树from sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier,export_graphvizimport pandas as pdtitan=pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")x=titan[["pclass","age","sex"]]y=titan["survived"]x["age"].fillna(x["age"].mean(),inplace=True)x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)dict=DictVectorizer(sparse=False)x_train=dict.fit_transform(x_train.to_dict(orient="records"))x_test=dict.transform(x_test.to_dict(orient="records"))dec=DecisionTreeClassifier()dec.fit(x_train,y_train)print("准确率为：",dec.score(x_test,y_test))#图形化export_graphviz(dec,out_file="./tree.dot",feature_names=["age","pclass=1st","pclass=2nd","pclass=3rd","female","male"]) 123456789101112131415161718192021222324#回归树import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltrng = np.random.RandomState(1)X = np.sort(5 * rng.rand(80,1), axis=0)y = np.sin(X).ravel()y[::5] += 3 * (0.5 - rng.rand(16))regr_1 = DecisionTreeRegressor(max_depth=2)regr_2 = DecisionTreeRegressor(max_depth=5)regr_1.fit(X, y)regr_2.fit(X, y)X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)plt.figure()plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
        <category>决策树</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[分类算法—朴素贝叶斯关键概念 ​ 联合概率：X取值为x和Y取值为y两个事件同时发生的概率，表示为P(X=x,Y=y) ​ 条件概率：在X取值为x的前提下，Y取值为y的概率。表示为P(Y=y|X=x) 这里的C代表类别，W代表特征。 我们学习的其它分类算法总是有一个特点：这些算法先从训练集中学习，获取某种信息来建立模型，然后用模型去对测试集进行预测。比如逻辑回归，我们要先从训练集中获取让损失函数最小的参数，然后用参数建立模型，再对测试集进行预测。在比如支持向量机，我们要先从训练集中获取让边际最大的决策边界，然后 用决策边界对测试集进行预测。相同的流程在决策树，随机森林中也出现，我们在ﬁt的时候必然已经构造好了能够让对测试集进行判断的模型。而朴素贝叶斯，似乎没有这个过程。这说明，朴素贝叶斯是一个不建模的算法。以往我们学的不建模算法，比如KMeans，比如PCA，都是无监督学习，而朴素贝叶斯是第一个有监督的，不建模的分类算法。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类。 sklearn中，基于高斯分布、伯努利分布、多项式分布为我们提供了四个朴素贝叶斯的分类器 sklearnnaive_bayes.GaussianNB(priors=None,var_smoothing=1e-09):高斯朴素贝叶斯，通过假设P(Xi|Y)是服从高斯分布(正态分布)，估计每个特征下每个类别上的条件概率。对于每个特征下的取值，有以下公式： 1234567891011121314from sklearn.naive_bayes import GaussianNBfrom sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitdigits = load_digits()X, y = digits.data, digits.targetXtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420)gnb = GaussianNB().fit(Xtrain,Ytrain)#查看分数acc_score = gnb.score(Xtest,Ytest)#查看预测结果Y_pred = gnb.predict(Xtest)#查看预测的概率结果prob = gnb.predict_proba(Xtest)print(prob)]]></content>
      <categories>
        <category>机器学习</category>
        <category>朴素贝叶斯</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2019%2F09%2F07%2Fk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[分类算法-k近邻算法(KNN)：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 如何求距离： k值取很小，容易受异常点影响 k值取很大,容易受k值数量的波动]]></content>
      <categories>
        <category>机器学习</category>
        <category>k-近邻算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读]]></title>
    <url>%2F2019%2F09%2F05%2FSVM%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[​ 支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。 支持向量机原理的三层理解： 支持向量机所做的事情其实非常容易理解，看下图： 上图是一组两种标签的数据，两种标签分别用圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在位置数据集上的分类误差(泛化误差)尽量小。 关键概念： ​ 超平面：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。 ​ 决策边界：在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就 说这个超平面是数据的“决策边界”。 决策边界一侧的所有点在分类为属于一个类，而另一个类的所有店分类属于另一个类。比如上面的数据集，我们很容易的在方块和圆的中间画一条线并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。 所以，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。 但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有B1和B2两条可能的决策边界。我们可以把决策边界B1向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到b11和b12的距离相等。在b11和b12中间的距离，叫做 这条决策边界的边际(margin)，通常记作d。对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。 接着我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误认成了圆，有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，拥有更大边际的决策边界在分类中的泛化误差更小，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。 结论：支持向量机，就是通过找出边际最多的决策边界，来对数据进行分类的分类器。 我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决 策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为： 我们将此表达式变换一下： 其中[a, -1]就是我们的参数向量 ， 就是我们的特征向量， 是我们的截距。 我们要求解参数向量w和截距b 。如果在决策边界上任意取两个点Xa，Xb，并带入决策边界的表达式，则有： 将两式相减，得： Xa和Xb是一条直线上的两个点，相减后的得到的向量方向是由Xb指向Xa，所以Xa-Xb的方向是平行于他们所在的直线——我们的决策边界的。而w与Xa-Xb相互垂直，所以参数向量w方向必然是垂直于我们的决策边界。 此时，我们有了我们得决策边界，任意一个紫色得点Xp就可以表示为： 由于紫色的点所代表的标签y是1，所以我们规定，p&gt;0。 同样的，对于任意一个红色的点Xr而言，我们可以将它表示为： 由于红色点所表示的标签y是-1，所以我们规定，r&lt;0 由上面得知：如果我们有新的测试数据Xt，则Xt的标签就可以根据以下式子来判定： 注意：p和r的符号是我们人为规定的 两类数据中距离我们的决策边界 最近的点，这些点就被称为支持向量。 紫色类的点为Xp ，红色类的点为Xr，则我们可以得到： 两个式子相减，得： 如下图所示： (Xp-Xr)可表示为两点之间的连线，而我们的边际d是平行于w的，所以我们现在，相当于是得到 了三角型中的斜边，并且知道一条直角边的方向。在线性代数中，向量有这样的性质：向量a除以向量b的模长||b|| ，可以得到向量a在向量b的方向上的投影的长度。所以，我们另上述式子两边同时除以||w||，则可以得到 最大边界所对应的决策边界，那问题就简单了，要最大化d，就求解w的最小值。极值问题可以相互转化，我们可以把求解w的最小值转化为，求解以下函数的最小值： 之所以要在模长上加上平方，是因为模长的本质是一个距离，所以它是一个带根号的存在，我们对它取平方，是为了消除根号。 我们得到我们SVM的损失函数： 至此，SVM的第一层理解就完成了。 用拉格朗日对偶函数求解线性SVM 当然，不是所有数据都是线性可分的，不是所有数据我们都能够一眼看出，有一条直线，或一个平面，甚至一个超平面可以将数据完全分开。比如下面的环形数据。对于这样的数据，我们需要对它进行一个升维变化，将数据从原始的空间x投射到新空间F(x)中。升维之后，我们明显可以找出一个平面，能够将数据切分开来。 F是一个映射函数，它代表了某种能够将数据升维的非线性的变换，我们对数据进行这样的变换，确保数据在自己的空间中一定能够线性可分。 这种变换非常巧妙，但也带有一些实现问题。 首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。而解决这些问题的是核函数。 关键概念：核函数是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
        <category>SVM解读</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库概述]]></title>
    <url>%2F2019%2F09%2F04%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[​ 大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。 数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。 数据库数据具有永久存储、有组织、可共享三个基本特点。 数据库系统(DBS)由计算机硬件、数据库、数据库管理系统、应用软件和数据库管理系统组成。 数据库设计时面向数据模型对象、数据库系统的数据冗余度小、数据共享度高、数据库系统的数据和程序之间具有较高的独立性、数据库中数据的最小存取单位是数据项、数据库系统通过DBMS进行数据安全性、完整性、并发控制和数据恢复控制。 数据库数据物理独立性高是指当数据的物理结构（存储结构）发生变化时，应用程序不需要修改也可以正常工作 数据库数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行 DBMS的功能结构 ​ 数据定义功能：能够提供数据定义语言(DDL)和相应的建库机制。用户利用DDL可以方便建立数据库。 ​ 数据操纵功能：实现数据的插入、修改、删除、查询、统计等数据存取操作的功能称为数据操纵功能。数据库管理系统通过提供数据操纵语言(DML)实现其数据操纵功能。 ​ 运行管理功能：包括并发控制、数据的存取控制、数据完整性条件的检查和执行、数据库内部的维护等 ​ 建立和维护功能：指数据的载入、转储、重组织功能及数据库的恢复功能，指数据库结构的修改、变更及扩充功能。 从数据库管理系统的角度查看，数据库系统通常采用外模式、模式、内模式三级模式结构。从数据库最终用户的角度看，数据库系统的就够分为单用户结构、主从结构、分布式结构、客户/服务器结构。 外模式：也称为子模式或用户模式。它是数据库用户看见和使用的、对局部数据的逻辑结构和特征的描述，是与某一应用有关的数据的逻辑表示。 模式：模式是数据库中数据的逻辑结构和特征的描述。它仅仅涉及到型的描述，不涉及到具体的值。模式的具体值被称为模式的一个实例。同一模式可以有很多实例。模式是相对稳定的，实例是相对变动的。模式反映的是数据的结构及其关系，而实例反映的是数据某一时刻的状态。 内模式：也称存储模式，它是对数据的物理结构和存储结构的描述，是数据在数据库内部的表示方式。 为了能够在内部实现这三个抽象层次的联系和转换，数据库系统在这三级模式之间提供了两层映像：外模式/概念模式映像、概念模式/内模式映像。这两层映像保证了数据库系统中的数据能够具有较高的逻辑独立性和物理独立性。 外模式/模式映像保证了数据与程序间的逻辑独立性，模式/内模式映像保证了数据的物理独立性。 逻辑独立性：当数据库的整体逻辑结构发生变化时，通过调整外模式和模式之间的映像，使得外模式中的局部数据及其结构不变，程序不在修改。 物理独立性：当数据库的存储结构发生变化时，通过调整模式和内模式之间的映像，使得整体模式不变，当然外模式及应用程序不用改变。]]></content>
      <categories>
        <category>数据库</category>
        <category>数据库概述</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数以及作用]]></title>
    <url>%2F2019%2F09%2F03%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[本片博客介绍以下激活函数以及激活函数的作用。 首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。 激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。 问题一：为什么我们不能在不激活输入信号的情况下完成此操作呢？ 如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。 问题二：那么为什么我们需要非线性函数？ 非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。 接下来介绍一下常用的激活函数：sigmoid、tanh、Relu sigmoid：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： 缺点：当 zz 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z)将接近 00 。这会导致权重 W 的梯度将接近 00 ，使得梯度更新十分缓慢，即梯度消失 tanh：该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1) 之间，其公式与图形为： tanh函数的缺点同sigmoid函数的第一个缺点一样，当 zz 很大或很小时，g′(z)接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 Relu函数： Leaky Relu：这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： 其中a取值在（0，1）之间]]></content>
      <categories>
        <category>深度学习</category>
        <category>激活函数</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD实战-tensorflow实现目标检测]]></title>
    <url>%2F2019%2F09%2F03%2FSSD%E5%AE%9E%E6%88%98-tensorflow%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[​ 因为caffe的环境实在是难搭，所以将实战的框架改为tensorflow]]></content>
      <categories>
        <category>目标检测</category>
        <category>one-stage</category>
        <category>SSD</category>
        <category>SSD实战</category>
      </categories>
      <tags>
        <tag>SSD实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸检测数据集构造]]></title>
    <url>%2F2019%2F08%2F28%2F%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E9%80%A0%2F</url>
    <content type="text"><![CDATA[在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。 Caffe-SSD数据集构造流程： 1、生成VOC个数数据集（图片、XML标注信息文件） 2、修改Caffe-SSD数据打包脚本相关路径配置 3、运行Caffe-SSD数据打包脚本 VOC格式数据集的目录下有三个文件夹： Annotation中保存的是xml格式的label信息 ImageSet中Main目录中存放不同照片列表文件 ​ train.txt：训练图片文件名列表 ​ val.txt：验证图片文件名列表 ​ trianval.txt：训练和验证的图片文件名列表 ​ test.txt：测试图片文件名列表 JPEGImages目录存放所有的图片集。Annotation和JPEGImages一一对应。 WIDERFace数据集：打开人脸检测数据集地址下载数据集。 下载数据的训练集、验证集、测试集和标注信息。 打开标注信息，可以看到文件夹中有很多txt文件，打开看一下： **** 对于VOC格式的数据集，最主要的是生成两个文件。一个是图片的标注数据，另一个是图片数据。对于图片数据，我们只需要将wider_face的图片放到对应目录下，而对于标注数据，我们需要将上面图片的文件进行解析来重新生成针对每张图片的标注信息，这个信息用XML格式存储。 对于wider_face转为voc，首先创建Annotation、ImageSets、JPEGImages三个文件夹，其中ImageSets中再创建一个Main文件夹。 将wider_face转为voc的coding： import os,cv2,sys,shutil from xml.dom.minidom import Document def writexml(filename,saveing,bboxes,xmlpath): doc=Document() annotation=doc.createElement(&quot;annotation&quot;) doc.appendChild(annotation) folder=doc.createElement(&quot;folder&quot;) folder_name=doc.createTextNode(&quot;widerface&quot;) folder.appendChild(folder_name) annotation.appendChild(folder) filenamenode=doc.createElement(&quot;filename&quot;) filename_name=doc.createTextNode(filename) filenamenode.appendChild(filename_name) annotation.appendChild(filenamenode) source=doc.createElement(&quot;source&quot;) annotation.appendChild(source) database=doc.createElement(&quot;database&quot;) database.appendChild(doc.createTextNode(&quot;wider face database&quot;)) source.appendChild(database) annotation_s=doc.createElement(&quot;annotation&quot;) annotation_s.appendChild(doc.createTextNode(&quot;PASCAL VOC2007&quot;)) source.appendChild(annotation_s) image=doc.createElement(&quot;image&quot;) image.appendChild(doc.createTextNode(&quot;flickr&quot;)) source.appendChild(image) flickrid=doc.createElement(&quot;flickrid&quot;) source.appendChild(doc.createTextNode(&quot;-1&quot;)) source.appendChild(flickrid) owner=doc.createElement(&quot;owner&quot;) annotation.appendChild(owner) flickrid_o=doc.createElement(&quot;flickrid&quot;) flickrid_o.appendChild(doc.createTextNode(&quot;yanyu&quot;)) owner.appendChild(flickrid_o) name_o=doc.createElement(&quot;name&quot;) name_o.appendChild(doc.createTextNode(&quot;yanyu&quot;)) owner.appendChild(name_o) size=doc.createElement(&quot;size&quot;) annotation.appendChild(size) width=doc.createElement(&quot;width&quot;) width.appendChild(doc.createTextNode(str(saveing.shape[1]))) height=doc.createElement(&quot;height&quot;) height.appendChild(doc.createTextNode(str(saveing.shape[0]))) depth=doc.createElement(&quot;depth&quot;) depth.appendChild(doc.createTextNode(str(saveing.shape[2]))) size.appendChild(width) size.appendChild(height) size.appendChild(depth) segmented=doc.createElement(&quot;segmented&quot;) segmented.appendChild(doc.createTextNode(&quot;0&quot;)) annotation.appendChild(segmented) for i in range(len(bboxes)): bbox=bboxes[i] objects=doc.createElement(&quot;object&quot;) annotation.appendChild(objects) object_name=doc.createElement(&quot;name&quot;) object_name.appendChild(doc.createTextNode(&quot;face&quot;)) objects.appendChild(object_name) pose=doc.createElement(&quot;pose&quot;) pose.appendChild(doc.createTextNode(&quot;Unspecified&quot;)) objects.appendChild(pose) truncated=doc.createElement(&quot;truncated&quot;) truncated.appendChild(doc.createTextNode(&quot;1&quot;)) objects.appendChild(truncated) difficult=doc.createElement(&quot;difficult&quot;) difficult.appendChild(difficult) difficult.appendChild(doc.createTextNode(&quot;0&quot;)) objects.appendChild(difficult) bndbox=doc.createElement(&quot;bndbox&quot;) objects.appendChild(bndbox) xmin=doc.createElement(&quot;xmin&quot;) xmin.appendChild(doc.createTextNode(str(bbox[0]))) bndbox.appendChild(xmin) ymin=doc.createElement(&quot;ymin&quot;) ymin.appendChild(doc.createTextNode(str(bbox[1]))) bndbox.appendChild(ymin) xmax=doc.createElement(&quot;xmax&quot;) xmax.appendChild(doc.createTextNode(str(bbox[0]+bbox[2]))) bndbox.appendChild(xmax) ymax=doc.createElement(&quot;ymax&quot;) ymax.appendChild(doc.createTextNode(str(bbox[1]+bbox[3]))) bndbox.appendChild(ymax) f=open(xmlpath,&quot;w&quot;) f.write(doc.toprettyxml(indent=&quot; &quot;)) f.close() rootdir=&quot;./wider_face&quot; def convertimgset(img_set): imgdir=rootdir+&quot;/WIDER_&quot;+img_set+&quot;/images&quot; gtfilepath=rootdir+&quot;/wider_face_split/wider_face_&quot;+img_set+&quot;_bbx_gt.txt&quot; fwrite=open(rootdir+&quot;/ImageSets/Main&quot;+img_set+&quot;.txt&quot;,&quot;w&quot;) index=0 with open(gtfilepath,&quot;r&quot;) as gtfiles: while(index&lt;1000): filename=gtfiles.readline()[:-1] if (filename==&quot;&quot;): continue imgpath=imgdir+&quot;/&quot;+filename img=cv2.imread(imgpath) if not img.data: break numbbox=int(gtfiles.readline()) bboxes=[] for i in range(numbbox): line=gtfiles.readline() lines=line.split() lines=lines[0:4] bbox=(int(lines[0]),int(lines[1]),int(lines[2]),int(lines[3])) bboxes.append(bbox) filename=filename.replace(&quot;/&quot;,&quot; &quot;) if len(bboxes)==0: print(&quot;no face&quot;) cv2.imwrite(&quot;{}/JPEGImages/{}&quot;.format(rootdir,filename),img) fwrite.write(filename.split(&quot;.&quot;)[0]+&quot;\n&quot;) xmlpath=&quot;{}/Annotations/{}&quot;.format(rootdir,filename.split(&quot;.&quot;)[0]) writexml(filename,img,bboxes,xmlpath) print(&quot;第%d张成功的图片&quot; % index) index+=1 fwrite.close() if __name__==&quot;__main__&quot;: img_sets=[&quot;train&quot;,&quot;val&quot;] for img_set in img_sets: convertimgset(img_set) shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;train.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;trainval.txt&quot;) shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;val.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;test.txt&quot;) 这篇博客废了。。。。caffe的环境太难搭了。。。。 我将框架换成tensorflow，进行另一个实战]]></content>
      <categories>
        <category>目标检测</category>
        <category>one—stage</category>
        <category>SSD</category>
        <category>SSD实战</category>
        <category>构造数据集</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD实战-人脸检测]]></title>
    <url>%2F2019%2F08%2F28%2FSSD%E5%AE%9E%E6%88%98-%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[人脸标注方法：矩形标注和椭圆形标注 矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、椭圆圆心x坐标、椭圆圆心y坐标。 判断算法性能好坏： 每一个标记只允许有一个检测与之相对应，也就是说，我们检测出来的每一个人脸图像只能同我们在标注中人脸图像中数据中的一个相对应。如果有多个，就会视为重复检测。重复检测会被视为错误检测。接着我们就可以得出正确率和错误率，可以画出ROC曲线和PR曲线。可以根据曲线看出算法的好坏。 人脸采集常用方法： 活体检测 判断用户是否为正常操作，通过指定用户做随机动作，一搬有张嘴、摇头、点头、凝视、眨眼等等，防止照片攻击。判断用户是否真实在操作，指定用户上下移动手机，防止视频攻击和非正常动作的攻击。 3D检测 验证采集到的是否为立体人像，能够防止平面照片、不同弯曲程度的照片等。 连续检测 通过连续的检测，验证运动轨迹是否正常，防止跳过活体检测直接替换采集的照片，也能防止中途换人。]]></content>
      <categories>
        <category>目标检测</category>
        <category>one-stage</category>
        <category>SSD</category>
        <category>SSD实战</category>
        <category>人脸检测综述</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测常用名词]]></title>
    <url>%2F2019%2F08%2F22%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[图像分类：一张图像中是否包含某种物体 物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。 语义分割：按对象得内容进行图像得分割，分割的依据是内容，即对象类别。 实例分割：按对象个体进行分割，分割的依据是单个目标。 滑动窗口——为什么要有候选区域？既然目标是在图像中的某一区域，那么最直接的方法就是滑窗法（sliding window approach），就是遍历图像的所有区域，用不同大小的窗口在整个图像上滑动，那么就会产生所有的矩形区域，然后再后续排查，思路很简单，但是开销巨大。 region proposal（RP）：候选区域 IOU：region proposal与Ground Truth的窗口的交集比并集的比值，相当于准确率。‘ SPP：Spatial Pyramid Pooling 空间金字塔采样 在pooing的过程中计算pooling后的结果对应的两个像素点映射到feature map上所占的范围，然后在那个范围中进行max或者average。 ROI Pooling：就是将一个个大小不同的box矩形框，都映射到大小为w*h的矩形框。 Anchor：请看Two-stage基本介绍 GT box:Ground Truth box 如上图所示，绿色的框为飞机的Ground Truth，红色的框是提取的Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU&lt;0.5)，那么这张图相当于没有正确的检测出飞机。如果我们能对红色的框进行微调，使得经过微调后的窗口跟Ground Truth更接近，这样岂不是定位会更准确。 带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。 如下图所示，(a)是普通的3×33×3卷积，其视野就是3×33×3，(b)是扩张率为2，此时视野变成7×77×7，(c)扩张率为4时，视野扩大为15×1515×15，但是视野的特征更稀疏了。 后面遇见会继续完善。。。。。]]></content>
      <categories>
        <category>目标检测</category>
        <category>常用名词解释</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD系列算法]]></title>
    <url>%2F2019%2F08%2F21%2FSSD%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[主干网络介绍 主干网络原始作者采用的VGG16，我们也可以将其他神经网络作为主干网络。例如：ResNet、MobileNets等 输入300300的image，将VGG16网络FC6、FC7换成conv6和conv7，同时将池化层变为stride=1，pool_size=3\3，这样做的目的是为了不减少feature map size，为了配合这种变化，conv6会使用扩张率为6的带孔卷积。 ​ 带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。具体看目标检测常用名词 接着然后移除dropout层和fc8层，并新增conv7，conv8，conv9，conv10，conv11，在检测数据集上做finetuing。其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是38×38，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet），以保证和后面的检测层差异不是很大，这个和Batch Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma。 多尺度feature map预测多尺度feature map预测，也就是在预测的时候，在接下来预测的时候，针对接下来六个不同的尺寸进行预测。如下图的六条连线，分别是38*38、19*19、10*10、5*5、3*3、1*1。将这六个不同尺度的feature map分别作为检测、预测层的输入，最后通过NMS进行筛选和合并。 对于六种不同尺度的网络，我们通常使用pooling来降采样。对于每一层的feature map，我们输入到相应的预测网络中。而预测网络中，我们会包括Prior box的提取过程。Prior box对应Fast R-CNN中Anchor的概念，也就是说，在Prior box中，feature map上的每一个点都作为一个cell（相当于Anchor）。以这个cell为中心，按照等比的放缩，找到它在原始图片的位置。接着以这个点为中心，提取不同尺度bounding box。而这些不同尺度的bounding box就是Prior box。然后对于每一个Prior box，我们通过和真值比较，就能够拿到它的label。对于每一个Prior box，我们都会分别预测它的类别概率和坐标（x，y，w，h）。也就是说，对于每一个cell，我们会将它对应到不同的Prior box，分别来预测当前这个Prior box所对应当前这个类别的概率分布和坐标。 对Prior Box的具体定义： 这里我们假设Prior Box的输入是m*n维的feature map。 如果每一个点都作为cell，那就会有mn个cell。接着每个cell上生成固定尺寸和不同长宽比例的box。每个cell对应k个bounging box，每个bounding box预测c个类别分数和4个偏移坐标。其中c个类别分数实际上是当前bounding box所对应的不同类别的概率分布。如果输入大小为m\n，那就会输出(c+4)*k*m*n。其中尺寸(scale)和比例(ratio)是超参数。 接下来我们看看Prior box是怎么生成的： 每个feature map上的点定义了六种长宽比的default box。也就是说，最后对于每一个anchor都会获得六个不同尺寸和长宽比的default box。对于3838层，每个feature map上的点，我们都会提取4个default box作为prior box。对于19\19层、10*10层、5*5，提取6个default box也就是全部都是prior box。而3*3、1*1提取4个default作为prior box。所以最后得到8732个prior box(38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4)。prior box就是选择的default box。尺寸和比例都是可以通过SSD的配置文件进行配置，后面实战详解。 对default box进行筛选成为prior box：每一个feature map cell不是k个default box都取，prior box与GT box(Ground Truth box)做匹配，IOU&gt;阈值为正样本。IOU&lt;阈值 为负样本。介于正样本和负样本中间阈值的default box去掉。 SSD系列算法优化及扩展SSD算法对小目标不够鲁棒，原因最主要是浅层feature map的表征能力不够强。 DSSD： DSSD相当原来的SSD模型主要作了两大更新。一是替换掉VGG，而改用了Resnet-101作为特征提取网络并在对不同尺度feature maps特征进行default boxes检测时使用了更新的检测单元；二则在网络的后端使用了多个deconvolution layers以有效地扩展低维度信息的contextual information，从而有效地提高了小尺度目标的检测。 下图为DSSD模型与SSD模型的整体网络结构对比： DSOD： SSD+DenseNet=DSOD DSOD可以从0开始训练数据，不需要预训练模型。 FSSD： 借鉴了FPN的思想，重构了一组pyramid feature map（金字塔特征），使得算法的精度有了明显特征，速度也没有下降很多。具体是把网络中某些feature调整为同一size再contact（连接），得到一个像素层，以此层为base layer来生成pyramid feature map，作者称之为Feature Fusion Module。 Feature Fusion 对上面图的解读： ​ (a) image pyramid ​ (b) rcnn系列，只在最后一层feature预测 ​ (c) FPN，语义信息一层传递回去，而且有很多相加的计算 ​ (d) SSD，在各个level的feature上直接预测，每个level之间没联系 ​ (e) FSSD的做法，把各个level的feature concat，然后从fusion feature上生成feature pyramid FSSD网络结构： RSSD： rainbow concatenation方式(pooling加deconvolution)融合不同层的特征，再增加不同层之间feature map关系的同时也增加了不同层的feature map个数。这种融合方式不仅解决了传统SSD算法存在的重复框问题，同时一定程度上解决了small object的检测问题。]]></content>
      <categories>
        <category>目标检测</category>
        <category>one-stage</category>
        <category>SSD</category>
        <category>SSD原理</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One-stage基本介绍]]></title>
    <url>%2F2019%2F08%2F20%2FOne-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ One-stage也是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。 One-stage常见算法： One-stage核心组件： CNN网络 CNN网络设计原则：从简到繁到简的卷积神经网 多尺度特征融合的网络 更轻量级的CNN网络 回归网络 One-stage和Two-stage的区别在于是否存在RPN网络。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
        <category>One-stage基本介绍</category>
      </categories>
      <tags>
        <tag>One-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Two-stage基本介绍]]></title>
    <url>%2F2019%2F08%2F18%2FTwo-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。 Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。 Two-stage核心组件： CNN网络 CNN网络设计原则： 从简到繁再到简的卷积神经网 多尺度特征融合的网络 更轻量级的CNN网络 RPN网络 区域推荐（Anchor机制）：首先我们需要知道anchor的本质是什么，本质是SPP(spatial pyramid pooling)思想的逆向。而SPP本身是做什么的呢，就是将不同尺寸的输入resize成为相同尺寸的输出。所以SPP的逆向就是，将相同尺寸的输出，倒推得到不同尺寸的输入。当前的feature map的大小是n*c*w*h（n是张数，c是层数，w是宽，h是高），对于当前这个feature map上，也就是wh上，选择其中的每一个点作为锚点（也就是候选区域的中心点），我们以每一个点作为中心点去提取候选区域。这样的每一个点都是Anchor。而这个候选区域通常都有比例：对于Fast R-CNN三个面积尺寸（128^2，256^2，512^2），然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）。一个feature map可以提取w\h*9个候选区域。我们示意图如下： ROI Pooling 分类和回归]]></content>
      <categories>
        <category>目标检测</category>
        <category>Two-stage算法</category>
        <category>Two-stage基本介绍</category>
      </categories>
      <tags>
        <tag>Two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMS算法]]></title>
    <url>%2F2019%2F08%2F18%2FNMS%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ NMS全称是非极大值抑制算法。 目的：为了消除多余的框，找到最佳的物体检测的位置。 思想:选取那些邻域里分数最高的窗口，同时抑制那些分数低的窗口。 其实NMS的处理不太合理。所以有人提出了Soft-NMS。 NMS和Soft-NMS的区别： 相邻区域内的检测框的分数进行调整而非彻底抑制，从而提高了高检索率情况下的准确率。再低检索率时仍能对物体检测性能有明显提升。]]></content>
      <categories>
        <category>目标检测</category>
        <category>NMS算法</category>
      </categories>
      <tags>
        <tag>目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DPM算法]]></title>
    <url>%2F2019%2F08%2F18%2FDPM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ DPM算法是传统目标检测方法的巅峰。 步骤： 1、计算DPM特征图 2、计算响应图 3、Latent SVM分类器训练 4、检测识别]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测</category>
        <category>DPM算法</category>
      </categories>
      <tags>
        <tag>传统目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HOG+SVM算法]]></title>
    <url>%2F2019%2F08%2F18%2FHOG-SVM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[HOG+SVM算法主要用于行人检测。 步骤： 1、提取HOG特征。如果彩色图需要用HOG特征，则需要先转化为灰度图。 2、训练SVM分类器 ３、利用滑动窗口提取目标区域，进行分类判断 4、NMS 5、输出检测结果]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
        <category>HOG+SVM</category>
      </categories>
      <tags>
        <tag>传统目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VJ算法]]></title>
    <url>%2F2019%2F08%2F18%2FVJ%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ VJ算法全称为Viola—Jones，多用于人脸检测。 步骤： 1、Haar特征抽取 Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。 2、训练人脸分类器（Adaboost算法等） 3、滑动窗口]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
        <category>VJ算法</category>
      </categories>
      <tags>
        <tag>传统目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql基础命令]]></title>
    <url>%2F2019%2F08%2F13%2Fmysql%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[sql语句最后需要以；号结尾，sql语句最后需要以；号结尾，sql语句最后需要以；号结尾。重要的事说三遍。 数据库操作select version()：显示数据库版本 select now()：显示时间 show databases：查看所有数据库 show create database 数据库名：查看创建数据库的语句 create database 数据库名字：创建数据库 create database 数据库名字 charset=utf8：创建指定编码格式的数据库 drop database 数据库名：删除数据库 use 数据库名：使用数据库 select database ()：查看当前使用的数据库 数据表操作show tables:查看当前数据库中所有表 drop table 表名：删除表 show create table 表名：查看创建表的语句 create table 表名（字段 类型 约束[，字段 类型 约束]）：创建表 参数：auto_increment表示自动增长、not null表示不能为空、primary key表示主键、default 默认值 desc 表名：查看表的状态 insert into 表名 values()：向表插入数据，按照参数类型写参数 select * from 表名：查看表中所有的数据 alter table 表名 add 列名 类型：向表中添加字段 alter table 表名 modify 列名 类型 约束：不重命列名版 alter table 表名 change 列的原名 列的新名 类型 约束 ：重命列名版 alter table 表名 drop 列名：删除字段 数据的增删改查增加全列插入：insert into 表名 values(数据) 主键字段：可以用0 null default 来占位。因为auto_increment是自动增加的 部分插入：insert into 表名(列名1) values(值1) 没有的值取默认值 多行插入：insert into 表名 values (数据1)，(数据2) update 表名 set 列名 ：整列都改 update 表名 set 列名 where 条件：根据条件改 通过其他表来更新一个表： update 其他表 as 新名 inner join 被更新的表 as 新名 on 条件 set 需要改的数据 查询：select * from 表名：查询整个表，*号代表全部 select * from 表名 where ：根据条件查询 select 查询的列名 from 表名：根据列名查询。查询多列时，列名间用,隔开 select 查询的列名 as 列的新名字 from 表名：将列查询后以新的名字显示出来 select 表名.列名 from 表名 select 表的新名字.列名 from 表名 as 表的新名字 select distinct 列名 from 表名：可以去重，只显示相同数据第一次数据出现的位置 条件查询：and、or、not都可以用，类似python的语法。判断是否为空，is NULL。 模糊查询：like ：%代表一个或多个，_代表一个。 select name from 表名 where name like “%小%”：查询名字中有”小”字的名字 select name from 表名 where name like “__”：查询两个字的名字 select name from 表名 where name like “__%”：查询两个字以上的名字 rlike：利用正则查询 范围查询：in表示在一个非连续的范围内 between 数字 and 数字表示在一个连续的范围内 排序：order by 默认从小到大排序asc：从小到大排 desc：从大到小排 如果排序字段相同，我们可以设置多个排序字段。若不设置，默认按照主键大小排。 聚合函数(count，max，min，avg，sum，round)：不能跟其他字段一起用count：计算个数，其他类似 max：计算最大的 min：计算最小的 avg：计算平均值 sum：求和 round：四舍五入 ，round(123.23,1)保留一位小数=123.2 分组：group by按照性别分组： 配合聚合函数使用计算每种性别的人数： 计算男性的人数： group_concat():查询同一组的其他字段 还可以用字符串分割 having：将达到条件的组输出，可以配合聚合函数一起使用 having和where的区别：having是在分组后进行筛选，where是在原表的基础上进行筛选 分页：limit start，count 限制查询出来的数据个数。start代表从哪开始，count 代表查询的数据个数。start默认为0 链接查询：内链接、左链接、右链接内链接：inner join ……on 在两个表中取交集，如果存在则将两表数据合并。不存在则跳过。 可以利用as化简语句： 可以根据需求修改需要显示的数据： 还可以修改数据显示的位置： 通过某个表的字段排序： 左链接：left join……on。谁在左边，谁就是左表。查询的结果为两个表匹配到的数据，左表特有的数据，对于右表不存在的数据使用null填充 将左表特有的数据提取出来： 右链接：right join……on：类似左表。可以直接将右链接的表中左表和右表的显示交换即可得右链接。自关联：补。。。。 子查询：查询里嵌套一个查询 删除删除分为物理删除和逻辑删除： 物理删除： delete from 表名:删除整个表 delete from 表名 where 条件：删除符合条件的数据 逻辑删除：（用一个字段表示，这条信息是否还能用） alter table 表名 add 字段 类型 default 默认值]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
        <category>基础命令</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络知识散记]]></title>
    <url>%2F2019%2F08%2F13%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[端口端口是英文port的意译，可以认为是设备与外界通讯交流的出口。端口可分为虚拟端口和物理端口，其中虚拟端口指计算机内部或交换机路由器内的端口，不可见。例如计算机中的80端口、21端口、23端口等。物理端口又称为接口，是可见端口，计算机背板的RJ45网口，交换机路由器集线器等RJ45端口。电话使用RJ11插口也属于物理端口的范畴。（用来区分哪个进程） 同一台电脑用pid区分进程，不同电脑用端口区分进程。 端口范围是0到65535 知名端口是众所周知的端口，范围是0到1023 动态端口的范围是1024到65535 查看端口状态：用netstat -an查看 TCP/IP协议TCP/IP:这不是两个协议，这是一个协议族，包含很多协议。主要是TCP/IP协议。 四层：物理层、网络层、传输层、应用层 七层：物理层、链路层、网络层、传输层、 会话层、表示层、应用层 IP地址用来标记唯一一台电脑。每一个IP地址都包括网络地址和主机地址 网络地址相同，则处于同一个网段，主机地址用来标记网里的电脑 A类网络的IP地址范围为：1.0.0.1－126.255.255.254； B类网络的IP地址范围为：128.1.0.1－191.255.255.254； C类网络的IP地址范围为：192.0.1.1－223.255.255.254 1．A类IP地址 一个A类IP地址由1字节（每个字节是8位）的网络地址和3个字节主机地址组成,即第一段数字范围为1～126。每个A类地址可连接16387064台主机(不能用0(产生冲突)和255(广播地址)，Internet有126个A类地址。 2．B类IP地址 一个B类IP地址由2个字节的网络地址和2个字节的主机地址组成，第一段数字范围为128～191。每个B类地址可连接64516(254*254)台主机(不能用0(产生冲突)和255(广播地址))，Internet有16256个B类地址。 3．C类IP地址 一个C类地址是由3个字节的网络地址和1个字节的主机地址组成，第一段数字范围为192～223。每个C类地址可连接254台主机(不能用0(产生冲突)和255(广播地址)，Internet有2054512个C类地址。 4．D类地址用于多点播送。 第一个字节的数字范围为224～239，是多点播送地址，用于多目的地信息的传输，和作为备用。全零（“0.0.0.0”）地址对应于当前主机，全“1”的IP地址（“255.255.255.255”）是当前子网的广播地址。多播和广播的区别：广播在同一个局域网都能收到，多播是指定那些人可以收得到，其他人收不到，常用于视频会议。 5.E类地址 第一段数字范围为240～254。E类地址保留，仅作实验和开发用。 全零（“0．0．0．0”）地址对应于当前主机。全“1”的IP地址（“255．255．255．255”）是当前子网的广播地址。 在IP地址3种主要类型里，各保留了3个区域作为私有地址，常见于局域网中。 ##常用术语 网络号：网络号等于ip地址和网络掩码按位与操作 网络掩码（子网掩码）的作用：取网络号、主机号 两台电脑能通信的前提是处于同一个网络号 集线器（hub）的作用：实现多台电脑连接在一起，组成一个小型局域网，交换机也是。 集线器和交换机的区别：集线器是广播发数据，交换机不是每次都是广播，效率高。 实际地址：代表网卡地址（MAC）。由六个字节组成，前三个字节代表厂商，后三个字节代表厂商生产 arp：根据ip找mac地址 rarp：根据mac地址找ip icmp：ping的时候用 arp -a即是查看本地局域网内所有用户ip和mac地址绑定关系的一个命令。 ARP -d 就是清除缓存中的数据。也是删除ip和mac绑定的项目。 路由器：连接不同的网络，使他们之间能够通信 rip：路由解析协议 mac：标记实际转发数据时的地址 ip：标记逻辑上的地址 natmask：和ip地址一起确定网络号 默认网关：发送的ip不在同一个网段内，那么会把这个数据转发给默认网关。 为什么TCP比UDP稳定？在TCP中，如果有一方接收到对方的数据，一定会发送ack确认包给对方。而在UDP中，没有这个过程。 TCP三次握手：确定一定发送数据到对方 四次挥手：调用close时使用 TCP长连接、短连接： TTL：表示经过的路由器数目。每经过一个路由器，TTL-1。 MSL：表示一个数据包存在的最多时间 CDN：内容分发 查看域名解析的IP地址： nslookup 域名 例子：nslookup baidu.com 常见的网络攻击： DDOS攻击：拒绝服务器攻击。 DNS攻击：1.DNS服务器被劫持：篡改IP ​ 2.DNS欺骗： ARP攻击：中间人攻击]]></content>
      <categories>
        <category>网络知识</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>网络知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测概述]]></title>
    <url>%2F2019%2F08%2F12%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[​ 目标检测方法分为传统目标检测方法和深度学习目标检测方法。 传统目标检测方法：Viola-Jones、HOG+SVM、DPM等 Viola-Jones：采用积分图特征，进行人脸检测 HOG+SVM：行人检测。通过HOG特征结合SVM分类器进行检测。 DPM：同样通过HOG特征，并加入许多其他额外的策略进行检测。传统目标检测最好的方法。 深度学习目标检测方法：One-state、Two-stage One-stage：YOLO和SSD系列，直接回归目标位置。 Two-stage：Faster RCNN系列，利用网络对候选区进行推荐。 目标检测问题基本流程： Viola-Jones（人脸检测）步骤 1、Haar特征抽取 2、训练人脸分类器（Adaboost算法） 3、滑动窗口 HOG+SVM（行人检测）步骤 DPM（物体检测）步骤 NMS（非极大值抑制算法） 目的：为了消除多余的框，找到最佳的物体检测的位置 Soft-NMS是对NMS算法的改进]]></content>
      <categories>
        <category>目标检测</category>
        <category>目标检测概述</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv颜色识别]]></title>
    <url>%2F2019%2F08%2F12%2Fopencv%E9%A2%9C%E8%89%B2%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[数字图像处理中常用的采用模型是RGB（红，绿，蓝）模型和HSV（色调，饱和度，亮度），RGB广泛应用于彩色监视器和彩色视频摄像机，我们平时的图片一般都是RGB模型。而HSV模型更符合人描述和解释颜色的方式，HSV的彩色描述对人来说是自然且非常直观的。 这里的颜色识别是指根据人们的意愿提取图片中对应的颜色区域。 颜色识别步骤： 1、读取一张图片或视频. 2、用cvtcolor将它从RGB转为HSV。 3、通过inrange得出掩膜。 4、用 图像的”与”操作(bitwise_and)得出对应区域的图像。 coding： 读者可以加一些其他的操作提高效果，例如什么开运算、滤波之类的。这里就不赘述了，接下来，贴一张HSV的颜色阈值表，可以参考：]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>颜色识别</category>
      </categories>
      <tags>
        <tag>颜色识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魔法方法]]></title>
    <url>%2F2019%2F08%2F10%2F%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ 魔法方法是指Python内部已经包含的，被双下划线所包围的方法，这些方法在进行特定的操作时会自动被调用。使用Python的魔法方法可以是Python的自由度变得更高，当不需要重写魔法方法也可以在规定的默认情况下生效。在需要重写时也可以让使用者根据自己的需求来重写部分方法来达到自己的期待。 常用的魔法方法：__doc__：表示类的描述信息 __module__:表示当前操作的对象在哪个模块 __class__:表示当前操作的对象的类是什么 __call__：让对象直接调用call方法 __dict__：类或对象的所有属性 __getitem__、__setitem__、__delitem__： 魔法方法集合]]></content>
      <categories>
        <category>python</category>
        <category>魔法方法</category>
      </categories>
      <tags>
        <tag>魔法方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（3）]]></title>
    <url>%2F2019%2F08%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 感知机：有n个输入数据，通过权重与各数据之间的计算和，比较激活函数结果，得出输出。感知机是解决分类问题。 神经网络的基本组成包括输入层、隐藏层、输出层。卷积神经网络的特点在于隐藏层分为卷积层和池化层。 神经网络的种类： 神经网络的策略是交叉熵损失，优化是通过反向传播算法（相当于梯度下降 ）。 简单神经网络： 卷积神经网络： 卷积层：通过在原始图像上平移来提取特征 零填充方式：SAME和VALID。 当填充方式为SAME时：全零填充，无法取整时，向上取值 当填充方式为VALID时，不使用全零填充，无法取整时，向上取值 卷积向下取整，池化向上取整 例：当卷积层100个filter，5*5，步长为1，零填充是2时。输入[28,28,1]大小的图像，输出[26,26,100]的图像。 池化层：通过特征后稀疏参数来减少学习的参数，降低网络的复杂度（最大池化和平均值化）]]></content>
      <categories>
        <category>深度学习</category>
        <category>深度学习（3）</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（2）]]></title>
    <url>%2F2019%2F08%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[数据读取通过线程和队列提高速度。可以边取数据边训练 模型的保存和加载 模型保存： 模型加载： 添加权重参数、损失值等的变化 首先收集变量： 而后在会话中运行：]]></content>
      <categories>
        <category>深度学习</category>
        <category>深度学习（2）</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习（1）]]></title>
    <url>%2F2019%2F08%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[tensorflow基本知识在tensorflow中把数据称为张量（tensor）。 张量的阶：相当于数组的维度。 张量的属性：graph、op、name、shape tensorflow中张量形状分为动态形状和静态形状，其在于有没有生成一个新的张量数据。静态形状的修改不能跨维度修改 把操作称为节点（OP），所有操作都是一个OP。 整个程序的结构称为图（graph） 运算程序的图称为会话（Session）。一次只能运行一个图 会话的作用：1、运行图的结构 2、分配资源运算 3、掌握资源 会话需要进行资源释放，需要run后进行close。否则可以使用with作为上下文管理器 可以在会话当中指定图去运行 sess.run(fetches，feed_dict=None,graph=None)启动整个图。 用来运行op和计算tensor feed_dict常与placeholder(占位符)一起使用 变量：tensorflow中的变量也是一种op，是一种特殊的张量能够进行存储持久化，它的值就是张量，默认被训练。其中有个trainable参数默认为True，如果改为False，变量将不再变化。 tf.reduce_mean()函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。 如果想设置为原来向量的维度，keep_dims=True。 图的可视化（tensorboard）首先通过pip安装，而后在会话中进行写入事件。 最后在命令行启动 双引号中间填绝对路径，注意不要出现中文和空格。 变量作用域：让模型更直观的显示。 深度学习中的线性回归：]]></content>
      <categories>
        <category>深度学习</category>
        <category>深度学习（1）</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法分类和数据分割]]></title>
    <url>%2F2019%2F08%2F09%2FML%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[机器学习算法分类 区分监督学习和非监督学习的方法：看是否有标准答案。无监督学习无标准答案，只有特征值。 区分分类问题和回归问题的方法:目标值是否是离散型。目标值是离散型，则是分类问题。目标值是连续型，则是回归问题。 数据分割模型=算法+数据 数据分为训练集和测试集 fit_transform=fit+transform：fit做的是计算平均值和标准差，transform做的是转化]]></content>
      <categories>
        <category>机器学习</category>
        <category>算法分类和数据分割</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程]]></title>
    <url>%2F2019%2F08%2F09%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 前些天我把python的处理图像的库——opencv总结了一下，但这终究是传统方法处理图像。现在都是用深度学习网络处理图像。所以，在学深度学习之前，我看了些机器学习的知识。但在看机器学习的算法前，我们先来看看特征工程。 sklearn中的数据堪称完美，各大教材的数据也是一样。但是到我们现实应用中时，发现模型调用效果差，这是因为现实中的数据离完美的数据集差十万八千里。所以数据预处理和特征工程是必要的。 sklearn中六大板块有两大板块是关于数据预处理和特征工程的：也就是黄色标识的两个模块 特征工程三大知识：特征抽取、特征预处理、特征降维。 特征抽取在机器学习中，大多数算法都只能处理数值型数据，不能处理文字。在sklearn当中，除了专用来处理文字的算法，其它的算法在fit的时候全部要求输入数组和矩阵，也不能导入文字型数据。然而在现实生活中，许多标签和特征在数据收集完毕后，都不是以数字来表现的。为了让数据适应算法，我们必须将数据进行编码，即将文字型数据转换为数值型。 preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值。这个没运行成功，好像是我的数据集有问题。我自己设了个数据 123456789101112131415from pandas import DataFrame,Seriesfrom sklearn.preprocessing import LabelEncoderimport pandas as pddata=DataFrame(data=[[-1,2],[-0.5,6],[0,18],[1,18]])print(data)y=data.iloc[:,-1]#要输入的是标签，所以容许一维数组print(y)le=LabelEncoder()le.fit(y) #导入数据，调取结果label=le.transform(y)#print(le.fit_transform(y))print("调取结果是：",label)print(le.classes_) #查看标签中有多少类型data.iloc[:,-1]=labelprint(data) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML68.png)**preprocessing.OrdinalEncoder**：特征专用，能够将分类特征转换为分类数值字典特征抽取，把数据中的以字符串标记的数据转化为one-hot编码。![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML1.png)由上面代码可得下面的稀疏（sparse）矩阵：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML2.png)将稀疏矩阵转为矩阵：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML3.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML4.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML6.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML7.png)将矩阵转化为字典：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML8.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML9.png)文本特征抽取：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML10.png)和字典特征抽取不一样的是CountVectorizer没有sparse参数，只能通过矩阵的toarray转化为数组![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML12.png)可得：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML11.png)**注意：单个字母不统计。因为单个英文字母没有依据**如果文本是中文，根据需要的词频给文本添加空格。同样，单个字无法统计![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML13.png)利用jieba分词对数据进行one-hot编码：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML14.png)TF-IDF特征抽取：用以评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML15.png)## 特征预处理数据归一化处理：通过对原始数据进行交换把数据映射到（默认为[0,1]）之间![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML16.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML17.png)归一化缺点：注意在特定场景最大值最小值是变化的。另外，最大值与最小值非常容易受异常点的影响，所以这种方法的鲁棒性差，只适合传统精确小数据场景。数据标准化处理:通过对原始数据进行变换把数据变换到均值为0，方差为1的范围。![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML18.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML19.png)标准化总结：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。除了StandardScaler和MinMaxScaler之外，sklearn也提供了各种其他的缩放处理：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML60.png)缺失值处理方法:老版本用Imputer，新版本用SimpleImputer删除：如果每列或者行数据缺失值达到一定比例时，建议放弃整行或者整列插补：可以通过缺失值每行或者每列的平均值、中位数来填充Imputer：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML20.png)SimpleImputer：![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML69.png)![](https://raw.githubusercontent.com/Brickexperts/Figurebed/master/ML66.png)```pythonimport pandas as pdfrom sklearn.preprocessing import LabelEncoderfrom sklearn.impute import SimpleImputerdata=pd.read_csv("train.csv",index_col=0)#打印行数，列数，列索引，列非空值个数，列类型，内存占用print(data.info())Age = data.loc[:,"Age"].values.reshape(-1,1) #sklearn当中特征矩阵必须是二维imp_mean = SimpleImputer() #实例化，默认均值填补imp_mean = imp_mean.fit_transform(Age) #fit_transform一步完成调取结果data.loc[:,"Age"]=imp_mean #使用中位数填补Ageprint(data["Age"]) 处理连续型特征：二值化和分段 sklearn.preprocessing.Binarizer 根据阈值将数据二值化(将特征值设置为0或1)，用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射为1。 1234from sklearn.preprocessing import BinarizerAge=data.loc[:,"Age"].values.reshape(-1,1)Bin=Binarizer(threshold=30).fit_transform(Age)print(Bin) sklearn.preprocessing.KBinsDiscretizer 这是将连续变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。三个重要参数： 12345678from sklearn.preprocessing import KBinsDiscretizerx=data.loc[:,"Age"].values.reshape(-1,1)est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')est.fit_transform(x)#查看转换后分的箱:变成了一列中的三箱 set(est.fit_transform(X).ravel()) ravel降维est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform') #查看转换后分的箱:变成了哑变量result=est.fit_transform(x).toarray()print(result) 数据降维：减少特征的数量。这里的维度代表特征的个数。两种方法：1.特征选择 2.主成分分析 特征选择：特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值。但是选择后的特征维数肯定比选择前小。主要方法：Filter（过滤式）、Embedded（嵌入式）、Wrapper（包裹式） 主成分分析：PCA技术，PCA是一种分析、简化数据集的技术。特征数达到数百才会使用 目的：是数据维数压缩，尽可能降低原数据的维数，损失少量信息 PCA语法： n_components取值可以是小数和整数：小数的取值范围是0-1，代表降维后的维度是原本维度的0-1倍数，整数的值是降维降几个维。]]></content>
      <categories>
        <category>机器学习</category>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(7)]]></title>
    <url>%2F2019%2F08%2F09%2Fopencv-7%2F</url>
    <content type="text"><![CDATA[直方图横坐标：图像中各个像素的灰度级 纵坐标：具有该灰度级的像素个数 归一化直方图： 横坐标：图像中各个像素的灰度级 纵坐标：出现这个灰度级的概率 掩膜：通过掩膜可以将一张图的某块区域的直方图画出来 直方图均值化： 注意:用cv2.equalizeHist进行直方图均衡化处理时，对彩色图像进行处理时，分通道进行。灰度图像直接均衡化 利用CLAHE有限对比适应性直方图均衡化，对彩色图像也是分通道进行。灰度图像直接均衡化。 背景建模：以高斯混合模型为基础的背景/前景分割算法 apply可以得到前景的掩膜 傅里叶变换补 。。。。。]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(7)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(6)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-6%2F</url>
    <content type="text"><![CDATA[特征提取图像特征：可以表达图像中对象的主要信息、并且以此为依据可以从其他未知图像中检测出相似或者相同图像。 常见的图像特征：边缘、角点、纹理 角点检测：cv2.cornerHarris() img:输入图像 blockSize：角点检测中要考虑的领域大小 ksize：Sobel求导中使用的窗口大小 k：方程参数，参数为[0.04，0.06] 亚像素级的角点检测：红色是原先的角点，绿色像素是修正后的像素 适合于跟踪的角点检测：cv2.goodFeaturesToTrack(): 输入灰度图像、检测的角点数目、设置角点的质量水平，0-1之间，低于这个数的都会被忽略、设置两个角点间的最短欧式距离。 SIFT尺度不变特征变换匹配算法： cv2.xfeatures2d.SIFT_create()：创建sift特征器 sift.detect()可以在图像找到关键点。如果想在图像的特定区域搜索，可以创建一个掩膜图像作为参数。属性如下： pt：表示图像中关键点的x坐标和y坐标 size：表示特征的直径 angle：表示特征的方向 response：表示关键点强度 octave：表示特征所在金字塔的层级 class_id：表示关键点的ID sift.compute():计算关键点的描述符，在sift.detect后使用。 sift.detectAndCompute()：直接找到关键点并计算出描述符 cv2.drawKeypoints(): image:原始图像 keypoints：特征点向量 outimage：特征点绘制的画布图像，可以是原图像 color：绘制的特征点颜色，可以是原图像。 flags：五种绘制模式： DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS:就会绘制代表关键点大小的圆圈甚至可以绘制除关键点的方向。 DEFAULT：只绘制特征点的坐标点,显示在图像上就是一个个小圆点,每个小圆点的圆心坐标都是特征点的坐标。 DRAW_OVER_OUTIMG：函数不创建输出的图像,而是直接在输出图像变量空间绘制,要求本身输出图像变量就是一个初始化好了的,size与type都是已经初始化好的变量 NOT_DRAW_SINGLE_POINTS：单点的特征点不被绘制 DRAW_RICH_KEYPOINTS：绘制特征点的时候绘制的是一个个带有方向的圆,这种方法同时显示图像的坐标,size，和方向,是最能显示特征的一种绘制方式 SURF：加速稳健特征算法，加快版的SIFT。与SIFT类似 BRIEF:这是一种特征描述符，他不提供查找特征的方法。所以我们需要使用其他的特征检测器。例如：SIFT或SURF。推荐使用STAR。 orb检测： bf暴力匹配： cv2.BFMatcher()：创建一个BFMatcher对象。参数： ​ normType：指定使用的距离测试类型。默认值为cv2.NORM_L2。cv2.NORM_L1也行。这两种适用于SIFT和SURF。对于ORB,BRIEF，应使用cv2.NORM_HAMMING。如果VTA_K==3或4，normType应设置为cv2.NORM_HAMMING2。 ​ crossCheck：默认为False。如果设置为True，匹配条件就会更加严格。 BFMatcher对象具有两个方法：match()和knnMatch()。第一种方法会返回最佳匹配，第二个方法为每个关键点返回k个最佳匹配（降序排列后取前k个）。 cv2.drawMatches cv2.drawMatchsKnn：就像使用 cv2.drawKeypoints() 绘制关点一样我们可以使用 cv2.drawMatches()来绘制匹配的点。它会将两幅图像先水平排列然后在最佳匹配的点之间绘制直线从原图像到目标图像。如果前面使用的是BFMatcher.knnMatch()现在我们可以使用函数 cv2.drawMatchsKnn 为每个关键点和它的 k 个最佳匹配点绘制匹配线。如果 k 等于 2就会为每个关键点绘制两条最佳匹配直线。如果我们选择性绘制就给函数传入一个掩模。 对ORB描述符进行暴力匹配： match=bf.match(des1，des2)：返回值是一个DMatch对象列表。具有以下属性： DMatch.distance - 描述符之间的距离。越小越好。 • DMatch.trainIdx - 目标图像中描述符的索引。 • DMatch.queryIdx - 查询图像中描述符的索引。 • DMatch.imgIdx - 目标图像的索引 对SIFT描述符进行暴力匹配： FLANN匹配器： cv2.FlannBasedMatcher( [, indexParams[, searchParams]] ) indexParams： searchParams：]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(6)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(5)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-5%2F</url>
    <content type="text"><![CDATA[视频处理Videocapture：VideoCapture(args)：如果args=0，则打开摄像头。如果args=路径， 则打开视频源 摄像头设置和查询： 调用摄像头读取图像数据，以及使用 cap.set( propId ， value ) cap.get( propId ) 图片合成视频： 视频分解图片： normalize：归一化函数为了消除指标之间的影响，需要对数据进行标准化处理，以解决数据指标之间的可比性 cv2.normalize(src[, dst[, alpha[, beta[, norm_type[, dtype[, mask]]]]]]) src:输入数组 dst:与src大小相同的输出数组 alpha:下限边界 beta:上限边界 norm_type:NORM_MINMAX NORM_INF NORM_L1 NORM_L2 dType:当输出为负时，输出数组具有与src相同的类型。否则，具有与src相同的信道数和深度 mask:掩膜 画图cv2.rectangle（）：若将五改为-1，则填充整个矩形 cv2.circle：要画圆的话，只需要指定圆形的中心点坐标和半径大小，颜色和粗细 cv2.ellispe:一个参数是中心点的位置坐标。下一个参数是长轴和短轴的长度。椭圆沿逆时针方向旋转的角度。椭圆弧沿顺时针方向起始的角度和结束角度，如果是 0 到 360，就是整个椭圆。 图像金字塔我们对同一图像的不同分辨率的子图像处理。比如我们在一幅图像中查找某个目标比如脸，我们不知目标在图像中的尺寸大小。这种情况下我们创建一组图，这些图像是具有不同分率的原始图像。我们把组图像叫做图像字塔简单来就是同一图像的不同分率的子图集合。如果我们把大的图像放在底部最小的放在顶部。看来像一座金字塔，故而得名图像金字塔。 图像金字塔：高斯金字塔和拉普拉斯金字塔 向下采样：将图像缩小，图像信息丢失 向上采样：将图像放大，图像会变模糊 向下采样和向上采样不是可逆的，无法将图像变为原始图像。 向下采样函数：cv2.pyrDown(原始图像) 向上采样函数：cv2.pyrUP 拉普拉斯金字塔： opencv删除窗口：删除所有窗口 ：cv2.destroyAllWindow() 删除指定窗口：cv2.destroyWindow(“original”) waitKey(x):参数为等待键盘触发的时间。单位为毫秒。如不输入参数，则输入任意键退出。 setMouseCallback():event为事件 x，y代表鼠标位于窗口的（x，y）位置 flags：代表鼠标的拖曳事件以及鼠标和键盘联合的事件。共有32种 param：函数指针，标识所相应的事件函数。 namewindow WINDOW_NORMAL 用户可以改变窗口的大小； WINDOW_AUTOSIZE 窗口大小会根据显示图像自动调整，用户必能手动改变窗口大小； WINDOW_OPENGL 支持OpenGL。]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(5)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(4)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-4%2F</url>
    <content type="text"><![CDATA[图像类型转换 cvtColor:颜色空间转换函数 cv2.cvtColor()支持多种颜色空间之间的转换，其 需要注意的是cvtColor()函数不能直接将RGB图像转换为二值图像(Binary Image)，需要借助threshold()函数 HSV中：H为色彩，取值范围是[0,179]，S为饱和度，取值范围是[0,255],V是亮度，取值范围是[0,255]。 支持的转换类型和转换码如下： 1、RGB和BGR（opencv默认的彩色图像的颜色空间是BGR）颜色空间的转换 COLOR_BGR2RGB COLOR_RGB2BGR COLOR_RGBA2BGRA COLOR_BGRA2RGBA 2、向RGB和BGR图像中增添alpha通道 COLOR_RGB2RGBA COLOR_BGR2BGRA 3、从RGB和BGR图像中去除alpha通道 COLOR_RGBA2RGB COLOR_BGRA2BGR 4、从RBG和BGR颜色空间转换到灰度空间 COLOR_RGB2GRAY COLOR_BGR2GRAY COLOR_RGBA2GRAY COLOR_BGRA2GRAY 5、从灰度空间转换到RGB和BGR颜色空间 COLOR_GRAY2RGB COLOR_GRAY2BGR COLOR_GRAY2RGBA COLOR_GRAY2BGRA 6、RGB和BGR颜色空间与BGR565颜色空间之间的转换 COLOR_RGB2BGR565 COLOR_BGR2BGR565 COLOR_BGR5652RGB COLOR_BGR5652BGR COLOR_RGBA2BGR565 COLOR_BGRA2BGR565 COLOR_BGR5652RGBA COLOR_BGR5652BGRA 7、灰度空间域BGR565之间的转换 COLOR_GRAY2BGR555 COLOR_BGR5552GRAY 8、RGB和BGR颜色空间与CIE XYZ之间的转换 COLOR_RGB2XYZ COLOR_BGR2XYZ COLOR_XYZ2RGB COLOR_XYZ2BGR 9、RGB和BGR颜色空间与uma色度（YCrCb空间）之间的转换 COLOR_RGB2YCrCb COLOR_BGR2YCrCb COLOR_YCrCb2RGB COLOR_YCrCb2BGR 10、RGB和BGR颜色空间与HSV颜色空间之间的相互转换 COLOR_RGB2HSV COLOR_BGR2HSV COLOR_HSV2RGB COLOR_HSV2BGR 11、RGB和BGR颜色空间与HLS颜色空间之间的相互转换 COLOR_RGB2HLS COLOR_BGR2HLS COLOR_HLS2RGB COLOR_HLS2BGR 12、RGB和BGR颜色空间与CIE Lab颜色空间之间的相互转换 COLOR_RGB2Lab COLOR_BGR2Lab COLOR_Lab2RGB COLOR_Lab2BGR 13、RGB和BGR颜色空间与CIE Luv颜色空间之间的相互转换 COLOR_RGB2Luv COLOR_BGR2Luv COLOR_Luv2RGB COLOR_Luv2BGR 14、Bayer格式（raw data）向RGB或BGR颜色空间的转换 COLOR_BayerBG2RGB COLOR_BayerGB2RGB COLOR_BayerRG2RGB COLOR_BayerGR2RGB COLOR_BayerBG2BGR COLOR_BayerGB2BGR COLOR_BayerRG2BGR COLOR_BayerGR2BGR inrange：实现二值化功能 image=cv2.inrance(hsv,lower_red,upper_red) 第一个参数：hsv指的是原图 第二个参数：lower_red指的是图像中低于这个lower_red的值，图像值变为0 第三个参数：upper_red指的是图像中高于这个upper_red的值，图像值变为0 而在lower_red～upper_red之间的值变成255 图像中的与、或、异或、非操作opencv中的bitwise_not，bitwise_xor，bitwise_or，bitwise_and的使用方法与效果。 bitwise_and是对二进制数据进行“与”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“与”操作，1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0。 bitwise_or是对二进制数据进行“或”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“或”操作，1|1=1，1|0=0，0|1=0，0|0=0。 bitwise_xor是对二进制数据进行“异或”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“异或”操作，1^1=0,1^0=1,0^1=1,0^0=0。 bitwise_not是对二进制数据进行“非”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“非”操作，1=0，0=1。 Hough直线变换cv2.HoughLines()，返回值就是距离和角度。这个函数的第一个参数是一个二值化图像。第二个和第三个值代表距离和角度的精确度。第四个参数是阈值，只有累加其中的值高于阈值时才被认为是一条直线。 cv2.HoughLinesP():比上面多两个参数，minLineLength和MaxLineGap。比较简单 minLineLength：线的最短长度，比这个短的线都忽略 MaxLineGap：两条线段的最大间断，如果小于此值，这两条直线被看成一条直线 Hough圆环变换：HoughCircles(image, method, dp, minDist[, circles[, param1[, param2[, minRadius[, maxRadius]]]]]) image参数表示8位单通道灰度输入图像矩阵。 method参数表示圆检测方法，目前唯一实现的方法是HOUGH_GRADIENT。 dp参数表示累加器与原始图像相比的分辨率的反比参数。例如，如果dp = 1，则累加器具有与输入图像相同的分辨率。如果dp=2，累加器分辨率是元素图像的一半，宽度和高度也缩减为原来的一半。 minDist参数表示检测到的两个圆心之间的最小距离。如果参数太小，除了真实的一个圆圈之外，可能错误地检测到多个相邻的圆圈。如果太大，可能会遗漏一些圆圈。 circles参数表示检测到的圆的输出向量，向量内第一个元素是圆的横坐标，第二个是纵坐标，第三个是半径大小。 param1参数表示Canny边缘检测的高阈值，低阈值会被自动置为高阈值的一半。 param2参数表示圆心检测的累加阈值，参数值越小，可以检测越多的假圆圈，但返回的是与较大累加器值对应的圆圈。 minRadius参数表示检测到的圆的最小半径。 maxRadius参数表示检测到的圆的最大半径]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(4)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(3)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-3%2F</url>
    <content type="text"><![CDATA[图像轮廓和图像边缘不一样，边缘不连续。将边缘连接成一个整体构成轮廓。 提取图像轮廓的方法：先调用cv2.findContours()，后调用cv2.drawCont() cv2.findCount函数使用方法: 参数mode的几种方式： 参数method的几种方法： 轮廓特征：cv2.moments()会将计算得到的矩以一个字典的形式返回。 轮廓面积可以使用函数 cv2.contourArea() 计算得到 轮廓周长：cv2.arcLength(cnt，True) 函数的第二个参数用来指定对象的形状是闭合的(True)，还是打开的。 轮廓近似：cv2.approxPolyDP(cnt，epsilon，True) 凸包：函数cv2.convexHull() 可以用来检测一个曲线是否具有凸性缺陷并能纠正凸性缺陷。凸性曲线总是凸出来的，至少是平的。如果有地方凹去了就叫做凸性缺陷。 凸性检测：cv2.isContourConvex()可以用来检测一个曲线是不是凸的，返回True和False。 边界矩形：直边界矩形和旋转的边界矩形。 直边界矩形：面积不是最小的。x,y,w,h=cv2.boundingRect()。x,y,为矩形左上角的坐标，w,h为矩形的宽和高 旋转的边界矩形：面积最小。考虑了对象的旋转。cv2.minAreaRect())。返回的是一个 Box2D 结构，其中包含矩形左上点的坐标x，y矩形的宽和高w，h以及旋度。但是绘制个矩形矩形的 4 个点可以函数cv2.boxPoints() 获 得。 最小外接圆：cv2.minEnlosingCircle() 椭圆拟合：旋转边界矩形的内切圆 cv2.fitEllipse(cnt) 形状匹配：cv2.matchShape()可以帮我们比较两个形状或轮廓的相似度。返回值越小，匹配越好。 getStructuringElement构建一个核。前面腐蚀膨胀的numpy构建的结构化元素是正方形的 getStructuringElement 与 Numpy 定义的元素结构是完全一样的这个函数的第一个参数表示内核的形状，有三种形状可以选择。 矩形：MORPH_RECT; 交叉形：MORPH_CROSS; 椭圆形：MORPH_ELLIPSE; 第二和第三个参数分别是内核的尺寸以及锚点的位置。对于锚点的位置，有默认值（-1,-1），表示锚点位于中心点。element形状唯一依赖锚点位置，其他情况下，锚点只是影响了形态学运算结果的偏移。 透视变换在不同的视觉拍摄同一个物体，会有不同的图像。透视变换就是类似于改变拍摄物体的角度 仿射变换：由平移、错切、缩放、反转、旋转复合而成，是透视变换的特殊形式 OpenCV提供了两个变换函数cv2.warpAﬃne(仿射变换)和cv2.warpPerspective(透视变换) ，cv2.warpAﬃne 接收的参数是 2×3 的变换矩阵，而cv2.warpPerspective 接收的参数是 3×3 的变换矩阵。 函数cv2.warpAﬃne() 的第三个参数的是输出图像的大小。它的格式应是图像的(宽,高)。注意的是图像的宽对应的是列数，高对应的是行数。可以实现图片平移。可以和cv2.getAffineTransform配合使用 其中两个参数是变换前后的位置关系 函数cv2.warpPerspective配合cv2.getPerspectiveTransform()使用。同时可以用findHomography返回的单应性矩阵。 getPersonspectiveTransform得出变换矩阵： 得出变换矩阵以后用warpPerspective()：第一个参数是输入图像，M是变换矩阵，第三个参数是输出图像的大小 findHomography：提供正确估计的好的匹配被叫做inliers，而其他的叫做outliers。cv2.findHomography()返回一个掩图来指定inline和outline。第一个和第二个参数分别是原图像和目的图像，第三个参数可选为cv2.RANSAC、cv2.LMEDS.第二个参数取值范围在1到10。 getPerspectiveTransform和findHomography的区别： 旋转：cv2.getRotationMatrix2D()]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(3)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(2)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-2%2F</url>
    <content type="text"><![CDATA[图像滤波：即在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像预处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。 图像滤波图像滤波的目的 a、消除图像中混入的噪声 b、为图像识别抽取出图像特征 均值滤波，用blur函数 方框滤波：进行归一化处理后，和均值滤波相同。当normalize为零时，不进行归一化处理。当normalize为一时，进行归一化处理（默认）。目标图像深度通常于原始图像一样，值为-1。 高斯滤波： 注意：ksize：核大小必须为单数 sigmaX、sigmaY：控制权重 中值滤波： 两种边缘保留滤波： biateraFilter：能在保持边界清晰的情况下有效的去除噪音 filter2D:对2D图像实施低通滤波。去除噪音，模糊图像 膨胀和腐蚀膨胀、腐蚀时用到的kernel的形状一般有下面三种： 矩形: MORPH_RECT 交叉形: MORPH_CROSS 椭圆形: MORPH_ELLIPSE 图像腐蚀：调用erode函数 图像膨胀：调用dialate 开运算：先通过图像腐蚀，后经过图像膨胀可以图像去噪。iteration表示次数 闭运算：先是先通过图像膨胀，后经过图像腐蚀 梯度运算：图像膨胀-图像腐蚀 图像礼帽（图像顶帽）：原始图像-开运算图像，得到噪声图像 图像黑帽：闭运算-原始图像 求梯度梯度简单来说就是求导。Sobel，Scharr是求一阶导数或二阶导数。Scharr是对Sobel的优化。Laplacian是求二阶导数。 Sobel算子：当一个像素右边的值减去左边的值不为零，该像素为边界。 计算梯度的函数： ddepth通常取cv2.CV_64F。 dx=0,dy=1计算y轴的边界,dx=1,dy=0计算x轴的边界。满足条件dx&gt;=0&amp;&amp;dy&gt;=0&amp;&amp;dx+dy=1 将图像中的负值转为正： 将一个图像的边缘提取出来：如果没有converScaleAbs，所有的负值都会被截断为0.换句话就是把边界丢掉。 Scharr算子：比sobel算子精准，使用方法基本一样 计算梯度调用函数：cv2.Scharr(src,ddpetch,dx,dy) des=cv2.Scharr(src,ddpetch,dx,dy) 等价于des=cv2.Sobel(src,ddpetch,dx,dy,-1) 为图像扩边：cv2.copyMakeBorder()src:原图图像 top,bottom,left,right分别表示在原图四周扩充边缘的大小 borderType：扩充边缘的类型，就是外插的类型，OpenCV中给出以下几种方式 * BORDER_REPLICATE * BORDER_REFLECT * BORDER_REFLECT_101 * BORDER_WRAP * BORDER_CONSTANT canny函数提取图片边缘cv2.canny(img,threshold1,threshold2) img代表原始图像，threshold1、threshold2为阈值。两个阈值越小，得出图像边缘越详细。反之，边框越边缘。 提取原理步骤：1、高斯模糊 - GaussianBlur 2、灰度转换 - cvtColor 3、计算梯度 – Sobel/Scharr 4、非最大信号抑制5、高低阈值输出二值图像]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(2)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(1)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-1%2F</url>
    <content type="text"><![CDATA[​ 因为我是对图像处理的方面比较感兴趣的。所以我也是对python的视觉处理模块opencv进行了学习。这个模块虽然是传统方法，但还是蛮有用的。而且它有上千个API，我总结了以下我所学到的。开始吧。 cv2是opencv的扩展模块。 imread第二个参数类型: cv2.imwrite:保存图片 下文中：绝对路径代表绝对路径也行 改变图片某行某列的像素： 批量改变图片像素： 获取图像属性： 拆分通道： 合并通道： 图片移位： 将两张图片加在一起：1、取模加法 2、饱和运算 注意：两张图片的大小和类型相等 减法(subtract)、乘法(multiply)和除法(divide)和加法(add)类似 图像融合：将两张或两张以上的图片融合到一张图片上 图像缩放（参数必须为整数）： 图像翻转：调用cv2.flip(原始图像，flipcode) 三种情况： filpcode=0：以x轴为对称轴的上下翻转 flipcode&gt;0:以Y轴为对称轴的左右翻转 flipcode&lt;0:X、Y轴各翻转一次 图像颜色反转： 图片打上马赛克： 图片上写字： 图片修补： 图片亮度增强： 图像阈值分割：调用了threshold函数 五种分割方法： 二进制阈值化（cv2.THRESH_BINARY）：选定一个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素阈点值设为最大，若小于该阈值，则将该像素点阈值设为零。 反二进制阈值化（cv.THRESH_BINARY_INV）：选定一个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素点的阈值设为零，若小于该阈值，则将该像素点阈值设为最大。 截断阈值化（cv.THRESH_TRUNC）：选定个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素点的阈值设为该阈值，小于该阈值的像素点的阈值不变。 阈值化为0（THRESH_TOZERO）：先选定一个阈值，像素点的灰度值大于该阈值的不进行任何改变；像素点的灰度值小于该阈值的，其灰度值全部变为0。 反阈值化为0(THRESH_TOZERO_INV)：先选定一个阈值，像素点的灰度值小于该阈值的不进行任何改变；像素点的灰度值大于该阈值的，其灰度值全部变为0。 还有另一种阈值分割函数：自适应阈值函数 cv2.adaptiveThreshold()]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
        <category>opencv(1)</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间模块]]></title>
    <url>%2F2019%2F08%2F08%2F%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python里面的时间模块。time 模块提供各种时间相关的功能。在 Python 中，与时间处理有关的模块包括：time，datetime 以及 calendar。 首先先来一波术语解释： 时间戳（timestamp）的方式：通常来说，时间戳表示的是从 1970 年 1 月 1 日 00:00:00 开始按秒计算的偏移量（time.gmtime(0)）此模块中的函数无法处理 1970 纪元年以前的日期和时间或太遥远的未来（处理极限取决于 C 函数库，对于 32 位系统来说，是 2038 年） UTC（Coordinated Universal Time，世界协调时）也叫格林威治天文时间，是世界标准时间。在中国为 UTC+8 DST（Daylight Saving Time）即夏令时的意思。一些实时函数的计算精度可能低于它们建议的值或参数，例如在大部分 Unix 系统，时钟一秒钟“滴答”50~100 次 gmtime()，localtime()和 strptime() 以时间元祖（struct_time）的形式返回。 注一：范围真的是0-61.这是基于历史原因。 ##time time.altzone：返回格林威治西部的夏令时地区的偏移秒数；如果该地区在格林威治东部会返回负值（如西欧，包括英国）；对夏令时启用地区才能使用。 time.asctime([t])：接受时间元组并返回一个可读的形式为”Tue Dec 11 18:07:14 2015”（2015年12月11日 周二 18时07分14秒）的 24 个字符的字符串。 time.clock()：用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） time.ctime([secs]) ：作用相当于 asctime(localtime(secs))，未给参数相当于 asctime() time.gmtime([secs])：接收时间辍（1970 纪元年后经过的浮点秒数）并返回格林威治天文时间下的时间元组 t（注：t.tm_isdst 始终为 0） time.daylight：如果夏令时被定义，则该值为非零。 time.localtime([secs])：接收时间辍（1970 纪元年后经过的浮点秒数）并返回当地时间下的时间元组 t（t.tm_isdst 可取 0 或 1，取决于当地当时是不是夏令时） time.mktime(t)：接受时间元组并返回时间辍（1970纪元后经过的浮点秒数） time.perf_counter()：返回计时器的精准时间（系统的运行时间），包含整个系统的睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 time.process_time() ：返回当前进程执行 CPU 的时间总和，不包含睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 time.sleep(secs)：推迟调用线程的运行，secs 的单位是秒。 time.clock()：用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。 Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） time.time()：返回当前时间的时间戳（1970 纪元年后经过的浮点秒数） time.timezone：time.timezone 属性是当地时区（未启动夏令时）距离格林威治的偏移秒数（美洲 &gt;0；大部分欧洲，亚洲，非洲 &lt;= 0） time.tzname：time.tzname 属性是包含两个字符串的元组：第一是当地非夏令时区的名称，第二个是当地的 DST 时区的名称。 time.strftime(format[, t]) ：把一个代表时间的元组或者 struct_time（如由 time.localtime() 和 time.gmtime() 返回）转化为格式化的时间字符串。如果 t 未指定，将传入 time.localtime()。如果元组中任何一个元素越界，将会抛出 ValueError 异常。 time.strptime(string[, format])：把一个格式化时间字符串转化为 struct_time。实际上它和 strftime() 是逆操作 format 格式如下： 注1：“%p”只有与“%I”配合使用才有效果。 注2：范围真的是*0 ~ 61（你没有看错哦）；60代表闰秒，61是基于历史原因保留。 注3：当使用 strptime() 函数时，只有当在这年中的周数和天数被确定的时候%U 和 %W 才会被计算。 datetimedatetime.today()：返回一个表示当前本地时间的 datetime 对象，等同于 datetime.fromtimestamp(time.time()) datetime.now(tz=None)：返回一个表示当前本地时间的 datetime 对象；如果提供了参数 tz，则获取 tz 参数所指时区的本地时间 datetime.utcnow():返回一个当前 UTC 时间的 datetime 对象 datetime.fromtimestamp(timestamp, tz=None): 根据时间戮创建一个 datetime 对象，参数 tz 指定时区信息 datetime.utcfromtimestamp(timestamp): 根据时间戮创建一个 UTC 时间的 datetime 对象 datetime.fromordinal(ordinal): 返回对应 Gregorian 日历时间对应的 datetime 对象 datetime.combine(date, time): 根据参数 date 和 time，创建一个 datetime 对象 datetime.strptime(date_string, format): 将格式化字符串转换为 datetime 对象 datetime.timedelta对象代表两个时间之间的时间差，两个date或datetime对象相减就可以返回一个timedelta对象。 *datetime.timedelta([days[, seconds[, microseconds[, milliseconds[, minutes[, hours[, weeks]]]]]]]) * 往前算： 往后算： datetime.date()：返回一个 date 对象datetime.time() - 返回一个 time 对象（tzinfo 属性为 None） datetime.timetz()： 返回一个 time() 对象（带有 tzinfo 属性） datetime.replace([year[, month[, day[, hour[, minute[, second[, microsecond[, tzinfo]]]]]]]])： 生成一个新的日期对象，用参数指定日期和时间代替原有对象中的属性 datetime.astimezone(tz=None)： 传入一个新的 tzinfo 属性，返回根据新时区调整好的 datetime 对象 datetime.utcoffset()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.utcoffset(self) datetime.dst()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.dst(self) datetime.tzname()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.tzname(self) datetime.timetuple()：返回日期对应的 time.struct_time 对象（类似于 time模块的 time.localtime()） datetime.utctimetuple()：返回 UTC 日期对应的 time.struct_time 对象 datetime.toordinal()：返回日期对应的 Gregorian Calendar 日期（类似于 self.date().toordinal()） datetime.timestamp()：返回当前时间的时间戳（类似于 time 模块的 time.time()） datetime.weekday()：返回 0 ~ 6 表示星期几（星期一是 0，依此类推） datetime.isoweekday()： 返回 1 ~ 7 表示星期几（星期一是1， 依此类推） datetime.isocalendar() ：返回一个三元组格式 (year, month, day) datetime.isoformat(sep=’T’)：返回一个 ISO 8601 格式的日期字符串，如 “YYYY-MM-DD” 的字符串 datetime.__str__()：对于 date 对象 d 来说，str(d) 相当于 d.isoformat() datetime.ctime()：返回一个表示日期的字符串，相当于 time模块的 time.ctime(time.mktime(d.timetuple())) datetime.strftime(format):返回自定义格式化字符串表示日期。 datetime.__format__(format)跟 datetime.strftime(format) 一样，这使得调用 str.format() 时可以指定 data 对象的字符串 calendarcalendar.calendar(year,w,l,c)(year为年份，w是每日宽度，c为间隔距离，l为每星期行数) 日历 *calendar.isleap(year) *：如果是闰年就返回True，否则返回False calendar.leapdays(y1,y2)：返回y1与y2两年间的闰年总数 *calendar.month(year,month,w,l) *：year代表年份，month代表月日历，每行宽度间隔为w，l是每星期的行数 *calendar.prcal *：相当于print(calendar.calendar(year,w,l,c)) calendar.prmonth(year,month,w,l)：相当于print(calendar.calendar(year,w,l,c)) calendar.weekday(year,month,day)： 返回指定日期的日期码，0-6（星期），1-12（月份） calendar.firstweekday()返回当前起始日期的设置 calendar.setfirstweekday()：设置每周的起始日期码 calendar.timegm(时间元祖) 和time.gmtime相反 calendar.monthrange(year,month)：返回两个整数。第一个是该月第一天是星期几的日期码，第二个是该月的日期码。日从0（星期一）到6（星期日）;月从1到12。 timeit测试一个函数的执行时间：timeit.timeit timeit.repeat:返回一个包含了每次实验的执行时间的列表 三个时间模块的函数特别多，我也没有一一去试。用到的时候才会深究，各位看官根据自己的情况来试着使用起来吧。]]></content>
      <categories>
        <category>python</category>
        <category>时间管理</category>
      </categories>
      <tags>
        <tag>python时间管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（3）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[textwrap用来重新格式化文本的输出 fill() 调整换行符,每行显示给定宽度 dedent()：去除缩进 indent():给定前缀 首行缩进： 多余的省略号：shorter() wrap():wrap(s,width) 以单词为单位(包括字符)最大长度不超过width个字符 itertoolsPython的内建模块itertools提供了非常有用的用于操作迭代对象的函数 count()会创建一个无限的迭代器，所以下述代码会打印出自然数序列，根本停不下来，只能按Ctrl+C退出。 cycle()会把传入的一个序列无限重复下去： repeat()负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数： chain()可以把一组迭代对象串联起来，形成一个更大的迭代器： groupby()把迭代器中相邻的重复元素挑出来放在一起： 实际上挑选规则是通过函数完成的，只要作用于函数的两个元素返回的值相等，这两个元素就被认为是在一组的，而函数返回值作为组的key。如果我们要忽略大小写分组，就可以让元素’A’和’a’都返回相同的key： permutations():输出输入序列的全排列,考虑顺序 combinations():同上，不过不考虑顺序 product():输出输入序列的笛卡儿积 compress：可以对一个序列的筛选结果施加到另一个相关的序列上 dropwhile():筛选满足条件的元素 islice()： zip_longest：zip可产生元祖。当其中摸个输入序列中没有元素可以继续迭代时，迭代过程结束。所以整个迭代的长度和最短的输入序列相同。 如果不想这样就用zip_longest：]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
        <category>常用模块（3）</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（2）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[collection：collections是Python内建的一个集合模块，提供了许多有用的集合类。 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。 利用namedtuple定义和使用具名元祖：第一个参数为类名，第二个参数为类的各个字段的名字 具名元祖有些专用的属性：类属性_fields,类方法_make(),实例方法_asdict() 如果需要修改任何属性，可以通过使用nametupled实例的_replace方法来实现。该方法会创建按一个新的命名元祖，并对相应的值进行替换。 deque：是为了高效实现插入和删除操作的双向列表，适合用于队列和栈。deque(maxlen=N)创建一个固定长度的队列，当有新记录加入而队列已满时会自动移除最老的那条记录。 defaultdict:使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict。defaultdict的一个特点就是会自动初始化第一个值。 用defaultdict的效率比不用高: OrderedDict 使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用OrderedDict。 OrderDict的大小是普通字典的2倍多，这是由于它额外创建的链表所导致。 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key Counter：一个简单的计数器 计数器的更新包括增加和减少两种，增加使用update，减少用subtract。 most_common(x):根据x返回频率前x的项。 itemgetter：可以通过公共键对字典列表排序。 ChainMap：可接受多个映射然后再逻辑上是它们表现为一个单独的映射结构。如果有重复的键，那么会采用第一个映射中所对应的值。修改映射的操作总是会作用在列出的第一个映射结构上。 randomrandom.random() 产生0-1的随机浮点数 random.uniform(a, b) 产生指定范围内的随机浮点数 random.randint(a, b) 产生指定范围内的随机整数 random.randrange([start], stop[, step]) 从一个指定步长的集合中产生随机数 random.choice(sequence) 从序列中产生一个随机数 random.shuffle(x[, random]) 将一个列表中的元素打乱 random.sample(sequence, k) 从序列中随机获取指定长度的片断 functoolsreduce(function, sequence, value)：对sequence中的item顺序迭代调用function，如果有value，还可以作为初始值调用。function接收的参数个数只能为2，先把sequence中第一个值和第二个值当参数传给function，再把function的返回值和第三个值当参数传给function，然后只返回一个结果。 partial：基于一个函数创建一个新的可调用对象，把原函数的某些参数固定。 偏函数：只需要传一次值，后面想传就传]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
        <category>常用模块（2）</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（1）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 这几年，python大火。其中的一个原因是python的库特别多，而且封装非常好。接下来我来总结以下我用过的一些库，虽然都不是很大的库，但还是有用的。 jsonjson主要执行序列化和反序列化的功能，通过Python的json模块，可以将字符串形式的json数据转化为字典，也可以将Python中的字典数据转化为字符串形式的json数据。 通过json字符串转为字典 json.loads 字典转换为json：json.dumps json.loads()、dumps解码python json格式 json.load、dump加载json格式文件 pickle使用方法与json一样 区别： json是可以在不同语言之间交换数据的，而pickle只在python之间使用。 json只能序列化最基本的数据类型，而pickle可以序列化所有的数据类型，包括类，函数都可以序列化。 hashlib该模块提供了常见的摘要算法。如MD5，SHA1……摘要算法又称哈希算法、散列算法，通过一个函数，把任意长度的数据转换为一个长度固定的数据串。 摘要算法是一个单向函数，通过摘要函数f()对任意长度的数据data计算出固定长度的摘要digest，目的是发现原始数据是否被改动过。计算f(data)很容易，但通过digest反推data却非常困难，对原始数据做一个字节的修改，都会导致计算出来的摘要不同。 stringstring.digits:包含数字0-9的字符串 string.letters:包含所有字母（大写或小写）的字符串 string.lowercase:包含所有小写字母的字符串 string.printable:包含所有可打印字符的字符串 string.punctuation:包含所有标点的字符串 string.uppercase:包含所有大写字母的字符串 string.ascii_letters和string.digits方法，其中ascii_letters是生成所有字母，从a-z和A-Z,digits是生成所有数字0-9. 词云 decimal如果期望获得更高的精度（并且愿意牺牲掉一些性能），可以使用decimal模块。 fractions模块可以用来处理涉及分数的数学计算问题 copy完成深拷贝和浅拷贝 is和==的区别：==是看值，is看是否指向同一个 浅拷贝：拷贝内容的地址 深拷贝：开发另一片空间存放要拷贝的内容 copy会判断数据类型是否为可变类型，如元祖为不可变类型，则只会完成浅拷贝。 fileinput可以快速对一个或多个文件进行循环遍历 fileinput.input([files[, inplace[, backup[, mode[, openhook]]]]]])功能:生成FileInput模块类的实例。能够返回用于for循环遍历的对象。注意:文件名可以提供多个 inplace：是否返回输出结果到源文件中，默认为零不返回。设置为1时返回。 backup：备份文件的扩展名 mode：读写模式。只能时读、写、读写、二进制四种模式。默认是读 openhook：必须是一个函数，有两个参数，文件名和模式。返回相应的打开文件对象 fileinput.filename()：返回当前正在读取的文件的名称。在读取第一行之前，返回None。 fileinput.fileno()：返回当前文件的整数“文件描述符”。如果没有打开文件（在第一行之前和文件之间），则返回-1。 fileinput.lineno()：返回刚读过的行的累计行号。在读取第一行之前，返回0。读取完最后一个文件的最后一行后，返回该行的行号。 fileinput.filelineno()：返回当前文件中的行号。在读取第一行之前，返回0。读取完最后一个文件的最后一行后，返回该文件中该行的行号。 fileinput.isfirstline()：如果刚刚读取的行是其文件的第一行，则返回true，否则返回false。 fileinput.isstdin()：如果读取了最后一行sys.stdin，则返回true，否则返回false。 fileinput.nextfile()：关闭当前文件，以便下一次迭代将读取下一个文件的第一行（如果有的话）; 未从文件中读取的行将不计入累计行数。直到读取下一个文件的第一行之后才会更改文件名。在读取第一行之前，此功能无效; 它不能用于跳过第一个文件。读取完最后一个文件的最后一行后，此功能无效。 fileinput.close()关闭序列 subprocess：subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用subprocess.PIPE将多个子进程的输入和输出连接在一起，构成管道(pipe) subprocess.call():父进程等待子进程完成，返回退出信息(returncode，相当于Linux exit code) subprocess.check_call():父进程等待子进程完成，返回0，检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查 subprocess.check_output():父进程等待子进程完成，返回子进程向标准输出的输出结果，检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try…except…来检查。 subprocess.Popen()，以下为参数： args：shell命令，可以是字符串，或者序列类型，如list,tuple。 bufsize：缓冲区大小，可不用关心 stdin,stdout,stderr：分别表示程序的标准输入，标准输出及标准错误 shell：与上面方法中用法相同 cwd：用于设置子进程的当前目录 env：用于指定子进程的环境变量。如果env=None，则默认从父进程继承环境变量 universal_newlines：不同系统的的换行符不同，当该参数设定为true时，则表示使用\n作为换行符 常用方法： poll() ： 检查子进程状态 kill() ： 终止子进程 send_signal() :向子进程发送信号 terminate() ： 终止子进程 communicate:从PIPE中读取PIPE的文本，该方法会阻塞父进程，直到子进程完成 常用属性：pid：子进程的pid，returncode：子进程的退出码。]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
        <category>常用模块（1）</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[元类]]></title>
    <url>%2F2019%2F08%2F07%2F%E5%85%83%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[​ 今天来讲一下python里面较难的一个东西。我学到的python内容里面有两个东西是比较难的，一个是描述符，另一个就是今天讲的元类。 通过python知识散记我们知道global的功能是将局部变量转为全局变量，接下来要说的是globals()。这个globals()函数会以字典类型返回当前位置的全部全局变量。 __builtins__模块是默认加载的，在ipython中打开。 python通过类创建对象，通过元类创建类。（类也是对象） 动态创建类： 函数choose_class根据name的不同而动态的创建不同的类，但这种做法效率相当low。因为如果类多了就会要很多if-else来判断创建哪个类。 通过type动态创建类。命名规则： type(类名，由父类名称构成的元祖（针对继承的情况，可以为空），包含属性的字典（名称和值）) 继承： 添加实例方法： 添加类方法： 添加静态方法： 元类应用： metaclass用来指定按照upper_attr来创建，如果不指定，则默认使用type创建。Foo传到class_name，父类(object)传到class_name，新的字典传到class_attr。 用类完成以上代码：]]></content>
      <categories>
        <category>python</category>
        <category>元类</category>
      </categories>
      <tags>
        <tag>python的元类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas用法总结]]></title>
    <url>%2F2019%2F08%2F07%2Fpandas%2F</url>
    <content type="text"><![CDATA[​ pandas也是一个非常强大的库，所以我也只是总结了我用到的方法。 pandas常用的数据类型：1、Series 一维 带标签的数组（标签就是索引）2、DataFrame 二维 Series的容器 Series：通过列表创建Series： 索引可以指定，默认从0开始： 通过字典创建Series：可以通过astype修改类型 取值： 可以将条件和value、index配合使用： pandas读取外部数据： read_csv:读取CSV文件 read_excel:读取excel文件 其他文件类似 DataFrame通过列表创建： 通过数组创建： 通过字典创建： DataFrame的基础属性： shape ：行数 列数 dtypes：列数据类型 ndim：数据维度 index：行索引 columns：列索引 values：对象值 DataFrame的方法： head(n):显示头n行。默认是前5行 tail(n)：显示尾n行。 info()：行数，列数，列索引，列非空值个数，列类型，内存占用 describe()：计数 均值 标准差 最大值 四分位数 最小值 DataFrame排序：sort_values()。通过设置by来确定排序的key。设置ascending确定升序or降序。 DataFrame的取值： 方括号写数字，表示取行。对行进行操作。根据实际情况写 对列进行操作： 配合使用： loc：DataFrame通过标签索引获取行数据 根据多个索引取多个对应的值： iloc：DataFrame通过位置获取列数据。与ioc类似，只是将索引换成数字。 数组合并： 1、join 按行索引合并 2、merge按列索引进行合并 on指定按哪一列合并 how：合并方式 inner(交集，默认) outer(并集) left(左边为准，NaN补全) right(右边为准，NaN补全) 如果列索引不同。可以left_on和right_on指定左边、右边DataFrame的合并列。 另一种写法： 分组:groupby(by) by:通过什么分组，可以设置多个条件分组 聚合：count 计算数量 sum 求和 mean 求平均值 median 求中位数 std、var 求标准差和方差 min、max 求最大和最小值 DataFrame的索引和复合索引：简单的索引操作： 获取index：df.index 指定index：df.index=[“”,””] 同理可得指定columns：df.columns=[“ “,” “] 重新设置index：df.reindex() 指定某一列成为index：df.set_index()。drop决定是否保留设定的列 可以设定多个列成为index： 返回index的唯一值：df.set_index().index.unique() 时间序列：date_range(start,end,period,freq) 生成一段时间范围。start和end表示范围，periods表示个数，freq表示频率(年、月、天) 频率类型： 时间段：PeriodIndex 重采样resample：指的是将时间序列从一个频率转化为另一个频率进行处理的过程。将高频率数据转化为低频率数据为降采样。低频率数据转化为高频率为升采样。 判断数据是否为NaN：pd.isnull() pd.notfull() 在DataFrame中对缺失数据（NaN）的处理： 方式1：删除NaN所在的行列dropna(axis,how,inplace):how=”any”时一行(列)里有一个为nan就删。how=”all”时，一行全部为nan时才删。inplace为True，原地修改。False为False，不修改。 方式2：填充数据，fillna() 处理为0的数据：t[t==0]=np.nan 计算平均值时：nan不参与计算，0参与]]></content>
      <categories>
        <category>python</category>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（3）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 今天的知识散记讲三个器，哪三个器呢？装饰器、迭代器、生成器。还有列表生成式、字典生成式。 装饰器：python装饰器就是用于拓展原来函数功能的一种函数，这个函数的特殊之处在于它的返回值也是一个函数，使用python装饰器的好处就是在不用更改原函数的代码前提下给函数增加新的功能。 使用闭包完成的装饰器原理： 当执行test(test)时，func已经指向test()函数。所以func()相当于test()。这就是装饰器的原理。但是实际不是这样写的，请看下图： 使用装饰器对有参数的函数装饰： 对不定长参数的装饰： 装饰器对有返回值的函数装饰： 带参数的装饰器 用类做装饰器： 装饰器的一个关键特性：函数装饰器在导入模块时立即执行，而被装饰的函数只在明确调用时运行。 装饰器完了，自己慢慢悟吧。 迭代器迭代器只能前进不能后退。使用迭代器不要求事先准备好整个迭代过程中的所有元素。迭代器仅仅在迭代到某个元素时才计算该元素，而在这之前或之后元素可以不存在或者被销毁。因此迭代器适合遍历一些数量巨大甚至无限的序列。 Python中迭代器的本质上每次调用__next__()方法都返回下一个元素或抛出StopIteration的容器对象 由于Python中没有“迭代器”这个类，因此具有以下两个特性的类都可以称为“迭代器”类： 1、有__next__()方法，返回容器的下一个元素或抛出StopIteration异常 2、有__iter__()方法，返回迭代器本身 all(iterable)：如果迭代器里面的所有元素都为True时,返回True;否则返回False any(iterable）：如果迭代器里面的所有元素为False,返回False;否则返回True. 生成器一个个的生成数据，但占用内存更少，生成器是特殊的迭代器。 当yield存在函数时，函数就变成一个生成器。yield不像return那样返回值，而是每次产生多个值。每次使用yield产生一个值，函数就会被冻结。 用aim.__next__()得出结果： aim.send()得出结果： 列表生成式和字典推导式：]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
        <category>知识散记（3）</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（2）]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 接着上次的知识散记，我们接着看。 闭包在函数内部定义一个函数，并且这个函数用到了外面的变量。将这个函数以及用到的一些变量称之为闭包。 当某个函数被当成对象返回时，夹带了外部 变量，就形成了一个闭包。 global：一般多用在函数内，声明变量的作用域为全局作用域。箭头变化的时候加，箭头不变的时候可以不加。（箭头类似指针） nonlocal：nonlocal关键字用来在函数或其他作用域中使用外层(非全局)变量 lambda匿名函数： 不要看匿名函数蛮简单的，其实还是有点注意事项的。对默认参数的赋值只会在函数定义的时候绑定一次。x是一个自由变量。在运行的时候绑定，而不是在定义的时候绑定。执行时，x的值是多少就是多少。如果希望匿名函数可以在定义的时候绑定，并保持值不变，则采用下面方法。 assert:断言用来直接让程序崩溃，在程序中置入检查点 条件后可以添加字符串，用来解释断言 format: 对*和**的解释：星号(*)和(**)作为形参的时候是起到“打包”的作用，相反，作为实参的时候是起到“解包”的作用。 星号(*)或(**)作为形参，表示调用可变参数函数：通过在形参前加一个星号(*)或两个星号(**)来指定函数可以接收任意数量的实参。 从两个示例的输出可以看出：当参数形如 *args 时，传递给函数的任意个实参会按位置打包成一个元 组（tuple）；当参数形如 **args 时，传递给函数的任意个 key = value 实参会被包装进一个字典（dict）。 星号(**)和(*)作为实参时，表示通过解包参数调用函数： 常用内置函数补充：复数可以通过complex（real，imag）来指定。conjugete提取共轭复数 hasattr：hasattr() 函数用于判断对象是否包含对应的属性。如果对象有该属性返回 True，否则返回 False。 上下文管理器（context manager）：任何实现了__enter__和__exit__方法的对象都可称为上下文管理器 __enter__():方法返回资源对象，这里就是你将要打开的那个文件对象，__exit__()处理一些清除工作。因为File类实现上下文管理器，现在就可以使用with语句了。 实现上下文管理器的其他方法：使用contextmanager装饰器 python的三种修饰符：staticmethod、classmethod 和 property，作用分别是把类中定义的实例方法变成静态方法、类方法和类属性。staticmethod、classmethod具体看python的类和对象。 注意： 函数先定义，再修饰它；反之会编译器不认识； 修饰符“@”后面必须是之前定义的某一个函数； 每个函数只能有一个修饰符，大于等于两个则不可以。 property用法:它的作用把方法当作属性来访问（注意getnum和setter的顺序，一定getnum在第一个）]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
        <category>知识散记（2）</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（1）]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 今天来讲一下python的一些散装知识，还是蛮多的。所以分了几个部分。今天的部分是最简单的，废话少说。开始吧 导入模块方法：1、最常见的方式，直接将要导入的模块名称写在后面导入。import xxxx 2、from .. import .. 与import类似，只是更明确的要导入的方法或变量。 3、from modname import *，导入所有的类和公有方法。 if __name__==”__main__“:让你写的脚本模块既可以导入到别的模块中用，另外该模块自己也可执行。 常用的一些内置函数callable(object)：检查对象object是否可调用。如果返回True，object仍然可能调用失败；但如果返回False，调用对象ojbect绝对不会成功。 divmod(a,b)：以元祖的方式放回a//b以及a%b。 ord(str)：把对应的字符转成整数. chr(integer)：把整数转化成对应的字母. bool(x)：把一个值转化为布尔值,如果该值为假或者省略返回False,否则返回True abs(x) ：返回一个数的绝对值.该参数可以是整数或浮点数.如果参数是一个复数,则返回其大小 round(number[, ndigits])：返回浮点数number保留ndigits位小数后四舍五入的值。 dir([object]): 没有参数,返回当前局部范围的名单列表。有参数，试图返回该对象的有效的属性列表 issubclass(class, classinfo):返回True如果参数class是classinfo的一个子类，否则返回False。 isinstance(object, classinfo):返回True如果参数object是classinfo的一个实例，否则返回False(适用于继承)。 zip(*iterables):生成一个迭代器，迭代器聚合了从每个可迭代数集里的元素。它的内容只能被消费一次 map：第一个参数 function 以参数序列中的每一个元素调用 function函数，返回包含每次 function 函数返回值的新列表。 filter：filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。应该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 enumerate是一个非常有用的函数，直接看效果。 eval(str [,globals [,locals ]])：用来计算存储在字符串中的有效python表达式。 exec(object[, globals[, locals]])， 用来执行存储在字符串或文件中的python语句 格式化输出格式： python语句中一些基本规则和特殊字符： python调试：python调试两种方法都有用到pdb模块 第一种：在代码的目录下，打开cmd，输入python -m 文件名 h：帮助命令 第二种：可以在交互界面进行调试]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
        <category>知识散记（1）</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一些协程。协程是python中独有的，在其他语言中是没有这个概念的。协程是利用线程在等待的时候做事情。 使用yield完成协程： 使用greenlet完成协程： 使用gevent完成协程： 要想用时间模块延迟，则必须打补丁： 我还没看完，后面补。未完待续……]]></content>
      <categories>
        <category>python</category>
        <category>协程</category>
      </categories>
      <tags>
        <tag>python协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的进程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多进程：multiprocessing模块 os.getpid()获取当前进程的id os.getppid()获取父进程的id 大量启动子进程，可以用进程池pool批量创建子进程。可以通过processes改变创建的进程数目。 apply_async(func[, args=()[, kwds={}[, callback=None]]])该函数用于传递不定参数，非阻塞且支持结果返回进行回调。 将函数添加到进程池： map(func, iterable[, chunksize=None])：Pool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到返回结果。 注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。 close()：关闭进程池（pool），使其不在接受新的任务。 terminate()：结束工作进程，不在处理未处理的任务。 join([timeout])：主进程阻塞等待子进程的退出，join方法必须在close或terminate之后使用。timeout表示等待最多时间。若超出，则会直接执行下列代码 Value、Array是通过共享内存的方式共享数据 Value：将一个值存放在内存中， Array：将多个数据存放在内存中，但要求数据类型一致 Value: Array:两种情况 若为数字，表示开辟的共享内存中的空间大小，（Value表示为该空间绑定一个数值） 若为数组，表示在共享内存中存入数组 说明：三个0表示开辟的共享内存容量为3，当再超过3时就会报错。 Manager（Value、Array、dict、list、Lock、Semaphore等）是通过共享进程的方式共享数据。 进程间通信：Queue ，Pipe 使用方法和队列差不多 q.get_nowait()：和get()差不多，不用等。 Pipe:Pipe可以是单向(half-duplex)，也可以是双向(duplex)。我们通过mutiprocessing.Pipe(duplex=False)创建单向管道 (默认为双向)。一个进程从PIPE一端输入对象，然后被PIPE另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。 这里的Pipe是双向的。 Pipe对象建立的时候，返回一个含有两个元素的表，每个元素代表Pipe的一端(Connection对象)。我们对Pipe的某一端调用send()方法来传送对象，在另一端使用recv()来接收。 生产者与消费者模式：在两者中找一个缓冲的东西（队列，缓冲池），解决数据生产方和数据处理方数据不分配的问题。 耦合：谁和谁的关系越强，耦合性就越强。耦合性越强，程序维护越难。 解耦的好处：哪块不合适，就改那块。 接上面：]]></content>
      <categories>
        <category>python</category>
        <category>进程</category>
      </categories>
      <tags>
        <tag>python进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的线程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[重要的话写在前面：进程间不共享全局变量，线程间共享全局变量。 同步：按预定的先后次序进行运行 异步：不确定的次序 对于操作系统来说，一个任务就是一个进程。进程内的子任务成为线程 ，一个进程至少有一个线程 多任务执行的方式： 多进程 多线程 多进程+多线程 多线程：Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 传入参数为元祖。也就是即使只有一个参数，也要写逗号。 join():将线程加入到当前线程，并等待其终止 判断线程是否在运行： 守护线程：将daemon属性设为True，则该线程无法被连接 daemon属性可以保证主线程结束时可以同时结束子线程或者使主线程等待子线程结束后在结束。故称为守护线程。daemon默认为False，如需修改，必须在调用start()方法启动线程之前进行设置。不适用与idle的交互模式或脚本模式 当daemon属性为False时，主线程会检测子线程是否结束，如果子线程还在运行，则主线程会等待他完成后在退出。当daemon属性为True时，子线程没执行的不再执行，主线程直接退出。 通过轮询终止线程： threading的常用方法： ​ active_count() 当前活动的 Thread 对象个数 ​ current_thread() 返回当前 Thread 对象 ​ current_thread().name返回当前的Thread对象的名字 ​ get_ident() 返回当前线程 ​ enumerater() 返回当前活动 Thread 对象列表 ​ main_thread() 返回主 Thread 对象 ​ settrace(func) 为所有线程设置一个 trace 函数 ​ setprofile(func) 为所有线程设置一个 profile 函数 ​ stack_size([size]) 返回新创建线程栈大小；或为后续创建的线程设定栈大小为 size ​ TIMEOUT_MAX Lock.acquire(), RLock.acquire(), Condition.wait() 允许的最大值 threading 可用对象列表： ​ Thread 表示执行线程的对象 ​ Lock 锁原语对象 ​ RLock 可重入锁对象，使单一进程再次获得已持有的锁(递归锁) Condition： 条件变量对象，使得一个线程等待另一个线程满足特定条件，比如改变状态或某个值 ​ wait(timeout): 线程挂起，直到收到一个notify通知或者超时（可选的，浮点数，单位是秒s）才会被唤醒继续运行。wait()必须在已获得Lock前提下才能调用，否则会触发RuntimeError。 ​ condition = threading.Condition(lock=None) # 创建Condition对象 参数可以不传 ​ condition.acquire() # 加锁 ​ condition.release() # 解锁 ​ condition.wait(timeout=None) # 阻塞，直到有调用notify(),或者notify_all()时再触发 ​ condition.wait_for(predicate, timeout=None) # 阻塞，等待predicate条件为真时执行 ​ condition.notify(n=1) # 通知n个wait()的线程执行, n默认为1 ​ condition.notify_all() # 通知所有wait着的线程执行 ​ with condition: # 支持with语法，不必每次手动调用acquire()/release() Semaphore 为线程间共享的有限资源提供一个”计数器”，如果没有可用资源会被阻塞 Events：它是由线程设置的信号标志，如果信号标志为真，则其他线程等待直到信号接触。 Event对象实现了简单的线程通信机制，它提供了设置信号，清除信号，等待等用于实现线程间的通信。 event = threading.Event() 创建一个event 1、设置信号 event.set() 使用Event的set（）方法可以设置Event对象内部的信号标志为真。Event对象提供了isSet（）方法来判断其内部信号标志的状态。 当使用event对象的set（）方法后，isSet（）方法返回真 2、清除信号 event.clear() 使用Event对象的clear（）方法可以清除Event对象内部的信号标志，即将其设为假，当使用Event的clear方法后，isSet()方法返回假 3 、等待 event.wait() Event对象wait的方法只有在内部信号为真的时候才会很快的执行并完成返回。当Event对象的内部信号标志为假时，则wait方法一直等待到其为真时才返回。也就是说必须set新号标志为真 主线程在等事件设置后才继续执行 event使用示范： Barrier :创建一个”阻碍”，必须达到指定数量的线程后才可以继续 每个线程中都调用了wait()方法，在所有（此处设置为3）线程调用wait方法之前是阻塞的。也就是说，只有等到3个线程都执行到了wait方法这句时，所有线程才继续执行。 计算处于alive的Thread对象数量： 多线程避免全局变量的改变：上锁。上锁后执行的代码越少越好。 互斥锁：Lock是比较低级的同步原语，当被锁定后不属于特定的线程。一个锁有两个状态:Locked和unLocked.刚创建的的Locked处于unlocked状态。如果锁处于unlocked状态，acquire()方法将其修改为Locked并立即返回。如果锁处于locked状态，则阻塞当前线程并等待其他线程释放锁，然后将其修改为locked并立即返回。release()方法用来将锁的状态从locked修改为unlocked并立即返回。如果锁的状态本来就是unlocked，则会抛出异常 可重入锁Rlock对象也是一种常用的线程同步原语，可被同一个线程acquire()多次。当locked状态时，某现场拥有该锁，当处于unlocked状态时，该锁不属于任何线程。Rlock对象的acquire()/release()调用对可以嵌套，仅当最后一个或者最外层的release执行结束后，锁才会被设置为unlocked状态 死锁：双方都在等待对方的条件满足 避免死锁的方法：1、添加超时事件 2、 尽量避免（银行家算法） Threadlocal：保存当前线程的专有状态，这个状态对其他线程不可见。 全局变量local就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local看成全局变量，但每个属性如local.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 线程池：threadpool模块或concerrent.futures模块 threadpool模块比较老旧，不是主流。 threadpool.ThreadPool(poolsize)：定义一个线程池，创建了poolsize个线程。 threadpool.makeRequest(开启多线程的函数，函数相关参数，[回调函数]) putRequest:将所有要运行多线程的请求扔进线程池。 concerrent.futures：这个模块轻松绕开GIL ThreadPoolExecutor构造实例的时候，传入max_workers参数来设置线程池中最多能同时运行的线程数目。 使用submit函数来提交线程需要执行的任务（函数名和参数）到线程池中，并返回该任务的句柄（类似于文件、画图），注意submit()不是阻塞的，而是立即返回。 通过submit函数返回的任务句柄，能够使用done()方法判断该任务是否结束。上面的例子可以看出，由于任务有2s的延时，在task1提交后立刻判断，task1还未完成，而在延时4s之后判断，task1就完成了。 使用cancel()方法可以取消提交的任务，如果任务已经在线程池中运行了，就取消不了。这个例子中，线程池的大小设置为2，任务已经在运行了，所以取消失败。如果改变线程池的大小为1，那么先提交的是task1，task2还在排队等候，这是时候就可以成功取消。 使用result()方法可以获取任务的返回值。查看内部代码，发现这个方法是阻塞的 as_completed:一次取出所有任务的结果。as_completed()方法是一个生成器，在没有任务完成的时候，会阻塞，在有某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞住，循环到所有的任务结束。从结果也可以看出，先完成的任务会先通知主线程。 map的作用和submit一样，但略有不同。输出顺序和参数列表的顺序相同 wait方法接受三个参数，等待的任务序列，超时时间，以及等待条件。等待条件return_when默认为ALL_COMPLTED，表明要等待所有的任务都结束。还可以设为FIRST_COMPLETED，表示第一个任务完成就结束等待。FITST_EXCEPTION(注意要导入) 通过类创建线程： t.start后一定调用run函数，不定义run函数该线程不执行。对其他函数的调用只能在run函数里执行。多线程可以共享全局变量，但当数据量大时，数据会出错（产生资源竞争）。 线程是真的多，看到最后。迷迷糊糊，有错一定要提醒我。而且很多我还没有用过。后面用到的话，会补充上去的。接下来说最后一个：GIL。何为GIL？ GIL：全局解释器锁 单CPU的系统中运行多个进程那样，内存中可以存放多个程序，但任意时刻，只有一个程序在CPU中运行。同样地，虽然Python解释器中可以“运行”多个线程，但在任意时刻，只有一个线程在解释器中运行。 GIL保证了多线程时只有一个线程被调用。 所以多进程效率比多线程高，但是进程间通信比线程难。 解决方法：用C语言写关键部分。模块（ctypes）]]></content>
      <categories>
        <category>python</category>
        <category>线程</category>
      </categories>
      <tags>
        <tag>python线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F08%2F06%2Fnumpy%2F</url>
    <content type="text"><![CDATA[​ 最近这几年，机器学习和深度学习大火。而这其中的数据计算是非常多，而这得益于python的numpy模块。多提一句：numpy是没有GIL（多线程解释器锁）的。所以，numpy中的计算是非常快的。那什么是GIL呢？请看python的多线程。话不多说，让我们开启今天的数学之旅。PS：numpy很多API，我还没有学完。我只是总结了一部分，后面会补充的。 通过array生成矩阵： 可以通过dtype设置矩阵数据的类型。astype可以修改数据类型。 还有其他生成矩阵的方式： numpy生成随机数： seed的使用方法： 可以通过reshape修改列表的行数和列数，resize改变数组的尺寸大小。根据reshape传入的参数判断转为哪种数组。 numpy运算：加、减、乘、除类似。如果两个矩阵形状相同，两个矩阵对应的元素做操作。若两个矩阵形状不相同，其中一个矩阵的维度与另一个矩阵的维度相同，可以在该维度上做操作。 通过axis求每行（列）的元素和或最大、最小值。0代表列，1代表行。 还可以获得最大、最小值： 其他一些计算： 对clip函数的解释：小于5的元素都设为5，大于9的元素都设为9 flatten：对数组展开为一维数组 numpy的合并： numpy的分割： 转置矩阵： 未完待续……]]></content>
      <categories>
        <category>python</category>
        <category>数学计算</category>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>python的数学计算模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一下网络编程。这里面用到一个库：socket。网络通信其实就是两个进程之间在编程。先说两个重要的协议：TCP协议 和 UDP协议。TCP协议是传输控制协议，UDP协议是数据传输协议。TCP和UDP的区别：TCP慢但是稳定，因为它经过了三次握手和四次挥手，不会丢失数据。UDP快。 socket:注意参数是一个tuple，包含地址和端口号。 在同一个os中，端口不允许相同，即如果某个端口已经被使用了，那么在这个进程释放之前，其他进程都不能使用这个端口。（端口用来区分进程，若相同，不能把数据发送到准确的进程） 创建Socket时，AF_INET指定使用IPv4协议，如果要用更先进的IPv6，就指定为AF_INET6。SOCK_STREAM指定使用面向流的TCP协议，这样，一个Socket对象就创建成功，但是还没有建立连接。 coding： 主机名可以通过调用socket.gethostname()获得 接收数据时，调用recv(max)方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到recv()返回空数据，表示接收完毕，退出循环。当我们接收完数据后，调用close()方法关闭Socket，这样，一次完整的网络通信就结束了 创建TCP连接时，主动发起连接的叫客户端，被动响应连接的叫服务器。 客户端要主动发起TCP连接，必须知道服务器的IP地址和端口号。 TCP服务端建立步骤： 一般是服务器（接受方）绑定端口，客户端（发送方）不绑定 UDP不需要调用listen（）方法。可以直接接收数据 TCP调用listen()方法开始监听端口，将主动套接字（默认）变为被动套接字，传入的参数指定等待连接的最大数量 TCP服务端： TCP客户端： 一般send()和recv()用于TCP，sendto()及recvfrom()用于UDP。sendto和recvfrom一般用于UDP协议中,但是如果在TCP中connect函数调用后也可以用。 服务器编程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立Socket连接，随后的通信就靠这个Socket连接了。 由于服务器会有大量来自客户端的连接，所以，服务器要能够区分一个Socket连接是和哪个客户端绑定的。一个Socket依赖4项：服务器地址、服务器端口、客户端地址、客户端端口来唯一确定一个Socket。但是服务器还需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。 然后，我们要绑定监听的地址和端口()。服务器可能有多块网卡，可以绑定到某一块网卡的IP地址上，也可以用0.0.0.0绑定到所有的网络地址，还可以用127.0.0.1绑定到本机地址。127.0.0.1是一个特殊的IP地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。 端口号需要预先指定。请注意，小于1024的端口号必须要有管理员权限才能绑定： 每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接 利用多线程和socket进行聊天室的创建（UDP）： 下面这段代码是单进程服务器，配合进程或线程也可以建立多任务服务器（TCP）： serverSocket：当这个套接字被关闭时，代表不再接收新的客户端连接 clientSocket：当这个套接字被关闭时，代表不能使用send和recv发收数据。 当利用线程建立多任务服务器时，clientSocket不能关闭。因为子线程共用数据 当利用进程建立时，clientSocket能关闭。子进程和父进程完全”一样“（实时拷贝） 单进程实现多任务： 最后，讲一下单播，多播和广播。 单播：一对一 多播：一对多 广播：一对所有 UDP有广播，TCP没有广播 UDP发送广播数据的条件：]]></content>
      <categories>
        <category>python</category>
        <category>网络编程（socket）</category>
      </categories>
      <tags>
        <tag>python的网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F08%2F05%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式（不能随意添加空格，不然会改变原来含义）： 元字符(不能匹配自身): . $ ^ ( ) { } [ ] $ + \ | *， |：A | B 会匹配 A 或 B 中出现的任何字符。为了能够更加合理的工作，| 的优先级非常低。例如 Fish|C 应该匹配 Fish 或 C，而不是匹配 Fis，然后一个 ‘h’ 或 ‘C’。同样，我们使用 | 来匹配 ‘|’ 字符本身；或者包含在一个字符类中，像这样 [|]。 ^:匹配字符串的起始位置。如果设置了 MULTILINE 标志，就会变成匹配每一行的起始位置。在 MULTILINE 中，每当遇到换行符就会立刻进行匹配。 $:匹配字符串的结束位置，每当遇到换行符也会离开进行匹配。 +：用于指定前一个字符匹配一次或多次 ：匹配的是零次或多次 ？：指定前一个字符匹配零次或者一次。 {m，n}(m和n都是十进制整数)：它的含义是前一个字符必须匹配m次到n（包括n次）次之间 原始字符串来表示正则表达式（就是在字符串前边加上 r） \A:只匹配字符串的起始位置。如果没有设置 MULTILINE 标志的时候，\A 和 ^ 的功能是一样的；但如果设置了 MULTILINE 标志，则会有一些不同：\A 还是匹配字符串的起始位置，但 ^ 会对字符串中的每一行都进行匹配。 \Z:只匹配字符串的结束位置 \B:另一个零宽断言，与 \b 的含义相反，\B 表示非单词边界的位置。 零宽断言： 有些元字符它们不匹配任何字符，只是简单地表示成功或失败，因此这些字符也称之为零宽断言 前向断言： （1）：前向肯定断言：如果当前包含的正则表达式（这里以 … 表示）在当前位置成功匹配，则代表成功，否则失败。一旦该部分正则表达式被匹配引擎尝试过，就不会继续进行匹配了；剩下的模式在此断言开始的地方继续尝试。 （2）：前向否定断言：这跟前向肯定断言相反（不匹配则表示成功，匹配表示失败） 假定我们要处理一段html，我们要替换掉相对url，例如text 这个a标签我们要替换成text，而对于代码 这样的a标签则要保留不做替换。这个应用场景下 就需要判断A标签的href属性如果不是以http://开头则匹配，即要做前向否定的断言. 脱字符：^ ,例如[^ 5 ]会匹配任何字符 “5”之外的任何字符 [ ]他们指定一个字符类用于存放你需要的字符集合。可以单独列出需要匹配字符，也可以两个字符和一个横杆-指定匹配的范围。元字符在方括号中不会触发“特殊功能”，在字符类中，它们只匹配自身。 反斜杠 \：如果在反斜杠后边紧跟着一个元字符，那么元字符的“特殊功能”也不会被触发。例如你需要匹配符号[ 或 \，你可以在他们前面加上一个反斜杠，以消除他们的特殊功能：\[ ,\\ 注意用小括号括住要重复的内容： 匹配ip（万能版）： 非捕获组和命名组： 非捕获组的语法是 (?:…)，这个 … 你可以替换为任何正则表达式。 “捕获”就是匹配的意思啦，普通的子组都是捕获组，因为它们能从字符串中匹配到数据 命名组：：(?P)。很明显，&lt; &gt; 里边的 name 就是命名组的名字啦，除了使用名字访问， 命名组仍然可以使用数字序号进行访问 正则表达式使用以下方法修改字符串： split(***string***[, maxsplit=0**])*：通过正则表达式匹配来分割字符串。如果在 RE 中，你使用了捕获组，那么它们的内容会作为一个列表返回。你可以通过传入一个 *maxsplit 参数来设置分割的数量。如果 maxsplit 的值是非 0，表示至多有 maxsplit 个分割会被处理，剩下的内容作为列表的最后一个元素返回。 .sub(***replacement***, string**[,* count=0**])*返回一个字符串，这个字符串从最左边开始，所有 RE 匹配的地方都替换成 *replacement。如果没有找到任何匹配，那么返回原字符串。可选参数 *count 指定最多替换的次数，必须是一个非负值。默认值是 0，意思是替换所有找到的匹配。下边是使用 sub() 方法的例子，它会将所有的颜色替换成 color： subn:subn() 方法跟 sub() 方法干同样的事情，但区别是返回值为一个包含有两个元素的元组：一个是替换后的字符串，一个是替换的数目。 匹配方法： 匹配的方法和属性： group(N) 返回第N组括号匹配的字符，groups() 返回所有括号匹配的字符，以tuple格式 match匹配的m： findall() 需要在返回前先创建一个列表，而 finditer() 则是将匹配对象作为一个迭代器返回 利用compile来先编译的方法是模式级别的方法（适用于多次使用该正则表达式），可以针对同一种模式做多次匹配，如下图：另一种是模式对象方法 import re import re p=re.compile() re.search(“”,””) p.search() 贪婪模式和非贪婪模式： 贪婪模式是让正则表达式尽可能的匹配符合的内容 在匹配的字符后面加一个问号，启动非贪婪模式 编译标志：编译标志让你可以修改正则表达式的工作方式。在 re模块下，编译标志均有两个名字：完整名和简写 ASCII(re.A) 使得 \w，\W，\b，\B，\s 和 \S 只匹配 ASCII 字符，而不匹配完整的 Unicode 字符。这个标志仅对 Unicode 模式有意义，并忽略字节模式。 DOTALL(re.S) 使得 . 可以匹配任何字符，包括换行符。如果不使用这个标志，. 将匹配除了换行符的所有字符。 IGNORECASE(re.I) 字符类和文本字符串在匹配的时候不区分大小写。举个例子，正则表达式 [A-Z] 也将会匹配对应的小写字母，像 FishC 可以匹配 FishC，fishc 或 FISHC 等。如果你不设置 LOCALE，则不会考虑语言（区域）设置这方面的大小写问题。 LOCALE(re.L) 使得 \w，\W，\b 和 \B 依赖当前的语言（区域）环境，而不是 Unicode 数据库。区域设置是 C 语言的一个功能，主要作用是消除不同语言之间的差异。例如你正在处理的是法文文本，你想使用 \w+ 来匹配单词，但是 \w 只是匹配 [A-Za-z] 中的单词，并不会匹配 ‘é’ 或 ‘&#231;’。如果你的系统正确的设置了法语区域环境，那么 C 语言的函数就会告诉程序 ‘é’ 或 ‘&#231;’ 也应该被认为是一个字符。当编译正则表达式的时候设置了 LOCALE 的标志，\w+ 就可以识别法文了，但速度多少会受到影响。 MULTILINE(re.M) 通常 ^ 只匹配字符串的开头，而 $ 则匹配字符串的结尾。当这个标志被设置的时候，^ 不仅匹配字符串的开头，还匹配每一行的行首；&amp; 不仅匹配字符串的结尾，还匹配每一行的行尾。 VERBOSE(re.X) 这个标志使你的正则表达式可以写得更好看和更有条理，因为使用了这个标志，空格会被忽略（除了出现在字符类中和使用反斜杠转义的空格）；这个标志同时允许你在正则表达式字符串中使用注释， 符号后边的内容是注释，不会递交给匹配引擎（除了出现在字符类中和使用反斜杠转义的 ） 正则表达式特殊符号及用法：]]></content>
      <categories>
        <category>python</category>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读取、写入excel]]></title>
    <url>%2F2019%2F08%2F05%2F%E8%AF%BB%E5%8F%96%E3%80%81%E5%86%99%E5%85%A5excel%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python如何对excel进行 读取和写入操作。 ##xlrd只支持对excel文件个是为xls文件的读取。 table = data.sheets()[0] #通过索引顺序获取 table = data.sheet_by_index(sheet_index)) #通过索引顺序获取 table = data.sheet_by_name(sheet_name)#通过名称获取 name=workbook_r.sheet_names() #获取文件的所有工作表名字 对行的操作： nrows = table.nrows #获取该sheet中的有效行数 table.row(rowx) #返回由该行中所有的单元格对象组成的列表 table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表 table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表 table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表 table.row_len(rowx) #返回该列的有效单元格长度 对列的操作： ncols = table.ncols #获取列表的有效列数 table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表 table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表 table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表 table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 获取单元格内容： 单元格 A1= table.cell(0,0).value C4=able.cell(2,3).value 使用行列索引 cell_A1 = table.row(0)[0].value cell_A2 = table.col(1)[0].value ##xlwt只支持对Excel文件格式为xls文件的写入 add_sheet(sheet_name): 添加sheet get_sheet(Sheet_name): 选择sheet save(file_name): 保存]]></content>
      <categories>
        <category>python</category>
        <category>读取、写入excel</category>
      </categories>
      <tags>
        <tag>xlrd、xlwt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收GC]]></title>
    <url>%2F2019%2F08%2F05%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6GC%2F</url>
    <content type="text"><![CDATA[​ 每种语言都有自己的垃圾回收机制。接下来我们来讲一下python的垃圾回收机制。 小整数对象池：python对小整数的定义为[-5，257)，这些整数对象是提前建立好的，不会被垃圾回收。单个字母也一样，但是当定义两个相同的字符串（没有空格等特殊符号），触发intern机制，引用计数为零，触发垃圾回收。 引用计数机制的优点：简单、实时性（一旦没有引用，内存就直接释放了）。 缺点：维护引用计数消耗资源、循环引用 python以引用计数为主，隔代回收为辅进行垃圾回收 GC模块（不能重写del方法）： 1、gc.set_debug(flags) 设置gc的debug日志，一般设置为gc.DEBUG_LEAK 2、gc.collect([generation]) 显式进行垃圾回收，可以输入参数，0代表只检查第一代的对象，1代表检查一，二代的对象，2代表检查一，二，三代的对象，如果不传参数，执行一个full collection，也就是等于传2。 返回不可达（unreachable objects）对象的数目 3、gc.get_threshold() 获取的gc模块中自动执行垃圾回收的频率 4、gc.set_threshold(threshold0[,threshold1[, threshold2]) 设置自动执行垃圾回收的频率。 5、gc.get_count() 获取当前自动执行垃圾回收的计数器，返回一个长度为3的列表 6、gc.disable() 把gc关闭,gc.enable()打开gc（默认打开） 7.gc.garbage 存储垃圾 导致引用计数+1的情况： 导致引用计数-1的情况： 查看一个对象的引用计数： 因为调用函数的时候传入a，所以是2.真正的引用计数=sys.getrefcount()-1]]></content>
      <categories>
        <category>python</category>
        <category>GC</category>
      </categories>
      <tags>
        <tag>python的GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂生产模式]]></title>
    <url>%2F2019%2F08%2F05%2F%E5%B7%A5%E5%8E%82%E7%94%9F%E4%BA%A7%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一下什么是工厂方法模式。工厂方法模式是在基类完成基本框架的搭建，在子类中具体实现方法的实现。工厂模式是一种典型的解耦模式。 函数或者类之间的关系越强，耦合性越强。代码就越难更新。 使用函数进行解耦： 使用类进行解耦：]]></content>
      <categories>
        <category>python</category>
        <category>工厂方法模式</category>
      </categories>
      <tags>
        <tag>python生产模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类和对象]]></title>
    <url>%2F2019%2F08%2F05%2F%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[​ 众所周知，面向过程是根据业务逻辑从上到下写代码，面向过程：根据业务逻辑从上到下写代码面向对象：将数据与函数绑定到一起，进行封装，这样能更快速的开发程序，减少了重复代码的重写过程。面向对象语言三个基本要素：封装 继承 多态把函数和全局变量和在一起就是封装。而python就属于面向对象的语言。 类与对象的关系和区别：类是抽象的概念，仅仅代表事物的模板。对象是一个能够看得到，摸得着的具体的实体比如：飞机是对象，飞机图纸是类。 类由三部分构成: 类的名称:类名 类的属性：一组数据 一个特殊的对象：能够知道这个对象的class. 类的方法：允许进行的操作 类的抽象：拥有相同或者类似属性和行为的对象都可以抽象出一个类 python调用__init__方法的作用：初始化对象 python调用__str__方法： 私有方法：外界不能直接调用 私有属性：可以添加，可以添加后取值，不可以取值。（以双下划线开头为私有变量，单下划线开头的为保护变量） 通过内部方法取值： 私有属性无法取值的原因是因为名字重整技术，其实可以通过_类名_属性可以访问： 在这里讲一下私有的原理：这是通过名字重整机制改变的。命名规则：_类名__num（尽量不要用） python可以自己调用__del__方法，__del__是该对象被删除后调用的方法，注意：这里的删除是没有引用对象 完全删除后（没有引用对象）： 测量对象的引用对象：用sys模块的sys.getrefcount，得出结果后减一。 __new__方法：创建对象 实例化对象相当于做三件事： 1.通过__init__来创建对象，然后找了一个变量来接收___init__的返回值。这个返回值表示创建出来的对象引用。 2.__init__（刚刚创建出来的对象的引用） 3.返回对象的引用。 __new__方法 和 \init__方法的区别: 1.\init__ 通常用于初始化一个新实例，控制这个初始化的过程，比如添加一些属性， 做一些额外的操作，发生在类实例被创建完以后。它是实例级别的方法。没有返回值。负责初始化 2._new_ 通常用于控制生成一个新实例的过程。它是类级别的方法，参数是cls。负责创建。__init__和\new__方法合起来相当于C++的构造函数。 下面讲一个新的概念：单例 单例是创建了多少个对象都是指向同一片内存。 只初始化一次对象： 继承：可以少写代码。继承父类的方法和属性 继承可以多类继承。当子类和父类有什么不同时，可以进行重写（在子类中写一个和父类方法名相同的方法进行不同操作） 私有方法，私有属性不能被继承，可以被间接调用，如果在子类中实现了一个公有方法调用的方法或属性，那么这个方法是不能调用继承的父类的私有方法和私有属性 多继承： 当子类中的方法和父类方法名一样时会按照以下顺序去进行执行： 子类调用父类的三种方法： 父类.方法 super().init(不用传self，传参数) super(父类，self).init（不传self，参数）可以根据父类指定调用哪一个父类 多态：在写完方法的时候并不知道调用的是什么方法。真正执行的时候才知道 接下来的内容也很重要哦： 类在程序中也属于一个对象，称之为类对象。同过类创建出来的对象称之为实例对象。 类属性(classmethod)：类对象里的属性 实例属性：实例对象里的属性 实例属性和类属性的区别：实例属性和具体的某个实例对象有关系，且一个实例对象和另外一个实例对象是不共享属性的。类属性属于类对象，并且多个实例对象共享同一个类属性。 实例方法和类方法： 静态方法： 实例方法 类方法和静态方法的区别：实例方法和类方法必须传一个参数（实例方法self用来接收对象，类方法cls用于接收类），静态方法不需要参数（可以有）。 动态添加属性和方法： slots可以限制添加属性：这可以告诉解释器这个类的所有实例属性都在这了。可以节省大量内存。每个子类都要定义slots属性，因为解释器会忽略继承的slots属性。 内建属性： getattribute:属性拦截器 使用类名调用类属性时，不会经过__getattribute__方法，其他均要调用。（可以用来做日志）]]></content>
      <categories>
        <category>python</category>
        <category>类和对象</category>
      </categories>
      <tags>
        <tag>python的类和对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python异常]]></title>
    <url>%2F2019%2F08%2F05%2Fpython%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python的异常。何为异常？即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。异常时python对象，表示一个错误。当python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。 ##try/except可以用来捕获异常 ##一个try捕获多个异常： ##一个try对多个except：根据不同的except做不同的操作 ##try-finally语句无论是否发生异常都将执行最后的代码。 ##异常传递 ##raise自定义异常： ##最后是python的标准异常：]]></content>
      <categories>
        <category>python</category>
        <category>异常</category>
      </categories>
      <tags>
        <tag>python异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python数据类型]]></title>
    <url>%2F2019%2F08%2F04%2Fpython%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ 每种编程语言都有属于自己的数据类型。今天，我们就来讲一下python的数据类型——列表，元祖，字典，字符串，堆，集合，队列。 列表​ 同时还有一些比较常用的方法，类似cmp(比较大小，python3已经找不到。如要使用，可以利用operator模块)，len(list)计算列表元素个数，max(list)求列表中的最大值，min(list)求最小值。下面对上面列表的排序方法(list.sort)进行讲解： 下张图片是对以上方法的coding： ##元祖（tuple） 元祖不像列表可以改变。元祖是不可变的。但可以利用切片灵活使用。 字典（dict）####注意：字典支持常见的集合操作（&amp;等操作） 字典有多个键与其对应的值构成的键-值对组成。键-值对称为项。每个键和它的值之间用:(冒号)隔开，项之间用,(逗号)隔开。 len(d)返回d中项(键-值对)的数量 d[k]返回关联到键k的值 d[k]=v将值关联到键k上 del d[k] 删除键为k的项 k in d 检查d中是否有含有键为k的值 clear()：清除字典中所有的项 fromkeys()：使用给定的键建立新的字典，每个键都对应一个默认的值None，也可以设立默认值 get():访问字典项，访问一个不存在的键时，没有异常。且可以定义默认值为None。 items和iteritems： items将所有的项以列表方式返回，列表中的每一项都表示为（键，值）对的形式。 iteritems方法的作用大致相同，但是会返回一个迭代器对象而不是列表。 keys和iterkeys： keys方法将字典中的键以列表形式返回，而iterkeys则返回针对键的迭代器 values和itervalues： values方法以列表的形式返回字典中的值，itervalues返回值的迭代器。与返回键的列表不同的是，返回值的列表中可以包含重复的元素。 pop：用来获得对应于给定键的值，然后将这个键-值对从字典中移除。 popitem：弹出随机的项 setdefault：获得与给定键相关联的值，能在字典中不含有给定键的情况下设立相应的键值。 update：可以利用一个字典项更新另外一个字典 堆（heapq）使用该数据类型前，我们先导入一个新的模块heapq heappush(heap,x):将x入对 heapop(heap)：将堆中最小的元素弹出 heapify(heap)：将heap属性强制应用到任意一个列表 heapreplace(heap,x)：将堆中最小的元素弹出，同时将x入堆 nlargest(n,iter)：返回iter中前n大的元素 nsmallest(n,iter)：返回iter中前n小的元素 集合（set）集合可取交集、取并集、取差集、对称差集： 利用set做去重操作： 集合分为可变集合和不可变集合： 队列（Queue）在python中，队列时线程间最常用的交换数据形式，Queue模块时提供队列操作的模块。 先进先出（FIFO）： 先进后出（LIFO）： 优先级队列：优先级队列put进去一个元祖，（优先级，数据），优先级数字越小，优先级越高。 注意：如果有两个元素优先级是一样的，那么在出队的时候是按照先进先出的顺序。 双端队列： 队列的方法： 使用put方法往队列中添加元素，需要考虑是否能放下的问题如果放不下了，默认会阻塞(block=True)，阻塞时可以定义超时时间timeout。可以使用block=False设置阻塞时立即报错 使用get()从队列里取数据。如果为空的话，blocking= False 直接报 empty异常。如果blocking = True，就是等一会，timeout必须为 0或正数。None为一直等下去，0为不等，正数n为等待n秒还不能读取，报empty异常。 字符串（string）用+拼接字符串： 将值转换为字符串的机制： 1、通过str函数，把值转换为合理形式的字符串，以便用户可以理解 2、通过repr函数创建一个字符串，以合法的python表达式的形式表示值 join和split： 字符串格式化方式： capitalize() 把字符串的第一个字符改为大写 casefold() 把整个字符串的所有字符改为小写 center(width) 将字符串居中，并使用空格填充至长度 width 的新字符串 count(sub[, start[, end]]) 返回 sub 在字符串里边出现的次数，start 和 end 参数表示范围，可选。 encode(encoding=’utf-8’, errors=’strict’) 以 encoding 指定的编码格式对字符串进行编码。 startswith(prefix[, start[, end]]) 检查字符串是否以 prefix 开头，是则返回 True，否则返回 False。start 和 end 参数可以指定范围检查，可选。 endswith(sub[, start[, end]])检查字符串是否以 sub 子字符串结束，如果是返回 True，否则返回 False。start 和 end 参数表示范围，可选。 startswith和endswith如果需要同时针对多个选项做检查，只需要给startswith和endswith提供包含可能选项的元祖。 expandtabs([tabsize=8]) 把字符串中的 tab 符号（\t）转换为空格，如不指定参数，默认的空格数是 tabsize=8。 find(sub[, start[, end]])检测 sub 是否包含在字符串中，如果有则返回索引值，否则返回 -1，start 和 end 参数表示范围，可选。 maketrans() 方法用于创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。 注：两个字符串的长度必须相同，为一一对应的关系。 translate(table)根据 table 的规则（可以由 str.maketrans(‘a’, ‘b’) 定制）转换字符串中的字符 index(sub[, start[, end]]) 跟 find 方法一样，不过如果 sub 不在 string 中会产生一个异常。 isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返回 True，否则返回 False。 isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True，否则返回 False。 isdecimal() 如果字符串只包含十进制数字则返回 True，否则返回 False。 isdigit() 如果字符串只包含数字则返回 True，否则返回 False。 islower() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是小写，则返回 True，否则返回 False。 isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。 isspace() 如果字符串中只包含空格，则返回 True，否则返回 False。 istitle() 如果字符串是标题化（所有的单词都是以大写开始，其余字母均小写），则返回 True，否则返回 False。 isupper() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是大写，则返回 True，否则返回 False。 join(sub) 以字符串作为分隔符，插入到 sub 中所有的字符之间。 ljust(width) 返回一个左对齐的字符串，并使用空格填充至长度为 width 的新字符串。 rjust(width) 返回一个右对齐的字符串，并使用空格填充至长度为 width 的新字符串。 format也可以完成对齐的任务。“&lt;”：左对齐 “&gt;”：右对齐 “^”：居中对齐 lower() 转换字符串中所有大写字符为小写。 lstrip() 去掉字符串左边的所有空格无法去除中间的字符 rstrip() 删除字符串末尾的空格。无法去除中间的字符 partition(sub) 找到子字符串 sub，把字符串分成一个 3 元组 (pre_sub, sub, fol_sub)，如果字符串中不包含 sub 则返回 (‘原字符串’, ‘’, ‘’) replace(old, new[, count]) 把字符串中的 old 子字符串替换成 new 子字符串，如果 count 指定，则替换不超过 count 次。 rfind(sub[, start[, end]]) 类似于 find() 方法，不过是从右边开始查找。返回值是下标 rindex(sub[, start[, end]]) 类似于 index() 方法，不过是从右边开始。 rpartition(sub) 类似于 partition() 方法，不过是从右边开始查找。 split(sep=None, maxsplit=-1) 不带参数默认是以空格为分隔符切片字符串，如果 maxsplit 参数有设置，则仅分隔 maxsplit 个子字符串，返回切片后的子字符串拼接的列表。 splitlines(([keepends])) 在输出结果里是否去掉换行符，默认为 False，不包含换行符；如果为 True，则保留换行符。。 strip([chars]) 删除字符串前边和后边所有的空格，chars 参数可以定制删除的字符，可选。 swapcase() 翻转字符串中的大小写。 title() 返回标题化（所有的单词都是以大写开始，其余字母均小写）的字符串。 upper() 转换字符串中的所有小写字符为大写。 zfill(width) 返回长度为 width 的字符串，原字符串右对齐，前边用 0 填充。]]></content>
      <categories>
        <category>python</category>
        <category>数据类型</category>
      </categories>
      <tags>
        <tag>python数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2019%2F08%2F03%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[​ 你好陌生人，欢迎来到湛蓝星空的博客。这是我的第一篇博客，纠结了很久要不要写，因为我通常都是用一些笔记软件来记录学的一些知识。在这里，希望你能学到你希望学到的东西。 PS：本人实在太菜。如果有错误，请谅解。]]></content>
      <categories>
        <category>你好</category>
      </categories>
      <tags>
        <tag>你好</tag>
      </tags>
  </entry>
</search>
