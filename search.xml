<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度卷积网络]]></title>
    <url>%2F2019%2F10%2F02%2F%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[经典网络LeNet - 5LeNet-5是专门为灰度图训练的。所以他的图像样本的深度都是1。 LeNet-5的网络结构： AlexNet网络结构： 实际上论文的原文是使用224x224x3的，但经过试验发现227x227x3效果更好。 VGG-16网络结构： 从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。 可以看到 VGG16 是13个卷积层+3个全连接层叠加而成。 残差网络网络越深越难训练，因为存在梯度消失和梯度爆炸的问题。本小节学习跳远连接。跳远连接可从某一个网络层激活，然后迅速反馈给另外一层甚至是神经网络的更深层。我们可以利用跳远连接构建能够训练深度网络的ResNets。 ResNets是由残差块构建的。那什么是残差块？看下图 这是一个两层的神经网络，它在L层进行激活，得到a[ l +1]再次进行激活，两层之后得到a[ l +2]。下图中的黑色部分即为计算过程。 而在残差网络中，我们直接将a^[ l ]拷贝到蓝色箭头所指位置。在线性激活之后、Relu非线性激活之前加上a[ l ]，不在沿着原来的主路径传递。这就意味者我们主路径过程的第四个式子替换为蓝色式子，也正是这个蓝色式子中加上的a[ l ]产生了一个残差块。 为什么残差网络有用？一个网络深度越深，它在训练集上训练网络的效率会有所减弱，但对于ResNets就不完全是这样了。 上一张图已经说过a^[ l +2]=g(z^[ l +2]+a^[ l ])，添加项a^[ l ]是刚添加的跳远连接的输入。解开这个式子，得： a^[ l+2]=g(w^[ l+2] * a^[ l+1]+b^[ l+2]+a^[ l ])，这里w和b为关键值。如果w和b均为0，那a^[ l+2]=g(a^[ l ])=a^[ l ](假设这里得激活函数是Relu)。结果表明，残差块学习这个恒等式函数残差块并不难，跳远连接让我们很容易的得到a^[ l+2]=a^[ l ] 这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络。因为对它来说，学习恒等函数对它来说很简单，尽管它多了两层，也只是把a^[ l ]的值赋给a^[ l+2]。 所以，残差网络有用的原因是这些函数残差块学习恒等函数非常容易。 1x1的卷积看到这个标题，你也许会迷惑，1x1的卷积能做什么呢？不就是乘以数字吗？结果并非如此 看上图，你会觉得这个1x1的过滤器没什么用，只是对输入矩阵乘以某个数字。但这个1x1的过滤器仅仅是对于6x6x1的信道图片效果不好 。如果是一张6x6x32的图片就不一样了。 具体来说，1x1卷积所实现的功能是遍历这36个单元格。计算输入图中32个数字和过滤器中32个数字的乘积，然后应用Relu函数。我们以一个单元格为例，用着36个数字乘以这个输入层上1x1的切片，得到一个实数画在下面图中。 这个1x1x32的过滤器中的32可以这样理解，一个神经元的输入是32个数字，乘以相同高度和宽度上某个切片上的32个数字。这三十二个数字具有不同信道，乘以32个权重，然后应用Relu非线性函数。一般来说，如果过滤器不止一个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6x6x过滤器数量。所以1x1卷积可以从根本上理解为这32个单元都应用了一个全连接神经网络。全连接层的作用是输入32个数字和过滤器数量，标记为nc^[ l+1]。在36个单元上重复此过程，输出结果是6x6x过滤器数量。这种方法通常称为1x1卷积，也成为Network in Network。 下面是一个1x1卷积的应用： 假设这是一个28x28x192的输入层，我们可以利用池化层压缩它的高度和宽度。但是如果信道数量很大，我们该如何把它压缩为28x28x32维度的层呢？我们可以用32个大小为1x1的过滤器，每个过滤器的大小都是1x1x192维，因为过滤器中信道的数量必须与输入层中信道的数量一致。因此过滤器数量为32，输出层为28x28x32。这就是压缩nc的方法。 Inception网络构建卷积层是，我们需要决定过滤器的大小是3x3还是5x5或者其它大小？或者要不要添加池化层？而我们接下来要讲的Inception就是代替我们来做决定的。虽然网络结构会变得非常复杂，但网络表现得非常好。我们来看一下原理。 基本思想：不需要人为的决定使用哪个过滤器，或者是否需要池化，而是由网络自行确定这些参数。人们只需给出这些参数的所有可能值，然后把这些输出连起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合。 举个例子： 如果我们直接计算上图，我们的计算成本为28x28x32x5x5x192 但是我们用1x1卷积后： 我们的计算成本变为28x28x16x192+28x28x32x5x5x16，使用1x1卷积后计算成本是没使用前的1/10。 下面再举一个例子： 例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=”SAME”)，输出数据的大小为100x100x256。其中，卷积层的参数为5x5x128x256。假如上一层输出先经过具有32个输出的1x1卷积层，这时得图片输出为100x100x32，接着再经过具有256个输出的5x5卷积层，那么最终的输出数据的大小仍为100x100x256，但卷积参数量已经减少为1x1x128x32 + 5x5x32x256，大约减少了4倍。 在inception结构中，大量采用了1x1的矩阵，主要是两点作用：1）对数据进行降维；2）引入更多的非线性，提高泛化能力，因为卷积后要经过ReLU激活函数。 搭建inception网络： inception就是将这些模块都组合到一起。 Inception network：下面这个图就是Inception的网络： 看起来很复杂，但我们截取其中一个环节，如上图的红色框。就会发现这不正是我们搭建的inception吗。另外，再一些inception模块前网络会使用最大池化层来修改高和宽的维度。 其实上图的Inception并不完整，看下图： 多出了红色框住的部分。这些分支是做什么的呢?这些分支是通过隐藏层做出预测。 迁移学习我们在做计算机视觉的应用时，相对于从头训练权重，下载别人训练好的网络结构的权重作为我们预训练，然后转换到感兴趣的任务上。别人的训练过程可能是需要花费好几周，并且需要很多的GPU找最优的过程，这就意味着我们可以下载别人开源的权重参数并把它当做一个很好的初始化，用在我们的神经网络上。这就是迁移学习。 假如我们在做一个识别任务，却没有很多的训练集。我们就可以把别人的网络下载，冻结所有层的参数。我们只需要训练和我们Softmax层有关的参数，然后把别人Softmax改成我们自己Softmax。通过别人的训练的权重，我们可能会得到一个好的结果，即使我们的训练集并不多。 由于前面的层都冻结了，相当于一个固定函数，不需要改变，因为我们不训练它。 网络层数越多，需要冻结的层数越少，需要训练的层数就越多。 数据扩充 数据扩充也叫数据增强。因为计算机视觉相对于机器学习，数据较少。所以数据增强为了增加数据的数量。下面我们讲一下数据增强的办法： 垂直镜像对称 随机裁剪 色彩转换 给RGB三个通道加上不同的失真值 这些可以轻易改变图像的颜色，但是对目标的识别还是保持不变的。所以使用这种数据增强方法使得模型对照片的颜色更改更具鲁棒性。]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN详解]]></title>
    <url>%2F2019%2F10%2F01%2FCNN%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[全连接神经网络的局限性过去的文章中，我们都是对 64×64 的小图片进行操作，实际上，因为每张图片都有 3 个颜色通道，它的数据量是 64×64×3=12288。 现代计算机中，64×64 真的是很小的图片，如果想处理大一点的图片，比如1000×1000，那么最终输入层的数据量是300万，假设我们有一个隐藏层，含有1000个神经元构成的全连接网络，那么数据量将达到 1000*300万，也就是30亿。在这样的情况下很难获得足够的数据防止过拟合，并且需要的内存大小很难得到满足。 本篇讲解的卷积神经网络（也称作 ConvNets 或 CNN）不仅可以达到减少参数的作用，而且在图像分类任务中还有其他优于全连接神经网络的特性。 卷积神经网络概览一个图像的相互靠近的局部具有联系性，卷积神经网络的思想就是用不同的神经元去学习图片的局部特征，每个神经元用来感受局部的图像区域，如下图不同颜色的圆圈所示： 然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。 卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了由卷积层和子采样层构成的特征抽取器，并且在最后有几个全连接层用于对提取的特征进行分类。一个简单的卷积神经网络模型如下： 接下来我们讲解以下这几个层究竟在做的是什么。 卷积层卷积层主要是做卷积运算。本文用 “*” 表示卷积操作。 卷积运算假如我们拥有一个66的灰度图，矩阵如下图表示，在矩阵右边还有一个3\3的矩阵。我们称之为过滤器(filter)或核。 卷积的第一步就是将过滤器覆盖在灰度图的左上角的数字就是过滤器对应位置上的数字，如下图蓝色部分： 蓝色矩阵每个小个子的左上角的数字就是过滤器对应位置上的数字，将每个格子的两个数字相乘： 然后将得到的矩阵所有元素相加，即3+1+2+0+0+0+(-1)+(-8)+(-2)= -5，然后将这个值放到新的矩阵的左上角，最后的新的矩阵是一个4*4的矩阵。规律：n*n维的矩阵经过一个f*f过滤器后，得到一个（n-f+1）*（n-f+1）维度的新矩阵。 我们以1为步长(stride)向右移动一格，再进行类似的计算： 0 × 1 + 5 × 1 + 7 × 1 + 1 × 0 + 8 × 0 + 2 × 0 + 2 × (-1) + 9 × (-1) + 5 × (-1) = -4 写在刚刚得到的-5的右边。 重复向右移动，直到第一行都计算完成： 然后将过滤器向下移动一格，从左边开始继续运算，得到-10，写在新的矩阵第二行第一个位置上： 不断的向右、向下移动，直到计算出所有的数据，这样就完成了卷积运算，得出下图： 下面这张图也可以帮助我们理解卷积运算(黄色的矩阵式过滤器)： 最终经过卷积得到的图像，我们称之为特征图(Feature Map) 对于上述过程，也许你有很多疑问，比如过滤器是什么，为什么是3×3的，作用是什么，滑动的步长stride为什么是1，为什么得到4×4的矩阵，卷积有什么作用等，我们一一解答。 过滤器(filter)我们来看看下面的例子，本例子中我们用正数表示白色，0表示灰色，负数表示黑色。 左边 6×6 的矩阵是一个灰度图，左半部分是白色，右半部分是灰色。下面的3×3的过滤器是一种垂直边缘过滤器。 对二者进行卷积得到新的矩阵，这个矩阵的中间部分数值大于0，显示为白色，而两边为 0， 显示为灰色。 上图中三个红色箭头都是我们过滤得到的边缘。在这里，你可能会疑问为什么会有这么大的边缘，那是因为我们的原始图片太小，如果我们把原始灰度图放大一点： 再进行卷积，就可以得到： 这样就能大致看出过滤器确实过滤出了垂直边缘。如果想得到水平的边缘，将过滤器转置一下即可。而如果想得到其他角度的边缘，则需要在过滤器中设置不同的数值。下图是水平边缘过滤器： 当然很难手动设置过滤器的值去过滤某个特殊角度（比如31°）的边缘，我们通常是将过滤器中的9个数字当成9个参数。随机初始化这些参数后，通过神经网络的学习来获得一组值，用于过滤某个角度的边缘。 目前主流网络的过滤器的大小一般是1x1、3x3、5x5等，数值一般不会太大，因为我们希望这些过滤器可以获得一些更细微的边缘特征。另外，一般将长和宽都取奇数，具体原因我们讲到Padding的时候再解释。 PaddingPadding是填充。什么是填充呢？ 在前面，我们的例子一个66的矩阵经过一个3\3的过滤器后，得到一个4*4的新矩阵。但是这样的话会有两个缺点，第一个缺点就是我们每次做卷积运算的时候，我们的图像都会缩小。可能我们在做完几次卷积之后，我们的图像就会变得很小。第二个缺点是角落边的像素点只会被使用一次，像下图的绿色。但是如果是中间的像素点(类似图中的红色框)，就会被使用很多次。 所以，那些边缘或者角落的像素点在输出时使用较少。这意味者，我们将丢掉图片边缘区域。如果这里有重要信息，那我们就得不偿失了。 那我们该怎么解决这个问题呢？这时候就用到Padding了。我们可以在卷积操作之前填充这个图像。我们可以沿着图像的边缘再填充一层像素(通常为0，因为用0填充不影响原来的图片特征)。这样做的话，我们的原始图片像素就有66变成了8\8。接着我们对这幅8*8的图像进行卷积，得到的新矩阵就不是4*4的，而是6*6的。规律：n*n维的矩阵填充了P个像素点后再经过一个f*f过滤器后，得到一个（n+2p-f+1）*（n+2p-f+1）维度的新矩阵。 由上面这个规律：我们得出p=(f-1)/2，为了使p是对称填充，所以我们的f通常都是奇数。还有一个原因，是因为当我们有一个奇数的过滤器，我们会有一个中间像素点。这就是为什么前面我们说过滤器通常都是奇数维度。 那我们Padding填充的时候到底选择多少呢？1还是2还是更大的数字？ 其实Padding有两个选择：一个Valid，一个是Same。 Valid：意味不填充。也就是之前的66矩阵经过一个3\3的矩阵后，得出一个4*4的矩阵。 Same：意味我们的图片输出大小和输入大小是一样的。也就是上面的66矩阵在Padding=1之后经过一个3\3的矩阵后，得出一个6*6的矩阵。图片输出大小和输入大小一样。 卷积步长我们先用 3x3 的过滤器以2为步长卷积 7x7 的图像，看看会得到什么。 第一步，对左上角的区域进行计算： 第二步，向右移动，得： 再继续向右向下移动过滤器，就可以得到下面的结果： 规律：我们用一个ff的过滤器卷积一个n*n的图像，Padding=p，步长为s，我们得到一个(n+2p-f)/s+1\(n+2p-f)/s+1 最后一个问题：如果我们的商不是整数呢？这时我们采用向下求整的策略。 三维卷积上面的讲解你已经知道如何对灰度图进行卷积运算了，现在看看如何在有三个通道(RGB)的图像上进行卷积。 假设彩色图像尺寸 6×6×3，这里的 3 指的是三个颜色通道。我们约定图像的通道数必须和过滤器的通道数匹配，所以需要增加过滤器的个数到3个。将他们叠在一起，进行卷积操作。6*6*3的图像经过一个33\3过滤器得出一个新的4*4*1图像。 如下图： 具体过程就是，首先，把这个 3×3×3 的过滤器先放到最左上角的位置，依次取原图和过滤器对应位置上的各27 个数相乘后求和，得到的数值放到新矩阵的左上角。 注意，由于我们是用三维的过滤器对三维的原图进行卷积操作，所以最终得到的矩阵只有一维。 然后不断向右向下进行类似操作，直到“遍历”完整个原图： 至于效果是怎么样的。结合之前的知识，我们假设了一组垂直边界过滤器： 它可以完成RGB三个通道上垂直边缘的检测工作。这也解释了为什么过滤器使用3个通道：也就是说如果你想获得RGB三个通道上的特征，那么你使用的过滤器也得是3个通道的。 如果你除了垂直边界，还想要检测更多的边界怎么办？可以使用更多 3维 甚至更多的过滤器，如下图框住的部分，我们增加了一组过滤器： 这样，两组过滤器就得到了两组边界，我们一般会将卷积后得到的图叠加在一起，得到上图左边的 4x4x2 的图像。如果你还想得到更多的边界特征，使用更多的3维的过滤器即可。 现在我们对维度进行总结： 首先我们假设我们的没有padding且步长为1，这时候使用 n × n × nc 原图和 nc’ 个 f × f × fc 的过滤器进行卷积，得到 (n-f+1) × (n-f+1) × nc’ 的图像 nc 是 原图的通道数(3)，过滤器的通道数和原图的通道数必须一样，都是 nc。 nc’ 是最终得到的图像的通道数(在上面的例子为2)，由使用的过滤器个数决定(而 nc’ 又是下一层的输入图像的通道数) 卷积层全貌 对于卷积层，我们同样需要进行以下操作，这也是前向传播的操作： z^[1] = w^[1] * a^[0]+b^[1] a^[1]=g(z^[1]) 上面所用到的变量解释： a^[0] 是我们原图的数据 X ，也就是上图 6x6x3 的RGB图。 w^[1] 是这一层的权重矩阵，由 2 个 3x3x3 的过滤器组成。 b^[1] 是第一层的偏置项，不同的过滤器对应不同的偏置值 g() 是激活函数，本例子中使用 ReLu 卷积层的工作说明： 式子中 w^[1] * a^[0] 是代表进行卷积运算得到两个特征图(也就是2个4x4的矩阵)，如下图的绿色框所示，接着再对两个特征图加上不同的偏置就得到了z^[1]，如下图红色框所示： 进一步应用激活函数就得到了两个处理过的图像，将他们叠加在一起，就得到了这一层的输出 a^[1](上图右下角)。 注意：上图中的加偏置(b1,b2)均使用了python的广播 简单来说，卷积层的工作就是：将输入的数据a^[l - 1]进行卷积操作，加上偏置，再经过激活函数非线性化处理得到a^[l]。到这里卷积层的任务就完成了。 为什么要使用卷积？卷积层的主要优势在稀疏连接就和权值共享 稀疏连接假设我们输入的是32×32×3的RGB图像。 如果是全连接网络，对于隐藏层的某个神经元，它不得不和前一层的所有输入（32×32×3）都保持连接。 但是，对卷积神经网络而言，神经元只和输入图像的局部相连接，如下图圆圈所示： 这个局部连接的区域，我们称之为“感受野”，大小等同于过滤器的大小(3*3*3)。 看下面的图： 右边的绿色或者红色的输出单元仅仅与36个输入特征中的九个相连接，其它像素值对输出不会产生任何影响。 相比之下，卷积神经网络的神经元与图像的连接要稀疏的多，我们称之为稀疏连接。 权值共享权值共享其实就是过滤器共享。特征检测如垂直边缘检测过滤器如果适用于图片的某个区域，那么它也可能适用于图片的其他区域，如果我们用垂直过滤器扫描整张图片，就可以得到整张图片的所有垂直边缘，而换做水平过滤器就可以扫描出图片的水平边缘。 在CNN中，我们用不同的神经元(过滤器)去学习整张图片的不同特征，而不是利用不同的神经元 学习图片不同的局部特征。因为图像的不同部分也可能有相同的特征。每个特征检测过滤器都可以在输入图片的不同区域中使用同样的参数(即方格内的9个数字)，以便提取垂直边缘或其它特征。右边两张图每个位置是被同样的过滤器扫描而得的，所以权重是一样的，也就是共享。假设有100个神经元，全连接神经网络就需要32×32×3×100=307200个参数。 因为共享了权值，提取一个特征只需要 f×f 个参数，上图右边两张图的每个像素只与使用 3x3 的过滤器相关，顶多再加上一个偏置项。在本例子中，得到一个feature map 只用到了 3*3+1=10 个参数。如果想得到100个特征，也不过是用去1000个参数。这就解释了为什么利用卷积神经网络可以减少参数个数了。 当你对提取到的 水平垂直或者其他角度 的特征，再进行卷积操作，就可以得到更复杂的特征，比如一个曲线。如果对得到的曲线再进行卷积操作，又能得到更高维度的特征，比如一个车轮，如此往复，最终可以提取到整个图像全局的特征。 到这里卷积层的内容就结束了。 池化层对于一个32x32像素的图像，假设我们使用400个3x3的过滤器提取特征，每一个过滤器和图像卷积都会得到含有 (32-3 + 1) × (32 - 3 + 1) = 900 个特征的feature map，由于有 400 个过滤器，所以每个样例 (example) 都会得到 900 × 400 = 360000 个特征。而如果是96x96的图像，使用400个8x8的过滤器提取特征，最终得到一个样本的特征个数为 3168400。对于如此大的输入，训练难度大，也容易发生过拟合。 为了减少下一个卷积层的输入，引入了采样层(也叫池化层)，采样操作分为最大值采样(Max pooling)，平均值采样(Mean pooling)。 最大值采样 举个例子，我们用3x3的过滤器以1为步长去扫描图像，每次将重叠的部分的最大值提取出来，作为这部分的特征，过程如下： 我们也可以使用2x2的过滤器。以2为步长进行特征值的提取。最终得到的图像长度和宽度都缩小一半： 平均值采样 与最大值采样不同的是 将覆盖部分特征的平均值作为特征，其他过程都一样。经过采样处理，可以提取更为抽象的特征，减少数据处理量的同时保留有效信息。 下图是过滤器为2x2，步长为2进行特征值的提取： 注意：池化层没有需要学习的参数。且padding大多数时候为0 全连接层在经过几次卷积和池化的操作之后，卷积神经网络的最后一步是全连接层，全连接层的个数可能不止一个： 最后一个采样层到全连接层的过渡是通过卷积来实现的，比如前层输出为 3x3x5 的feature map，那我们就用 3x3x5 的过滤器对前层的这些feature map进行卷积，于是得到了一个神经元的值： 全连接层有1024个神经元，那我们就用1024个3x3x5 的过滤器进行卷积即可。 第一个全连接层到第二个全连接层的过渡也是通过卷积实现的，若前层有1024个神经元，这层有512个，那可以看做前层有1024x1x1个feature map，然后用1x1的卷积核对这些feature map进行卷积，则得到100x1x1个feature map。 卷积神经网络要做的事情，大致分为两步： 卷积层、采样层负责提取特征 全连接层用于对提取到的特性进行分类 最后一步就和普通的神经网络没有什么区别，充当一个分类器的作用，输入是不同特征的特征值，输出是分类。在全连接层后，就可以根据实际情况用softmax对图片进行分类了。]]></content>
      <categories>
        <category>深度学习</category>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深层神经网络]]></title>
    <url>%2F2019%2F09%2F29%2F%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[深度神经网络深层神经网络样子也许是下面这样的，也有可能是更多层的。 上图是一个四层的神经网络(输入层一般记为零层或不记住)，我们用L表示神经网络的层数，n^[l] 表示第 l 层的神经元数量 由于神经网络具有许多层，在下图中用方框代表一层，每个层都要完成各自的任务，流程图大致如下所示： 蓝框中的部分完成正向传播： 该过程的输入是 X 也就是 A^[0]，一层一层向后计算，最后得到A^[L]。 并且，在各层 l 计算出 A^[l] 的同时，缓存各层的输出值 A^[l] 到变量 cache 中，因为反向传播将会用上。图中这里写错了，应该是A^[1]、A^[2]、A^[3]的。 绿框中的部分完成反向传播： 该过程的输入是dA^[L]，并根据dA^[L]一步步向前计算，得到各层对应的dZ^[l]，dW^[l]，db^[l] 此外，还要计算出 dA^[l - 1] 作为前一层的输入 具体过程请看浅层神经网络的正向传播 和 浅层神经网络的反向传播 核对矩阵的维数为了在写代码的过程中减少出现bug，尤其是在进行反向传播的时候，我们要注意核对矩阵的维数，这个知识点我们前面将 浅层神经网络的反向传播的文章中也提到了，大致规律如下： W^[l] ： (n^[l], n^[l - 1]) Z^[l] ： (n^[l], m), A^[l] ： (n^[l], m) 另外，还有一个规律： 对任意层而言： dW 的维度和 W 的维度相同 dZ 的维度和 Z 的维度相同 dA 的维度和 A 的维度相同 为什么要使用深度神经网络？我们都知道深度神经网络可以解决很多问题，网络并不一定要很大，但一定要有深度，需要比较多的隐藏层。这到底为什么呢？ 如果我们在建立一个人脸识别或者检测系统，当我们输入一张脸部的图片时，我们可以把深度神经网络第一层当成一个特征探测器或者边缘检测器。在这个例子中，我们建立一个大概有20个隐藏单元的深度神经网络。看下图： 隐藏单元就是这个图里这些小方块。每个小方块都是一个隐藏单元，它会去寻找一张图的边缘方向。它可能在水平方向找、也可能在竖直方向找。这个东西在卷积神经网络我们会细讲(别问，问就是当时不会)。这里就告诉你们我们可以先把神经网络的第一层当作看图，然后去找这张图片的各个边缘。接着我们把组成图片边缘的像素放在一起看。它可以把探测到的边缘组合成面部的不同部分。比如说，可能有一个神经元回去找眼睛的部分，另外还有找鼻子或其它的部分。然后把这许多边缘结合在一起，就可以开始检测人脸的不同部分，最后再把这些部分放在一起，就可以识别或检测不同的人脸。我们把前几层的神经网络当作简单的检测函数，例如：边缘检测等，之后把它们跟后几层结合在一起。注意：边缘探测器其实相对来说都是针对图片中非常小块的面积。 具体原因移步网易云课堂的吴恩达的深度学习。链接 超参数在学习算法中还有其它参数，需要输入到学习算法中，比如学习率&alpha;、隐藏层的数量、使用的激活函数种类等都是超参数。因为它们影响着最终W和b的值。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上传项目到GitHub]]></title>
    <url>%2F2019%2F09%2F28%2F%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E5%88%B0GitHub%2F</url>
    <content type="text"><![CDATA[首先登录GitHub，没有账号的申请一个。很简单，跳过。 新建一个仓库： 记住这个网址，之后用到 进入项目的目录，点击空白处，选择git Bash。 输入git init，会发现当前目录下多了一个.git文件夹 接着输入git add . 这个是将项目上所有的文件添加到仓库中的意思，如果想添加某个特定的文件，只需把.换成这个特定的文件名即可。 输入git commit -m “first commit”，表示你对这次提交的注释，双引号里面的内容可以根据个人的需要 输入git remote add origin https://自己的仓库url地址（上面有说到） 将本地的仓库关联到github上， 输入git push -u origin master，这是把代码上传到github仓库的意思]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github的使用]]></title>
    <url>%2F2019%2F09%2F28%2FGithub%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[github的账户创建和仓库创建比较简单，就不赘述了 github添加ssh账户首先，点击账户，选择setting 直接添加就完事了。那我们怎么生成ssh公钥呢？ 先回到用户的主目录下，编辑文件.gitconfig，修改某台机器的git配置。修改为注册github的邮箱，填写用户名 接着执行命令，ssh-keygen -t rsa -C “邮箱地址”，一路yes。最后生成三个文件： id_rsa是机器私钥，自己保留。id_rsa.pub就是我们的公钥。把公钥内容复制到第一张图New SSH Key后的位置。名字可以随便取。 克隆项目 接着git clone SSH 这个再cmd下执行同样有效。 如果出错了，执行以下代码：先执行eval “$(ssh-agent -s)”，再执行ssh-add 不只是可以使用SSH，也可以使用HTTPS。 git clone 网址 在cmd上同样是有效的。 推送代码推送前： 图中的红框是分支。 推送分支就是把给分支上的所有本地提交库推送到远程库，推送时要指定本地分支，这样，git就会把该分支推送到远程库对应的远程分支上。 git push origin 分支名称 本地分支跟踪远程分支git branch –set-upstream-to=origin/远程分支名词 本地分支名称 跟踪后，如果本地分支和远程分支的进度不一样，使用命令 git status 会提醒。 拉取代码git pull orgin 分支名称]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git的分支管理]]></title>
    <url>%2F2019%2F09%2F27%2Fgit%E7%9A%84%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[分支原理git把我们之前每次提交的版本串成一条时间线，这条时间线就是一条分支。在git里，这个分支就是主分支，即master分支。HEAD严格来说不是指向提交，而是指向master。master才是指向提交的版本，所以，HEAD指向的就是当前分支。 每次提交，master分支都会向前移动一步。这样，随着不断提交，master分支的线也越来越长。 当我们创建新的分支dev，git创建了一个指针叫dev。指向master相同的提交。再把HEAD指向dev，就表示当前分支在dev上。 git创建一个分支很快，因为除了增加一个dev指针，改变HEAD的指向，工作区的文件没有任何变化。 假如我们在dev上的工作完成了，就可以把dev合并到master上。怎么合并的呢？git直接把master指向dev的当前提交，就完成了合并。 合并分支也很快，就改改指针。工作区的内容不变。 合并完分支后，甚至可以删除dev分支，删除dev分支就是把dev指针给删掉。删掉后，我们只剩下一条master指针。 分支作用分支在实际中有很大用处。假设你准备开发一个新功能，但是需要两周才能完成，第一周只写了一半的代码。如果立刻提交，由于代码还没写完，不完整的代码库会导致别人无法干活，如果等代码全部写完，又存在丢失每天进度的巨大风险。有了分支，就不用怕这些事情。你创建自己的一个分支，别人看不到，也可以在原来的分支上工作。而你还在自己的分支上干活，想提交就提交。直到全部开发完，一次性合并到主分支。这样既安全，又不影响别人工作。 分支基本操作查看当前几个分支且能看到在哪个分支工作git branch 创建分支git branch 分支名 创建分支并切换到其上工作git checkout -b dev 切换回master分支git checkoout master 合并分支git merge 分支名字 注意上面的Fast-forward，也就是红色框。git告诉我们，这次合并是快速合并，也就是直接把master指向dev的当前提交，所以合并速度非常快 解决冲突合并冲突并不是一番风顺的。在两个分支上修改同一个文件并提交，就会起冲突。解决办法：手动解决冲突，再提交一次 看下图： git告诉我们，git_test2.txt文件存在冲突，必须手动解决冲突再提交。 打开刚才修改的文件，发现文件修改了 将文件中新增加的&lt;、= 和 &gt;手动删掉，再提交一次 删除分支git branch -d 分支名字 这个操作也是非常快的，直接把dev这个指针删了就好了。 分支管理策略通常，合并分支时，如果可能，git会用fast forward模式。但是有些快速合并并不能合并但不会起冲突，合并之后并做一次新的提交。在这种模式下，删除分支后，会丢掉部分信息。 如上图，merge并不会起冲突(因为不是同一个文件)。但是，会出来一个框： 为什么会出现这个框？前面我们说了，合并分支无法合并但不会起冲突且做一次提交。在这次提交中，需要输入描述信息。就在弹窗输入描述信息 PS：我太难了，我接下来一直退不出这个框。所以这个例子就这样吧。可以的跟我说一下，拜托了。 禁用快速合并： git merge –no-ff -m “描述” 分支名 Bug分支软件开发中，bug就像家常便饭，有了bug就要修复。在git中，由于分支是如此强大，所以，每个bug都可以通过一个新的临时分支来修复。修复后，合并分支，然后将临时分支删除。这个例子不难，就不贴图了。 假如你正在写代码，突然老大让你修改一个bug。你需要先保存一下工作现场，修复完bug后，再恢复工作现场。]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git介绍和基本操作]]></title>
    <url>%2F2019%2F09%2F26%2Fgit%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近经常用到git和GitHub，但是命令很多都忘记了。故写一篇博客回顾并总结一下 git介绍git是目前世界上最先进的分布式版本系统。 git的两大特点 版本控制：可以解决多人同时开发的代码问题，也可以解决找回历史代码的问题 分布式：git是分布式版本控制系统，同一个git仓库，可以分布到不同的机器上，首先找一台电脑充当服务器的角色。每天24小时开机，其它每个人都从这个“服务器”仓库克隆一份自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交，可以自己搭建服务器，也可以使用github网站。 git的安装就不赘述了，直接下好安转包。一路next就完事了(不用改安装路径)。 git基本操作初始化通过git我们可以管理一个目录下的代码，首先我们通过git init创建一个版本库 随机切到一个目录，点击空白区域。点击Git Bash Here，如图中所示： 这样我们就有一个格式为 .git 文件 版本创建首先，git add 文件名.文件格式：添加某个特定的文件 或 git add . ：将目录上所有的文件添加到仓库 接着，git commit -m “对这个版本的说明信息” 查看版本记录使用 git log 图中的commit 12c6a……代表版本序列号 随便放了一张图片进项目，第二次提交项目 如果版本记录过程，以简短形式显示： 版本回退在git中，有个HEAD指针一直指向当前的版本。用HEAD^代表HEAD前一个版本，HEAD^^代表前两个版本。其它版本类推。那前一百个要写100个^吗？答案当然是不用的，用HEAD~100。HEAD~1 等价于 HEAD^。 git reset –hard HEAD git reset –hard 版本号：这里的版本号就是commit后的版本序列号 版本号不需要写完，写完前几个号码就好了。 查看操作记录git reflog 工作区和暂存区工作区就相当于我们的目录 在工作区中，有一个隐藏目录 .git，这个不是工作区。而是git的版本库。其中存了很多东西，其中最重要的是称为stage(或者叫index)的暂存区，还有git为我们自动创建的第一个分支master，以及指向master的一个指针HEAD。 前面讲了我们把文件往git版本库里添加进去，是分两步执行的： 第一步是git add把文件添加进去，实际上就是把文件添加到暂存区 第二步是git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支 用git status查看当前的状态。如果有新的文件或有文件在修改之后、在add之前，会出现以下情况： 把新的文件add之后，得出以下情况： 经历add和commit后使用status： git管理文件的修改，它只会提交暂存区的修改来创建版本。也就是说，在修改或创建一个新文件后，不add到暂存区而直接commit，git也会说文件被修改。 撤销修改git checkout –文件名.文件格式 如果是add后，先利用reset取消暂存的变更重新回到工作区： 接着用git checkout –文件名.文件格式 丢弃工作区的改动 如果是commit了，就版本回退 对比文件的不同对比工作区和某个版本中文件的不同 git diff 某个版本库 – 比较的文件 对比两个版本间的不同 git diff 某个版本库 某个版本库 – 比较的文件 两个版本交换位置，得出的效果是不同的 删除文件 如果是add到暂存区，也是和之前一样，先reset，再checkout。]]></content>
      <categories>
        <category>git和github</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现浅层神经网络]]></title>
    <url>%2F2019%2F09%2F24%2FPython%E5%AE%9E%E7%8E%B0%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[开始前的准备本文用到的库：numpy、sklearn、matplotlib 另外，我们还要借助一下吴恩达老师的工具函数testCases.py 和 planar_utils.py。地址 ​ testCases：提供测试样本来评估我们的模型。 ​ planar_utils：提供各种有用的函数。 这两个文件的函数就不具体阐述了，有兴趣的自己研究去吧。 导入必要的库1234567import numpy as npimport matplotlib.pyplot as pltfrom testCases import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets 数据集介绍12#X是训练集，Y是测试集X,Y=load_planar_dataset() 数据集的说明： X是维度为(2, 400)的训练集，两个维度分别代表平面的两个坐标。 Y是为(1, 400)的测试集，每个元素的值是0（代表红色），或者1（代表蓝色） 利用 matplotlib 进行数据的可视化操作，这是一个由红点和蓝色的点构成的类似花状的图像 12345 # np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉 # 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23 # 目的是确保 cost 是一个浮点数plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral);plt.show() 构建神经网络模型 对于某个样本x^[i]： 成本函数J： 接下来我们要用代码实现神经网络的结构，步骤大致如下： 1. 定义神经网络结构 2. 随机初始化参数 3. 不断迭代: - 正向传播 - 计算成本函数值 - 利用反向传播得到梯度值 - 更新参数（梯度下降） 定义神经网络的结构主要任务是描述网络各个层的节点个数 12345def layer_sizes(X, Y): n_x = X.shape[0] # 输入层单元数量 n_h = 4 # 隐藏层单元数量，在这个模型中，我们设置成4即可 n_y = Y.shape[0] # 输出层单元数量 return (n_x, n_h, n_y) 随机初始化参数参数W的随机初始化是很重要的，如果W和b都初始化成0，那么这一层的所有单元的输出值都一样，导致反向传播时，所有的单元的参数都做出完全相同的调整，这样多个单元的效果和只有一个单元的效果是相同的。那么多层神经网络就没有任何意义。为了避免这样的情况发生，我们会把参数W初始化成非零。 另外，随机初始化W之后，我们还会乘上一个较小的常数，比如 0.01 ，这样是为了保证输出值 Z 数值都不会太大。为什么这么做？设想我们用的激活函数是 sigmoid 或者 tanh ，那么，太大的 Z 会导致最终 A 会落在函数图像中比较平坦的区域内，这样的地方梯度都接近于0，会降低梯度下降的速度，因此我们会将权重初始化为较小的随机值。 1234567891011def initialize_parameters(n_x, n_h, n_y): #随机初始化 W1 = np.random.randn(n_h, n_x) * 0.01 b1 = np.zeros((n_h, 1)) W2 = np.random.randn(n_y, n_h) * 0.01 b2 = np.zeros((n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 循环迭代实现正向传播123456789101112131415161718def forward_propagation(X, parameters): # 从parameters中取出参数 W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # 执行正向传播操作 Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) #如果不相等，直接报错 assert (A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cache 计算成本函数J 1234567891011def compute_cost(A2, Y, parameters): m = Y.shape[1] # 样本个数 # 下一行的结果是 (1, m)的矩阵 logprobs = np.multiply(Y, np.log(A2)) + np.multiply(1 - Y, np.log(1 - A2)) # 将 (1, m)的矩阵求和后取平均值 cost = - 1 / m * np.sum(logprobs) # np.squeeze()函数对矩阵进行压缩维度，删除单维度条目，即把shape中为1的维度去掉 # 例如： np.squeeze([[ 1.23 ]]) 可以将维度压缩，最终变成 1.23 # 目的是确保 cost 是一个浮点数 cost = np.squeeze(cost) return cost 利用正向传播函数返回的cache，我们可以实现反向传播了。 实现反向传播 说明： 我们使用的 g^[1] () 是 tanh ，并且 A1 = g^[1] ( Z^[1] ) ,那么求导变形后可以得到 g‘ ^[1] ( Z^[1] ) = 1-( A^[1] )^2 用python表示为： 1 - np.power(A1, 2) 我们使用的 g^[2] () 是 sigmoid ，并且 A2 = g^[2] ( Z^[2] ) ,那么求导变形后可以得到 g‘ ^[2] ( Z^[2] ) =A^2 - Y 1234567891011121314151617181920def backward_propagation(parameters, cache, X, Y): # 获取样本的数量 m = X.shape[1] # 从 parameters 和 cache 中取得参数 W1 = parameters["W1"] W2 = parameters["W2"] A1 = cache["A1"] A2 = cache["A2"] # 计算梯度 dW1, db1, dW2, db2. dZ2 = A2 - Y dW2 = 1 / m * np.dot(dZ2, A1.T) db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True) dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) dW1 = 1 / m * np.dot(dZ1, X.T) db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True) grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return grads 说明 1、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。 2、 keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。 更新参数利用上面反向传播的代码段获得的梯度去更新 (W1, b1, W2, b2)： 式子中 α 是学习率，较小的学习率的值会降低梯度下降的速度，但是可以保证成本函数 J 可以在最小值附近收敛，而较大的学习率让梯度下降速度较快，但是可能会因为下降的步子太大而越过最低点，最终无法在最低点附近出收敛。 本篇博客选择1.2作为学习率 123456789101112131415161718192021def update_parameters(parameters, grads, learning_rate=1.2): W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # 更新参数 W1 -= learning_rate * dW1 b1 -= learning_rate * db1 W2 -= learning_rate * dW2 b2 -= learning_rate * db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 构建完整的神经网络模型123456789101112131415161718192021222324252627282930313233def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): # 输入层单元数 n_x = layer_sizes(X, Y)[0] # 输出层单元数 n_y = layer_sizes(X, Y)[2] # 随机初始化参数 parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # 10000次梯度下降的迭代 for i in range(0, num_iterations): # 正向传播，得到Z1、A1、Z2、A2 A2, cache = forward_propagation(X, parameters) # 计算成本函数的值 cost = compute_cost(A2, Y, parameters) # 反向传播，得到各个参数的梯度值 grads = backward_propagation(parameters, cache, X, Y) # 更新参数 parameters = update_parameters(parameters, grads, learning_rate = 1.2) # 每迭代1000下就输出一次cost值 if print_cost and i % 1000 == 0: print ("Cost after iteration %i: %f" %(i, cost)) # 返回最终训练好的参数 return parameters 对样本进行预测我们约定 预测值大于0.5的之后取1，否则取0： 对一个矩阵M而言，如果你希望它的每个元素 如果大于某个阈值 threshold 就用1表示，小于这个阈值就用 0 表示，那么，在python中可以这么实现：M_new = (M &gt; threshold) 1234def predict(parameters, X): A2, cache = forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) return predictions 利用模型预测123456789# 利用含有4个神经元的单隐藏层的神经网络构建分类模型parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)# 可视化分类结果plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))plt.show()# 输出准确率predictions = predict(parameters, X)print('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%') 尝试换一下隐藏层的单元个数12345678910plt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print ("Accuracy for &#123;&#125; hidden units: &#123;&#125; %".format(n_h, accuracy)) 换数据集12345678910111213141516171819202122# Datasetsnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()datasets = &#123;"noisy_circles": noisy_circles, "noisy_moons": noisy_moons, "blobs": blobs, "gaussian_quantiles": gaussian_quantiles&#125;# 在这里选择你要选用的数据集dataset = "noisy_circles"X, Y = datasets[dataset]X, Y = X.T, Y.reshape(1, Y.shape[0])# make blobs binaryif dataset == "blobs": Y = Y%2# 可视化数据plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral);plt.show()# 测试代码如下：parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# 画出边界plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))plt.show()]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅层神经网络的反向传播]]></title>
    <url>%2F2019%2F09%2F23%2F%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[符号约定首先看下图： 我们假设n[l] 表示第 l 层的神经元的数量，那么这个单隐层的神经网络的输入层、隐含层和输出层的维度分别是：n[0]= n_x =3、n[1]=4、n[2]=1。 那么根据浅层神经网络的正向传播的分析，如果一次性输入带有 m 个样本的矩阵 X 我们可以得到： W^[1] = (4, 3), Z^[1] = (4, m), A^[1] = (4, m) W^[2] = (1, 4), Z^[2] = (1, m), A^[2] = (1, m) 可以总结出如下规律，对第 l 层的单元而言： W^[l] = (n^[l], n^[l-1]), Z^[l] = (n^[l], m), A^[l] = (n^[l], m). 另外，还有一个规律： 对任意层而言： dW 的维度和 W 的维度相同 dZ 的维度和 Z 的维度相同 dA 的维度和 A 的维度相同 注意：以上出现的符号都是将m个样本向量化后的表达。 单个样本的梯度计算讲反向传播前，我们先回顾一下单个样本的正向传播的过程： 其中计算z^[1]时用到的 x 可以看做是 a^[0]，σ( ) 是激活函数中一种，一般情况下，我们更习惯用用 g( ) 来表示激活函数。于是将上述式子一般化可以得到第 l 层的公式为： z^[l] = W^[l]a^[l-1] + b^[l] ——— ① a^[l] = g^[l] ( z^[l] ) ——————- ② 由于我们输入的样本只有一个，所以各个向量的维度如下： W^[l] = (n^[l], n^[l-1]), z^[l] = (n^[l], 1), a^[1] = (n^[l], 1). 我们现在根据式子 ① 和 ② 来讨论反向传播的过程。 因为正向传播的时候我们依次计算了z^[1], a^[1], z^[2], a^[2]最终得到了损失函数L。所以反向传播的时候，我们要从L向前计算梯度。 第一步计算dz^[2]和da^[2]进而算出 dW^[2] 和 db^[2]： da^[2]=dL/da^[2]= -y/a^[2]+(1-y)/(1-a^[2]) dz^[2]= dL/dz^[2] = dL/da^[2]*da^[2]/dz^[2]=-y(1-a^[2])+(1-y)a^[2] = a^[2] - y 说明1： dz^[2] 的维度和z^[2]相同，da^[2] 的维度和a^[2]相同，为(1, 1) g’( z^[2] ) 的维度与 z^[2]维度相同，为(1, 1) 在第二层中，a^[2] 与 z^[2]的维度也相同，为(1, 1) 实际上，dz^[2] 应该等于da^[2] 与 g’( z^[2] ) 的内积的结果，理由我们我们先向下看 说明2 ： 上图中我们可以得到 dW^[2] = dz^[2]乘上 a^[1] 的转置，这里是因为 : dW^[2] 和 W^[2] 的形状是一样的(n^[2], n^[1])也就是(1, 4)， dz^[2] 和 z^[2] 的形状是一样的(n^[2], 1)也就是(1, 1)， a^[1] 的形状是 (n^[1], 1) 也就是(4, 1)，可以得到 a^[1] 的转置 a^( [1]T ) 的形状是 (1, 4)， 这样 dz^[2] 和 a^[1] 的转置 的乘积的形状才能是 dW^[2] 的形状 (1, 4)； 因此 dW^[2] = dz^[2]乘上 a^[1] 的转置。 这里给我们的启发是：向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行转置操作。 第二步计算 da^[1] 和 dz^[1]进而算出 dW^[1] 和 db^[1] 检查矩阵形状是否匹配：（4，1）* （1，1），匹配 说明3： dz^[1] 的维度和z^[1]相同，da^[1] 的维度和a^[1]相同，均为(4, 1) W^[2]T 的维度是 (4, 1)，dz^[2] 的维度是 (1, 1) ，二者的乘积与da^[1] 维度相同。 g’( z^[1] ) 的维度与 z^[1]维度相同，也与da^[1] 维度相同，为(4, 1)。 所以想得到维度为 (4, 1) 的dz^[1] ，da^[1] 与 g’( z^[1] ) 直接的关系为内积 这里给我们的启发是：向前求导的时候注意矩阵的维度变化，通过分析矩阵的维度来判断矩阵是否需要进行内积操作。这里就解释了说明（1）留下的问题。 检查维度：（4，3）=（4，1）*（1，3），正确 公式： 多个样本的梯度运算用m个样本作为输入，并进行向量化后正向传播的公式： Z^[1] = W^[1]X + b^[1] A^[1] = g^[1] ( Z^[1] ) Z^[2] = W^[2]A^[1] + b^[2] A^[2] = g^[2] ( Z^[2] ) 由于引入了m个样本，我们也要有个成本函数来衡量整体的表现情况，我们的目的是让J达到最小值 下图中左边列举了我们上面所推导的各种式子，其中图中所用的激活函数 g^[2] () 为sigmoid函数，因而dz^[2] = a^[2] - y。这些左边的式子都是针对单个样本而言的，而右边则是将m个样本作为输入并向量化后的公式的表达形式。 上图的说明： 1、由于成本函数 J 是所有的 损失函数 L 的值求和后的平均值， 那么 J 对 W^[1], b^[1], W^[2], b^[2] 的导数 等价于 各个 L 对 W^[1], b^[1], W^[2], b^[2] 的导数求和后的平均值。所以dW^[1], db^[1], dW^[2], db^[2]的式子中需要乘上 1 / m。 2、计算db^[1]和db^[2]时的程序代码的sum函数的第二个参数 axis=1 表示水平相加求和。 3、keepdims=True是为了保持矩阵的shape属性值是我们熟悉的()中有两个数字的形式，例如(1, m)，(4, m)等，如果不加上可能得到的是奇怪的(1, )，(4, )这样的形式。 更新参数最后，不要忘了要更新参数（式子中的&alpha;是学习率）]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅层神经网络的正向传播]]></title>
    <url>%2F2019%2F09%2F23%2F%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[神经网络概览在逻辑回归中，我们的模型大概是这样的，得到的a即是预测结果y帽。 如果把上面的图片抽象化，可以得到以下模型： 再进一步简化得到： 上图里，小圆圈是sigmoid单元，它要完成的工作就是先计算 z 然后计算 a 最终作为 y帽 输出。 进而，我们看看下图，这是较为简单的神经网络的样子，它无非就是将多个 sigmoid 单元组合在一起，形成了更复杂的结构。图中每个单元都需要接收数据的输入，并完成数据的输出，其每个单元的计算过程与 logistic回归 的正向传播类似。可以看到，图片里给神经网络分了层次，最左边的是输入层，也就是第0层；中间的是隐藏层，也就是第一层；最右边的是输出层，是第二层。通常，我们不将输入层看做神经网络的一层，因而下图是一个2层的神经网络。 另外要清楚的是，本图中隐藏层只有一个，但实际上，隐藏层可以有多个。由于对用户而言，隐藏层计算得到的数据用户不可预见，也没有太大必要知道，所以称之为隐藏层。也正是因为如此，神经网络的解释器很差。 再进一步认识下这张图片里的标记，隐藏层的每个单元都需要接收输入层的数据，并且各个单元都需要计算 z , 并经过 sigmoid 函数得到各自的 a ，为了便于区分不同层的不同单元的 a，我们做如下约定： a 的右上角有个角标[i]，表示这是第i层的单元；a 的右下角有个角标 j 用于区分这是该层自上向下的第 j 个单元。例如我们用 a^[1]_3 表示这是第一层的第三个单元。输入层的 x1, x2, … xn 可以看做是 a^[0]_1, a^[0]_2 … a^[0]_n. 前一层a[ i ] 的输出，便是后一层 a[ i+1 ] 的所有单元输入。除了输入层外的其它层，也就是隐藏层和输出层的单元都有各自的参数 w 和 b 用于计算 z ，同样是用w^[i]_j, b^[i]_j, z^[i]_j 来区分他们；得到 z^[i]_j 后用sigmoid函数计算 a^[i]_j ，再将本层n个单元计算得到的n个 a 作为下一层的输入，最终在输出层得到预测值 y帽。其计算过程大致如下： 计算一个样本的神经网络输出下图是输入单个样本(该样本含有特征x1, x2, x3)的神经网络图，隐藏层有4个单元： 根据上面的说明，要计算该神经网络的输出，我们首先要计算隐藏层4个单元的输出 第一步就是计算第一层各个单元的 z^[1]_j ，第二步是计算出各个单元的 a^[i]_j，不难想到可以用向量化计算来化简上述操作，我们将所有 w^[1]_j 的转置纵向叠加得到下图的内容，我们将这个大的矩阵记为W^[1]，得出下图： 由于输入的 x 是三维的列向量，所以每个分量x1, x2, x3 都需要一个 w1, w2, w3 对应，因此 w^[1]_j 的转置是一个(1, 3) 的矩阵，又因为隐藏层有4个单元，即 j 的取值为1, 2, 3, 4，故 W^[1] 是 (4, 3) 的矩阵。 同理，第二层，也就是输出层的 W^[2] 由于有4个输入的特征，1个单元，所以 W^[2] 是 (1, 4)的矩阵。 对于只有一个样本的情况，我们不难得到如下式子： z^[1] 是个 (4, 1) 的矩阵。可以再进一步通过 sigmoid 函数得到 至此，第一层神经网络得任务已完成 第二层也就是输出层的工作，无非就是把第一层的 a^[1] 作为输入，继续用 W^[2] 与 a^[1] 相乘 再加上 b^[2] 得到 z^[2]，再通过 sigmoid 函数得到 a^[2] 也就是最终的预测结果 y帽。 至此，我们就完成了一个样本的输出。接下来看看如何用矩阵实现多样本的正向输出。 计算多样本的输出假设我们有m个样本，每个样本有3个特征，并且隐藏层也是4个单元。 那么，通常我们需要使用一个 for 循环将 从 1 遍历至 m 完成以下操作： 角标 ^(i) 表示第 i 个样本。我们可以构造这样一个矩阵 x： 它将所有样本的特征，按列叠加在一起，构成 (3, m) 的矩阵。 如果我们替换上面计算 z^[1] 的过程中使用的 单个样本 x^(1) 为(3, m) 的矩阵 x (也就是下图的绿色框)： 就可以得到下面的式子（为了方便表达，下面的公式中假设 b 等于0）： 到这里，我们求出了 Z^[1] ，并且由于 Z^[1] 是(4, 3)的矩阵W^[1]乘以(3, m)的矩阵x 再加上b，所以它是 (4, m) 的矩阵。再经过 sigmoid 函数即可得到同样是 (4, m) 的矩阵 A^[1]，到此隐藏层的工作完成了。 输出数据矩阵 A^[1]，作为下一层（也就是输出层）的输入参数，经过类似的计算也可以得到 Z^[2] = W^[2] × A^[1] + b^[2]，上面我们分析到 W^[2] 是 (1, 4)的矩阵，所以得到的Z^[2]是 (1, m) 的矩阵，同样经过sigmoid函数处理得到的 A^[2] 也是 (1, m) 的矩阵，A^[2]的每个元素，代表一个样本输出的预测值。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系数据库]]></title>
    <url>%2F2019%2F09%2F19%2F%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[域域是一组具有相同数据类型的值的集合。基数是域中数据的个数 笛卡儿积笛卡尔积直观意义是诸集合各元素间一切可能的组合，可表示为一个二维表。 关系关系是笛卡尔积的有限集合 关系可以有三种类型： ​ 基本表：实际存储数据的逻辑表示 ​ 查询表：查询结果对应的标 ​ 视图表：是虚表，由基本表或其他试图导出，不对应实际存储的数据 关系的基本性质： ​ 列是同质的，每一列中的分量是同一类型的数据，来自同一个域。 ​ 不同的列可出自同一个域，其中的每一列称为一个属性，不同的属性要给予不同的属性名。 ​ 列的顺序无所谓，列的次序可以任意交换。 ​ 行的顺序无所谓，行的次序可以任意交换 ​ 任意两个元祖不能完全相同 ​ 分量必须取原子值，每一个分量都必须是不可分的数据项。这是规范条件中最基本的一条 几个术语：若关系中的某一属性组的值能唯一识别一个元祖，则称该属性为候选码。 若一个关系有多个候选码，则选定其中一个作为主码。 候选码的诸属性称为非码属性 不包含在任何候选码中的属性称为非码属性 若关系模式的所有属性组是这个关系模式的候选码，则称为全码 关系模式关系模式是型，关系是值。关系模式是对关系的描述。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib画图总结]]></title>
    <url>%2F2019%2F09%2F18%2Fmatplotlib%E7%94%BB%E5%9B%BE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本片博客介绍了matplotlib画图的API，API很多，只总结了部分自己用到的。开始吧 没有这个库的，pip安装一下。 首先将matplotlib中的pyplot导入，如下： import matplotlib.pyplot as plt，所以以下的plt都是pyplot。 plt.scatter画散点图 123456789101112131415import matplotlib.pyplot as pltx = list(range(1,1001))y = [x**2 for x in x]#s代表点的面积#c代表颜色plt.scatter(x,y,c="green",s=20)#设置标题并加上轴标签plt.title("Squares Numbers",fontsize=24)plt.xlabel("Value",fontsize=14)plt.xlabel("Square of Value",fontsize=14)#设置刻度标记的大小plt.tick_params(axis='both',which='major',labelsize=14)#设置每个坐标的取值范围plt.axis([0,1100,0,1100000])plt.show() 上面的代码运行后，是一条线。那是因为点太多了。绘制很多点的时候，轮廓会连在一起。要删除数据点的轮廓可用scatter，传递参数edgecolor=”none” 模块pyplot内置了一组颜色映射，要使用这些颜色映射，你需要告诉pyplot该如何设置数据集中每个点的颜色。 123456789101112131415import matplotlib.pyplot as pltx = list(range(1,1001))y = [x**2 for x in x]#s代表点的面积#cmap设置颜色映射plt.scatter(x,y,s=20,c=y,cmap=plt.cm.Greens)#设置标题并加上轴标签plt.title("Squares Numbers",fontsize=24)plt.xlabel("Value",fontsize=14)plt.xlabel("Square of Value",fontsize=14)#设置刻度标记的大小plt.tick_params(axis='both',which='major',labelsize=14)#设置每个坐标的取值范围plt.axis([0,1100,0,1100000])plt.show() 保存散点图：savefig 保存图片之前，一定要把plt.show()注释掉，否则会保存一张空白的图片 12#plt.show()plt.savefig("save.jpg",bbox_inches="tight") plt.bar画柱状图，默认是竖直条形图。 1234567import matplotlib.pyplot as pltimport numpy as npy = [20, 10, 30, 25, 15]x = np.arange(5)p1 = plt.bar(x, height=y, width=0.5, )# 展示图形plt.show() 水平条形图：需要把orientation改为horizontal，然后x与y的数据交换 123456789import matplotlib.pyplot as pltimport numpy as npx = [20, 10, 30, 25, 15]y = np.arange(5)# x= 起始位置，bottom= 水平条的底部(左侧), y轴， height：水平条的宽度#width：水平条的长度p1 = plt.bar(x=0, bottom=y, height=0.5, width=x, orientation="horizontal")# 展示图形plt.show() plt.plot(x,y,color=’red’,linewidth=2.5,linestyle=’-‘) 画线，也可以用来画折线图。x是横坐标的值。y是纵坐标的值。color参数设置曲线颜色，linewidth设置曲线宽度，linestyle设置曲线风格。 linestyle的可选参数： ‘-‘ solid line style ‘–’ dashed line style ‘-.’ dash-dot line style ‘:’ dotted line style plt.figure()自定义画布大小，画图之前首先设置figure对象，此函数相当于设置一块自定义大小的画布，使得后面的图形输出在这块规定了大小的画布上，其中参数figsize设置画布大小。 figure语法说明 figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True) num:图像编号或名称，数字为编号 ，字符串为名称 figsize:指定figure的宽和高，单位为英寸； dpi参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为80 facecolor:背景颜色 edgecolor:边框颜色 frameon:是否显示边框 plt.xticks()设置x轴刻度的表现方式 plt.yticks()设置y轴刻度的表现方式 plt.xlim()设置x轴刻度的取值范围 plt.ylim()设置y轴刻度的取值范围 plt.text(1, 2, “I’m a text”)前两个参数表示文本坐标, 第三个参数为要添加的文本 plt.legend()函数实现了图例功能, 他有两个参数, 第一个为样式对象, 第二个为描述字符,都可以为空 plt.xlabel()添加x轴名字 plt.ylabel()添加y轴名字 plt.tight_layout()tight_layout自动调整subplot(s)参数，以便subplot(s)适应图形区域 plt.subplot()设置画布划分以及图像在画布上输出的位置，将figure设置的画布大小分成几个部分，参数‘221’表示2(row)x2(colu),即将画布分成2x2，两行两列的4块区域，1表示选择图形输出的区域在第一块，图形输出区域参数必须在“行x列”范围 plt.subplots()subplots(nrows,ncols,sharex,sharey,squeeze,subplot_kw,gridspec_kw,**fig_kw) :创建画布和子图 1、nrows,ncols表示将画布分割成几行几列。shares，sharey表示坐标轴的属性是否相同，可选的参数：True、False、row、col。默认值为False。 2、 squeeze bool a.默认参数为True：额外的维度从返回的Axes(轴)对象中挤出，对于N1或1N个子图，返回一个1维数组，对于N*M，N&gt;1和M&gt;1返回一个2维数组。 b.为False，不进行挤压操作：返回一个元素为Axes实例的2维数组，即使它最终是1x1。 3、subplot_kw:字典类型，可选参数。把字典的关键字传递给add_subplot()来创建每个子图。 4、gridspec_kw:字典类型，可选参数。把字典的关键字传递给GridSpec构造函数创建子图放在网格里(grid)。 5、**fig_kw：把所有详细的关键字参数传给figure()函数。 plt.grid()是否开启方格，True为开，False为不显示。默认为False plt.gca()获取当前的子图 plt.xcale()改变x轴的比例。pyplot不仅支持线性轴刻度，还支持对数和logit刻度。如果数据跨越许多数量级，则通常使用此方法 plt.xcale(“logit”)，还有log、symmlog等选择。还可以添加自己的比例。 plt.yscale()改变y轴的比例。用法同plt.xcale一样 随便写了个例子，其他具体的用法还是要自己去练习。 123456789101112import numpy as npimport matplotlib.pyplot as pltplt.scatter([1,3,9],[5,8,2], label="Example one",color="red",s=25,marker="o")plt.plot([2,4,6,8,10],[8,6,2,5,6], label="Example two", color='g')plt.legend()plt.xticks([1,10])plt.yticks([1,15])plt.xlabel('bar number')plt.ylabel('bar height')plt.title('test')plt.grid(True)plt.show()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(4)]]></title>
    <url>%2F2019%2F09%2F17%2FSVM%E8%A7%A3%E8%AF%BB(4)%2F</url>
    <content type="text"><![CDATA[使用SVC时的其他考虑SVC处理多分类问题之前的所有的SVM的(1)-(3)内容，全部是基于二分类的情况来说明的。因为支持向量机是天生二分类的模型。但是，它也可以做多分类。但是SVC在多分类情况的推广是很难的。因为要研究透彻多分类状况下的SVC，就必须研究透彻多分类时所需要的决策边界个数，每个决策边界所需要的支持向量个数，以及这些支持向量如何组合起来计算拉格朗日乘数。本小节只说一小部分。支持向量机是天在生二分类的模型，所以支持向量机在处理多分类问题的时候，是把多分类问题转换成了二分类问题来解决。这种转换有两种模式，一种叫做“一对一”模式（one vs one），一种叫做“一对多”模式(one vs rest)。 在ovo模式下(一对一模式)上，标签中的所有类别都会被两两组合，每两个类别建立一个SVC模型，每个模型生成一个决策边界，分别进行二分类，这种模式下，对于n_class个标签类别的数据来说，SVC会生成总共$C^2_{n_class}$个模型，即会生成总共$C^2_{n_class}$个超平面，其中： ovo模式下，二维空间的三分类状况： 首先让提出紫色点和红色点作为一组，然后求解出两个类之间的SVC和绿色决策边界。然后让绿色点和红色点作为一组，求解出两个类之间的SVC和灰色边界。最后让绿色和紫色组成一组，组成两个类之间的SVC和黄色边界。然后基于三个边界，分别对三个类别进行分类。 ovr模式下，标签中所有的类别会分别于其他类别进行组合，建立n_class个模型，每个模型生成一个决策边界。分别进行二分类。ovr模式下，则会生成以下的决策边界： 紫色类 vs 剩下的类，生成绿色的决策边界。红色类 vs 剩下的类，生成黄色的决策边界。绿色类 vs 剩下的类，生成灰色的决策边界，当类别更多的时候，如此类推下去，我们永远需要n_class个模型。 当类别更多的时候，无论是ovr还是ovo模式需要的决策边界都会越来越多，模型也会越来越复杂，不过ovo模式下的模型计算会更加复杂，因为ovo模式中的决策边界数量增加更快，但相对的，ovo模型也会更加精确。ovr模型计算更快，但是效果往往不是很好。在硬件可以支持的情况下，还是建议选择ovo模式。 模型和超平面的数量变化了，SVC的很多计算、接口、属性都会发生变化，而参数decision_function_shape决定我们究竟使用哪一种分类模式。 decision_function_shape可输入“ovo”，”ovr”，默认”ovr”，对所有分类器，选择使用ovo或者ovr模式。选择ovr模式，则返回的decision_function结构为(n_samples，n_class)。但是当二分类时，尽管选用ovr模式，却会返回(n_samples，)的结构。 选择ovo模式，则使用libsvm中原始的，结构为(n_samples,n_class(n_class-1)/2)的decision_function接口。在ovo模式并且核函数为线性核的情况下，属性coef_和intercepe_会分别返回(n_class\(n_class-1)/2,n_features) 和(n_class*(n_class-1)/2,)的结构，每行对应一个生成的二元分类器。ovo模式只在多分类的状况下使用。 SVC的其它参数、属性和接口的列表在SVM解读(2) 线性支持向量机类LinearSVC 线性支持向量机其实与SVC类中选择”linear”作为核函数的功能类似，但是其背后的实现库是liblinear而不是libsvm，这使得在线性数据上，linearSVC的运行速度比SVC中的“linear”核函数要快，不过两者的运行结果相似。在现实中，许多数据都是线性的，因此我们可以依赖计算得更快得LinearSVC类。除此之外，线性支持向量可以很容易地推广到大样本上，还可以支持稀疏矩阵，多分类中也支持ovr方案。 和SVC一样，LinearSVC也有C这个惩罚参数，但LinearSVC在C变大时对C不太敏感，并且在某个阈值之后就不能再改善结果了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(3)]]></title>
    <url>%2F2019%2F09%2F16%2FSVM%E8%A7%A3%E8%AF%BB(3)%2F</url>
    <content type="text"><![CDATA[参数C的理解进阶在SVM(2)说到有一些数据，可能是线性可分，但在线性可分状况下训练准确率不能达到100%，即无法让训练误差为0，这样的数据被我们称为“存在软间隔的数据”。此时此刻，我们需要让我们决策边界能够忍受一小部分训练误差，我们就不能单纯地寻求最大边际了。 因为对于软间隔地数据来说，边际越大被分错的样本也就会越多，因此我们需要找出一个”最大边际“与”被分错的样本数量“之间的平衡。因此，我们引入松弛系数ζ和松弛系数的系数C作为一个惩罚项，来惩罚我们对最大边际的追求。 那我们的参数C如何影响我们的决策边界呢？在硬间隔的时候，我们的决策边界完全由两个支持向量和最小化损失函数（最大化边际）来决定，而我们的支持向量是两个标签类别不一致的点，即分别是正样本和负样本。然而在软间隔情况下我们的边际依然由支持向量决定，但此时此刻的支持向量可能就不再是来自两种标签类别的点了，而是分布在决策边界两边的，同类别的点。回忆一下我们的图像： 此时我们的虚线超平面&omega;*x+b=1-ζi是由混杂在红色点中间的紫色点来决定的，所以此时此刻，这个蓝色点就是我们的支持向量了。所以软间隔让决定两条虚线超平面的支持向量可能是来自于同一个类别的样本点，而硬间隔的时候两条虚线超平面必须是由来自两个不同类别的支持向量决定的。而C值会决定我们究竟是依赖红色点作为支持向量（只追求最大边界），还是我们要依赖软间隔中，混杂在红色点中的紫色点来作为支持向量（追求最大边界和判断正确的平衡）。如果C值设定比较大，那SVC可能会选择边际较小的，能够更好地分类所有训练点的决策边界，不过模型的训练时间也会更长。如果C的设定值较小，那SVC会尽量最大化边界，尽量将掉落在决策边界另一方的样本点预测正确，决策功能会更简单，但代价是训练的准确度，因为此时会有更多红色的点被分类错误。换句话说，C在SVM中的影响就像正则化参数对逻辑回归的影响。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import svmfrom sklearn.datasets import make_circles, make_moons, make_blobs,make_classificationn_samples = 100datasets = [make_moons(n_samples=n_samples, noise=0.2, random_state=0),make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),make_blobs(n_samples=n_samples, centers=2, random_state=5),make_classification(n_samples=n_samples,n_features =2,n_informative=2,n_redundant=0, random_state=5)]Kernel = ["linear"]#四个数据集分别是什么样子呢？for X,Y in datasets: plt.figure(figsize=(5,4)) plt.scatter(X[:,0],X[:,1],c=Y,s=50,cmap="rainbow")nrows=len(datasets)ncols=len(Kernel) + 1fig, axes = plt.subplots(nrows, ncols,figsize=(10,16)) #第一层循环：在不同的数据集中循环for ds_cnt, (X,Y) in enumerate(datasets): ax = axes[ds_cnt, 0] if ds_cnt == 0: ax.set_title("Input data") ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,edgecolors='k') ax.set_xticks(()) ax.set_yticks(()) for est_idx, kernel in enumerate(Kernel): ax = axes[ds_cnt, est_idx + 1] clf = svm.SVC(kernel=kernel, gamma=2).fit(X, Y) score = clf.score(X, Y) ax.scatter(X[:, 0], X[:, 1], c=Y , zorder=10 , cmap=plt.cm.Paired, edgecolors='k') ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', zorder=10, edgecolors='white') x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape) ax.pcolormesh(XX, YY, Z &gt; 0, cmap=plt.cm.Paired) ax.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1]) ax.set_xticks(()) ax.set_yticks(()) if ds_cnt == 0: ax.set_title(kernel) ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0') , size=15 , bbox=dict(boxstyle='round', alpha=0.8, facecolor='white') # 为分数添加一个白色的格子作为底色 , transform=ax.transAxes # 确定文字所对应的坐标轴，就是ax子图的坐标轴本身 , horizontalalignment='right' # 位于坐标轴的什么方向 )plt.tight_layout()plt.show() 白色圈圈出的就是我们的支持向量，大家可以看到，所有在两条虚线超平面之间的点，和虚线超平面外，但属于另一个类别的点，都被我们认为是支持向量。并不是因为这些点都在我们的超平面上，而是因为我们的超平面由所有的这些点来决定，我们可以通过调节C来移动我们的超平面，让超平面过任何一个白色圈圈出的点。参数C就是这样影响了我们的决策，可以说是彻底改变了SVM的决策过程。 样本不均衡问题分类问题永远逃不过的痛点是样本不均衡问题。样本不均衡是指在一组数据集中，标签的一类天生占有很大的比例，但我们有着捕捉出某种特定的分类的需求的状况。 分类问题天生会倾向于多数的类，让多数类更容易被判断准确，少数类被牺牲掉。因此对于模型而言，样本量越大的标签可以学习的信息越多，算法就会更加依赖于从多数类中学到的信息来进行判断。如果我们希望捕获少数类，模型就会失败。其次，模型评估指标会失去意义。 class_weight可输入字典或者”balanced”，可不填，默认None 对SVC，将类i的参数C设置为class_weight [i] * C。如果没有给出具体的class_weight，则所有类都被假设为占有相同的权重1，模型会根据数据原本的状况去训练。如果希望改善样本不均衡状况，请输入形如{“标签的值1”：权重1，”标签的值2”：权重2}的字典，则参数C将会自动被设为：标签的值1的C：权重1 * C，标签的值2的C：权重2*C或者，可以使用“balanced”模式，这个模式使用y的值自动调整与输入数据中的类频率成反比的权重为n_samples/(n_classes * np.bincount(y)) 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)#其中红色点是少数类，紫色点是多数类clf = svm.SVC(kernel='linear', C=1.0)clf.fit(X, y)#设定class_weightwclf = svm.SVC(kernel='linear', class_weight=&#123;1: 10&#125;)wclf.fit(X, y)#给两个模型分别打分看看，这个分数是accuracy准确度print("没设定class_weight：",clf.score(X,y))print("设定class_weight：",wclf.score(X,y))plt.figure(figsize=(6,5))plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)ax = plt.gca() #获取当前的子图，如果不存在，则创建新的子图xlim = ax.get_xlim()ylim = ax.get_ylim()xx = np.linspace(xlim[0], xlim[1], 30)yy = np.linspace(ylim[0], ylim[1], 30)YY, XX = np.meshgrid(yy, xx)xy = np.vstack([XX.ravel(), YY.ravel()]).T#第二步：找出我们的样本点到决策边界的距离Z_clf = clf.decision_function(xy).reshape(XX.shape)a = ax.contour(XX, YY, Z_clf, colors='black', levels=[0], alpha=0.5, linestyles=['-'])Z_wclf = wclf.decision_function(xy).reshape(XX.shape)b = ax.contour(XX, YY, Z_wclf, colors='red', levels=[0], alpha=0.5, linestyles=['-'])#第三步：画图例 a.collections调用这个等高线对象中画的所有线，返回一个惰性对象plt.legend([a.collections[0], b.collections[0]], ["non weighted", "weighted"],loc="upper right")plt.show() 从图像上可以看出，灰色是我们做样本平衡之前的决策边界。灰色线上方的点被分为一类，下方的点被分为另一类。可以看到，大约有一半少数类（红色）被分错，多数类（紫色点）几乎都被分类正确了。红色是我们做样本平衡之后的决策边界，同样是红色线上方一类，红色线下方一类。可以看到，做了样本平衡后，少数类几乎全部都被分类正确了，但是多数类有许多被分错了。 从准确率的角度来看，不做样本平衡的时候准确率反而更高，做了样本平衡准确率反而变低了，这是因为做了样本平衡后，为了要更有效地捕捉出少数类，模型误伤了许多多数类样本，而多数类被分错的样本数量 &gt; 少数类被分类正确的样本数量，使得模型整体的精确性下降。现在，如果我们的目的是模型整体的准确率，那我们就要拒绝样本平衡，使用class_weight被设置之前的模型。而在现实情况中，我们往往都在捕捉少数类。因为有些情况是宁愿误伤，也要尽量的捕捉少数类。 SVC的模型评估指标上面说了，我们往往都在捕捉少数类的。但是单纯地追求捕捉出少数类，就会成本太高，而不顾及少数类，又会无法达成模型的效果。所以在现实中，我们往往在寻找捕获少数类的能力和将多数类判错后需要付出的成本的平衡。如果一个模型在能够尽量捕获少数类的情况下，还能够尽量对多数类判断正确，则这个模型就非常优秀了。为了评估这样的能力，我们将引入新的模型评估指标：混淆矩阵和ROC曲线来帮助我们 混淆矩阵(Confusion Matrix)精确率和召回率精确度Precision，又叫查准率，表示所有被我们预测为是少数类的样本中，真正的少数类所占的比例 召回率Recall，又被称为敏感度(sensitivity)，真正率，查全率，表示所有真实为1的样本中，被我们预测正确的样本所占的比例。 如果我们希望不计一切代价，找出少数类，那我们就会追求高召回率，相反如果我们的目标不是尽量捕获少数类，那我们就不需要在意召回率。 注意召回率和精确度的分子是相同的，只是分母不同。而召回率和精确度是此消彼长的，两者之间的平衡代表了捕捉少数类的需求和尽量不要误伤多数类的需求的平衡。究竟要偏向于哪一方，取决于我们的业务需求：究竟是误伤多数类的成本更高，还是无法捕捉少数类的代价更高。 为了同时兼顾精确度和召回率，我们创造了两者的调和平均数作为考量两者平衡的综合性指标，称之为F1measure。两个数之间的调和平均倾向于靠近两个数中比较小的那一个数，因此我们追求尽量高的F1 measure，能够保证我们的精确度和召回率都比较高。F1 measure在[0,1]之间分布，越接近1越好： 假负率、特异度(真负率)、假正率从Recall延申出来的另一个评估指标叫做假负率（False Negative Rate），它等于 1 - Recall，用于衡量所有真实为1的样本中，被我们错误判断为0的，通常用得不多。 特异度(Specificity)，也叫做真负率。表示所有真实为0的样本中，被正确预测为0的样本所占的比例。在支持向量机中，可以形象地表示为，决策边界下方的点占所有蓝色点的比例。特异度衡量了一个模型将多数类判断正确的能力，而1 - specificity就是一个模型将多数类判断错误的能力，叫做假正率。 sklearn当中提供了大量的类来帮助我们了解和使用混淆矩阵： ROC曲线ROC曲线，全称The Receiver Operating Characteristic Curve，译为受试者操作特性曲线。这是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。让我们先从概率和阈值开始看起。 阈值(threshold)1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [1.0, 1.0]] #设定两个类别的中心clusters_std = [0.5, 1] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=30)plt.show()from sklearn.linear_model import LogisticRegression as LogiRclf_lo=LogiR().fit(X,y)prob=clf_lo.predict_proba(X)import pandas as pdprob=pd.DataFrame(prob)prob.columns=["0","1"]for i in range(prob.shape[0]): #将0.5设立为阈值 if prob.loc[i,"1"] &gt; 0.5: #创建了一个新的列名为pred的列 prob.loc[i,"pred"] = 1 else: prob.loc[i,"pred"] = 0#创建了名为y_true的列，且将y赋值为该列prob["y_true"] = y#通过“1”列拍序，ascending表示正序or逆序prob = prob.sort_values(by="1",ascending=False)from sklearn.metrics import confusion_matrix as CM, precision_score as P, recall_score as Rfrom sklearn.metrics import accuracy_score as Aprint("混淆矩阵：",CM(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0]))print("精确度：",P(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0]))print("召回率：",R(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0]))print("准确率：",A(prob.loc[:,"y_true"],prob.loc[:,"pred"])) 我们可以设置不同阈值来实验模型的结果，在不同阈值下，我们的模型评估指标会发生变化。我们正利用这一点来观察Recall和FPR之间如何互相影响。但是注意，并不是升高阈值，就一定能够增加或者减少Recall，一切要根据数据的实际分布来进行判断。 概率(probability)我们在画等高线，也就是决策边界的时候曾经使用SVC的接口decision_function，它返回我们输入的特征矩阵中每个样本到划分数据集的超平面的距离。我们在SVM中利用超平面来判断我们的样本，本质上来说，当两个点的距离是相同的符号的时候，越远离超平面的样本点归属于某个标签类的概率就很大。比如说，一个距离超平面0.1的点，和一个距离超平面100的点，明显是距离为0.1的点更有可能是负类别的点混入了边界。同理，一个距离超平面距离为-0.1的点，和一个离超平面距离为-100的点，明显是-100的点的标签更有可能是负类接口decision_function返回的值也因此被我们认为是SVM中的置信度(confidence)。 不过，置信度始终不是概率，它没有边界，可以无限大。大部分时候不是也小数的形式呈现，而SVC的判断过程又不像决策树一样求解出一个比例。为了解决这个矛盾，SVC有重要参数probability。 probability：是否启用概率估计，布尔值，可不填，默认时False。必须在调用fit之前使用它，启用此功能会减慢SVM的计算速度。设置为True会启动，SVC的接口predict_proba和predict_log_proba将生效。在二分类情况下，SVC将使用Platt缩放生成概率，即在decision_function生成的距离上进行Sigmoid压缩，并附加训练数据的交叉验证拟合，来生成类逻辑回归的SVM分数。 1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)plt.show()#其中红色点是少数类，紫色点是多数类clf_proba = svm.SVC(kernel="linear",C=1.0,probability=True).fit(X,y)print("返回预测某标签的概率：",clf_proba.predict_proba(X))print("行数和列数：",clf_proba.predict_proba(X).shape)print("每个样本到划分数据集的超平面的距离：",clf_proba.decision_function(X)) 绘制SVM的ROC曲线现在，我们理解了什么是阈值（threshold），了解了不同阈值会让混淆矩阵产生变化，也了解了如何从我们的分类算法中获取概率。现在，我们就可以开始画我们的ROC曲线了。ROC是一条以不同阈值下的假正率FPR为横坐标，不同阈值下的召回率Recall为纵坐标的曲线。简单地来说，只要我们有数据和模型，我们就可以在python中绘制出我们的ROC曲线。思考一下，我们要绘制ROC曲线，就必须在我们的数据中去不断调节阈值，不断求解混淆矩阵，然后不断获得我们的横坐标和纵坐标，最后才能够将曲线绘制出来。接下来，我们就来执行这个过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobsclass_1 = 500 #类别1有500个样本class_2 = 50 #类别2只有50个centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [1.5, 0.5] #设定两个类别的方差，通常来说，样本量比较大的类别会更加松散X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)#看看数据集长什么样plt.scatter(X[:, 0], X[:, 1], c=y, cmap="rainbow",s=10)plt.show()from sklearn.metrics import confusion_matrix as CMfrom sklearn.linear_model import LogisticRegression as LogiRclf_lo=LogiR().fit(X,y)prob=clf_lo.predict_proba(X)import pandas as pdprob=pd.DataFrame(prob)prob.columns=["0","1"]for i in range(prob.shape[0]): #将0.5设立为阈值 if prob.loc[i,"1"] &gt; 0.5: #创建了一个新的列名为pred的列 prob.loc[i,"pred"] = 1 else: prob.loc[i,"pred"] = 0#创建了名为y_true的列，且将y赋值为该列prob["y_true"] = yprob = prob.sort_values(by="1",ascending=False)cm = CM(prob.loc[:,"y_true"],prob.loc[:,"pred"],labels=[1,0])#开始绘图recall = []FPR = []clf_proba = svm.SVC(kernel="linear",C=1.0,probability=True).fit(X,y)probrange = np.linspace(clf_proba.predict_proba(X)[:,1].min(),clf_proba.predict_proba(X)[:,1].max(),num=50,endpoint=False)from sklearn.metrics import confusion_matrix as CM, recall_score as Rimport matplotlib.pyplot as plotfor i in probrange: y_predict = [] for j in range(X.shape[0]): if clf_proba.predict_proba(X)[j,1] &gt; i: y_predict.append(1) else: y_predict.append(0) cm = CM(y,y_predict,labels=[1,0]) recall.append(cm[0,0]/cm[0,:].sum()) FPR.append(cm[1,0]/cm[1,:].sum())#排序recall.sort()FPR.sort()plt.plot(FPR,recall,c="red")plt.plot(probrange+0.05,probrange+0.05,c="black",linestyle="--")plt.show() 运行后，得出下图： 我们建立ROC曲线的根本目的是找寻Recall和FPR之间的平衡，让我们能够衡量模型在尽量捕捉少数类的时候，误伤多数类的情况会如何变化。横坐标是FPR，代表着模型将多数类判断错误的能力，纵坐标Recall，代表着模型捕捉少数类的能力，所以ROC曲线代表着，随着Recall的不断增加，FPR如何增加。我们希望随着Recall的不断提升，FPR增加得越慢越好，这说明我们可以尽量高效地捕捉出少数类，而不会将很多地多数类判断错误。所以，我们希望看到的图像是，纵坐标急速上升，横坐标缓慢增长，也就是在整个图像左上方的一条弧线。这代表模型的效果很不错，拥有较好的捕获少数类的能力。 中间的虚线代表着，当recall增加1%，我们的FPR也增加1%，也就是说，我们每捕捉出一个少数类，就会有一个多数类被判错，这种情况下，模型的效果就不好，这种模型捕获少数类的结果，会让许多多数类被误伤，从而增加我们的成本。ROC曲线通常都是凸型的。对于一条凸型ROC曲线来说，曲线越靠近左上角越好，越往下越糟糕，曲线如果在虚线的下方，则证明模型完全无法使用。但是它也有可能是一条凹形的ROC曲线。对于一条凹型ROC曲线来说，应该越靠近右下角越好，凹形曲线代表模型的预测结果与真实情况完全相反，那也不算非常糟糕，只要我们手动将模型的结果逆转，就可以得到一条左上方的弧线了。最糟糕的就是，无论曲线是凹形还是凸型，曲线位于图像中间，和虚线非常靠近，那我们拿它无能为力。 现在，我们虽然拥有了这条曲线，但是还是没有具体的数字帮助我们理解ROC曲线和模型效果。接下来将AUC面加，它代表了ROC曲线下方的面积，这个面积越大，代表ROC曲线越接近左上角，模型就越好。 AUC面积sklearn中，我们有帮助我们计算ROC曲线的横坐标假正率FPR，纵坐标Recall和对应的阈值的类sklearn.metrics.roc_curve。同时，我们还有帮助我们计算AUC面积的类sklearn.metrics.roc_auc_score。 AUC面积的分数使用以上类来进行计算，输入的参数也比较简单，就是真实标签，和与roc_curve中一致的置信度分数或者概率值。 1234567891011121314151617from sklearn.metrics import roc_curveFPR, recall, thresholds = roc_curve(y,clf_proba.decision_function(X), pos_label=1)#阈值可以也为负print(FPR,recall,thresholds)from sklearn.metrics import roc_auc_score as AUCarea = AUC(y,clf_proba.decision_function(X))plt.figure()plt.plot(FPR, recall, color='red',label='ROC curve (area = %0.2f)' % area)plt.plot([0, 1], [0, 1], color='black', linestyle='--')plt.xlim([-0.05, 1.05])plt.ylim([-0.05, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('Recall')plt.title('Receiver operating characteristic example')plt.legend(loc="lower right")plt.show() 利用ROC曲线找出最佳阈值对ROC曲线的理解来：ROC曲线反应的是recall增加的时候FPR如何变化，也就是当模型捕获少数类的能力变强的时候，会误伤多数类的情况是否严重。我们的希望是，模型在捕获少数类的能力变强的时候，尽量不误伤多数类，也就是说，随着recall的变大，FPR的大小越小越好。所以我们希望找到的最有点，其实是Recall和FPR差距最大的点。这个点，又叫做约登指数。 12345678910111213141516maxindex=(recall-FPR).tolist().index(max(recall-FPR))#得出最佳阈值print("最佳阈值：",thresholds[maxindex])plt.scatter(FPR[maxindex],recall[maxindex],c="black",s=30)plt.figure()plt.plot(FPR, recall, color='red',label='ROC curve (area = %0.2f)' % area)plt.plot([0, 1], [0, 1], color='black', linestyle='--')plt.scatter(FPR[maxindex],recall[maxindex],c="black",s=30)plt.xlim([-0.05, 1.05])plt.ylim([-0.05, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('Recall')plt.title('Receiver operating characteristic example')plt.legend(loc="lower right")plt.show() 这样，最佳阈值和最好的点都找了出来。]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(2)]]></title>
    <url>%2F2019%2F09%2F15%2FSVM%E8%A7%A3%E8%AF%BB(2)%2F</url>
    <content type="text"><![CDATA[非线性SVM与核函数SVC在非线性数据上的推广为了能够找出非线性数据的线性决策边界，我们需要将数据从原始的空间x投射到新空间Φ(x)中，Φ是一个映射函数，它代表了某种非线性的变换，如同我们之前所做过的使用r来升维一样，这种非线性变换看起来是一种非常有效的方式。使用这种变换，线性SVM的原理可以被很容易推广到非线性情况下，其推到过程核逻辑都与线性SVM一摸一样，只不过在第一决策边界之前，我们必须先对数据进行升维，即将原始的x转换成Φ。 重要参数kernel这种变换非常巧妙，但也带有一些实现问题。首先，我们可能不清楚应该什么样的数据应该使用什么类型的映射函数来确保可以在变换空间中找出线性决策边界。极端情况下，数据可能会被映射到无限维度的空间中，这种高维空间可能不是那么友好，维度越多，推导和计算的难度都会随之暴增。其次，已知适当的映射函数，我们想要计算类似于Φ(x)i* Φ(xtest)这样的点积，计算量无法巨大。要找出超平面所付出的代价是非常昂贵的。 关键概念：核函数 而解决上面这些问题的数学方式，叫做”核技巧”。是一种能够使用数据原始空间中的向量计算来表示升维后的空间中的点积结果的数学方式。具体体现为，K(u,v)= Φ(u)*Φ(v)。而这个原始空间中的点积函数K(u,v)，就被叫做“核函数”。 核函数能够帮助我们解决三个问题： 第一，有了核函数之后，我们无需去担心$\Phi$究竟应该是什么样，因为非线性SVM中的核函数都是正定核函数(positive definite kernel functions)，他们都满足美世定律(Mercer’s theorem)，确保了高维空间中任意两个向量的点积一定可以被低维空间中的这两个向量的某种计算来表示。 第二，使用核函数计算低维度中的向量关系比计算原来的Φ(xi) * Φ(xtest)要简单太多了。 第三，因为计算是在原始空间中进行，所以避免了维度诅咒的问题。 选用不同的核函数，就可以解决不同数据分布下的寻找超平面问题。在SVC中，这个功能由参数“kernel”和一系列与核函数相关的参数来进行控制。之前的代码中我们一直使用这个参数并输入”linear”，但却没有给大家详细讲解，也是因为如果不先理解核函数本身，很难说明这个参数到底在做什么。参数“kernel”在sklearn中可选以下几种选项： 可以看出，除了选项”linear”之外，其他核函数都可以处理非线性问题。多项式核函数有次数d，当d为1的时候它就是再处理线性问题，当d为更高次项的时候它就是在处理非线性问题。我们之前画图时使用的是选项“linear”，自然不能处理环形数据这样非线性的状况。而刚才我们使用的计算r的方法，其实是高斯径向基核函数所对应的功能，在参数”kernel“中输入”rbf“就可以使用这种核函数。 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npfrom sklearn.datasets import make_circlesX,y = make_circles(100, factor=0.1, noise=.1)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plt.show()def plot_svc_decision_function(model,ax=None): if ax is None: ax = plt.gca() xlim = ax.get_xlim() ylim = ax.get_ylim() x = np.linspace(xlim[0],xlim[1],30) y = np.linspace(ylim[0],ylim[1],30) Y,X = np.meshgrid(y,x) xy = np.vstack([X.ravel(), Y.ravel()]).T P = model.decision_function(xy).reshape(X.shape) ax.contour(X, Y, P,colors="k",levels=[-1,0,1],alpha=0.5,linestyles=["--","-","--"]) ax.set_xlim(xlim) ax.set_ylim(ylim) plt.show()clf = SVC(kernel = "linear").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")r = np.exp(-(X**2).sum(1))rlim = np.linspace(min(r),max(r),0.2)from mpl_toolkits import mplot3ddef plot_3D(elev=30,azim=30,X=X,y=y): ax = plt.subplot(projection="3d") ax.scatter3D(X[:,0],X[:,1],r,c=y,s=50,cmap='rainbow') ax.view_init(elev=elev,azim=azim) ax.set_xlabel("x") ax.set_ylabel("y") ax.set_zlabel("r") plt.show()plot_3D()clf = SVC(kernel = "rbf").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plot_svc_decision_function(clf) 运行后，从效果图可以看到，决策边界被完美的找了出来。 探索核函数在不同数据集上的表现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import svmfrom sklearn.datasets import make_circles, make_moons, make_blobs,make_classificationn_samples = 100datasets = [make_moons(n_samples=n_samples, noise=0.2, random_state=0),make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),make_blobs(n_samples=n_samples, centers=2, random_state=5),make_classification(n_samples=n_samples,n_features =2,n_informative=2,n_redundant=0, random_state=5)]Kernel = ["linear","poly","rbf","sigmoid"]#四个数据集分别是什么样子呢？for X,Y in datasets: plt.figure(figsize=(5,4)) plt.scatter(X[:,0],X[:,1],c=Y,s=50,cmap="rainbow")nrows=len(datasets)ncols=len(Kernel) + 1fig, axes = plt.subplots(nrows, ncols,figsize=(20,16))for ds_cnt, (X,Y) in enumerate(datasets):#在图像中的第一列，放置原数据的分布 ax = axes[ds_cnt, 0] if ds_cnt == 0: ax.set_title("Input data") ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,edgecolors='k') ax.set_xticks(()) ax.set_yticks(()) #第二层循环：在不同的核函数中循环 #从图像的第二列开始，一个个填充分类结果 for est_idx, kernel in enumerate(Kernel): #定义子图位置 ax = axes[ds_cnt, est_idx + 1] #建模 clf = svm.SVC(kernel=kernel, gamma=2).fit(X, Y) score = clf.score(X, Y) #绘制图像本身分布的散点图 ax.scatter(X[:, 0], X[:, 1], c=Y ,zorder=10 ,cmap=plt.cm.Paired,edgecolors='k') #绘制支持向量 ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=50, facecolors='none', zorder=10, edgecolors='k') #绘制决策边界 x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 #np.mgrid，合并了我们之前使用的np.linspace和np.meshgrid的用法 #一次性使用最大值和最小值来生成网格 #表示为[起始值：结束值：步长] #如果步长是复数，则其整数部分就是起始值和结束值之间创建的点的数量，并且结束值被包含在内 XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] #np.c_，类似于np.vstack的功能 Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]).reshape(XX.shape) #填充等高线不同区域的颜色 ax.pcolormesh(XX, YY, Z &gt; 0, cmap=plt.cm.Paired) #绘制等高线 ax.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1]) #设定坐标轴为不显示 ax.set_xticks(()) ax.set_yticks(()) #将标题放在第一行的顶上 if ds_cnt == 0: ax.set_title(kernel) #为每张图添加分类的分数 ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0') , size=15 , bbox=dict(boxstyle='round', alpha=0.8, facecolor='white') #为分数添加一个白色的格子作为底色 , transform=ax.transAxes #确定文字所对应的坐标轴，就是ax子图的坐标轴本身 , horizontalalignment='right' #位于坐标轴的什么方向 )plt.tight_layout()plt.show() 可以观察到，线性核函数和多项式核函数在非线性数据上表现会浮动，如果数据相对线性可分，则表现不错，如果是像环形数据那样彻底不可分的，则表现糟糕。在线性数据集上，线性核函数和多项式核函数即便有扰动项也可以表现不错，可见多项式核函数是虽然也可以处理非线性情况，但更偏向于线性的功能。Sigmoid核函数就比较尴尬了，它在非线性数据上强于两个线性核函数，但效果明显不如rbf，它在线性数据上完全比不上线性的核函数们，对扰动项的抵抗也比较弱，所以它功能比较弱小，很少被用到。rbf，高斯径向基核函数基本在任何数据集上都表现不错，属于比较万能的核函数。我个人的经验是，无论如何先试试看高斯径向基核函数，它适用于核转换到很高的空间的情况，在各种情况下往往效果都很不错，如果rbf效果不好，那我们再试试看其他的核函数。另外，多项式核函数多被用于图像处理之中。 选取与核函数相关的参数 在知道如何选取核函数后，我们还要观察一下除了kernel之外的核函数相关的参数。对于线性核函数，”kernel”是唯一能够影响它的参数，但是对于其他三种非线性核函数，他们还受到参数gamma，degree以及coef0的影响。参数gamma就是表达式中的&gamma;，degree就是多项式核函数的次数d，参数coef0就是常数项&gamma;。其中，高斯径向基核函数受到gamma的影响，而多项式核函数受到全部三个参数的影响。 12345678910111213141516171819#使用交叉验证得出最好的参数和准确率from sklearn.model_selection import StratifiedShuffleSplitfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_breast_cancerfrom sklearn import svmimport numpy as npdata = load_breast_cancer()X = data.datay = data.targetgamma_range = np.logspace(-10,1,20)coef0_range = np.linspace(0,5,10)param_grid = dict(gamma = gamma_range,coef0 = coef0_range)cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=420)grid = GridSearchCV(svm.SVC(kernel = "poly",degree=1,cache_size=5000),param_grid=param_grid, cv=cv)grid.fit(X, y)print("The best parameters are %s with a score of %0.5f" % (grid.best_params_,grid.best_score_)) 重要参数C关键概念：硬件隔与软件隔 当两组数据是完全线性可分，我们可以找出一个决策边界使得训练集上的分类误差为0，这两种数据就被称为是存在“硬间隔“的。当两组数据几乎是完全线性可分的，但决策边界在训练集上存在较小的训练误差，这两种数据就被称为是存在”软间隔“。 看上图，原来的决策边界&omega;x+b=0，原本的平行于决策边界的两个虚线超平面&omega;\x+b=1和&omega;*x+b=-1都依然有效。我们的原始判别函数是: 不过，这些超平面现在无法让数据上的训练误差等于0了，因为此时存在了一个混杂在红色点中的紫色点。于是，我们需要放松我们原始判别函数中的不等条件，来让决策边界能够适用于我们的异常点。于是我们引入松弛系数Φ来帮助我们优化原始的判别函数： 其中ζi&gt;0。可以看的出，这其实是将原来的虚线超平面向图像的上方和下方平移。松弛系数其实蛮好理解的，看上图，在红色点附近的蓝色点在原本的判别函数中必定会被分为红色，所有一定会判断错。我们现在做一条与决策边界平行，且过蓝色点的直线&omega;xi+b=1- ζi(图中的蓝色虚线)。这条直线是由&omega;*xi+b=1平移得到，所以两条直线在纵坐标上的差异就是ζi。而点&omega;xi+b=1的距离就可以表示为 ζi\&omega;/||&omega;||，即ζi在&omega;方向上的投影。由于单位向量是固定的，所以ζi可以作为蓝色点在原始的决策边界上的分类错误的程度的表示。隔得越远，分的越错。注意：ζi并不是点到决策超平面的距离本身。 不难注意到，我们让&omega;*xi+b&gt;=1-ζi作为我们的新决策超平面，是有一定的问题的。虽然我们把异常的蓝色点分类正确了，但我们同时也分错了一系列红色的点。所以我们必须在我们求解最大边际的损失函数中加上一个惩罚项，用来惩罚我们具有巨大松弛系数的决策超平面。我们的拉格朗日函数，拉格朗日对偶函数，也因此都被松弛系数改变。现在，我们的损失函数为 C是用来控制惩罚力度的系数 我们的拉格朗日函数为(其中$\mu$是第二个拉格朗日乘数) 需要满足的KKT条件为 拉格朗日对偶函数为 sklearn类SVC背后使用的最终公式。公式中现在唯一的新变量，松弛系数的惩罚力度C，由我们的参数C来进行控制。 在实际使用中，C和核函数的相关参数（gamma，degree等等）们搭配，往往是SVM调参的重点。与gamma不同，C没有在对偶函数中出现，并且是明确了调参目标的，所以我们可以明确我们究竟是否需要训练集上的高精确度来调整C的方向。默认情况下C为1，通常来说这都是一个合理的参数。 如果我们的数据很嘈杂，那我们往往减小C。当然，我们也可以使用网格搜索或者学习曲线来调整C的值。 123456789101112131415161718192021222324252627282930313233from sklearn.model_selection import StratifiedShuffleSplitfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_breast_cancerimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn import svmimport numpy as npdata = load_breast_cancer()X = data.datay = data.targetC1_range = np.linspace(0.01,30,50)C2_range=np.linspace(0.01,30,50)C3_range=np.linspace(5,7,50)param_grid1 = dict(C=C1_range)cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=420)grid1 = GridSearchCV(svm.SVC(kernel = "linear",gamma= 0.012742749857031322,cache_size=5000),param_grid=param_grid1, cv=cv)grid1.fit(X, y)print("linear The best parameters are %s with a score of %0.5f" % (grid1.best_params_,grid1.best_score_))param_grid2=dict(C=C2_range)grid2 = GridSearchCV(svm.SVC(kernel = "rbf",gamma= 0.012742749857031322,cache_size=5000),param_grid=param_grid2, cv=cv)grid2.fit(X, y)print("rbf(0.01,30,50) The best parameters are %s with a score of %0.5f" % (grid2.best_params_,grid2.best_score_))param_grid3=dict(C=C3_range)grid3=GridSearchCV(svm.SVC(kernel="rbf",gamma=0.012742749857031322,cache_size=5000),param_grid=param_grid3)grid3.fit(X,y)print("rbf(5,7,50) ",grid3.best_params_)print("rbf(5,7,50) ",grid3.best_score_) SVC的参数、属性和接口参数 属性 接口]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2F2019%2F09%2F12%2FXGBoost%2F</url>
    <content type="text"><![CDATA[前沿在使用XGBoost之前，要先安装XGBoost库。xgboost库要求我们必须要提供适合的Scipy环境。以下为大家提供在windows中和MAC使用pip来安装xgboost的代码： windows： pip install xgboost #安装xgboost库pip install –upgrade xgboost #更新xgboost库 我在这步遇到超时报错，查了以下，改成如下安装： 后面看到一些帖子，发现下面这个方法才是真的好用，在C:\Users\湛蓝星空 这个路径下创建一个pip文件夹，在文件夹创建一个txt，将下面内容加入文件里面，再将文件后缀名改为 .ini。 [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple 这其实就是将源改为清华的源，防止被墙。非常管用 MAC： brew install gcc@7pip3 install xgboost 安装好XGBoost库后，我们有两种方式来使用我们的XGBoost库。第一种方式。是直接使用XGBoost库自己的建模流程。 其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。params可能的取值以及xgboost.train的列表： 我们也可以选择第二种方法，使用xgboost库中的sklearn的API： 有人发现，这两种方法的参数是不同的。其实只是写法不同，功能是相同的。使用xgboost中设定的建模流程来建模，和使用sklearnAPI中的类来建模，模型效果是比较相似的，但是xgboost库本身的运算速度（尤其是交叉验证）以及调参手段比sklearn要简单。 XGBoostXGBoost本身的核心是基于梯度提升树实现的集成算法，整体来说可以有三个核心部分：集成算法本身，用于集成的弱评估器，以及应用中的其他过程。 梯度提升树 Boosting过程XGBoost的基础是梯度提升算法，因此我们必须先从了解梯度提升算法开始。梯度提升（Gradient boosting）是构建预测模型的最强大技术之一，它是集成算法中提升法（Boosting）的代表算法。集成算法通过在数据上构建多个弱评估器，汇总所有弱评估器的建模结果，以获取比单个模型更好的回归或分类表现。弱评估器被定义为是表现至少比随机猜测更好的模型，即预测准确率不低于50%的任意模型。 集成不同弱评估器的方法有很多种。有像我们曾经在随机森林的课中介绍的，一次性建立多个平行独立的弱评估器的装袋法。也有像我们今天要介绍的提升法这样，逐一构建弱评估器，经过多次迭代逐渐累积多个弱评估器的方法。提升法的中最著名的算法包括Adaboost和梯度提升树，XGBoost就是由梯度提升树发展而来的。梯度提升树中可以有回归树也可以有分类树，两者都以CART树算法作为主流，XGBoost背后也是CART树，这意味着XGBoost中所有的树都是二叉的。 接下来，我们来了解一些Boosting算法是上面工作：首先，梯度提升回归树是专注于回归的树模型的提升集成模型，其建模过程大致如下：最开始先建立一棵树，然后逐渐迭代，每次迭代过程中都增加一棵树，逐渐形成众多树模型集成的强评估器。 参数 123456789101112131415161718from xgboost import XGBRegressor as XGBRfrom sklearn.ensemble import RandomForestRegressor as RFRfrom sklearn.linear_model import LinearRegression as LinearRfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTSfrom sklearn.metrics import mean_squared_error as MSEimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltdata = load_boston()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)reg = XGBR(n_estimators=100).fit(Xtrain,Ytrain)print("预测：",reg.predict(Xtest)) #传统接口predictprint("准确率：",reg.score(Xtest,Ytest))print("均方误差：",MSE(Ytest,reg.predict(Xtest)))print("模型的重要性分数：",reg.feature_importances_) 使用参数学习曲线观察n_estimators对模型的影响 1234567891011axisx = range(10,1010,50)cv = KFold(n_splits=5, shuffle = True, random_state=42)rs = []for i in axisx: reg = XGBR(n_estimators=i,random_state=420) rs.append(CVS(reg,Xtrain,Ytrain,cv=cv).mean())print(axisx[rs.index(min(rs))],max(rs))plt.figure(figsize=(20,5))plt.plot(axisx,rs,c="red",label="XGB")plt.legend()plt.show() 方差与泛化误差机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定。其中偏差就是训练集上的拟合程度决定，方差是模型的稳定性决定，噪音是不可控的。而泛化误差越小，模型就越理想。 12345678910111213141516171819202122232425262728cv=KFold(n_splits=5,shuffle=True,random_state=42)axisx = range(100,300,10)rs = []var = []ge = []for i in axisx: reg = XGBR(n_estimators=i,random_state=420) cvresult = CVS(reg,Xtrain,Ytrain,cv=cv) rs.append(cvresult.mean()) var.append(cvresult.var()) ge.append((1 - cvresult.mean())**2+cvresult.var())#得出最好的n_estimatorsprint(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))rs = np.array(rs)var = np.array(var)*0.01plt.figure(figsize=(20,5))plt.plot(axisx,rs,c="black",label="XGB")#添加方差线plt.plot(axisx,rs+var,c="red",linestyle='-.')plt.plot(axisx,rs-var,c="red",linestyle='-.')plt.legend()plt.show()plt.figure(figsize=(20,5))#泛化误差的可控部分plt.plot(axisx,ge,c="gray",linestyle='-.')plt.show() 从这个过程中观察n_estimators参数对模型的影响，我们可以得出以下结论： 首先，XGB中的树的数量决定了模型的学习能力，树的数量越多，模型的学习能力越强。只要XGB中树的数量足够 了，即便只有很少的数据， 模型也能够学到训练数据100%的信息，所以XGB也是天生过拟合的模型。但在这种情况 下，模型会变得非常不稳定。 第二，XGB中树的数量很少的时候，对模型的影响较大，当树的数量已经很多的时候，对模型的影响比较小，只能有 微弱的变化。当数据本身就处于过拟合的时候，再使用过多的树能达到的效果甚微，反而浪费计算资源。当唯一指标 或者准确率给出的n_estimators看起来不太可靠的时候，我们可以改造学习曲线来帮助我们。 第三，树的数量提升对模型的影响有极限，开始，模型的表现会随着XGB的树的数量一起提升，但到达某个点之 后，树的数量越多，模型的效果会逐步下降，这也说明了暴力增加n_estimators不一定有效果。这些都和随机森林中的参数n_estimators表现出一致的状态。在随机森林中我们总是先调整n_estimators，当 n_estimators的极限已达到，我们才考虑其他参数，但XGB中的状况明显更加复杂，当数据集不太寻常的时候会更加 复杂。这是我们要给出的第一个超参数，因此还是建议优先调整n_estimators，一般都不会建议一个太大的数目， 300以下为佳。 subsample我们训练模型之前，必然会有一个巨大的数据集。我们都知道树模型是天生过拟合的模型，并且如果数据量太过巨 大，树模型的计算会非常缓慢，因此，我们要对我们的原始数据集进行有放回抽样（bootstrap）。有放回的抽样每 次只能抽取一个样本，若我们需要总共N个样本，就需要抽取N次。每次抽取一个样本的过程是独立的，这一次被抽 到的样本会被放回数据集中，下一次还可能被抽到，因此抽出的数据集中，可能有一些重复的数据。在无论是装袋还是提升的集成算法中，有放回抽样都是我们防止过拟合。 在sklearn中，我们使用参数subsample来控制我们的随机抽样。在xgb和sklearn中，这个参数都默认为1且不能取到0，所以取值范围是(0,1]。这说明我们无法控制模型是否进行随机有放回抽样，只能控制抽样抽出来的样本量大概是多少。 eta or learning_rate在逻辑回归中，我们自定义步长&alpha;来干涉我们的迭代速率，在XGB中看起来却没有这样的设置，但其实不然。在XGB中，我们完整的迭代决策树的公式应该写作： 其中&eta;读作”eta”，是迭代决策树时的步长（shrinkage），又叫做学习率（learning rate）。和逻辑回归中的&alpha;类似，&eta;越大，迭代的速度越快，算法的极限很快被达到，有可能无法收敛到真正的最佳。&eta;越小，越有可能找到更精确的最佳值，更多的空间被留给了后面建立的树，但迭代速度会比较缓慢。 在sklearn中，我们使用参数learning_rate来干涉我们的学习速率： 梯度提升树是XGB的基础，本节中已经介绍了XGB中与梯度提升树的过程相关的四个参数：n_estimators，learning_rate ，silent，subsample。这四个参数的主要目的，其实并不是提升模型表现，更多是了解梯度提升树的原理。现在来看，我们的梯度提升树可是说是由三个重要的部分组成： 一个能够衡量集成算法效果的，能够被最优化的损失函数 一个能够实现预测的弱评估器 一种能够让弱评估器集成的手段，包括我们讲解的迭代方法，抽样手段，样本加权等等过程 booster梯度提升算法中不只有梯度提升树，XGB作为梯度提升算法的进化，自然也不只有树模型一种弱评估器。在XGB中，除了树模型，我们还可以选用线性模型，比如线性回归，来进行集成。虽然主流的XGB依然是树模型，但我们也可以使用其他的模型。基于XGB的这种性质，我们有参数“booster”来控制我们究竟使用怎样的弱评估器。 两个参数都默认为”gbtree”，如果不想使用树模型，则可以自行调整。当XGB使用线性模型的时候，它的许多数学过程就与使用普通的Boosting集成非常相似。 1234for booster in ["gbtree","gblinear","dart"]:reg = XGBR(n_estimators=180,learning_rate=0.1,random_state=420,booster=booster).fit(Xtrain,Ytrain) print(booster) print(reg.score(Xtest,Ytest)) objective在众多机器学习算法中，损失函数的核心是衡量模型的泛化能力，即模型在未知数据上的预测的准确与否，我们训练模型的核心目标也是希望模型能够预测准确。在XGB中，预测准确自然是非常重要的因素，但我们之前提到过，XGB的是实现了模型表现和运算速度的平衡的算法。普通的损失函数，比如错误率，均方误差等，都只能够衡量模型的表现，无法衡量模型的运算速度。回忆一下，我们曾在许多模型中使用空间复杂度和时间复杂度来衡量模型的运算效率。XGB因此引入了模型复杂度来衡量算法的运算效率。因此XGB的目标函数被写作：传统损失函数 + 模型复杂度。 其中i代表数据集中的第i个样本，m表示导入第K棵树的数据总量，K代表建立的所有树(n_estimators)。 第二项代表模型的复杂度，使用树模型的某种变换$\Omega$表示，这个变化代表了一个从树的结构来衡量树模型的复杂度的式子,可以有多种定义。我们在迭代每一颗的过程中，都最小化Obj来求最优的yi。 在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error）。一个集成模型(f)在未知数据集(D)上的泛化误差 ，由方差(var)，偏差(bais)和噪声(ε)共同决定，而泛化误差越小，模型就越理想。从下面的图可以看出来，方差和偏差是此消彼长的，并且模型的复杂度越高，方差越大，偏差越小。 方差可以被简单地解释为模型在不同数据集上表现出来地稳定性，而偏差是模型预测的准确度。那方差-偏差困境就可以对应到我们的Obj中了： 第一项是衡量我们的偏差，模型越不准确，第一项就会越大。第二项是衡量我们的方差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。所以我们求解Obj的最小值，其实是在求解方差与偏差的平衡点，以求模型的泛化误差最小，运行速度最快。 在应用中，我们使用参数“objective”来确定我们目标函数的第一部分，也就是衡量损失的部分。 xgb自身的调用方式： 1234567891011121314151617181920212223242526from xgboost import XGBRegressor as XGBRfrom sklearn.metrics import mean_squared_error as MSEfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_split as TTSdata = load_boston()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#默认reg:linearreg = XGBR(n_estimators=180,random_state=420).fit(Xtrain,Ytrain)print("sklearn中的XGboost准确率：",reg.score(Xtest,Ytest))print("sklearn中的xgb均方误差：",MSE(Ytest,reg.predict(Xtest)))#xgb实现法import xgboost as xgb#使用类Dmatrix读取数据dtrain = xgb.DMatrix(Xtrain,Ytrain)dtest = xgb.DMatrix(Xtest,Ytest)#写明参数，silent默认为False，通常需要手动将它关闭param = &#123;'silent':False,'objective':'reg:linear',"eta":0.1&#125;num_round = 180#类train，可以直接导入的参数是训练数据，树的数量，其他参数都需要通过params来导入bst = xgb.train(param, dtrain, num_round)#接口predictfrom sklearn.metrics import r2_scoreprint("xgb中的准确率：",r2_score(Ytest,bst.predict(dtest)))print("xgb中的均方误差：",MSE(Ytest,bst.predict(dtest))) 看得出来，无论是从R2还是从MSE的角度来看，都是xgb库本身表现更优秀。 (alpha or reg_alpha) &amp; (lambda or reg_lambda)对于XGB来说，每个叶子节点上会有一个预测分数（prediction score），也被称为叶子权重。这个叶子权重就是所有在这个叶子节点上的样本在这一棵树上的回归取值,用fk(xi)或者&omega;来表示。 当有多棵树的时候，集成模型的回归结果就是所有树的预测分数之和，假设这个集成模型中总共有K棵决策树，则整个模型在这个样本i上给出的预测结果为 如下图： 设一棵树上总共包含了T个叶子节点，其中每个叶子节点的索引为j，则这个叶子节点上的样本权重是wj。依据这个，我们定义模型的复杂度$\Omega$(f)为（注意这不是唯一可能的定义，我们当然还可以使用其他的定义，只要满足叶子越多/深度越大，复杂度越大的理论): 使用L2正则项： 使用L1正则项： 还可以两个一起用： 这个结构中有两部分内容，一部分是控制树结构的&gamma;，另一部分则是我们的正则项。叶子数量T可以代表整个树结构，这是因为在XGBoost中所有的树都是CART树（二叉树），所以我们可以根据叶子的数量T判断出树的深度，而&gamma;是我们自定的控制叶子数量的参数。 至于第二部分正则项，类比一下我们岭回归和Lasso的结构，参数&alpha;和&lambda;的作用其实非常容易理解，他们都是控制正则化强度的参数，我们可以二选一使用，也可以一起使用加大正则化的力度。当 和 都为0的时候，目标函数就是普通的梯度提升树的目标函数。 来看正则化系数分别对应的参数： gamma 回忆一下决策树中我们是如何进行计算：我们使用基尼系数或信息熵来衡量分枝之后叶子节点的不纯度，分枝前的信息熵与分治后的信息熵之差叫做信息增益，信息增益最大的特征上的分枝就被我们选中，当信息增益低于某个阈值时，就让树停止生长。在XGB中，我们使用的方式是类似的：我们首先使用目标函数来衡量树的结构的优劣，然后让树从深度0开始生长，每进行一次分枝，我们就计算目标函数减少了多少，当目标函数的降低低于我们设定的某个阈值时，就让树停止生长。 原理还不是很明白，先贴最后的Gain函数 从上面的Gain函数，从上面的目标函数和结构分数之差Gain的式子来看，&gamma;使我们每增加一片叶子就会被剪去的惩罚项。增加的叶子越多，结构分数之差Gain会被惩罚越重，所以&gamma;也被称为”复杂性控制“。所以&gamma;是我们用来防止过拟合的重要参数。&gamma;是对梯度提升树影响最大的参数之一，其效果不逊色与n_estimators和放过拟合神器max_depth。同时&gamma;还是我们让树停止生长的重要参数。 在XGB中，规定只要结构分数之差Gain大于0，即只要目标函数还能减小，我们就允许继续进行分枝。也就是说，我们对于目标函数减小量的要求是： 因此，我们可以直接通过设定&gamma;的大小让XGB的树停止生长。&gamma;因此被定义为，在树的叶节点上进行进一步分枝所需的最小目标函数减少量，在决策树和随机森林中也有类似的参数(min_split_loss，min_samples_split)。 设定越大，算法就越保守，树的叶子数量就越少，模型的复杂度就越低。 12345678910111213141516171819202122232425262728293031from xgboost import XGBRegressor as XGBRfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_split as TTSimport numpy as npfrom matplotlib import pyplot as pltfrom sklearn.model_selection import cross_val_score as CVSdata = load_boston()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)axisx = np.arange(0,5,0.05)rs = []var = []ge = []for i in axisx: reg = XGBR(n_estimators=180,random_state=420,gamma=i) result = CVS(reg,Xtrain,Ytrain,cv=20) rs.append(result.mean()) var.append(result.var()) ge.append((1 - result.mean())**2+result.var())print(axisx[rs.index(max(rs))],max(rs),var[rs.index(max(rs))])print(axisx[var.index(min(var))],rs[var.index(min(var))],min(var))print(axisx[ge.index(min(ge))],rs[ge.index(min(ge))],var[ge.index(min(ge))],min(ge))rs = np.array(rs)var = np.array(var)*0.1plt.figure(figsize=(20,5))plt.plot(axisx,rs,c="black",label="XGB")plt.plot(axisx,rs+var,c="red",linestyle='-.')plt.plot(axisx,rs-var,c="red",linestyle='-.')plt.legend()plt.show() 为了调整&gamma;，我们需要引入新的工具，xgboost库中的类xgboost.cv 为了使用xgboost.cv，我们必须要熟悉xgboost自带的模型评估指标。xgboost在建库的时候本着大而全的目标，和sklearn类似，包括了大约20个模型评估指标，然而用于回归和分类的其实只有几个，大部分是用于一些更加高级的功能比如ranking。来看用于回归和分类的评估指标都有哪些： 1234567891011121314151617181920from matplotlib import pyplot as pltimport xgboost as xgbfrom sklearn.datasets import load_breast_cancerdata2 = load_breast_cancer()x2 = data2.datay2 = data2.targetdfull2 = xgb.DMatrix(x2,y2)param1 = &#123;'silent':True,'obj':'binary:logistic',"gamma":0,"nfold":5&#125;param2 = &#123;'silent':True,'obj':'binary:logistic',"gamma":2,"nfold":5&#125;num_round = 100cvresult1 = xgb.cv(param1, dfull2, num_round,metrics=("error"))cvresult2 = xgb.cv(param2, dfull2, num_round,metrics=("error"))plt.figure(figsize=(20,5))plt.grid()plt.plot(range(1,101),cvresult1.iloc[:,0],c="red",label="train,gamma=0")plt.plot(range(1,101),cvresult1.iloc[:,2],c="orange",label="test,gamma=0")plt.plot(range(1,101),cvresult2.iloc[:,0],c="green",label="train,gamma=2")plt.plot(range(1,101),cvresult2.iloc[:,2],c="blue",label="test,gamma=2")plt.legend()plt.show() scale_pos_weightXGB中存在着调节样本不平衡的参数scale_pos_weight,这个参数非常类似于之前随机森林和支持向量机中我们都使用到过的class_weight参数。 其它参数XGBoost应用的核心之一就是减轻过拟合带来的影响。作为树模型，减轻过拟合的方式主要是靠对决策树剪枝来降低模型的复杂度，以求降低方差。在之前的讲解中，我们已经学习了好几个可以用来防止过拟合的参数，包括上一节提到的复杂度控制&lambda;，正则化的两个参数&lambda;和&alpha;，控制迭代速度的参数 以及管理每次迭代前进行的随机有放回抽样的参数subsample。所有的这些参数都可以用来减轻过拟合。但除此之外，我们还有几个影响重大的，专用于剪枝的参数： 使用Pickle保存和调用模型pickle是python编程中比较标准的一个保存和调用模型的库，我们可以使用pickle和open函数的连用，来讲我们的模型保存到本地。 1234567891011121314151617181920212223242526#保存模型的codingimport pickleimport xgboost as xgbfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_breast_cancerdata2 = load_breast_cancer()x2 = data2.datay2 = data2.targetXtrain,Xtest,Ytrain,Ytest=train_test_split(x2,y2,test_size=0.3,random_state=420)dtrain = xgb.DMatrix(Xtrain,Ytrain)#设定参数，对模型进行训练param = &#123;'silent':True,'obj':'reg:linear',"subsample":1,"eta":0.05,"gamma":20,"lambda":3.5,"alpha":0.2,"max_depth":4,"colsample_bytree":0.4,"colsample_bylevel":0.6,"colsample_bynode":1&#125;num_round = 180bst = xgb.train(param, dtrain, num_round)#保存模型pickle.dump(bst, open("xgboostonboston.dat","wb")) 1234567891011121314151617181920#调用模型的codingfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split as TTSfrom sklearn.metrics import mean_squared_error as MSEimport pickleimport xgboost as xgbdata = load_breast_cancer()X = data.datay = data.targetXtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#注意，如果我们保存的模型是xgboost库中建立的模型，则导入的数据类型也必须是xgboost库中的数据类型dtest = xgb.DMatrix(Xtest,Ytest)#导入模型loaded_model = pickle.load(open("xgboostonboston.dat", "rb"))print("Loaded model from: xgboostonboston.dat")#做预测ypreds = loaded_model.predict(dtest)from sklearn.metrics import mean_squared_error as MSE, r2_scoreprint("均方误差：",MSE(Ytest,ypreds))print(r2_score(Ytest,ypreds)) 使用Joblib保存和调用模型Joblib是SciPy生态系统中的一部分，它为Python提供保存和调用管道和对象的功能，处理NumPy结构的数据尤其高效，对于很大的数据集和巨大的模型非常有用。Joblib与pickle API非常相似 123456789from xgboost import XGBRegressor as XGBRbst = XGBR(n_estimators=200,eta=0.05,gamma=20,reg_lambda=3.5,reg_alpha=0.2,max_depth=4,colsample_bytree=0.4,colsample_bylevel=0.6).fit(Xtrain,Ytrain)#保存模型joblib.dump(bst,"xgboost-boston.dat")#调用模型loaded_model = joblib.load("xgboost-boston.dat")#这里可以直接导入Xtestypreds = loaded_model.predict(Xtest)print（MSE(Ytest, ypreds)）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn中的神经网络]]></title>
    <url>%2F2019%2F09%2F12%2Fsklearn%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[神经网络概述人工神经网络(Artificial Neural Network，简称ANN)，通常简称为神经网络，是深度学习的基础。神经网络算法试图模拟生物神经系统的学习过程，以此实现强大的预测性能。不过由于是模仿人类大脑，所以神经网络的模型复杂度很高也是众所周知。在现实应用中，神经网络可以说是解释性最差的模型之一 。 神经网络原理最开始是基于感知机提出——感知机是最古老的机器学习分类算法之一 。和今天的大部分模型比较起来，感知机的泛化能力比较弱。但支持向量机和神经网络都基于它的原理来建立。感知机的原理在支持向量机中介绍过，使用一条线性决策边界z=&omega;*x+b来划分数据集，决策边界的上方是一类数据(z&gt;=0)，决策边界的下方是另一类数据(z&lt;0)的决策过程。使用神经元表示，如下图： 不同的特征数据被输入后，我们通过神经键将它输入我们的神经元。每条神经键上对应不同的参数&omega;，b。因此特征数据会经由神经键被匹配到一个参数向量&omega;，b。基于参数向量&omega;算法可以求解出决策边界z=&omega;*x+b，然后用决策函数sign(z)进行判断，最终预测出标签y并且将结果输出，其中函数sign(z)被称为”激活函数“。这是模拟人类的大脑激活神经元的过程所命名的，其实本质就是决定了预测标签的输出会是什么内容的预测函数。 注意：在这个过程中，有三个非常重要的点： ​ 1、每个输入的特征都会匹配到一个参数&omega;，我们都知道参数向量&omega;中含有的参数数量与我们的特征数目是一致的，在感知机中也是如此。也就是说，任何基于感知机的算法，必须至少要有参数向量&omega;可求。 ​ 2、一个线性关系z，z是由参数和输入的数据共同决定的。这个线性关系，往往就是我们的决策边界，或者它也可以是多元线性回归，逻辑回归等算法的线性表达式 ​ 3、激活函数的结果，是基于激活函数的本身，参数向量&omega;和输入的数据一同计算出来的。也就是说，任何基于感知机的算法。必须要存在一个激活函数。 神经网络就相当于众多感知机的集成，因此，确定激活函数，并找出参数向量&omega;也是神经网络的计算核心。我们来看看神经网络的基本结构： 首先，神经网络有三层。第一层叫做输入层（Input layer），输入特征矩阵用，因此每个神经元上都是一个特征向量。极端情况下，如果一个神经网络只训练一个样本，则每个输入层的神经元上都是这个样本的一个特征取值。 最后一层叫做输出层（output layer），输出预测标签用。如果是回归类，一般输出层只有一个神经元，回归的是所有输入的样本的标签向量。如果是分类，可能会有多个神经元。二分类有两个神经元，多分类有多个神经元，分别输出所有输入的样本对应的每个标签分类下的概率。但无论如何，输出层只有一层，是用于输出预测结果用。 输入层和输出层中间的所有层，叫做隐藏层（Hidden layers），最少一层。也就是说整个神经网络是最少三层。隐藏层是我们用来让算法学习的网络层级，从更靠近输入层的地方开始叫做”上层”，相对而言，更靠近输出层的一层，叫做”下层”。在隐藏层上，每个神经元中都存在一个激活函数，我们的数据被逐层传递，每个下层的神经元都必须处理上层的神经元中的激活函数处理完毕的数据，本质是一个感知器嵌套的过程。隐藏层中上层的每个神经元，都与下层中的每个神经元相连，因此隐藏层的结构随着神经元的变多可以变得非常非常复杂。神经网络的两个要点：参数&omega;和激活函数，也都在这一层发挥作用，因此理解隐藏层是神经网络原理中最难的部分。 神经网络的每一层的结果之间的关系是嵌套，不是迭代。由于我们执行的是嵌套，不是迭代。所以我们的每一个系数之间是相互独立的，每一层的系数之间也是相互独立的，我们不是在执行使用上一层或者上一个神经元的参数来求解下一层或者下一个神经元的参数的过程。我们不断求解，是激活函数的结果a，不是参数&omega;。在一次神经网络计算中，我们没有迭代参数&omega; 上面这张图还不算真实数据中特别复杂的情况，但已经含有总共8*9*9*9*4=23328个&omega;。神经网络可能是我们遇到的最难调参的算法。接下来看看sklearn中的神经网络。 sklearn中的神经网络sklearn是专注于机器学习的库，它在神经网络的模块中特地标注：sklearn不是用于深度学习的平台，因此这个神经网络不具备做深度学习的功能，也不具备处理大型数据的能力。 neural_network.MLPClasifier 重要参数hidden_layer_sizes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import numpy as npfrom sklearn.neural_network import MLPClassifier as DNNfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import cross_val_score as cvimport matplotlib.pyplot as pltfrom sklearn.datasets import load_breast_cancerfrom sklearn.tree import DecisionTreeClassifier as DTCfrom sklearn.model_selection import train_test_split as TTSfrom time import timeimport datetimedata = load_breast_cancer()X = data.datay = data.targetXtrain, Xtest, Ytrain, Ytest = TTS(X,y,test_size=0.3,random_state=420)times = time()dnn = DNN(hidden_layer_sizes=(100,),random_state=420)print("dnn的交叉验证：",cv(dnn,X,y,cv=5).mean())print("dnn所用的时间：",time() - times)#使用决策树进行一个对比times = time()clf = DTC(random_state=420)print("决策树的交叉验证：",cv(clf,X,y,cv=5).mean())print("决策树所用的时间：",time() - times)dnn = DNN(hidden_layer_sizes=(100,),random_state=420).fit(Xtrain,Ytrain)print("准确率为：",dnn.score(Xtest,Ytest))#使用重要参数n_layers_，得出层数print("n_layers_：",dnn.n_layers_)#可见，默认层数是三层，由于必须要有输入和输出层，所以默认其实就只有一层隐藏层#增加一个隐藏层上的神经元个数dnn = DNN(hidden_layer_sizes=(200,),random_state=420)dnn = dnn.fit(Xtrain,Ytrain)print("增加一个隐藏层的准确率是：",dnn.score(Xtest,Ytest))s = []for i in range(100,2000,100): dnn = DNN(hidden_layer_sizes=(int(i),),random_state=420).fit(Xtrain,Ytrain) s.append(dnn.score(Xtest,Ytest)) print(i,max(s))plt.figure(figsize=(20,5))plt.plot(range(200,2100,100),s)plt.show()#增加隐藏层，控制神经元个数s = []layers = [(100,),(100,100),(100,100,100),(100,100,100,100),(100,100,100,100,100),(100,100,100,100,100,100)]for i in layers: dnn = DNN(hidden_layer_sizes=(i),random_state=420).fit(Xtrain,Ytrain) s.append(dnn.score(Xtest,Ytest)) print(i,max(s))plt.figure(figsize=(20,5))plt.plot(range(3,9),s)plt.xticks([3,4,5,6,7,8])plt.xlabel("Total number of layers")plt.show()#增加隐藏层，控制神经元个数s = []layers = [(100,),(150,150),(200,200,200),(300,300,300,300)]for i in layers: dnn = DNN(hidden_layer_sizes=(i),random_state=420).fit(Xtrain,Ytrain) s.append(dnn.score(Xtest,Ytest)) print(i,max(s))plt.figure(figsize=(20,5))plt.plot(range(3,7),s)plt.xticks([3,4,5,6])plt.xlabel("Total number of layers")plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维算法]]></title>
    <url>%2F2019%2F09%2F12%2F%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更快，效果更好。对图像来说，维度就是图像中特征向量的数量。 sklearn中的降维算法sklearn中降维算法都被包括在模块decomposition中 PCA在降维过程中，我们会减少特征的数量，这意味着删除数据，数据量变少则表示模型可以获取的信息会变少，模型的表现可能会因此受影响。同时，在高维数据中，必然有一些特征是不带有有效的信息的（比如噪音），或者有一些特征带有的信息和其他一些特征是重复的（比如一些特征可能会线性相关）。我们希望能够找出一种办法来帮助我们衡量特征上所带的信息量，让我们在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息的，特征更少的，新特征矩阵。 在特征过程中，我们有说过一种特别的特征选择方法：方差过滤。如果一个特征的方差很小，则意味着这个特征上很可能有大量取值都相同（比如90%都是1，只有10%是0，甚至100%是1），那这一个特征的取值对样本而言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说明这个特征上带有大量的信息。因此，在降维中，PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多。 Var代表一个特征的方差，n代表样本量，xi代表一个特征中的每个样本取值，xhat代表这一列样本的均值。 PCA和特征选择技术都是特征工程的一部分，他们有什么不同？ 特征工程中有三种方式：特征提取，特征创造和特征选择。仔细观察上面的降维例子和上周我们讲解过的特征选择，你发现有什么不同了吗?特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。而PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。以PCA为代表的降维算法因此是特征创造（feature creation，或feature construction）的一种。 重要参数n_compontents：n_components是我们降维后需要的维度，即降维后需要保留的特征数量。 1234567891011121314151617181920212223242526import matplotlib.pyplot as pltfrom sklearn.datasets import load_irisfrom sklearn.decomposition import PCAiris = load_iris()y = iris.targetX = iris.datapca = PCA(n_components=2) #实例化#也可以一步到位，x_dr=pca.fit_transform(X)pca = pca.fit(X) #拟合模型X_dr = pca.transform(X) #获取新矩阵"""可以写三行代码，也可以写成for循环plt.figure()plt.scatter(X_dr[y==0, 0], X_dr[y==0, 1], c="red", label=iris.target_names[0])plt.scatter(X_dr[y==1, 0], X_dr[y==1, 1], c="black", label=iris.target_names[1])plt.scatter(X_dr[y==2, 0], X_dr[y==2, 1], c="orange", label=iris.target_names[2])plt.legend()plt.title('PCA of IRIS dataset')plt.show()"""colors = ['red', 'black', 'orange']plt.figure()for i in [0, 1, 2]: plt.scatter(X_dr[y == i, 0],X_dr[y == i, 1],alpha=.7,c=colors[i],label=iris.target_names[i])plt.legend()plt.title('PCA of IRIS dataset')plt.show() 选择最好的n_components累积可解释方差贡献率曲线当参数n_components中不填写任何值，则默认返回min(X.shape)个特征，一般来说，样本量都会大于特征数目，所以什么都不填就相当于转换了新特征空间，但没有减少特征的个数。一般来说，不会使用这种输入方式。但我们却可以使用这种输入方式来画出累计可解释方差贡献率曲线，以此选择最好的n_components的整数取值。累积可解释方差贡献率曲线是一条以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标的曲线，能够帮助我们决定n_components最好的取值。 12345678910#接上面的代码import numpy as nppca_line = PCA().fit(X)#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比#又叫做可解释方差贡献率plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))plt.xticks([1,2,3,4]) #这是为了限制坐标轴显示为整数plt.xlabel("number of components after dimension reduction")plt.ylabel("cumulative explained variance ratio")plt.show() 最大似然估计自选超参数除了输入整数，n_components还有哪些选择呢？让PCA用最大似然估计(maximum likelihood estimation)自选超参数的方法，输入“mle”作为n_components的参数输入，就可以调用这种方法。 12345pca_mle = PCA(n_components="mle")pca_mle = pca_mle.fit(X)X_mle = pca_mle.transform(X)#从这里看出，mle自动为我们选了几个特征print(X_mle.shape) 按信息占比选超参数输入[0,1]之间的浮点数，并且让参数svd_solver ==’full’，表示希望降维后的总解释性方差占比大于n_components指定的百分比，即是说，希望保留百分之多少的信息量。 12345pca_f = PCA(n_components=0.97,svd_solver="full")pca_f = pca_f.fit(X)X_f = pca_f.transform(X)#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比print(pca_f.explained_variance_ratio_) PCA中的SVD其实上面的svd_solver是奇异值分解器。其实SVD可以跳过数学，不计算协方差矩阵，直接找出一个新特征向量组成的n维向量。也就是说，奇异值分解可以不计算协方差矩阵等等计算冗长的矩阵，就直接求出新特征空间和降维后的特征矩阵。简而言之，SVD在矩阵分解中的过程比PCA简单快速。 重要参数svd_solver参数svd_solver是在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选：”auto”, “full”, “arpack”,”randomized”，默认”auto”。 “auto”：基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80％，就启用效率更高的”randomized“方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生 “full”：从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时间充足的情况。 “arpack”：从scipy.sparse.linalg.svds调用ARPACK分解器来运行截断奇异值分解(SVD truncated)，分解时就将特征数量降到n_components中输入的数值k，可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况，此过程包含一定的随机性。 “randomized”，通过Halko等人的随机方法进行随机SVD。在”full”方法中，分解器会根据原始数据和输入的n_components值去计算和寻找符合需求的新特征向量，但是在”randomized”方法中，分解器会先生成多个随机向量，然后一一去检测这些随机向量中是否有任何一个符合我们的分解需求，如果符合，就保留这个随机向量，并基于这个随机向量来构建后续的向量空间。这个方法已经被Halko等人证明，比”full”模式下计算快很多，并且还能够保证模型运行效果。适合特征矩阵巨大，计算量庞大的情况。 random_state参数random_state在参数svd_solver的值为”arpack” or “randomized”的时候生效，可以控制这两种SVD模式中的随机模式。通常我们就选用”auto“，不必对这个参数纠结太多。 PCA参数、属性和接口参数列表 属性列表 接口列表 PCA对手写数字数据集的降维123456789101112131415from sklearn.decomposition import PCAfrom sklearn.ensemble import RandomForestClassifier as RFCfrom sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata = pd.read_csv(r"digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]pca_line = PCA().fit(X)plt.figure(figsize=[20,5])plt.plot(np.cumsum(pca_line.explained_variance_ratio_))plt.xlabel("number of components after dimension reduction")plt.ylabel("cumulative explained variance ratio")plt.show() 接着降维后维度的学习曲线，继续缩小最佳维度的范围 123456789score = []for i in range(1,101,10): X_dr = PCA(i).fit_transform(X) once = cross_val_score(RFC(n_estimators=10,random_state=0) ,X_dr,y,cv=5).mean() score.append(once)plt.figure(figsize=[20,5])plt.plot(range(1,101,10),score)plt.show() 细化学习曲线，找出降维后的最佳维度 12345678score = []for i in range(10,25): X_dr = PCA(i).fit_transform(X) once = cross_val_score(RFC(n_estimators=10,random_state=0),X_dr,y,cv=5).mean() score.append(once)plt.figure(figsize=[20,5])plt.plot(range(10,25),score)plt.show() 导入找出的最佳维度进行降维，查看模型效果 12x_dr=PCA(23).fit_transform(X)print(cross_val_score(RFC(n_estimators=100,random_state=0),x_dr,y,cv=5).mean())]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林]]></title>
    <url>%2F2019%2F09%2F08%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[集成算法概述集成学习是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型建模结果。基本上所有的机器学习领域都可以看到集成学习的身影。 集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类表现。 多个模型集成称为的模型叫做集成评估器(ensemble estimator)，组成集成评估器的每个模型都叫做基评估器(base estimator)。通常来说，有三类集成算法：装袋法(Bagging)、提升法(Boosting)、stacking 装袋法的核心思想是构建多个相互独立的评估器，然后对其预测进行平均或多数表决原则来决定集成评估器的结果。装袋法的代表模型就是随机森林。提升法中，基评估器是相关的，是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本进行预测，从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树 sklearn中的集成算法sklearn集成算法模块ensemble 分类树参数、属性和接口 参数n_estimators这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，n_estimators越大，模型的效果往往越好。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。 random_state随机森林的本质是一种装袋集成算法(bagging)，装袋集成算法是对基评估器的预测结果进行平均或用多数表决元则来决定集成评估器的结果。决策树从最重要的特征中随机选择出一个特征来进行分枝，因此每次生成的决策树都不一样，这个功能由参数random_state控制。随机森林中其实也有random_state，用法和分类树中相似，只不过在分类树中，一个random_state只控制生成一棵树，而随机森林中的random_state控制的是生成森林的模式，而非让一个森林中只有一棵树。 bootstrap&amp;oob_score要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。bootstrap参数默认True，代表采用这种有放回的随机抽样技术。通常，这个参数不会被我们设置为False 如果希望用袋外数据来测试，则需要在实例化时就将oob_score这个参数调整为True，训练完毕之后，我们可以用随机森林的另一个重要属性：oob_score_来查看我们的在袋外数据上测试的结果。 参数的详细解释和其它控制基评估器的参数请参考决策树 属性随机森林中有三个非常重要的属性：.estimators_，.oob_score_以及.feature_importances_。.estimators_是用来查看随机森林中所有树的列表的。.oob_score_指的是袋外得分。随机森林为了确保林中的每棵树都不尽相同，所以采用了对训练集进行有放回抽样的方式来不断组成信的训练集，在这个过程中，会有一些数据从来没有被随机挑选到，他们就被叫做“袋外数据”。这些袋外数据，没有被模型用来进行训练，sklearn可以帮助我们用他们来测试模型，测试的结果就由这个属性oob_score_来导出，本质还是模型的精确度。而.feature_importances_和决策树中的.feature_importances_用法和含义都一致，是返回特征的重要性 接口随机森林的接口与决策树完全一致，因此依然有四个常用接口：apply, fit, predict和score。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。 回归树 所有的参数，属性与接口，全部和随机森林分类器一致。仅有的不同就是回归树与分类树的不同，不纯度的指标，参数Criterion不一致。 criterion：回归树衡量分枝质量的指标，支持的标准有三种：1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。 2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ： 其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 随机森林coding：1234567891011121314151617181920212223242526272829303132333435363738from sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import load_winewine = load_wine()from sklearn.model_selection import train_test_splitXtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)clf = DecisionTreeClassifier(random_state=0)rfc = RandomForestClassifier(random_state=0)clf = clf.fit(Xtrain,Ytrain)rfc = rfc.fit(Xtrain,Ytrain)score_c = clf.score(Xtest,Ytest)score_r = rfc.score(Xtest,Ytest)print("Single Tree:&#123;&#125;".format(score_c),"Random Forest:&#123;&#125;".format(score_r))#画出随机森林和决策树一组交叉验证的对比from sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as pltrfc = RandomForestClassifier(n_estimators=25)rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10)clf = DecisionTreeClassifier()clf_s = cross_val_score(clf,wine.data,wine.target,cv=10)plt.plot(range(1,11),rfc_s,label = "RandomForest")plt.plot(range(1,11),clf_s,label = "Decision Tree")plt.legend()plt.show()#画出随机森林和决策树十组交叉验证的对比rfc_l = []clf_l = []for i in range(10): rfc = RandomForestClassifier(n_estimators=25) rfc_s = cross_val_score(rfc,wine.data,wine.target,cv=10).mean() rfc_l.append(rfc_s) clf = DecisionTreeClassifier() clf_s = cross_val_score(clf,wine.data,wine.target,cv=10).mean() clf_l.append(clf_s)plt.plot(range(1,11),rfc_l,label = "Random Forest")plt.plot(range(1,11),clf_l,label = "Decision Tree")plt.legend()plt.show() 机器学习中调参的基本思想泛化误差在机器学习中，我们用来衡量模型在未知数据上的准确率的指标，叫做泛化误差（Genelization error） 当模型在未知数据（测试集或者袋外数据）上表现糟糕时，我们说模型的泛化程度不够，泛化误差大，模型的效果不好。泛化误差受到模型的结构（复杂度）影响。看下面这张图，它准确地描绘了泛化误差与模型复杂度的关系，当模型太复杂，模型就会过拟合，泛化能力就不够，所以泛化误差大。当模型太简单，模型就会欠拟合，拟合能力就不够，所以误差也会大。只有当模型的复杂度刚刚好的才能够达到泛化误差最小的目标。 1）模型太复杂或者太简单，都会让泛化误差高，我们追求的是位于中间的平衡点2）模型太复杂就会过拟合，模型太简单就会欠拟合3）对树模型和树的集成模型来说，树的深度越深，枝叶越多，模型越复杂4）树模型和树的集成模型的目标，都是减少模型复杂度，把模型往图像的左边移动 偏差和方差观察下面的图像，每个点就是集成算法中的一个基评估器产生的预测值。红色虚线代表着这些预测值的均值，而蓝色的线代表着数据本来的面貌。偏差：模型的预测值与真实值之间的差异，即每一个红点到蓝线的距离。在集成算法中，每个基评估器都会有自己的偏差，集成评估器的偏差是所有基评估器偏差的均值。模型越精确，偏差越低。方差：反映的是模型每一次输出结果与模型预测值的平均水平之间的误差，即每一个红点到红色虚线的距离，衡量模型的稳定性。模型越稳定，方差越低。 方差和偏差对模型的影响： 然而，方差和偏差是此消彼长的，不可能同时达到最小值 从图上可以看出，模型复杂度大的时候，方差高，偏差低。偏差低，就是要求模型要预测得“准”。模型就会更努力去学习更多信息，会具体于训练数据，这会导致，模型在一部分数据上表现很好，在另一部分数据上表现却很糟糕。模型泛化性差，在不同数据上表现不稳定，所以方差就大。而要尽量学习训练集，模型的建立必然更多细节，复杂程度必然上升。所以，复杂度高，方差高，总泛化误差高。相对的，复杂度低的时候，方差低，偏差高。方差低，要求模型预测得“稳”，泛化性更强，那对于模型来说，它就不需要对数据进行一个太深的学习，只需要建立一个比较简单，判定比较宽泛的模型就可以了。结果就是，模型无法在某一类或者某一组数据上达成很高的准确度，所以偏差就会大。所以，复杂度低，偏差高，总泛化误差高。 我们调参的目标是，达到方差和偏差的完美平衡 ！ 随机森林的调参12345678910111213141516171819from sklearn.datasets import load_breast_cancerfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCV,cross_val_scoreimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata=load_breast_cancer()rfc=RandomForestClassifier(n_estimators=100,random_state=90)score_pre=cross_val_score(rfc,data.data,data.target,cv=10).mean()scorel = []for i in range(0,200,10): rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1,random_state=90) score = cross_val_score(rfc,data.data,data.target,cv=10).mean() scorel.append(score)plt.figure(figsize=[20,5])plt.plot(range(1,201,10),scorel)plt.show() 调n_estimators1234567891011121314#从曲线看，n_estimators较平稳且准确率高的范围在35-45之间scorel = []#在划分好的范围内，继续细化学习曲线for i in range(35,45): rfc = RandomForestClassifier(n_estimators=i, n_jobs=-1, random_state=90) score = cross_val_score(rfc,data.data,data.target,cv=10).mean() scorel.append(score)#得出最高准确率的n_estimators，为39print(max(scorel),([*range(35,45)][scorel.index(max(scorel))]))plt.figure(figsize=[20,5])plt.plot(range(35,45),scorel)plt.show() 调整max_depth12345678rfc = RandomForestClassifier(n_estimators=39,random_state=90)param_grid=&#123;"max_depth":np.arange(1,20,1)&#125;GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)#得出最好的深度print("参数为：",GS.best_params_)print(GS.best_score_) 调整max_feature1234567rfc=RandomForestClassifier(n_estimators=39,max_depth=11,random_state=90)param_grid=&#123;"max_features":np.arange(1,20,1)&#125;GS=GridSearchCV(rfc,param_grid=param_grid,cv=10)GS.fit(data.data,data.target)#得出最好的max_featureprint("参数为：",GS.best_params_)print(GS.best_score_) 注意：在这步的max_features升高之后，模型的准确率却没有变化。说明模型本身已经处于泛化误差最低点，已经达到了模型的预测上限，没有参数可以左右的部分了。剩下的那些误差，是噪声决定的，已经没有方差和偏差的舞台了。如果是现实案例，我们到这一步其实就可以停下了，因为复杂度和泛化误差的关系已经告诉我们，模型不能再进步了。调参和训练模型都需要很长的时间，明知道模型不能进步了还继续调整，不是一个有效率的做法。如果我们希望模型更进一步，我们会选择更换算法，或者更换做数据预处理的方式 。但我让我们的探究继续。ps：我不要你觉得，我要我觉得 1234567param_grid=&#123;"min_samples_leaf":np.arange(1,10,1)&#125;rfc = RandomForestClassifier(n_estimators=39,max_depth=11,random_state=90)GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)print(GS.best_params_)print(GS.best_score_) 这步后的准确率还是没有变化，不要改变参数，让它默认就好 调整min_samples_split12345678param_grid=&#123;'min_samples_split':np.arange(2, 2+20, 1)&#125;rfc = RandomForestClassifier(n_estimators=39,random_state=90,max_depth=11)GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)print(GS.best_params_)print(GS.best_score_) 还是没有变化 调整criterion1234567param_grid = &#123;'criterion':['gini', 'entropy']&#125;rfc = RandomForestClassifier(n_estimators=39,random_state=90,max_depth=11)GS = GridSearchCV(rfc,param_grid,cv=10)GS.fit(data.data,data.target)print(GS.best_params_)print(GS.best_score_) 在整个调参过程之中，我们首先调整了n_estimators（无论如何都请先走这一步），然后调整max_depth，通max_depth产生的结果，来判断模型位于复杂度-泛化误差图像的哪一边，从而选择我们应该调整的参数和调参的方向。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库的数据模型]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[建立数据库系统离不开数据模型，本文介绍了常用的数据模型。重点介绍了关系数据模型的数据结构、数据操作、数据约束。 由于计算机不可能直接处理现实中的具体事物，所以人们必须事先把具体事物转换成计算机能够处理的数据。在数据库中用数据模型这个工具来抽象、表示和处理现实世界中的数据和信息。通俗的说，数据模型就是对现实世界的模拟、描述、表示。数据模型应该满足以下三个要求： 1、比较真实地描述现实世界 2、易为用户所理解 3、易于在计算机上实现 为什么需要数据模型？ 由于数据的定义与操作从应用程序中剥离出来，交由DBMS来统一管理。于是DBMS需要采用某种数据结构来定义、存储所要管理的数据。这种狭义的数据结构类似于DBMS的数据模型。 数据模型含有哪些内容？ 数据模型含有数据的静态结构、数据的动态操作(增删改查)、数据的完整性约束(描述数据、数据之间的联系、数据语义及完整性限制)。 ​ 1、数据结构 ​ 用于描述系统的静态特性，数据结构不仅要描述数据本身，还要描述数据之间的联系。数据结构是刻画一个数据模型性质最重要方面 ​ 2、数据操作 ​ 用于描述系统的动态特性。包括操作及有关的操作规则。数据库的主要操作：增删改查 ​ 3、数据的约束条件 ​ 是一组完整性规则的集合。完整性规则是数据模型中的数据及其联系所具有的约束规则，用来限定数据库状态以及状态的变化， 以保证数据的正确。 实体联系数据模型的地位与作用： 实体联系模型(Entity Relationship Model，ERM)是用得最多且最成熟的概念数据模型。 数据模型是用来描述数据的一组概念和定义，是描述数据的手段。 ​ 概念数据模型：面向用户、面向现实世界。它是按用户的观点对数据和信息建模。与DBMS无关，如E-R模型。 ​ 逻辑模型：用户从数据库管理系统看到的数据模型。与DBMS有关，如层次数据模型、网状数据模型、关系数据模型。它是按计算机系统的观点对数据建模 ​ 物理模型：是对数据最底层的抽象，描述数据在系统内部的表述方式和存取方法。，在磁盘或磁带上的存储方式和存取方法。 逻辑模型和物理模型都属于数据模型。概念数据模型只用于数据库设计，数据模型用于DBMS的实现。 数据模式是对数据结构、联系和约束的描述。数据模型是描述数据的手段，而数据模式是用给定的数据模型对数据的具体描述。 信息世界中的基本概念： ​ (1) 实体(Entity)：客观存在并可相互区别的事物称为实体。可以是具体的人、事、物体或抽象的概念。 ​ (2) 属性(Attribute)：实体所具有的某一个特性称为属性。一个实体可以由若干个属性来刻画 ​ (3) 键(Key)或称为码：唯一标识实体的属性集称为码 ​ (4) 域(Domain)：属性的取值范围称为该属性的域 ​ (5) 实体型(Entity Type)：用实体名及其属性名集合来抽象和刻画同类实体称为实体型 ​ (6) 实体集(Entity Set)：同一类型实体的集合称为实体集 ​ (7) 联系(Relationship)：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体内部的联系和实体之间的联系。实体内部的联系通常是指组成实体的各属性之间的联系实体之间的联系通常是指不同实体集之间的联系。 概念模型的表示方法：实体-联系方法，该方法用E-R图来描述现实世界的概念模型。 ​ 实体型：用矩形表示，矩形框内写明实体名 ​ 属性：用椭圆表示，并用无向边将其与相应的实体连接起来。 ​ 联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n） ​ 键：用下划线表示。 最常用的数据模型 ​ 非关系模型 ​ 层次模型：层次数据模型用树形结构表示各类实体以及实体之间的联系。现实世界中许多实体之间的联系就呈现出一种很自然的层次关系。如：行政机构、家庭关系等。层次模型是以记录为结点的有向树。 ​ 满足以下两个条件称为层次模型： ​ (1) 有且仅有一个结点无双亲。这个结点称为“根节点” ​ (2) 其它节点有且仅有一个双亲，但可以有多个后继 ​ 网状模型：用层次模型表示非树形结构很不直接，网状模型可克服这一弊病。网状模型比层次模型更具普遍性。它去掉了层次模型的两个限制，允许结点有多个双亲结点。比层次模型更能直接地描述现实世界。 ​ 网状结构特点： ​ (1) 允许一个以上的结点无双亲； ​ (2) 一个结点可以有多于一个的双亲，也可以有多个后继。两个结点间可有两种或多种联系(复合联系)。可能有回路存在。 ​ 关系数据模型：简称关系模型，关系模型中的所谓“关系”是有特定含义的。广义地说，任何数据模型都描述一定事物数据之间的关系。层次模型描述数据之间的从属层次关系，网状模型描述数据之间的多种从属的网状关系。 ​ 关系数据模型的数据结构： ​ 关系(Relation)：一个关系对应通常说的一张表 ​ 元祖(Tuple)：表中的一行即为一个元祖 ​ 属性(Attribute)：表中的一列即为一个属性。给每一个属性起一个名称即属性名。 ​ 主码(Key)：表中的某个属性组，它可以唯一确定一个元祖。 ​ 域(Domain)：属性的取值范围 ​ 分量：元祖中的一个属性值 ​ 关系模式：对关系的一个属性值。关系名(属性1，属性2，……，属性n) 关系数据模型的操作主要包括：查询、插入、删除、更新 关系的完整性约束条件：实体完整性、参照完整性、用户定义完整性]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的评判和调优]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E5%88%A4%E5%92%8C%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[精确率和召回率 分类模型评估API：F1-score,反应了模型的稳健性 模型选择和调优交叉验证交叉验证为了让被评估的模型更加准确可信 网格搜索网格搜索是用来调参数的。通常情况下，有很多参数是需要手动指定的（如k-近邻算法的k）但是手动过于繁杂。所以需要对模型预设几种超参数组合，每组超参数都采用交叉验证来进行评估。 sklearn.model_Selection.GridSearchCV]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means]]></title>
    <url>%2F2019%2F09%2F07%2FK-means%2F</url>
    <content type="text"><![CDATA[无监督学习与聚类算法有监督学习的模型在训练的时候，即需要特征矩阵X，也需要真实标签y。机器学习当中，还有相当一部分算法属于“无监督学习”，无监督的算法在训练的时候只需要特征矩阵X，不需要标签。而聚类算法，就是无监督学习的代表算法。 聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。 聚类算法在sklearn中有两种表现形式，一种是类（和我们目前为止学过的分类算法以及数据预处理方法们都一样），需要实例化，训练并使用接口和属性来调用结果。另一种是函数（function），只需要输入特征矩阵和超参数，即可返回聚类的结果和各种指标。 KMeans是如何工作的关键概念：簇与质心 ​ 簇：KMeans算法将一组N个样本的特征矩阵x划分为k个无交集的簇，直观上来看是簇是一组组聚集在一起的数据，在一个簇中的数据被认为是同一类。簇就是聚类的结果表现。 ​ 质心：簇中所有数据的均值通常被称为这个簇的质心。在一个二维平面中，一簇数据点的质心的横坐标就是这一簇数据点的横坐标的均值，质心的纵坐标就是这一簇数据点纵坐标的均值。同理可推广到高维空间。 在KMeans算法中，簇的个数K是一个超参数，需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好的K，找出K个最优的质心，并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如下 ： ​ 1、随机抽取k个样本作为最初的质心 ​ 2、开始循环 ​ 2.1、将每个样本点分配到离他们最近的质心，生成k个簇 ​ 2.2、对于每个簇，计算所有被分到该簇的样本点的平均值作为新的质心 ​ 3、当质心的位置不在发生变化，迭代停止，聚类完成 那什么情况下，质心的位置会不再变化呢？当我们找到一个质心，在每次迭代中被分配到这个质心上的样本都是一致的，即每次新生成的簇都是一致的，所有的样本点都不会再从一个簇转移到另一个簇，质心就不会变化了。 对于一个簇来说，所有样本点到质心的距离之和越小，我们就认为这个簇中的样本越相似，簇内差异就越小。而距离的衡量方法有多种，令x表示簇中的一个样本点，ui表示该簇中的质心，n表示每个样本点中的特征数目，i表示组成点x的每个特征，则该样本点到质心的距离可以由以下距离来度量： 如我们采用欧几里得距离，则一个簇中所有样本点到质心的距离的平方和为: 其中，m为一个簇中样本的个数，j是每个样本的编号。这个公式被称为簇内平方和（cluster Sum of Square），又叫做Inertia。而将一个数据集中的所有簇的簇内平方和相加，就得到了整体平方和（Total Cluster Sum ofSquare），又叫做total inertia。Total Inertia越小，代表着每个簇内样本越相似，聚类的效果就越好。因此,KMeans追求的是，求解能够让Inertia最小化的质心。实际上，在质心不断变化不断迭代的过程中，总体平方和是越来越小的。我们可以使用数学来证明，当整体平方和最小的时候，质心就不再发生变化了。如此，K-Means的求解过程，就变成了一个最优化问题 损失函数本质是用来衡量模型的拟合效果的，只有有着求解参数需求的算法，才会有损失函数。Kmeans不求解什么参数，它的模型本质也没有在拟合数据，而是在对数据进行一种探索。所以，K-Means不存在什么损失函数。Inertia更像是Kmeans的模型评估指标，而非损失函数。 重要参数和属性 参数n_clusters是KMeans中的k，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认为8类，但通常我们的聚类结果会是一个小于8的结果。通常，在开始聚类之前，我们并不知道n_clusters究竟是多少，因此我们要对它进行探索。 在之前描述K-Means的基本流程时我们提到过，当质心不再移动，Kmeans算法就会停下来。但在完全收敛之前， 我们也可以使用max_iter，大迭代次数，或者tol，两次迭代间Inertia下降的量，这两个参数来让迭代提前停下 来。有时候，当我们的n_clusters选择不符合数据的自然分布，或者我们为了业务需求，必须要填入与数据的自然 分布不合的n_clusters，提前让迭代停下来反而能够提升模型的表现。 max_iter：整数，默认300，单次运行的k-means算法的大迭代次数tol：浮点数，默认1e-4，两次迭代间Inertia下降的量，如果两次迭代之间Inertia下降的值小于tol所设定的值，迭代就会停下 属性labels_：查看聚好的类别，每个样本所对应的类 cluster_centers_：查看质心 inertia_：查看总距离平方和 n_iter_：实际的迭代次数 coding： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.datasets import make_blobsimport matplotlib.pyplot as plt#这是自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数#random_state代表随机性X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)fig, ax1 = plt.subplots(1)ax1.scatter(X[:, 0], X[:, 1]#画点图,marker='o'#代表点的形状,s=8)#代表点的大小#最开始数据集的形状plt.show()color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(4): ax1.scatter(X[y==i, 0], X[y==i, 1] ,marker='o' ,s=8 ,c=color[i])plt.show()from sklearn.cluster import KMeans#簇为3n_clusters = 3cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)#查看聚好的列表y_pred = cluster.labels_pre = cluster.fit_predict(X)#查看质心centroid = cluster.cluster_centers_#查看总距离平方和inertia = cluster.inertia_print(inertia)color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(n_clusters): ax1.scatter(X[y_pred==i, 0], X[y_pred==i, 1] ,marker='o' ,s=8 ,c=color[i])#画出质心ax1.scatter(centroid[:,0],centroid[:,1],marker="x",s=15,c="black")plt.show()#簇为4n_clusters=4cluster = KMeans(n_clusters=n_clusters, random_state=0).fit(X)#查看聚好的列表y_pred = cluster.labels_pre = cluster.fit_predict(X)#查看质心centroid = cluster.cluster_centers_#查看总距离平方和inertia = cluster.inertia_print(inertia)color = ["red","pink","orange","gray"]fig, ax1 = plt.subplots(1)for i in range(n_clusters): ax1.scatter(X[y_pred==i, 0], X[y_pred==i, 1] ,marker='o' ,s=8 ,c=color[i])#画出质心ax1.scatter(centroid[:,0],centroid[:,1],marker="x",s=15,c="black")plt.show() 聚类算法的模型评估上面的算法的簇随着数字的增大，总的距离和在不断的变小。难道我们我们就这样实验得出簇的个数吗？难道簇越小越好吗？看看算法的评估。聚类模型的结果不是某种标签输出，并且聚类的结果是不确定的，其优劣由业务需求或者算法需求来决定，并且没有永远的正确答案。那我们如何衡量聚类的效果呢？ KMeans的目标是确保“簇内差异小，簇外差异大”，我们就可以通过衡量簇内差异来衡量聚类的效果。我们刚才说过，Inertia是用距离来衡量簇内差异的指标，因此，我们是否可以使用Inertia来作为聚类的衡量指标呢？Inertia越小模型越好嘛。可以，但是这个指标的缺点和极限太大。 当真实标签未知的时候使用轮廓系数这样的聚类，是完全依赖于评价簇内的稠密程度（簇内差异小）和簇间的离散程度（簇外差异大）来评估聚类的效果。其中轮廓系数是最常用的聚类算法的评价指标。它是对每个样本来定义的，它能够同时衡量：1）样本与其自身所在的簇中的其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离2）样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中得所有点之间的平均距离根据聚类的要求”簇内差异小，簇外差异大“，我们希望b永远大于a，并且大得越多越好。单个样本的轮廓系数计算为 这个公式可以看作： 很容易理解轮廓系数范围是(-1,1)，其中值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似，当样本点与簇外的样本更相似的时候，轮廓系数就为负。当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。 在sklearn中，我们使用模块metrics中的类silhouette_score来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。 12345#接上面的代码from sklearn.metrics import silhouette_samplesfrom sklearn.metrics import silhouette_scoreprint("4个簇的时候的轮廓系数：",silhouette_score(X,y_pred))print("四个簇的每个样本的轮廓系数：",silhouette_samples(X,y_pred)) 当真实标签未知的时候用CHI除了轮廓系数是常用的，我们还有卡林斯基-哈拉巴斯指数（Calinski-Harabaz Index，简称CHI，也被称为方差比标准），戴维斯-布尔丁指数（Davies-Bouldin）以及权变矩阵（Contingency Matrix）可以使用。 在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言，Calinski-Harabaz指数s(k)写作如下公式： 其中N为数据集中的样本量，k为簇的个数(即类别的个数)，Bk是组间离散矩阵，即不同簇之间的协方差矩阵， Wk是簇内离散矩阵，即一个簇内数据的协方差矩阵，而tr表示矩阵的迹。在线性代数中，一个n×n矩阵A的主对角线（从左上方至右下方的对角线）上各个元素的总和被称为矩阵A的迹（或迹数），一般记作tr(A) 。数据之间的离散程度越高，协方差矩阵的迹就会越大。组内离散程度低，协方差的迹就会越小，Tr(Wk)也就越小，同时，组间离散程度大，协方差的的迹也会越大， Tr(Bk)就越大，这正是我们希望的，因此Calinski-harabaz指数越高越好。 123#接上面的代码from sklearn.metrics import calinski_harabaz_scoreprint(calinski_harabaz_score(X, y_pred)) 基于轮廓系数选择簇的个数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107from sklearn.datasets import make_blobsimport matplotlib.pyplot as pltfrom sklearn.metrics import silhouette_samplesfrom sklearn.metrics import silhouette_scorefrom sklearn.cluster import KMeansimport matplotlib.cm as cmimport numpy as np#自己制造数据集，n_samples代表数据个数，n_features代表数据特征个数#random_state代表随机性X, y = make_blobs(n_samples=500,n_features=2,centers=4,random_state=1)n_clusters = 4fig, (ax1, ax2) = plt.subplots(1, 2)fig.set_size_inches(18, 7)ax1.set_xlim([-0.1, 1])ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10])clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X)cluster_labels = clusterer.labels_silhouette_avg = silhouette_score(X, cluster_labels)print("For n_clusters =", n_clusters,"The average silhouette_score is :", silhouette_avg)sample_silhouette_values = silhouette_samples(X, cluster_labels)y_lower = 10for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10ax1.set_title("The silhouette plot for the various clusters.")ax1.set_xlabel("The silhouette coefficient values")ax1.set_ylabel("Cluster label")ax1.axvline(x=silhouette_avg, color="red", linestyle="--")ax1.set_yticks([])ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1],marker='o' #点的形状,s=8 #点的大小,c=colors)centers = clusterer.cluster_centers_ax2.scatter(centers[:, 0], centers[:, 1], marker='x',c="red", alpha=1, s=200)ax2.set_title("The visualization of the clustered data.")ax2.set_xlabel("Feature space for the 1st feature")ax2.set_ylabel("Feature space for the 2nd feature")plt.suptitle(("Silhouette analysis for KMeans clustering on sample data ""with n_clusters = %d" % n_clusters),fontsize=14, fontweight='bold')plt.show()for n_clusters in [2,3,4,5,6,7]: n_clusters = n_clusters fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) ax1.set_xlim([-0.1, 1]) ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10]) clusterer = KMeans(n_clusters=n_clusters, random_state=10).fit(X) cluster_labels = clusterer.labels_ silhouette_avg = silhouette_score(X, cluster_labels) print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg) sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i)/n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper) ,ith_cluster_silhouette_values ,facecolor=color ,alpha=0.7 ) ax1.text(-0.05 , y_lower + 0.5 * size_cluster_i , str(i)) y_lower = y_upper + 10ax1.set_title("The silhouette plot for the various clusters.")ax1.set_xlabel("The silhouette coefficient values")ax1.set_ylabel("Cluster label")ax1.axvline(x=silhouette_avg, color="red", linestyle="--")ax1.set_yticks([])ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)ax2.scatter(X[:, 0], X[:, 1],marker='o' #点的形状,s=8 #点的大小,c=colors)centers = clusterer.cluster_centers_ax2.scatter(centers[:, 0], centers[:, 1], marker='x', c="red", alpha=1, s=200)ax2.set_title("The visualization of the clustered data.")ax2.set_xlabel("Feature space for the 1st feature")ax2.set_ylabel("Feature space for the 2nd feature")plt.suptitle(("Silhouette analysis for KMeans clustering on sample data ""with n_clusters = %d" % n_clusters),fontsize=14, fontweight='bold')plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2019%2F09%2F07%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归推导过程逻辑回归是一种名为“回归”的线性分类器，其本质是由线性回归变化而来。想理解逻辑回归，先回顾以下线性回归。线性回归写作一个人人熟悉的方程： &theta;被统称为模型的参数，其中&theta;被称为截距。&theta;1—&theta;n被称为系数。我们可以用矩阵表示这个方程，其中x和&theta;都可以被看作是一个列矩阵： 线性回归的任务，就是构造一个预测函数z来映射输入的特征矩阵x和标签值y的线性关系，而构造预测函数的核心就是找出模型的参数：&theta;的转置矩阵和&theta;0，著名的最小二乘法就是用来求解线性回归中参数的数学方法。 通过函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务（比如预测产品销量，预测股价等等）。那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型变量，我们要怎么办呢？我们可以通过引入联系函数(link function)，将线性回归方程z变换为g(z)，并且令g(z)的值分布在(0,1)之间，且当g(z)接近0时样本的标签为类别0，当g(z)接近1时样本的标签为类别1，这样就得到了一个分类模型。而这个联系函数对于逻辑回归来说，就是Sigmoid函数： Sigmoid函数的公式和性质：Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但Sigmoid函数只是无限趋近于0和1。 将z带入Sigmoid，得到二元逻辑回归的一般形式： 而g(z)就是我们逻辑回归返回的标签值。此时,y(x)的取值都在[0,1]之间，因此 y(x)和1-y(x)相加必然为1。令y(x)除以1-y(x)可以得到形似几率得y(x)/(1-y(x))，在此基础上取对数，得： 不难发现，g(z)的形似几率取对数的本质其实就是我们的线性回归z，我们实际上是在对线性回归模型的预测结果取对数几率来让其的结果无限逼近0和1。因此，其对应的模型被称为”对数几率回归“（logistic Regression），也就是我们的逻辑回归，这个名为“回归”却是用来做分类工作的分类器。 逻辑回归有着基于训练数据求解参数&theta;的需求，并且希望训练出来的模型能够尽可能地拟合训练数据，即模型在训练集上的预测准确率越靠近100%越好。因此，我们使用”损失函数“这个评估指标，来衡量参数&theta;的优劣，即这一组参数能否使模型在训练集上表现优异。 ​ 关键概念：损失函数，衡量参数&theta;的优劣的评估指标，用来求解最优参数的工具损失函数小，模型在训练集上表现优异，拟合充分，参数优秀损失函数大，模型在训练集上表现差劲，拟合不足，参数糟糕我们追求，能够让损失函数最小化的参数组合。注意：没有”求解参数“需求的模型没有损失函数，比如KNN，决策树。 损失函数只适用于单个训练样本，所以引入了成本函数来。损失函数是衡量单一训练样例的效果。成本函数用于衡量参数w和b的效果。 我们要找到一组w和b是成本函数J最小。 推导过程： 既然是最大似然，我们的目标当然是要最大化似然概率： 对于二分类问题有： 用一个式子表示上面这个分段的函数(写成相乘的形式)： 如果用hθ(xi)表示p0，1 - hθ(xi)表示p1，将max函数换成min，则得到最终形式： 由于我们追求成本函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题：如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。虽然逻辑回归和线性回归是天生欠拟合的模型，但我们还是需要控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。 逻辑回归的类 linear_model.LogisticRegression 正则化重要参数正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量&theta;的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为”惩罚项”。损失函数改变，基于损失函数的最优化来求解的参数取值必然改变，我们以此来调节模型拟合的程度。其中L1范数表现为参数向量中的每个参数的绝对值之和，L2范数表现为参数向量中的每个参数的平方和的开方值。 其中J(&theta;)是我们之前提过的损失函数，C是用来控制正则化程度的超参数，n是方程中特征的总数，也是方程中参数的总数。j代表每个参数。在这里，J要大于等于1，因为我们的参数向量&theta;中，第一个参数是&theta;0，使我们的截距。它通常不参与正则化。上面的式子还有另一种写法，本质是一样的。 L1正则化和L2正则化虽然都可以控制过拟合，但它们的效果并不相同。当正则化强度逐渐增大（即C逐渐变小），参数 &theta;的取值会逐渐变小，但L1正则化会将参数压缩为0，L2正则化只会让参数尽量小，不会取到0。在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程，掌管了参数的“稀疏性”。L1正则化越强，参数向量中就越多的参数为0，参数就越稀疏，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。由于L1正则化的这个性质，逻辑回归的特征选择可以由Embedded嵌入法来完成。相对的，L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。 multi_class输入”ovr”, “multinomial”, “auto”来告知模型，我们要处理的分类问题的类型。默认是”ovr”。 ‘ovr’(one-vs-rest)：表示分类问题是二分类，或让模型使用”一对多”的形式来处理多分类问题。 ‘multinomial’：表示处理多分类问题，这种输入在参数solver是’liblinear’时不可用。 “auto”：表示会根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型。比如说，如果数据是二分类，或者solver的取值为”liblinear”，”auto”会默认选择”ovr”。反之，则会选择”nultinomial”。注意：默认值将在0.22版本中从”ovr”更改为”auto”。 12345678from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressioniris = load_iris()for multi_class in ['multinomial', 'ovr']: clf = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=multi_class).fit(iris.data, iris.target)#打印两种multi_class模式下的训练分数 print("training score : %.3f (%s)" % (clf.score(iris.data, iris.target),multi_class)) solver对于小数据集，‘liblinear’是一个不错的选择，而’sag’和’saga’对于大数据集来说更快。对于多类问题，只有’newton-cg’，‘sag’，’saga’和’lbfgs’处理多项损失。‘newton-cg’，’lbfgs’和’sag’只处理L2 penalty，而’liblinear’和’saga’处理L1 penalty。 参数列表 属性 接口 逻辑回归的特征工程高效的embedded嵌入法我们已经说明了，由于L1正则化会使得部分特征对应的参数为0，因此L1正则化可以用来做特征选择，结合嵌入法的模块SelectFromModel，我们可以很容易就筛选出让模型十分高效的特征。注意，此时我们的目的是，尽量保留原数据上的信息，让模型在降维后的数据上的拟合效果保持优秀，因此我们不考虑训练集测试集的问题，把所有的数据都放入模型进行降维。 12345678910111213from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import cross_val_scorefrom sklearn.feature_selection import SelectFromModeldata = load_breast_cancer()print(data.data.shape)LR_ = LR(solver="liblinear",C=0.9,random_state=420)print(cross_val_score(LR_,data.data,data.target,cv=10).mean())X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)print(X_embedded.shape)print(cross_val_score(LR_,X_embedded,data.target,cv=10).mean()) 看看结果，特征数量被减小到个位数，并且模型的效果却没有下降太多，如果我们要求不高，在这里其实就可以停下了。但是，能否让模型的拟合效果更好呢？在这里，我们有两种调整方式：1）调节SelectFromModel这个类中的参数threshold，这是嵌入法的阈值，表示删除所有参数的绝对值低于这个阈值的特征。现在threshold默认为None，所以SelectFromModel只根据L1正则化的结果来选择了特征，即选择了所有L1正则化后参数不为0的特征。我们此时，只要调整threshold的值（画出threshold的学习曲线），就可以观察不同的threshold下模型的效果如何变化。一旦调整threshold，就不是在使用L1正则化选择特征，而是使用模型的属性.coef_中生成的各个特征的系数来选择。coef_虽然返回的是特征的系数，但是系数的大小和决策树中的feature_ importances_以及降维算法中的可解释性方差explained_vairance_概念相似，其实都是衡量特征的重要程度和贡献度的，因此SelectFromModel中的参数threshold可以设置为coef_的阈值，即可以剔除系数小于threshold中输入的数字的所有特征。 12345678910111213141516fullx = []fsx = []threshold = np.linspace(0,abs((LR_.fit(data.data,data.target).coef_)).max(),20)k=0for i in threshold: X_embedded = SelectFromModel(LR_,threshold=i).fit_transform(data.data,data.target) fullx.append(cross_val_score(LR_,data.data,data.target,cv=5).mean()) fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=5).mean()) print((threshold[k],X_embedded.shape[1])) k+=1plt.figure(figsize=(20,5))plt.plot(threshold,fullx,label="full")plt.plot(threshold,fsx,label="feature selection")plt.xticks(threshold)plt.legend()plt.show() 这种方法其实是比较无效的，大家可以用学习曲线来跑一跑：当threshold越来越大，被删除的特征越来越多，模型的效果也越来越差。2）第二种调整方法，是调逻辑回归的类LR_，通过画C的学习曲线来实现： 123456789101112131415161718192021222324252627282930313233343536373839fullx = []fsx = []C=np.arange(0.01,10.01,0.5)for i in C: LR_ = LR(solver="liblinear",C=i,random_state=420) fullx.append(cross_val_score(LR_,data.data,data.target,cv=10).mean()) X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target) fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())print("最好的准确率和对应的index：",max(fsx),C[fsx.index(max(fsx))])plt.figure(figsize=(20,5))plt.plot(C,fullx,label="full")plt.plot(C,fsx,label="feature selection")plt.xticks(C)plt.legend()plt.show()#继续细化了学习曲线fullx = []fsx = []C=np.arange(6.05,7.05,0.005)for i in C: LR_ = LR(solver="liblinear",C=i,random_state=420) fullx.append(cross_val_score(LR_,data.data,data.target,cv=10).mean()) X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target) fsx.append(cross_val_score(LR_,X_embedded,data.target,cv=10).mean())print(max(fsx),C[fsx.index(max(fsx))])plt.figure(figsize=(40,5))plt.plot(C,fullx,label="full")plt.plot(C,fsx,label="feature selection")plt.xticks(C)plt.legend()plt.show()#验证模型效果：降维之前LR_ = LR(solver="liblinear",C=6.069999999999999,random_state=420)print("降维之前的模型效果：",cross_val_score(LR_,data.data,data.target,cv=10).mean())#验证模型效果：降维之后LR_ = LR(solver="liblinear",C=6.069999999999999,random_state=420)X_embedded = SelectFromModel(LR_,norm_order=1).fit_transform(data.data,data.target)print("降维之后的模型效果：",cross_val_score(LR_,X_embedded,data.target,cv=10).mean())print(X_embedded.shape) 系数累加法系数累加法的原理非常简单。在PCA中，我们通过绘制累积可解释方差贡献率曲线来选择超参数，在逻辑回归中我们可以使用系数coef_来这样做，并且我们选择特征个数的逻辑也是类似的：找出曲线由锐利变平滑的转折点，转折点之前被累加的特征都是我们需要的，转折点之后的我们都不需要。不过这种方法相对比较麻烦，因为我们要先对特征系数进行从大到小的排序，还要确保我们知道排序后的每个系数对应的原始特征的位置，才能够正确找出那些重要的特征。如果要使用这样的方法，不如直接使用嵌入法来得方便。 包装法相对的，包装法可以直接设定我们需要的特征个数。 梯度下降之前提到过，逻辑回归的数学目的是求解能够让模型最优化的参数&theta;的值，即求解能够让损失函数最小化的的值，而这个求解过程，对于二元逻辑回归来说，有多种方法可以选择，最常见的有梯度下降法(Gradient Descent)，坐标轴下降法(Coordinate Descent)，牛顿法(Newton-Raphson method)等，每种方法都涉及复杂的数学原理，但这些计算在执行的任务其实是类似的。 成本函数就是1/m的损失函数之和。损失函数可以衡量算法的结果，成本函数可以看出参数w和b在训练集上的效果。 单个样本的梯度下降 其中，&sigma;就是sigmoid函数： 假设某个样本的特征只有 x1 和 x2 ，即(2,1)的列向量： x = [x1, x2]^T, w 是一个(2,1)的列向量， 那么 根据 z = w^T*X + b 可以得到下面的图，其中w1, w2, b 是未知的参数，之后我们会通过训练获得一组最佳的取值。 首先，让我们计算损失函数的导数 dL(a,y)/da 和 da/dz 不难得到： da / dz = a * (1 - a)； ① ​ da/dz=e^(-z)/(1+e^(-z))^2=(1+e^(-z)-1)/((1+e^(-z))^2)=1/(1+e^(-z))-1/(1+e^(-z))^2=a-a^2=a(1-a) dL / da = - y / a + (1 - y) / (1 - a)； ② 根据链式求导法则，从图的右边向左推，得 dz = dL / dz = ② * ① = a - y； 再进一步： dw1 = dL / dw1 = ( dL / dz ) * ( dz / dw1) = dz * x1 = ( a - y ) * x1 dw2 = dL / dw2 = ( dL / dz ) * ( dz / dw2) = dz * x2 = ( a - y ) * x2 db = dL / db = ( dL / dz ) * ( dz / db) = dz * 1 = ( a - y ) 然后更新w1, w2, b 的值即可： w1 = w1 - dw1 * α （α 是学习率）， w2 = w2 - dw2 * α， b = b - α * db； 这样我们就完成了单个样本的参数更新 多个样本的梯度下降先回顾一下成本函数 J(w, b) 的含义, 它是m个样本损失函数求和后的平均值. 注意到公式里有上角标 i ,表示这是第 i 个样本的数据. 既然 成本函数 J 是 所有样本的 L 累加后的平均值, 那么 J 对 w1 的导数 等价于 各个 L 对 w1 的导数求和后的平均值, 同理, 那么 J 对 w2, b 的导数 也是 各个L 对 w2, b 的导数求和后的平均值. 于是得出各个参数更新过程的伪代码： 1234567891011121314151617181920212223242526# 初始化变量J=0,dw_1=0,dw_2=0,db=0# 遍历m个数据集for i = 1 to m z(i) = w^T*x(i)+b a(i) = sigmoid(z(i)) J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i)) dz(i) = a(i)-y(i) # dw1 dw2 db 用作累加器, 循环结束后除以m即可得到平均值 dw1 += x1(i)dz(i) dw2 += x2(i)dz(i) # 假设每个样本只有两个维度, 因此只要累加 dw1, dw2 即可 # 如果x有n个维度 ,就需要使用 for循环 遍历x所有的特征,累加 dw3, dw4 .... dwn # dw3 += x3(i)dz(i) # ....... # dwn += xn(i)dz(i) db += dz(i) J /= m # 取平均值 dw1 /= m dw2 /= m db /= m # 更新dw1 dw2 db， alpha 是学习率 w1 = w1 - alpha*dw1 w2 = w2 - alpha*dw2 b = b - alpha*db 这样, m 个样本的各个参数也就能得到调整了. 但是这部分代码, 仅仅更新了这些参数一次, 需要多次执行, 才能让 J 沿着梯度下降到最低点。这部分代码使用的 for 循环会造成执行效率不高, 如果我们可以把部分数据向量化, 利用矩阵乘法代替 for 循环可以大幅度提高程序执行效率。 在梯度下降过程中，应该有两个for循环。第一个大的for循环代表特征数量，第二个for嵌在第一个for循环里，代表样本数量。两个显式for循环会大大减慢运行速度。为了加快速度，可以通过向量化。初始化的时候用np.zeros初始化成零向量，利用np.dot实现&omega;*x。 向量化我们对代码的改动： ① 使用 numpy 生成 (n_x, 1) 的列矩阵存放w1, w2, w3..wn , 并初始化为0 ② 利用矩阵的加法，将第 i 个样本 x^(i) 的每个特征全部乘以 dz^(i) 并与 列向量 dw 相加 ③ 利用 numpy 的特性， dw /= m 可以使得 dw 中每个元素都除以 m 去掉内层for循环后的伪代码： 1234567Z = np.dot(w^T, X) + bA = σ(Z)dZ = A - Ydw = 1 / m * np.dot(X, dZ^T)db = 1 / m * np.sum( dZ )w = w - dw * alphab = b - db * alpha 这样就完成了m个样本的梯度下降的一次迭代。然而，我们需要多次迭代才能使得成本函数 J 到达最低点，因此 我们仍然需要使用 for 循环，外层for循环暂时还没有办法将其去掉。 步长的概念与解惑： 许多博客和教材在描述步长的时候，声称它是”梯度下降中每一步沿梯度的反方向前进的长度“，”沿着最陡峭最易下山的位置走的那一步的长度“或者”梯度下降中每一步损失函数减小的量“，甚至有说，步长是二维平面著名的求导三角形中的”斜边“或者“对边”的。这些说法都是错误的 请看下图，A(&theta;a,J(&theta;a))就是小球最初的位置，B(&theta;b,J(&theta;b))就是一次滚动后小球移动到的位置。从A到B的方向就是梯度向量的反方向，指向损失函数在A点下降最快的方向。而梯度向量的大小是点A在图像上对&theta;求导后的结果，也是点A切线方向的斜率，橙色角的tan结果，记作d。 梯度下降每走一步，损失函数减小的量，是损失函数在&theta;变化之后的取值的变化，写作J(&theta;b)-J(&theta;a)。梯度下降每走一步，参数变量的变化，写作&theta;a-&theta;b，根据我们参数向量的迭代公式，就是我们的步长梯度向量的大小。记作&alpha;\d，这是三角形的邻边。梯度下降中每走一步，也就是三角形中的斜边。 所以，步长不是任何物理距离，它甚至不是梯度下降过程中任何距离的直接变化，它是梯度向量的大小d上的一个比例，影响着参数向量&theta;每次迭代后改变的部分。 不难发现，既然参数迭代是靠梯度向量的大小d * 步长&alpha;来实现的，而J(&theta;)的降低又是靠调节&theta;来实现的，所以步长可以调节损失函数下降的速率。在损失函数降低的方向上，步长越长,&theta;的变动就越大。相对的，步长如果很短，&theta;的每次变动就很小。具体地说，如果步长太大，损失函数下降得就非常快，需要的迭代次数就很少，但梯度下降过程可能跳过损失函数的最低点，无法获取最优值。而步长太小，虽然函数会逐渐逼近我们需要的最低点，但迭代的速度却很缓慢，迭代次数就需要很多。 在彩色图中，小球在进入深蓝色区域后，并没有直接找到某个点，而是在深蓝色区域中来回震荡了数次才停下，这种”震荡“其实就是因为我们设置的步长太大的缘故。但是在我们开始梯度下降之前，我们并不知道什么样的步长才合适，但梯度下降一定要在某个时候停止才可以，否则模型可能会无限地迭代下去。因此，在sklearn当中，我们设置参数max_iter最大迭代次数来代替步长，帮助我们控制模型的迭代速度并适时地让模型停下。max_iter越大，代表步长越小，模型迭代时间越长，反之，则代表步长设置很大，模型迭代时间很短。 迭代结束，获取到J(&theta;)的最小值后，我们就可以找出这个最小值&theta;对应的参数向量 ，逻辑回归的预测函数也就可以根据这个参数向量&theta;来建立了。 接下来的是max_iter的学习曲线coding： 1234567891011121314151617181920212223242526272829from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoredata = load_breast_cancer()X = data.datay = data.targetl2 = []l2test = []Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)for i in np.arange(1,201,10): lrl2 = LR(penalty="l2",solver="liblinear",C=0.9,max_iter=i) lrl2 = lrl2.fit(Xtrain,Ytrain) l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain)) l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))graph = [l2,l2test]color = ["black","gray"]label = ["L2","L2test"]plt.figure(figsize=(20,5))for i in range(len(graph)): plt.plot(np.arange(1,201,10),graph[i],color[i],label=label[i])plt.legend(loc=4)plt.xticks(np.arange(1, 201, 10))plt.show()lr = LR(penalty="l2",solver="liblinear",C=0.9,max_iter=300).fit(Xtrain,Ytrain)#.n_iter_来调用本次调用本次求解中真正实现的迭代次数print(lr.n_iter_) 运行上面的代码。会弹出红色警告。因为max_iter中限制的步数已经走完了，逻辑回归却还没找到损失函数的最小值。参数&theta;还没有收敛，sklearn就会弹出警告： 当参数solver=”liblinear”： 当参数solver=“sag”： 虽然写法看起来略有不同，但其实都是一个含义，这是在提醒我们：参数没有收敛，请增大max_iter中输入的数字。但我们不一定要听sklearn的。max_iter很大，意味着步长小，模型运行得会更加缓慢。虽然我们在梯度下降中追求的是损失函数的最小值，但这也可能意味着我们的模型会过拟合（在训练集上表现得太好，在测试集上却不一定），因此，如果在max_iter报红条的情况下，模型的训练和预测效果都已经不错了，那我们就不需要再增大max_iter中的数目了，毕竟一切都以模型的预测效果为基准——只要最终的预测效果好，运行又快，那就一切都好，无所谓是否报红色警告了 coding1234567891011121314151617181920212223242526272829303132333435363738394041#建立两个逻辑回归，L1正则化和L2正则化。效果一目了然from sklearn.linear_model import LogisticRegression as LRfrom sklearn.datasets import load_breast_cancerimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoredata = load_breast_cancer()X = data.datay = data.targetdata.data.shapelrl1 = LR(penalty="l1",solver="liblinear",C=0.5,max_iter=1000)lrl2 = LR(penalty="l2",solver="liblinear",C=0.5,max_iter=1000)#逻辑回归的重要属性coef_，查看每个特征所对应的参数lrl1 = lrl1.fit(X,y)lrl1.coef_(lrl1.coef_ != 0).sum(axis=1)lrl2 = lrl2.fit(X,y)lrl2.coef_l1 = []l2 = []l1test = []l2test = []Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)for i in np.linspace(0.05,1,19): lrl1 = LR(penalty="l1",solver="liblinear",C=i,max_iter=1000) lrl2 = LR(penalty="l2",solver="liblinear",C=i,max_iter=1000) lrl1 = lrl1.fit(Xtrain,Ytrain) l1.append(accuracy_score(lrl1.predict(Xtrain),Ytrain)) l1test.append(accuracy_score(lrl1.predict(Xtest),Ytest)) lrl2 = lrl2.fit(Xtrain,Ytrain) l2.append(accuracy_score(lrl2.predict(Xtrain),Ytrain)) l2test.append(accuracy_score(lrl2.predict(Xtest),Ytest))graph = [l1,l2,l1test,l2test]color = ["green","black","lightgreen","gray"]label = ["L1","L2","L1test","L2test"]plt.figure(figsize=(6,6))for i in range(len(graph)): plt.plot(np.linspace(0.05,1,19),graph[i],color[i],label=label[i])plt.legend(loc=4) #图例的位置在哪里? 4表示，右下角plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归]]></title>
    <url>%2F2019%2F09%2F07%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归原理回归是一种应用广泛的预测建模技术，这种技术的核心在于预测的结果是连续型变量。决策树，随机森林，支持向量机的分类器等分类算法的预测标签是分类变量，多以{0，1}来表示，而无监督学习算法比如PCA，KMeans的目标根本不是求解出标签，注意加以区别。 线性回归的类可能是我们目前为止学到的最简单的类，仅有四个参数就可以完成一个完整的算法。并且看得出，这些参数中并没有一个是必填的，更没有对我们的模型有不可替代作用的参数。这说明，线性回归的性能，往往取决于数据本身，而并非是我们的调参能力，线性回归也因此对数据有着很高的要求。幸运的是，现实中大部分连续型变量之间，都存在着或多或少的线性联系。所以线性回归虽然简单，却很强大。顺便一提，sklearn中的线性回归可以处理多标签问题，只需要在fit的时候输入多维度标签就可以了。 线性回归是机器学习中最简单的回归算法，多元线性回归指的就是一个样本有多个特征的线性回归问题。对于一个有n个特征的样本i而言，它的回归结果可以写作一个几乎人人熟悉的方程 ： &omega;被统称为模型的参数。&omega;0被称为截距&omega;1~&omega;n被称为回归系数。这个表达式，其实就和我们小学时就无比熟悉的y=ax+b是同样的性质。其中y是我们的目标变量，也就是标签。 Xi1-Xin是样本i上的特征不同特征。如果考虑我们有m个样本，则m个样本的回归结果可以被写作： 我们可以使用矩阵来表示这个方程： y=X&omega;，其中y是包含了m个全部的样本的回归结果的列向量。粗体的大写字母表示矩阵。我们可以使用矩阵来表示这个方程，其中&omega;可以被看做是一个结构为(1,n)的列矩阵，X是一个结构为(m,n)的特征矩阵。这个预测函数的本质就是我们需要构建的模型，而构造预测函数的核心就是找出模型的参数向量&omega;。 在多元线性回归中，我们在损失函数如下定义： 在这个平方结果下，我们的真实标签和预测值分别如上图表示，也就是说，这个损失函数是在计算我们的真实标签和预测值之间的距离。因此，我们认为这个损失函数衡量了我们构造的模型的预测结果和真实标签的差异，因此我们固然希望我们的预测结果和真实值差异越小越好。所以我们的求解目标就可以转化成 ： 第一次看不明白上面红字，后面看了书，才明白。这个2表示L2范式。也就是我们损失函数代表的含义，在L2范式上开平方，就是我们的损失函数。上面这个式子，正是sklearn当中，用在类Linear_model.LinerRegression背后使用的损失函数，我们往往称呼这个式子为SSE(残差平方和)或RSS(误差平方和)。 现在问题转换成了求解让RSS最小化的参数向量 ，这种通过最小化真实值和预测值之间的RSS来求解参数的方法叫做最小二乘法。接下来，我们对&omega;求导。 我们让求导后的一阶导数为0，并且我们的特征矩阵X肯定不会是一个所有元素都为0的矩阵。得： 线性回归APIsklearn中的线性模型模块是linear_model。 coding： 模型评估指标回归类算法的模型评估一直都是回归算法中的一个难点，但不像我们曾经讲过的无监督学习算法中的轮廓系数等等评估指标，回归类与分类型算法的模型评估其实是相似的法则——找真实标签和预测值的差异。只不过在分类型算法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不同的角度来看待回归的效果：这两种角度，分别对于着不同得模型评估指标。第一，我们是否预测到了正确的数值。第二，我们是否拟合到了足够的信息。 sklearn当中，我们有两种方式调用这个评估指标，一种是使用sklearn专用的模型评估模块metrics里的类mean_squared_error，另一种是调用交叉验证的类cross_val_score并使用里面的scoring参数来设置使用均方误差 ​ 注：y_lr_predict表示std_y.inverse_transform(ypredict),也就是预测得最终价格 12345678910111213141516171819from sklearn.linear_model import LinearRegression as LRfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import cross_val_scorefrom sklearn.metrics import mean_squared_errorfrom sklearn.datasets import fetch_california_housing as fchhousevalue=fch()xtrain,xtest,ytrain,ytest=train_test_split(housevalue.data,housevalue.target,test_size=0.3,random_state=420)std_x=StandardScaler()xtrain=std_x.fit_transform(xtrain)xtest=std_x.transform(xtest)std_y=StandardScaler()ytrain=std_y.fit_transform(ytrain.reshape(-1,1))ytest=std_y.transform(ytest.reshape(-1,1))reg=LR().fit(xtrain,ytrain)ypredict=reg.predict(xtest)print(std_y.inverse_transform(ypredict))print("均方误差：",mean_squared_error(std_y.inverse_transform(ytest),std_y.inverse_transform(ypredict)))print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="mean_squared_error")) 这里如果我们运行上面的代码，会在第十九行报错： 我们在决策树和随机森林中都提到过，虽然均方误差永远为正，但是sklearn中的参数scoring下，均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)。在sklearn当中，所有的损失都使用负数表示，因此均方误差也被显示为负数了。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 我们可以将scoring改为”neg_mean_absolute_error” 。 1print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="neg_mean_squared_error")) 为了衡量模型对数据上的信息量的捕捉，我们定义了R^2和可解释性方差分数(explained_variance_score，EVS)来帮助: 在R^2和EVS中，分子是真实值和预测值之差的差值，也就是我们的模型没有捕获到的信息总量，分母是真实标签所带的信息量，所以两者都衡量1-我们的模型没有捕获到的信息量占真实标签中所带的信息量的比例，所以，两者都是越接近1越好。 R^2我们也可以使用三种方式来调用，一种是直接从metrics中导入r2_score，输入预测值和真实值后打分。第二种是直接从线性回归LinearRegression的接口score来进行调用。第三种是在交叉验证中，输入”r2”来调用。EVS有两种调用方法，可以从metrics中导入，也可以在交叉验证中输入”explained_variance“来调用 。 12print("R2",r2_score(ypredict,ytest))print(reg.score(xtest,ytest)) ????为什么结果不一样呢？然而看R2的计算公式，R2明显和分类模型的指标中的accuracy或者precision不一样，R2涉及到的计算中对预测值和真实值有极大的区别，必须是预测值在分子，真实值在分母。所以我们在调用metrcis模块中的模型评估指标的时候，必须要检查清楚，指标的参数中，究竟是要求我们先输入真实值还是先输入预测值 。所以我们要指定参数： 1234#直接用r2_scoreprint("R2",r2_score(y_true=ytest,y_pred=ypredict))#利用接口scoreprint(reg.score(xtest,ytest)) 123456#EVS的两种调用方法from sklearn.metrics import explained_variance_score as evs#第一种print(cross_val_score(reg,housevalue.data,housevalue.target,cv=10,scoring="explained_variance"))#第二种print("evs",evs(ytest,ypredict)) 多重共线性 矩阵A中第一行和第三行的关系，被称为“精确相关关系”，即完全相关，一行可使另一行为0。在这种精确相关关系下，矩阵A的行列式为0，则矩阵A的逆不可能存在。在我们的最小二乘法中，如果矩阵中存在这种精确相关关系，则逆不存在，最小二乘法完全无法使用，线性回归会无法求出结果。 矩阵B中第一行和第三行的关系不太一样，他们之间非常接近于”精确相关关系“，但又不是完全相关，一行不能使另一行为0，这种关系被称为”高度相关关系“。在这种高度相关关系下，矩阵的行列式不为0，但是一个非常接近0数，矩阵A的逆存在，不过接近于无限大。在这种情况下，最小二乘法可以使用，不过得到的逆会很大，直接影响我们对参数向量w的求解： 这样求解出来的参数向量w会很大，因此会影响建模的结果，造成模型有偏差或者不可用。精确相关关系和高度相关关系并称为”多重共线性”。在多重共线性下，模型无法建立，或者模型不可用。 相对的，矩阵C的行之间结果相互独立，梯形矩阵看起来非常正常，它的对角线上没有任何元素特别接近于0，因此其行列式也就不会接近0或者为0，因此矩阵C得出的参数向量w就不会有太大偏差，对于我们拟合而言是比较理想的。 从上面的所有过程我们可以看得出来，一个矩阵如果要满秩，则要求矩阵中每个向量之间不能存在多重共线性。这也构成了线性回归算法对于特征矩阵的要求。 多重共线性与相关性多重共线性如果存在，则线性回归就无法使用最小二乘法来进行求解，或者求解就会出现偏差。幸运的是，不能存在多重共线性，不代表不能存在相关性——机器学习不要求特征之间必须独立，必须不相关，只要不是高度相关或者精确相关就好。 关键概念：多重共线性与相关性 多重共线性是一种统计现象，是指线性模型中的特征（解释变量）之间由于存在精确相关关系或高度相关关系，多重共线性的存在会使模型无法建立，或者估计失真。多重共线性使用指标方差膨胀因子（variance inflation factor，VIF）来进行衡量（from statsmodels.stats.outliers_influence import variance_inflation_factor），通常当我们提到“共线性”，都特指多重共线性。 相关性是衡量两个或多个变量一起波动的程度的指标，它可以是正的，负的或者0。当我们说变量之间具有相关性，通常是指线性相关性，线性相关一般由皮尔逊相关系数进行衡量，非线性相关可以使用斯皮尔曼相关系数或者互信息法进行衡量。 处理多重共线性的方法 这三种手段中，第一种相对耗时耗力，需要较多的人工操作，并且会需要混合各种统计学中的知识和检验来进行使用。第二种手段在现实中应用较多，不过由于理论复杂，效果也不是非常高效。我们的核心是使用第三种方法：改进线性回归来处理多重共线性。为此，岭回归、Lasso、弹性网就被研究出来了。 岭回归在线性模型之中，除了线性回归之外，最知名的就是岭回归与Lasso了。这两个算法非常神秘，他们的原理和应用都不像其他算法那样高调，学习资料也很少。这可能是因为这两个算法不是为了提升模型表现，而是为了修复漏洞而设计的。 岭回归在多元线性回归的损失函数上加上了正则项，表达为系数w的L2范式（即系数w的平方项）乘以正则化系数&alpha;。岭回归的损失函数的完整表达式写作： linear_model.Ridge在sklearn中，岭回归由线性模型库中的Ridge类来调用 和线性回归相比，岭回归的参数稍微多了那么一点点，但是真正核心的参数就是我们的正则项的系数&alpha; ，其他的参数是当我们希望使用最小二乘法之外的求解方法求解岭回归的时候才需要的，通常我们完全不会去触碰这些参数。所以,大家只需要了解&alpha;的用法就可以了。 123456789101112131415161718192021222324252627282930313233343536import numpy as npimport pandas as pdfrom sklearn.linear_model import Ridge, LinearRegression, Lassofrom sklearn.model_selection import train_test_split as TTSfrom sklearn.datasets import fetch_california_housing as fchfrom sklearn.model_selection import cross_val_scoreimport matplotlib.pyplot as plthousevalue = fch()X = pd.DataFrame(housevalue.data)#print(housevalue.data)print(X.head())y = housevalue.targetX.columns = ["住户收入中位数","房屋使用年代中位数","平均房间数目","平均卧室数目","街区人口","平均入住率","街区的纬度","街区的经度"]Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#数据集索引恢复print(Xtest.shape[0])for i in [Xtrain,Xtest]: i.index = range(i.shape[0])#使用岭回归来进行建模reg = Ridge(alpha=1).fit(Xtrain,Ytrain)reg.score(Xtest,Ytest)alpharange = np.arange(1,1001,100)ridge, lr = [], []for alpha in alpharange: reg = Ridge(alpha=alpha) linear = LinearRegression() regs = cross_val_score(reg,X,y,cv=5,scoring = "r2").mean() linears = cross_val_score(linear,X,y,cv=5,scoring = "r2").mean() ridge.append(regs) lr.append(linears)plt.plot(alpharange,ridge,color="red",label="Ridge")plt.plot(alpharange,lr,color="orange",label="LR")plt.title("Mean")plt.legend()plt.show() 运行上面代码，可以看出，加利佛尼亚数据集上，岭回归的结果轻微上升，随后骤降。可以说，加利佛尼亚房屋价值数据集带有很轻微的一部分共线性，这种共线性被正则化参数 消除后，模型的效果提升了一点点，但是对于整个模型而言是杯水车薪。在过了控制多重共线性的点后，模型的效果飞速下降，显然是正则化的程度太重，挤占了参数 本来的估计空间。从这个结果可以看出，加利佛尼亚数据集的核心问题不在于多重共线性，岭回归不能够提升模型表现。 123456789101112131415#观察模型方差alpharange = np.arange(1,1001,100)ridge, lr = [], []for alpha in alpharange: reg = Ridge(alpha=alpha) linear = LinearRegression() varR = cross_val_score(reg,X,y,cv=5,scoring="r2").var() varLR = cross_val_score(linear,X,y,cv=5,scoring="r2").var() ridge.append(varR) lr.append(varLR)plt.plot(alpharange,ridge,color="red",label="Ridge")plt.plot(alpharange,lr,color="orange",label="LR")plt.title("Variance")plt.legend()plt.show() 可以发现，模型的方差上升快速。虽然岭回归和Lasso不是设计来提升模型表现，而是专注于解决多重共线性问题的，但当&alpha;在一定范围内变动的时候，消除多重共线性也许能够一定程度上提高模型的泛化能力。 不是很明白这句话，后面补 选取最佳的正则化参数既然要选择&alpha;的范围，我们就不可避免的进行最优参数的选择。在各种机器学习教材中，总是教导使用岭迹图来判断正则项参数的最佳取值。传统的岭迹图长成下图： 这一个以正则化参数为横坐标，线性模型求解的系数&alpha;为纵坐标的图像，其中每一条彩色的线都是一个系数&alpha;。其目标是建立正则化参数与系数&alpha;之间的直接关系，以此来观察正则化参数的变化如何影响了系数w的拟合。岭迹图认为，线条交叉越多，则说明特征之间的多重共线性越高。我们应该选择系数较为平稳的喇叭口所对应的&alpha;取值作为最佳的正则化参数的取值。绘制岭迹图的方法非常简单，代码如下: 123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model#创造10*10的希尔伯特矩阵X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])y = np.ones(10)#计算横坐标n_alphas = 200alphas = np.logspace(-10, -2, n_alphas)coefs = []for a in alphas: ridge = linear_model.Ridge(alpha=a, fit_intercept=False) ridge.fit(X, y) coefs.append(ridge.coef_)#绘图展示结果ax = plt.gca()ax.plot(alphas, coefs)ax.set_xscale('log')ax.set_xlim(ax.get_xlim()[::-1]) #将横坐标逆转plt.xlabel('正则化参数alpha')plt.ylabel('系数w')plt.title('岭回归下的岭迹图')plt.axis('tight')plt.show() 我非常不建议大家使用岭迹图来作为寻找最佳参数的标准。有这样的两个理由： 1、岭迹图的很多细节，很难以解释。比如为什么多重共线性存在会使得线与线之间有很多交点？当&alpha;很大了之后看上去所有的系数都很接近于0，难道不是那时候线之间的交点最多吗？ 2、岭迹图的评判标准，非常模糊。哪里才是最佳的喇叭口？哪里才是所谓的系数开始变得”平稳“的时候？一千个读者一千个哈姆雷特的画像？未免也太不严谨了 我们应该使用交叉验证来选择最佳的正则化系数。在sklearn中，我们有带交叉验证的岭回归可以使用： RidgeCV的重要参数、属性和接口： 123456789101112131415161718import numpy as npimport pandas as pdfrom sklearn.linear_model import RidgeCV, LinearRegressionfrom sklearn.datasets import fetch_california_housing as fchhousevalue = fch()X = pd.DataFrame(housevalue.data)y = housevalue.targetX.columns = ["住户收入中位数","房屋使用年代中位数","平均房间数目","平均卧室数目","街区人口","平均入住率","街区的纬度","街区的经度"]Ridge_ = RidgeCV(alphas=np.arange(1,1001,100),store_cv_values=True).fit(X, y)#无关交叉验证的岭回归结果print("没有交叉验证的岭回归：",Ridge_.score(X,y))#调用所有交叉验证的结果print("调用所有交叉验证：",Ridge_.cv_values_.shape)#进行平均后可以查看每个正则化系数取值下的交叉验证结果print("平均后的每个正则化系数交叉验证结果：",max(Ridge_.cv_values_.mean(axis=0)))#查看被选择出来的最佳正则化系数print("最佳正则化系数：",Ridge_.alpha_) Lasso除了岭回归之外，最常被人们提到还有模型Lasso。Lasso全称最小绝对收缩和选择算子（least absolute shrinkage and selection operator），由于这个名字过于复杂所以简称为Lasso。和岭回归一样，Lasso是被创造来作用于多重共线性问题的算法，不过Lasso使用的是系数w的L1范式（L1范式则是系数w的绝对值）乘以正则化系数&alpha; ，所以,Lasso的损失函数表达式为： 岭回归VSLasso：岭回归可以解决特征间的精确相关关系导致的最小二乘法无法使用的问题，而Lasso不行。 Lasso不是从根本上解决多重共线性问题，而是限制多重共线性带来的影响。 Lasso的核心作用：特征选择sklearn中我们使用类Lasso来调用lasso回归，众多参数中我们需要比较在意的就是参数&alpha; ，正则化系数。另外需要注意的就是参数positive。当这个参数为”True”的时候，是我们要求Lasso回归出的系数必须为正数，以此来保证我们的&alpha;一定以增大来控制正则化的程度。 需要注意的是，在sklearn中我们的Lasso使用的损失函数是： 红色框框住的只是作为系数存在。用来消除我们对损失函数求导后多出来的那个2的（求解w时所带的1/2），然后对整体的RSS求了一个平均而已，无论时从损失函数的意义来看还是从Lasso的性质和功能来看，这个变化没有造成任何影响，只不过计算上会更加简便一些。 1234567891011121314151617181920212223import pandas as pdfrom sklearn.linear_model import Ridge, LinearRegression, Lassofrom sklearn.model_selection import train_test_split as TTSfrom sklearn.datasets import fetch_california_housing as fchimport matplotlib.pyplot as plthousevalue = fch()X = pd.DataFrame(housevalue.data)y = housevalue.targetX.columns = ["住户收入中位数","房屋使用年代中位数","平均房间数目","平均卧室数目","街区人口","平均入住率","街区的纬度","街区的经度"]Xtrain,Xtest,Ytrain,Ytest = TTS(X,y,test_size=0.3,random_state=420)#恢复索引for i in [Xtrain,Xtest]: i.index = range(i.shape[0])#线性回归进行拟合reg = LinearRegression().fit(Xtrain,Ytrain)print("线性回归：",(reg.coef_*100).tolist())#岭回归进行拟合Ridge_ = Ridge(alpha=0).fit(Xtrain,Ytrain)print("岭回归：",(Ridge_.coef_*100).tolist())#Lasso进行拟合lasso_ = Lasso(alpha=0).fit(Xtrain,Ytrain)print("Lasso",(lasso_.coef_*100).tolist()) 可以看到，岭回归没有报出错误，但Lasso就不一样了，虽然依然对系数进行了计算，但是报出了整整三个警告： 这三条分别是这样的内容： ​ 1、正则化系数为0，这样算法不可收敛！如果你想让正则化系数为0，请使用线性回归吧 ​ 2、没有正则项的坐标下降法可能会导致意外的结果，不鼓励这样做！ ​ 3、目标函数没有收敛，你也许想要增加迭代次数，使用一个非常小的alpha来拟合模型可能会造成精确度问题！ sklearn不推荐我们使用0这样的正则化系数。如果我们的确希望取到0，那我们可以使用一个比较很小的数，比如0.01这样的值： 123456#岭回归拟合Ridge_ = Ridge(alpha=0.01).fit(Xtrain,Ytrain)print((Ridge_.coef_*100).tolist())#Lasso进行拟合lasso_ = Lasso(alpha=0.01).fit(Xtrain,Ytrain)print((lasso_.coef_*100).tolist()) 这样就不会报任何警告了。 选取最佳的正则化 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.linear_model import LassoCVfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.datasets import fetch_california_housing as fchimport numpy as nphousevalue=fch()Xtrain,Xtest,Ytrain,Ytest=train_test_split(housevalue.data,housevalue.target,test_size=0.3,random_state=420)#自己建立Lasso进行alpha选择的范围alpharange = np.logspace(-10, -2, 200,base=10)lasso_ = LassoCV(alphas=alpharange #自行输入的alpha的取值范围,cv=5 #交叉验证的折数).fit(Xtrain, Ytrain)#查看被选择出来的最佳正则化系数print("最佳的正则化系数：",lasso_.alpha_)#调用所有交叉验证的结果print("所有交叉验证的结果：",lasso_.mse_path_)print(lasso_.mse_path_.shape) #返回每个alpha下的五折交叉验证结果print(lasso_.mse_path_.mean(axis=1)) #有注意到在岭回归中我们的轴向是axis=0吗？#在岭回归当中我们的交叉验证结果返回的是，每一个样本在每个alpha下的交叉验证结果#因此我们要求每个alpha下的交叉验证均值，就是axis=0，跨行求均值#而在这里，我们返回的是，每一个alpha取值下，每一折交叉验证的结果#因此我们要求每个alpha下的交叉验证均值，就是axis=1，跨列求均值#最佳正则化系数下获得的模型的系数结果print("最佳正则化系数获得的模型结果：",lasso_.coef_)print("最佳正则化系数的准确率：",lasso_.score(Xtest,Ytest))#与线性回归相比如何？reg = LinearRegression().fit(Xtrain,Ytrain)print("线性回归的准确率：",reg.score(Xtest,Ytest))#使用lassoCV自带的正则化路径长度和路径中的alpha个数来自动建立alpha选择的范围ls_ = LassoCV(eps=0.00001,n_alphas=300,cv=5).fit(Xtrain, Ytrain)print(ls_.alpha_)print(ls_.alphas_) print(ls_.alphas_.shape)print(ls_.score(Xtest,Ytest))print(ls_.coef_) 模型效果上表现和普通的Lasso没有太大的区别，不过他们都在各个方面对原有的Lasso做了一些相应的改进（比如说提升了本来就已经很快的计算速度，增加了模型选择的维度，因为均方误差作为损失函数只考虑了偏差，不考虑方差的在。除了解决多重共线性这个核心问题之外，线性模型还有更重要的事情要做：提升模型表现。这才是机器学习最核心的需求，而Lasso和岭回归不是为此而设计的。为了提升模型表现而做出的改进：多项式回归。 多项式回归首先，“线性”这个词用于描述不同事物时有着不同的含义，我们最常使用的线性是指“变量之间的线性关系(linear relationship)”。它表示两个变量之间的关系可以展示为一条直线，即可以使用方程y=ax+b来进行拟合。要探索两个变量之间的关系是否线性，最简单的方式就是绘制散点图，如果散点图能够相对均匀地分布在一条直线的两端，则说明这两个变量之间的关系是线性的。从线性关系这个概念出发，我们有了一种说法叫做“线性数据”。通常来说，一组数据由多个特征和标签组成。当这些特征分别与标签存在线性关系的时候，我们就说这一组数据是线性数据。 但当我们在进行分类的时候，我们的数据分布往往是这样的： 这些数据都不能由一条直线来进行拟合，他们也没有均匀分布在某一条线的周围，那我们怎么判断，这些数据是线性数据还是非线性数据呢？在这里就要注意了，当我们在回归中绘制图像时，绘制的是特征与标签的关系图，横坐标是特征，纵坐标是标签，我们的标签是连续型的，所以我们可以通过是否能够使用一条直线来拟合图像判断数据究竟属于线性还是非线性。然而在分类中，我们绘制的是数据分布图，横坐标是其中一个特征，纵坐标是另一个特征，标签则是数据点的颜色。因此在分类数据中，我们使用“是否线性可分”（linearly separable）这个概念来划分分类数据集。当分类数据的分布上可以使用一条直线来将两类数据分开时，我们就说数据是线性可分的。反之，数据不是线性可分的。 ps：上面那张图我也不知道是不是线性可分的，大家知道以下这个概念就好。 12345678910111213141516171819202122232425262728293031323334import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LinearRegressionfrom sklearn.tree import DecisionTreeRegressorrnd = np.random.RandomState(42) #设置随机数种子X = rnd.uniform(-3, 3, size=100) #random.uniform，从输入的任意两个整数中取出size个随机数#生成y的思路：先使用NumPy中的函数生成一个sin函数图像，然后再人为添加噪音y = np.sin(X) + rnd.normal(size=len(X)) / 3 #random.normal，生成size个服从正态分布的随机数#使用散点图观察建立的数据集是什么样子plt.scatter(X, y,marker='o',c='k',s=20)plt.show()#为后续建模做准备：sklearn只接受二维以上数组作为特征矩阵的输入X = X.reshape(-1, 1)#使用原始数据进行建模LinearR = LinearRegression().fit(X, y)TreeR = DecisionTreeRegressor(random_state=0).fit(X, y)#放置画布fig, ax1 = plt.subplots(1)#创建测试数据：一系列分布在横坐标上的点line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)#将测试数据带入predict接口，获得模型的拟合效果并进行绘制ax1.plot(line, LinearR.predict(line), linewidth=2, color='green',label="linear regression")ax1.plot(line, TreeR.predict(line), linewidth=2, color='red',label="decision tree")#将原数据上的拟合绘制在图像上ax1.plot(X[:, 0], y, 'o', c='k')#其他图形选项ax1.legend(loc="best")ax1.set_ylabel("Regression output")ax1.set_xlabel("Input feature")ax1.set_title("Result before discretization")plt.tight_layout()plt.show() 从图像上可以看出，线性回归无法拟合出这条带噪音的正弦曲线的真实面貌，只能够模拟出大概的趋势，而决策树却通过建立复杂的模型将几乎每个点都拟合出来了。可见，使用线性回归模型来拟合非线性数据的效果并不好，而决策树这样的模型却拟合得太细致，相比之下，还是决策树的拟合效果更好一些。线性模型可以用来拟合非线性数据，而非线性模型也可以用来拟合线性数据，更神奇的是，有的算法没有模型也可以处理各类数据，而有的模型可以既可以是线性，也可以是非线性模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型的保存和加载]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[模型的保存和加载 joblib.dump():保存模型，第一个参数是估计器，第二个参数是保存模型的目录。模型的文件格式是pkl。 joblib.load()：读取模型。参数是模型的目录]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树推导首先看看下面这组数据集： 得出下面这颗决策树： 关键概念： ​ 信息熵公式： ​ 信息增益公式：就是熵和特征条件熵的差 ​ 随机变量的不确定性（熵）越小，信息增益越大，这个特征的表现就越好 决策树算法的需要解决的核心问题： ​ 1、如何从数据表中找出最佳节点和最佳分支？ ​ 2、如何让决策树停止生长，防止过拟合？ 决策树的基本过程： 直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。 决策树五大模块sklearn中决策树的类都在”tree“模块下，这个模块包含了五个类： 分类树参数、属性和接口 参数criterion为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是说，在同一棵决策树上，叶子节点的不纯度一定是最低的。 criterion这个参数正是用来决定不纯度的计算方法的： ​ 1、输入”entropy“，使用信息熵 ​ 2、输入”gini“，使用基尼系数 其中t代表给定的节点，i代表标签的任意分类，$p(i|t)$代表标签分类i在节点t上所占的比例。注意，当使用信息熵时，sklearn实际计算的是基于信息熵的信息增益(Information Gain)，即父节点的信息熵和子节点的信息熵之差。信息熵对不纯度更加敏感。 random_state&amp;splitterrandom_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合 max_depth限制树的最大深度，超过设定深度的树枝全部剪掉这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从=3开始尝试，看看拟合的效果再决定是否增加设定深度。 min_samples_leaf&amp;min_samples_splitmin_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生。一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。 max_features&amp;min_impurity_decreasemax_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本种更新的功能，在0.19版本之前时使用min_impurity_split。 class_weight&amp;min_weight_fraction_leaf完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更少偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。 属性分类树的七个参数，一个属性，四个接口，以及绘图所用的代码。七个参数：Criterion，两个随机性相关的参数（random_state，splitter），四个剪枝参数（max_depth， min_sample_leaf，max_feature，min_impurity_decrease）一个属性：feature_importances_ ，属性是在模型训练之后，能够调用查看的模型的各种性质。 接口四个接口：ﬁt，score，apply，predicd apply：返回每个测试样本所在叶子节点的索引 predict：返回每个测试样本的分类/回归结果 coding： 回归树参数解读 几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因此没有class_weight这样的参数。 criterion：回归树衡量分枝质量的指标，支持的标准有三种：1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失。 2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失，属性中最重要的依然是feature_importances_，接口依然是apply, fit, predict, score最核心 其中N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。所以MSE的本质，其实是样本真实数据与回归结果的差异。在回归树中，MSE不只是我们的分枝质量衡量指标，也是我们最常用的衡量回归树回归质量的指标，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作为我们的评估（在分类树中这个指标是score代表的预测准确率）。在回归中，我们追求的是，MSE越小越好。然而，回归树的接口score返回的是R平方，并不是MSE。R平方被定义如下 ： 其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，fi是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。值得一提的是，虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误差“（neg_mean_squared_error）。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。 决策树的本地保存：Graphvizwindows版本下载地址：https://graphviz.gitlab.io/_pages/Download/Download_windows.html 双击msi文件，一直next就完事了。 找到bin文件夹 在下面这张图片的位置加入环境变量 用dot -version检查是否安装成功 将dot文件转为png文件的命令：dot -Tpng *.dot -o *.png 123456789101112131415161718#分类树from sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier,export_graphvizimport pandas as pdtitan=pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")x=titan[["pclass","age","sex"]]y=titan["survived"]x["age"].fillna(x["age"].mean(),inplace=True)x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)dict=DictVectorizer(sparse=False)x_train=dict.fit_transform(x_train.to_dict(orient="records"))x_test=dict.transform(x_test.to_dict(orient="records"))dec=DecisionTreeClassifier()dec.fit(x_train,y_train)print("准确率为：",dec.score(x_test,y_test))#图形化export_graphviz(dec,out_file="./tree.dot",feature_names=["age","pclass=1st","pclass=2nd","pclass=3rd","female","male"]) 123456789101112131415161718192021222324#回归树import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltrng = np.random.RandomState(1)X = np.sort(5 * rng.rand(80,1), axis=0)y = np.sin(X).ravel()y[::5] += 3 * (0.5 - rng.rand(16))regr_1 = DecisionTreeRegressor(max_depth=2)regr_2 = DecisionTreeRegressor(max_depth=5)regr_1.fit(X, y)regr_2.fit(X, y)X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)plt.figure()plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data")plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[贝叶斯的性质关键概念 ​ 联合概率：X取值为x和Y取值为y两个事件同时发生的概率，表示为P(X=x,Y=y) ​ 条件概率：在X取值为x的前提下，Y取值为y的概率。表示为P(Y=y|X=x) 这里的C代表类别，W代表特征。 我们学习的其它分类算法总是有一个特点：这些算法先从训练集中学习，获取某种信息来建立模型，然后用模型去对测试集进行预测。比如逻辑回归，我们要先从训练集中获取让损失函数最小的参数，然后用参数建立模型，再对测试集进行预测。在比如支持向量机，我们要先从训练集中获取让边际最大的决策边界，然后 用决策边界对测试集进行预测。相同的流程在决策树，随机森林中也出现，我们在ﬁt的时候必然已经构造好了能够让对测试集进行判断的模型。而朴素贝叶斯，似乎没有这个过程。这说明，朴素贝叶斯是一个不建模的算法。以往我们学的不建模算法，比如KMeans，比如PCA，都是无监督学习，而朴素贝叶斯是第一个有监督的，不建模的分类算法。我们认为，训练集和测试集都来自于同一个不可获得的大样本下，并且这个大样本下的各种属性所表现出来的规律应当是一致的，因此训练集上计算出来的各种概率，可以直接放到测试集上来使用。即便不建模，也可以完成分类。 sklearn中的朴素贝叶斯sklearn中，基于高斯分布、伯努利分布、多项式分布为我们提供了四个朴素贝叶斯的分类器 高斯朴素贝叶斯sklearnnaive_bayes.GaussianNB(priors=None,var_smoothing=1e-09) 高斯朴素贝叶斯，通过假设P(Xi|Y)是服从高斯分布(正态分布)，估计每个特征下每个类别上的条件概率。对于每个特征下的取值，有以下公式： 1234567891011121314from sklearn.naive_bayes import GaussianNBfrom sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitdigits = load_digits()X, y = digits.data, digits.targetXtrain,Xtest,Ytrain,Ytest = train_test_split(X,y,test_size=0.3,random_state=420)gnb = GaussianNB().fit(Xtrain,Ytrain)#查看分数acc_score = gnb.score(Xtest,Ytest)#查看预测结果Y_pred = gnb.predict(Xtest)#查看预测的概率结果prob = gnb.predict_proba(Xtest)print(prob) 概率类模型的评估指标布里尔分数Brier Score 概率预测的准确程度被称为“校准程度”，是衡量算法预测出的概率和真实结果的差异的一种方式。一种比较常用的指标叫做布里尔分数，它被计算为是概率预测相对于测试样本的均方误差，表示为： 其中N是样本数量，Pi为朴素贝叶斯预测出的概率，Oi是样本所对应的真实结果，只能取到0或者1，如果事件发生则为1，如果不发生则为0。这个指标衡量了我们的概率距离真实标签结果的差异，其实看起来非常像是均方误差。布里尔分数的范围是从0到1，分数越高则预测结果越差劲，校准程度越差，因此布里尔分数越接近0越好。由于它的本质也是在衡量一种损失，所以在sklearn当中，布里尔得分被命名为brier_score_loss。我们可以从模块metrics中导入这个分数来衡量我们的模型评估结果： 12345#接上面的代码from sklearn.metrics import brier_score_loss#注意，第一个参数是真实标签，第二个参数是预测出的概率值#我们的pos_label与prob中的索引一致，就可以查看这个类别下的布里尔分数是多少print(brier_score_loss(Ytest, prob[:,1], pos_label=1)) 对数似然函数另一种常用的概率损失衡量是对数损失（log_loss），又叫做对数似然，逻辑损失或者交叉熵损失。由于是损失，因此对数似然函数的取值越小，则证明概率估计越准确，模型越理想。值得注意得是，对数损失只能用于评估分类型模型 在sklearn，我们可以从metrics模块中导入我们的对数似然函数： 12from sklearn.metrics import log_lossprint(log_loss(Ytest,prob)) 第一个参数是真实标签，第二个参数是我们预测的概率。真正的概率必须要以接口predict_proba来调用，千万避免混淆。 那什么时候使用对数似然，什么时候使用布里尔分数？ 可靠性曲线可靠性曲线（reliability curve），又叫做概率校准曲线（probability calibration curve），可靠性图（reliabilitydiagrams），这是一条以预测概率为横坐标，真实标签为纵坐标的曲线。我们希望预测概率和真实值越接近越好，最好两者相等，因此一个模型/算法的概率校准曲线越靠近对角线越好。 12345678910111213141516171819202122232425262728293031323334import matplotlib.pyplot as pltimport pandas as pdfrom sklearn.datasets import make_classification as mcfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import brier_score_lossfrom sklearn.model_selection import train_test_splitX, y = mc(n_samples=100000,n_features=20 #总共20个特征,n_classes=2 #标签为2分类,n_informative=2 #其中两个代表较多信息,n_redundant=10 #10个都是冗余特征,random_state=42)#样本量足够大，因此使用1%的样本作为训练集Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,test_size=0.99,random_state=42)gnb = GaussianNB()gnb.fit(Xtrain,Ytrain)y_pred = gnb.predict(Xtest)prob_pos = gnb.predict_proba(Xtest)[:,1] #我们的预测概率 - 横坐标clf_score=brier_score_loss(Ytest,prob_pos,pos_label=1)#Ytest - 我们的真实标签 - 横坐标#在我们的横纵表坐标上，概率是由顺序的（由小到大），为了让图形规整一些，我们要先对预测概率和真实标签按照预测#概率进行一个排序，这一点我们通过DataFrame来实现df = pd.DataFrame(&#123;"ytrue":Ytest[:500],"probability":prob_pos[:500]&#125;)df = df.sort_values(by="probability")df.index = range(df.shape[0])#紧接着我们就可以画图了fig = plt.figure()ax1 = plt.subplot()ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated") #得做一条对角线来对比呀ax1.plot(df["probability"],df["ytrue"],"s-",label="%s (%1.3f)" % ("Bayes", clf_score))ax1.set_ylabel("True label")ax1.set_xlabel("predcited probability")ax1.set_ylim([-0.05, 1.05])ax1.legend()plt.show() 为什么存在这么多上下穿梭的直线？因为我们是按照预测概率的顺序进行排序的，而预测概率从0开始到1的过程中，真实取值不断在0和1之间变化，而我们是绘制折线图，因此无数个纵坐标分布在0和1的被链接起来了，所以看起来如此混乱。那我们换成散点图来试试看呢？ 12345678910#接上面的代码fig = plt.figure()ax1 = plt.subplot()ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")ax1.scatter(df["probability"],df["ytrue"],s=10)ax1.set_ylabel("True label")ax1.set_xlabel("predcited probability")ax1.set_ylim([-0.05, 1.05])ax1.legend()plt.show() 可以看到，由于真实标签是0和1，所以所有的点都在y=1和y=0这两条直线上分布，这完全不是我们希望看到的图像。回想一下我们的可靠性曲线的横纵坐标：横坐标是预测概率，而纵坐标是真实值，我们希望预测概率很靠近真实值，那我们的真实取值必然也需要是一个概率才可以，如果使用真实标签，那我们绘制出来的图像完全是没有意义的。但是，我们去哪里寻找真实值的概率呢？这是不可能找到的——如果我们能够找到真实的概率，那我们何必还用算法来估计概率呢，直接去获取真实的概率不就好了么？所以真实概率在现实中是不可获得的。但是，我们可以获得类概率的指标来帮助我们进行校准。一个简单的做法是，将数据进行分箱，然后规定每个箱子中真实的少数类所占的比例为这个箱上的真实概率trueproba，这个箱子中预测概率的均值为这个箱子的预测概率predproba，然后以trueproba为纵坐标，predproba为横坐标，来绘制我们的可靠性曲线。 在sklearn中，这样的做法可以通过绘制可靠性曲线的类calibration_curve来实现。和ROC曲线类似，类calibration_curve可以帮助我们获取我们的横纵坐标，然后使用matplotlib来绘制图像。该类有如下参数： 123456789101112131415#接上面的代码from sklearn.calibration import calibration_curve#从类calibiration_curve中获取横坐标和纵坐标trueproba, predproba = calibration_curve(Ytest, prob_pos,n_bins=10 #输入希望分箱的个数)fig = plt.figure()ax1 = plt.subplot()ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")ax1.plot(predproba, trueproba,"s-",label="%s (%1.3f)" % ("Bayes", clf_score))ax1.set_ylabel("True probability for class 1")ax1.set_xlabel("Mean predcited probability")ax1.set_ylim([-0.05, 1.05])ax1.legend()plt.show() 根据不同的n_bins取值得出不同的曲线 123456789101112#接上面的代码fig, axes = plt.subplots(1,3,figsize=(18,4))for ind,i in enumerate([3,10,100]): ax = axes[ind] ax.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated") trueproba, predproba = calibration_curve(Ytest, prob_pos,n_bins=i) ax.plot(predproba, trueproba,"s-",label="n_bins = &#123;&#125;".format(i)) ax.set_ylabel("True probability for class 1") ax.set_xlabel("Mean predcited probability") ax.set_ylim([-0.05, 1.05]) ax.legend()plt.show() 很明显可以看出，n_bins越大，箱子越多，概率校准曲线就越精确，但是太过精确的曲线不够平滑，无法和我们希望的完美概率密度曲线相比较。n_bins越小，箱子越少，概率校准曲线就越粗糙，虽然靠近完美概率密度曲线，但是无法真实地展现模型概率预测地结果。因此我们需要取一个既不是太大，也不是太小的箱子个数，让概率校准曲线既不是太精确，也不是太粗糙，而是一条相对平滑，又可以反应出模型对概率预测的趋势的曲线。通常来说，建议先试试看箱子数等于10的情况。箱子的数目越大，所需要的样本量也越多，否则曲线就会太过精确。 校准可靠性曲线sklearn中的概率校正类CalibratedClassifierCV来对二分类情况下的数据集进行概率校正 base_estimator需要校准其输出决策功能的分类器，必须存在predict_proba或decision_function接口。 如果参数cv = prefit，分类器必须已经拟合数据完毕。 cv整数，确定交叉验证的策略。可能输入是：None，表示使用默认的3折交叉验证。在版本0.20中更改：在0.22版本中输入“None”，将由使用3折交叉验证改为5折交叉验证 任意整数，指定折数对于输入整数和None的情况下来说，如果时二分类，则自动使用类sklearn.model_selection.StratifiedKFold进行折数分割。如果y是连续型变量，则使用sklearn.model_selection.KFold进行分割。 已经使用其他类建好的交叉验证模式或生成器cv。 可迭代的，已经分割完毕的测试集和训练集索引数组。 输入”prefit”，则假设已经在分类器上拟合完毕数据。在这种模式下，使用者必须手动确定用来拟合分类器的数据与即将倍校准的数据没有交集 method进行概率校准的方法，可输入”sigmoid”或者”isotonic” 输入’sigmoid’，使用基于Platt的Sigmoid模型来进行校准 输入’isotonic’，使用等渗回归来进行校准当校准的样本量太少（比如，小于等于1000个测试样本）的时候，不建议使用等渗回归，因为它倾向于过拟合。样本量过少时请使用sigmoids，即Platt校准。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import matplotlib.pyplot as pltimport pandas as pdfrom sklearn.datasets import make_classification as mcfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import brier_score_lossfrom sklearn.calibration import calibration_curvefrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.model_selection import train_test_splitX, y = mc(n_samples=100000,n_features=20 #总共20个特征,n_classes=2 #标签为2分类,n_informative=2 #其中两个代表较多信息,n_redundant=10 #10个都是冗余特征,random_state=42)#样本量足够大，因此使用1%的样本作为训练集Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,test_size=0.99,random_state=42)def plot_calib(models,name,Xtrain,Xtest,Ytrain,Ytest,n_bins=10): fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6)) ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated") for clf, name_ in zip(models, name): clf.fit(Xtrain, Ytrain) y_pred = clf.predict(Xtest) # hasattr(obj,name)：查看一个类obj中是否存在名字为name的接口，存在则返回True if hasattr(clf, "predict_proba"): prob_pos = clf.predict_proba(Xtest)[:, 1] else: # use decision function prob_pos = clf.decision_function(Xtest) prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min()) # 返回布里尔分数 clf_score = brier_score_loss(Ytest, prob_pos, pos_label=y.max()) trueproba, predproba = calibration_curve(Ytest, prob_pos, n_bins=n_bins) ax1.plot(predproba, trueproba, "s-", label="%s (%1.3f)" % (name_, clf_score)) ax2.hist(prob_pos, range=(0, 1), bins=n_bins, label=name_, histtype="step", lw=2) ax2.set_ylabel("Distribution of probability") ax2.set_xlabel("Mean predicted probability") ax2.set_xlim([-0.05, 1.05]) ax2.legend(loc=9) ax2.set_title("Distribution of probablity") ax1.set_ylabel("True probability for class 1") ax1.set_xlabel("Mean predcited probability") ax1.set_ylim([-0.05, 1.05]) ax1.legend() ax1.set_title('Calibration plots(reliability curve)') plt.show()from sklearn.calibration import CalibratedClassifierCVname = ["GaussianBayes","Logistic","Bayes+isotonic","Bayes+sigmoid"]gnb = GaussianNB()models = [gnb ,LR(C=1., solver='lbfgs',max_iter=3000,multi_class="auto")#定义两种校准方式,CalibratedClassifierCV(gnb, cv=2, method='isotonic'),CalibratedClassifierCV(gnb, cv=2, method='sigmoid')]if __name__=="__main__": plot_calib(models,name,Xtrain,Xtest,Ytrain,Ytest) 多项式朴素贝叶斯在sklearn中，用来执行多项式朴素贝叶斯的类MultinomialNB包含如下的参数和属性： 1234567891011121314151617181920212223242526272829from sklearn.preprocessing import MinMaxScalerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_blobsfrom sklearn.metrics import brier_score_lossclass_1 = 500class_2 = 500 #两个类别分别设定500个样本centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [0.5, 0.5] #设定两个类别的方差X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)mnb = MultinomialNB().fit(Xtrain_, Ytrain)mnb.predict(Xtest_)mnb.predict_proba(Xtest_)mnb.score(Xtest_,Ytest)print(brier_score_loss(Ytest,mnb.predict_proba(Xtest_)[:,1],pos_label=1)) 其实效果不是很理想，来试试看把Xtrain转换成分类型数据吧。注意我们的Xtrain没有经过归一化，因为做哑变量之后自然所有的数据就不会又负数了。看下面的代码： 1234567from sklearn.preprocessing import KBinsDiscretizerkbs = KBinsDiscretizer(n_bins=10, encode='onehot').fit(Xtrain)Xtrain_ = kbs.transform(Xtrain)Xtest_ = kbs.transform(Xtest)mnb = MultinomialNB().fit(Xtrain_, Ytrain)mnb.score(Xtest_,Ytest)print(brier_score_loss(Ytest,mnb.predict_proba(Xtest_)[:,1],pos_label=1)) 伯努利朴素贝叶斯 12345678910111213141516171819202122232425262728293031323334from sklearn.preprocessing import MinMaxScalerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_blobsfrom sklearn.metrics import brier_score_lossclass_1 = 500class_2 = 500 #两个类别分别设定500个样本centers = [[0.0, 0.0], [2.0, 2.0]] #设定两个类别的中心clusters_std = [0.5, 0.5] #设定两个类别的方差X, y = make_blobs(n_samples=[class_1, class_2],centers=centers,cluster_std=clusters_std,random_state=0, shuffle=False)Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,y,test_size=0.3,random_state=420)#先归一化，保证输入多项式朴素贝叶斯的特征矩阵中不带有负数mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)from sklearn.naive_bayes import BernoulliNB#普通来说我们应该使用二值化的类sklearn.preprocessing.Binarizer来将特征一个个二值化#然而这样效率过低，因此我们选择归一化之后直接设置一个阈值mms = MinMaxScaler().fit(Xtrain)Xtrain_ = mms.transform(Xtrain)Xtest_ = mms.transform(Xtest)#不设置二值化bnl_ = BernoulliNB().fit(Xtrain_, Ytrain)bnl_.score(Xtest_,Ytest)brier_score_loss(Ytest,bnl_.predict_proba(Xtest_)[:,1],pos_label=1)#设置二值化阈值为0.5bnl = BernoulliNB(binarize=0.5).fit(Xtrain_, Ytrain)bnl.score(Xtest_,Ytest)print(brier_score_loss(Ytest,bnl.predict_proba(Xtest_)[:,1],pos_label=1)) 补集朴素贝叶斯在sklearn中，补集朴素贝叶斯由类ComplementNB完成，它包含的参数和多项式贝叶斯也非常相似：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2019%2F09%2F07%2Fk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[分类算法-k近邻算法(KNN)：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 如何求距离： k值取很小，容易受异常点影响 k值取很大,容易受k值数量的波动]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM解读(1)]]></title>
    <url>%2F2019%2F09%2F05%2FSVM%E8%A7%A3%E8%AF%BB(1)%2F</url>
    <content type="text"><![CDATA[​ 支持向量机(SVM)是机器学习中获得关注最多的算法。它是我们除了集成算法之外，接触的第一个强学习器。从学术的角度看，SVM是最接近深度学习的机器学习方法。 sklearn中的支持向量机 注意，除了特别表明是线性的两个类LinearSVC和LinearSVR之外，其他的所有类都是同时支持线性和非线性的。NuSVC和NuSVC可以手动调节支持向量的数目，其他参数都与最常用的SVC和SVR一致。注意OneClassSVM是无监督的类。除了本身所带的类之外，sklearn还提供了直接调用libsvm库的几个函数。libsvm是一个简单、易于使用和快速有效的英文的SVM库。 SVM原理解读支持向量机原理的三层理解： 第一层理解支持向量机所做的事情其实非常容易理解，看下图： 上图是一组两种标签的数据，两种标签分别用圆和方块代表。支持向量机的分类方法，是在这组分布中找出一个超平面作为决策边界，使模型在数据上的分类误差尽量接近于小，尤其是在位置数据集上的分类误差(泛化误差)尽量小。 关键概念： ​ 超平面：在几何中，超平面是一个空间的子空间，它是维度比所在空间小一维的空间。 如果数据空间本身是三维的， 则其超平面是二维平面，而如果数据空间本身是二维的，则其超平面是一维的直线。 ​ 决策边界：在二分类问题中，如果一个超平面能够将数据划分为两个集合，其中每个集合中包含单独的一个类别，我们就 说这个超平面是数据的“决策边界”。 决策边界一侧的所有点在分类为属于一个类，而另一个类的所有店分类属于另一个类。比如上面的数据集，我们很容易的在方块和圆的中间画一条线并让所有落在直线左边的样本被分类为方块，在直线右边的样本被分类为圆。如果把数据当作我们的训练集，只要直线的一边只有一种类型的数据，就没有分类错误，我们的训练误差就会为0。 所以，对于一个数据集来说，让训练误差为0的决策边界可以有无数条。 但在此基础上，我们无法保证这条决策边界在未知数据集（测试集）上的表现也会优秀。对于现有的数据集来说， 我们有B1和B2两条可能的决策边界。我们可以把决策边界B1向两边平移，直到碰到离这条决策边界最近的方块和圆圈后停下，形成两个新的超平面，分别是b11和b12，并且我们将原始的决策边界移动到b11和b12的中间，确保B1到b11和b12的距离相等。在b11和b12中间的距离，叫做 这条决策边界的边际(margin)，通常记作d。对B2也执行同样的操作，然后我们来对比一下两个决策边界。现在两条决策边界右边的数据都被判断为圆，左边的数据都被判断为方块，两条决策边界在现在的数据集上的训练误差都是0，没有一个样本被分错。 接着我们引入和原本的数据集相同分布的测试样本（红色所示），平面中的样本变多了，此时我们可以发现，对于B1而言，依然没有一个样本被分错，这条决策边界上的泛化误差也是0。但是对于B2而言，却有三个方块被误认成了圆，有两个圆被误分类成了方块，这条决策边界上的泛化误差就远远大于B1了。这个例子表现出，拥有更大边际的决策边界在分类中的泛化误差更小，这一点可以由结构风险最小化定律来证明（SRM）。如果边际很小，则任何轻微扰动都会对决策边界的分类产生很大的影响。边际很小的情况，是一种模型在训练集上表现很好，却在测试集上表现糟糕的情况，所以会“过拟合”。所以我们在找寻决策边界的时候，希望边际越大越好。 结论：支持向量机，就是通过找出边际最多的决策边界，来对数据进行分类的分类器。 我们让所有紫色点的标签为1，红色点的标签为-1。我们要在这个数据集上寻找一个决策边界，在二维平面上，决 策边界（超平面）就是一条直线。二维平面上的任意一条线可以被表示为： 我们将此表达式变换一下： 其中[a, -1]就是我们的参数向量 ， 就是我们的特征向量， 是我们的截距。 我们要求解参数向量w和截距b 。如果在决策边界上任意取两个点Xa，Xb，并带入决策边界的表达式，则有： 将两式相减，得： Xa和Xb是一条直线上的两个点，相减后的得到的向量方向是由Xb指向Xa，所以Xa-Xb的方向是平行于他们所在的直线——我们的决策边界的。而w与Xa-Xb相互垂直，所以参数向量w方向必然是垂直于我们的决策边界。 此时，我们有了我们得决策边界，任意一个紫色得点Xp就可以表示为： 由于紫色的点所代表的标签y是1，所以我们规定，p&gt;0。 同样的，对于任意一个红色的点Xr而言，我们可以将它表示为： 由于红色点所表示的标签y是-1，所以我们规定，r&lt;0 由上面得知：如果我们有新的测试数据Xt，则Xt的标签就可以根据以下式子来判定： 注意：p和r的符号是我们人为规定的 两类数据中距离我们的决策边界 最近的点，这些点就被称为支持向量。 紫色类的点为Xp ，红色类的点为Xr，则我们可以得到： 两个式子相减，得： 如下图所示： (Xp-Xr)可表示为两点之间的连线，而我们的边际d是平行于w的，所以我们现在，相当于是得到 了三角型中的斜边，并且知道一条直角边的方向。在线性代数中，向量有这样的性质：向量a除以向量b的模长||b|| ，可以得到向量a在向量b的方向上的投影的长度。所以，我们另上述式子两边同时除以||w||，则可以得到 最大边界所对应的决策边界，那问题就简单了，要最大化d，就求解w的最小值。极值问题可以相互转化，我们可以把求解w的最小值转化为，求解以下函数的最小值： 之所以要在模长上加上平方，是因为模长的本质是一个距离，所以它是一个带根号的存在，我们对它取平方，是为了消除根号。 我们得到我们SVM的损失函数： 至此，SVM的第一层理解就完成了。 关键概念：函数间隔与几何间隔 对于给定的数据集T和超平面(&omega;,b),定义超平面(&omega;,b)关于样本点(xi,yi)的函数间隔为： 这其实是我们的虚线超平面的表达式整理过后得到的式子。函数间隔可以表示分类预测的正确性以及确信度。再在这个函数间隔的基础上除以&omega;的模长||&omega;||来得到几何间隔： 几何间隔的本质其实是点xi到超平面(&omega;,b)，即到我们的决策边界的带符号的距离(signed distance)。 第二层理解用拉格朗日对偶函数求解线性SVM有了我们的损失函数以后，我们就需要对损失函数进行求解。我们之前得到了线性SVM损失函数的最初形态。 这个损失函数分为两部分：需要最小化的函数，以及参数求解后必须满足的约束条件。这是一个最优化问题。 为什么要进行转换？我们的目标是求解让损失函数最小化的&omega;,但其实很容易看的出来，如果||&omega;||为0，f(&omega;)必然最小了。但是，||&omega;||=0其实是一个无效的值，原因很简单：首先，我们的决策边界是&omega;x+b=0，如果&omega;为0，则这个向量的所有元素都为0，那就有b=0这个唯一值。如果b和&omega;都为0，决策边界就不在是一条直线了，函数间隔yi(&omega;\xi+b)就会为0，条件中的y(&omega;*xi+b)&gt;=1就不可能实现，所有&omega;不可以是一个0向量。可见，单纯让f(&omega;)为0，是不能求解出合理的&omega;的。我们希望能够能够找出一种方式，能够让我们的条件yi(&omega;*xi+b)&gt;=1在计算中也被纳入考虑，其中一种方法就是使用拉格朗日乘数法。 为什么可以进行转换？我们的损失函数是二次的(quadratic)，并且我们损失函数中的约束条件在参数&omega;和b下是线性的，求解这样的损失函数被称为“凸优化问题”(convex optimization problem)。拉格朗日乘数法正好可以用来解决凸优化问题，这种方法也是业界常用的，用来解决带约束条件，尤其是带有不等式的约束条件的函数的数学方法。首先第一步，我们需要使用拉格朗日乘数来将损失函数改写为考虑了约束条件的形式： 上面被称为拉格朗日函数。其中&alpha;i就叫做拉格朗日乘数。此时此刻，我们要求解的就不只有参数向量&omega;和截距b了，我们也要求拉格朗日乘数&alpha;，而我们的xi和yi都是我们已知的特征矩阵和标签。 怎么进行转换？ 拉格朗日函数也分为两部分。第一部分和我们原始的损失函数一样，第二部分呈现了我们带有不等式的约束条件。我们希望，L(&omega;,b,&alpha;)不仅能够代表我们原有的损失函数f(&omega;)和约束条件。还能够表示我们想要最小化损失函数来求解&omega;和b的意图，所以我们要先以&alpha;为参数，L(&omega;,b,&alpha;)的最大值，再以&omega;和b为参数，求解L(&omega;,b,&alpha;)的最小值。因此，我们的目标可以写作： 首先，我们先执行max，即最大化L(&omega;,b,&alpha;)，那就有两种情况： 若把函数第二部分当作一个惩罚项来看待，则yi(&omega;*xi+b)大于1时函数没有收到惩罚。而yi(&omega;*xi+b)小于1时函数受到极致的惩罚，即加上一个正无穷项，函数整体永远不可能渠道最小值。所以第二部，我们执行min的命令，求解函数整体的最小值，我们就永远不能让&alpha;必须取到正无穷的状况出现，即是说永远不让yi(&omega;*xi+b)&lt;1的状况出现。从而实现了求解最小值的同时让约束条件满足。现在，L(&omega;,b,&alpha;)就是我们新的损失函数。我们的目标时通过先最大化，再最小化它来求解参数向量&omega;和截距b的值。 拉格朗日函数转换为拉格朗日对偶函数为什么要进行转换？要求极值，最简单的方法还是对参数求导后让一阶导数等于0。我们先来试试看对拉格朗日函数求极值，在这里我们对参数向量&omega;和截距b分别求偏导并且让他们等于0。 求导过程如下： 由于两个求偏导结果中都带有未知的拉格朗日乘数&alpha;i，因此我们还是无法求解出&omega;和b，我们必须想出一种方法来求解拉格朗日乘数&alpha;i。幸运地是，拉格朗日函数可以被转换成一种只带有&alpha;i，而不带有&omega;和b的形式，这种形式被称为拉格朗日对偶函数。在对偶函数下，我们就可以求解出拉格朗日乘数&alpha;i，然后带入到上面推导出的(1)和(2)式中来求解&omega;和b。 为什么能够进行转换？对于任何一个拉格朗日函数，都存在一个与它对于的对偶函数g(&alpha;)，只带有拉格朗日乘数&alpha;作为唯一的参数。如果L(x,a)的最优解存在并可以表示为min L(x,a)，并且对偶函数的最优解也存在并可以表示为max g(&alpha;)，则可以定义对偶差异。即拉格朗日函数的最优解与其对偶函数的最优解之间的差值： 如果$\Delta$=0，则称L(x,&alpha;)与其对偶函数之间存在强对偶关系(strong duality property)，此时我们就可以通过求解其对偶函数的最优解来替代求解原始函数的最优解。 那强对偶关系上面时候存在呢？那就是这个拉格朗日函数必须满足KTT(Karush-Kuhn-Tucker)条件： 这里的条件其实都比较好理解。首先是所有参数的一阶导数必须为0，然后约束条件中的函数本身需要小于等于0，拉格朗日乘数需要大于等于0，以及约束条件乘以拉格朗日乘数必须等于0，即不同i的取值下，两者之中至少有一个为0。当所有限制都被满足，则拉格朗日函数L(x,&alpha;)的最优解与其对偶函数的最优解相等，我们就可以将原始的最优化问题转换成为对偶函数的最优化问题。而不难注意到，对于我们的损失函数L(&alpha;,b,&alpha;)而言，KTT条件都是可以操作的。如果我们能够人为让KTT条件全部成立，我们就可以求解出L(&omega;,b,&alpha;)的对偶函数来解出&alpha;。 之前我们已经让拉格朗日函数上对参数&omega;和b的求导为0，得到了式子： 并且在我们的函数中，我们通过先求解最大值再求解最小值的方法使得函数天然满足： 接下来，我们只需要再满足一个条件： 这个条件其实很容易满足，能够让yi(&omega;*xi+b)-1=0的就是落在虚线的超平面上的样本点，即我们的支持向量。所有不是支持向量的样本点则必须满足&alpha;i=0。满足这个式子说明了，我们求解的参数&omega;和b以及求解的超平面的存在，只与支持向量相关，与其他样本点无关。五个条件满足后，就可以使用L(&omega;,b,&alpha;)的对偶函数来求解&alpha;了。 怎么进行转换？首先让拉格朗日函数对参数&omega;和b求导后的结果为0，本质时再探索拉格朗日函数的最小值。然后： 这个Ld就是我们的对偶函数。对所有存在对偶函数的拉格朗日函数我们有对偶差异如下： 则对于我们的L(&omega;,b,&alpha;)和Ld，我们则有： 我们推到Ld的第一步是对L(&omega;,b,&alpha;)求偏导并让偏导数都为0。所有我们求解对偶函数的过程其实是在求解L(&omega;,b,&alpha;)的最小值。所以我们又可以把公式写成： 如此，我们只需要求解对偶函数的最大值，就可以求出&alpha;了。最终，我们的目标函数变化为： 求解拉格朗日对偶函数到了这一步，我们就需要使用梯度下降，smo或者二次规划(QP，quadratic programming)来求解&alpha;。由于数学太难且我的数学也是太差，我不看了也就不讲了，头都要秃了。大家只需要知道，一但我们求得了&alpha;值，我们就可以使用求导后得到的(1)式求解&omega;，并可以使用(1)式子和决策边界的表达式结合，得到下面的式子来求解b： 当求得特征向量&omega;和b，我们就得到了我们的决策边界的表达式。也就可以利用决策边界和其有关的超平面来进行分类，我们的决策函数就可以写作： 其中xtest是任意测试样本，sign(h)是h&gt;0时返回-1的符号函数。到这里，我们也就完成了对SVM的第二层理解的大部分内容，我们了解了线性SVM的四种相关函数：损失函数的初始形态、拉格朗日函数、拉格朗日对偶函数以及最后的决策函数。 第三层理解熟练以上的推导过程，就是我们的第三层理解。 线性SVM决策过程的可视化我们可以使用sklearn的式子来可视化我们的决策边界、支持向量、以及决策边界平行的两个超平面。 画决策边界的函数：contour contour是专门用来画等高线的函数。等高线，本质上是在二维图像表现三维图像的一种形式，其中两维X和Y是两条坐标轴上的取值，而Z表示高度。Contour就是将由X和Y构成平面上的所有点中，高度一致的点连接成线段的函数，在同一条等高线上的点一定具有相同的Z值。我们可以利用这个性质来绘制我们的决策边界。 回忆一下，我们的决策边界是&omega;*x+b=0，并在决策边界的两边找出两个超平面，使得超平面到决策边界的相对距离为1。那其实，我们只需要在我们的样本构成的平面上，把所有到决策边界的距离为0的点相连，就是我们的决策边界，而把所有到决策边界的相对距离为1的点相连，就是我们的两个平行于决策边界的超平面了。此时，我们的Z就是平面上的任意点到达超平面的距离。那首先，我们需要获取样本构成的平面，作为一个对象。 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.datasets import make_blobsfrom sklearn.svm import SVCimport matplotlib.pyplot as pltimport numpy as npX,y = make_blobs(n_samples=50, centers=2, random_state=0,cluster_std=0.6)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plt.xticks([])plt.yticks([])#制作数据的散点图plt.show()def plot_svc_decision_function(model,ax=None):#获取当前的子图，如果没有，就创建 if ax is None: ax = plt.gca()#获取x轴的最小最大值 xlim = ax.get_xlim()#获取y轴的最小最大值 ylim = ax.get_ylim()#在两个最小最大值间生成30个值 x = np.linspace(xlim[0],xlim[1],30) y = np.linspace(ylim[0],ylim[1],30)#使用meshgrid函数将两个一维向量转换为特征矩阵 Y,X = np.meshgrid(y,x)#列合并numpy xy = np.vstack([X.ravel(), Y.ravel()]).T#decision_function是一个重要接口，返回每个输入的样本所对应到决策边界的距离，#P的本质是输入的样本到决策边界的距离，而contour函数中的level其实是输入了这个距离 P = model.decision_function(xy).reshape(X.shape) ax.contour(X, Y, P,colors="k",levels=[-1,0,1],alpha=0.5,linestyles=["--","-","--"]) ax.set_xlim(xlim) ax.set_ylim(ylim) plt.show()clf = SVC(kernel = "linear").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plot_svc_decision_function(clf)print("预测值：",clf.predict(X))print("准确度：",clf.score(X,y))print("返回支持向量：",clf.support_vectors_)print("返回每个类中支持向量的个数：",clf.n_support_) 那如果推广到非线性数据上呢，比如环形数据 1234567from sklearn.datasets import make_circlesX,y = make_circles(100, factor=0.1, noise=.1)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plt.show()clf = SVC(kernel = "linear").fit(X,y)plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap="rainbow")plot_svc_decision_function(clf) 运行代码后看效果图，很明显，现在线性SVM已经不适合于我们的状况了，我们无法找出一条直线来划分我们的数据集，让直线的两边分别是两种类别。这个时候，如果我们能够在原本的X和y的基础上，添加一个维度r，变成三维，我们可视化这个数据，来看看添加维度让我们的数据如何变化。 1234567891011121314#即使下面的代码没有看出来用了这个模块，但是必须要引入from mpl_toolkits import mplot3d#定义一个绘制三维图像的函数#elev表示上下旋转的角度#azim表示平行旋转的角度def plot_3D(elev=30,azim=30,X=X,y=y): ax = plt.subplot(projection="3d") ax.scatter3D(X[:,0],X[:,1],r,c=y,s=50,cmap='rainbow') ax.view_init(elev=elev,azim=azim) ax.set_xlabel("x") ax.set_ylabel("y") ax.set_zlabel("r") plt.show()plot_3D() 可以看见，此时此刻我们的数据明显是线性可分的了：我们可以使用一个平面来将数据完全分开，并使平面的上方的所有数据点为一类，平面下方的所有数据点为另一类。此时我们的数据在三维空间中，我们的超平面就是一个二维平面。明显我们可以用一个平面将两类数据隔开，这个平面就是我们的决策边界了。我们刚才做的，计算r，并将r作为数据的第三维度来将数据升维的过程，被称为“核变换”，即是将数据投影到高维空间中，以寻找能够将数据完美分割的超平面，即是说寻找能够让数据线性可分的高维空间。为了详细解释这个过程，我们需要引入SVM中的核心概念：核函数 后续内容请看SVM解读(2)]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库概述]]></title>
    <url>%2F2019%2F09%2F04%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[​ 大三老狗已正式上线，这学期学了新的课程。其中一门就是数据库，总结一下所学的内容。 数据与信息的联系：数据是信息的符号表示或载体。信息是数据的内涵，是对数据的语义解释。 数据库数据具有永久存储、有组织、可共享三个基本特点。 数据库系统(DBS)由计算机硬件、数据库、数据库管理系统、应用软件和数据库管理系统组成。 数据库设计时面向数据模型对象、数据库系统的数据冗余度小、数据共享度高、数据库系统的数据和程序之间具有较高的独立性、数据库中数据的最小存取单位是数据项、数据库系统通过DBMS进行数据安全性、完整性、并发控制和数据恢复控制。 数据库数据物理独立性高是指当数据的物理结构（存储结构）发生变化时，应用程序不需要修改也可以正常工作 数据库数据逻辑独立性高是指当数据库系统的数据全局逻辑结构改变时，它们对应的应用程序不需要改变仍可以正常运行 DBMS的功能结构 ​ 数据定义功能：能够提供数据定义语言(DDL)和相应的建库机制。用户利用DDL可以方便建立数据库。 ​ 数据操纵功能：实现数据的插入、修改、删除、查询、统计等数据存取操作的功能称为数据操纵功能。数据库管理系统通过提供数据操纵语言(DML)实现其数据操纵功能。 ​ 运行管理功能：包括并发控制、数据的存取控制、数据完整性条件的检查和执行、数据库内部的维护等 ​ 建立和维护功能：指数据的载入、转储、重组织功能及数据库的恢复功能，指数据库结构的修改、变更及扩充功能。 从数据库管理系统的角度查看，数据库系统通常采用外模式、模式、内模式三级模式结构。从数据库最终用户的角度看，数据库系统的就够分为单用户结构、主从结构、分布式结构、客户/服务器结构。 外模式：也称为子模式或用户模式。它是数据库用户看见和使用的、对局部数据的逻辑结构和特征的描述，是与某一应用有关的数据的逻辑表示。 模式：模式是数据库中数据的逻辑结构和特征的描述。它仅仅涉及到型的描述，不涉及到具体的值。模式的具体值被称为模式的一个实例。同一模式可以有很多实例。模式是相对稳定的，实例是相对变动的。模式反映的是数据的结构及其关系，而实例反映的是数据某一时刻的状态。 内模式：也称存储模式，它是对数据的物理结构和存储结构的描述，是数据在数据库内部的表示方式。 为了能够在内部实现这三个抽象层次的联系和转换，数据库系统在这三级模式之间提供了两层映像：外模式/概念模式映像、概念模式/内模式映像。这两层映像保证了数据库系统中的数据能够具有较高的逻辑独立性和物理独立性。 外模式/模式映像保证了数据与程序间的逻辑独立性，模式/内模式映像保证了数据的物理独立性。 逻辑独立性：当数据库的整体逻辑结构发生变化时，通过调整外模式和模式之间的映像，使得外模式中的局部数据及其结构不变，程序不在修改。 物理独立性：当数据库的存储结构发生变化时，通过调整模式和内模式之间的映像，使得整体模式不变，当然外模式及应用程序不用改变。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数以及作用]]></title>
    <url>%2F2019%2F09%2F03%2F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[本片博客介绍激活函数以及激活函数的作用。 首先激活函数是非线性连续的，激活函数有Sigmoid、tanh、Relu、Leaky Relu、Maxout、ELU。 激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。 激活函数三问问题一：为什么我们要使用激活函数呢？ 如果不使用激活函数，我们的每一层输出只是承接了上一层输入函数的线性变换，无论神经网络有多少层，输出都是输入的线性组合。如果使用的话，激活函数给神经元引入了非线性的因素，使得神经网络可以逼近任何非线性函数，这样神经网络就可以应用到非线性模型中。 问题二：那么为什么我们需要非线性函数？ 非线性函数是那些一级以上的函数，而且当绘制非线性函数时它们具有曲率。现在我们需要一个可以学习和表示几乎任何东西的神经网络模型，以及可以将输入映射到输出的任意复杂函数。神经网络被认为是通用函数近似器（Universal Function Approximators）。这意味着他们可以计算和学习任何函数。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。 问题三：如何选择激活函数？ 1、sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。 2、tanh 激活函数： tanh 是非常优秀的， 几乎适合所有场合。 3、ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。 常见激活函数接下来介绍一下常用的激活函数： sigmoid该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下： 缺点：当 z 值非常大或者非常小时，通过上图我们可以看到，sigmoid函数的导数 g′(z)将接近 00 。这会导致权重 W 的梯度将接近 00 ，使得梯度更新十分缓慢，即梯度消失 tanh该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到(−1,1) 之间，其公式与图形为： tanh函数的缺点同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z)接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。因此再介绍一个机器学习里特别受欢迎的激活函数 Relu函数。 Relu函数 只要z是正值的情况下，导数恒等于 1，当z是负值的时候，导数恒等于 0。z 等于0的时候没有导数，但是我们不需要担心这个点，假设其导数是0或者1即可。 激活函数选择的经验：如果输出是0和1的二分类问题，则输出层选择 sigmoid 函数，其他层选择 Relu 函数 Leaky Relu这是一种对ReLU函数改进的函数，又称为PReLU函数，但其并不常用。其公式与图形如下： 其中a取值在（0，1）之间 总结一下Relu 激活函数的优点： Relu 函数在 z&gt;0 的部分的导数值都大于0，并且不趋近于0，因而梯度下降速度较快。 Relu 函数在 z&lt;0 的部分的导数值都等于0，此时神经元（就是模型中的圆圆）就不会得到训练，产出所谓的稀疏性，降低训练出来的模型过分拟合的概率。但即使这样，也有足够多的神经元使得 z&gt;0。 激活函数的导数在进行神经网络反向传播的时候，需要计算激活函数的斜率或者导数 sigmoid的求导 或者让 a=g(z)，得 tanh的求导 我们同样让a=g(z)，得 Relu的求导]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸检测数据集构造]]></title>
    <url>%2F2019%2F08%2F28%2F%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E9%80%A0%2F</url>
    <content type="text"><![CDATA[在这次实战中，我们使用了caffe来作为我们的框架。所以，在打包数据格式的时候，打包成VOC数据格式的数据集，得到的数据集将采用LMDB的格式对数据进行封装。如果用的是tensorflow来训练SSD模型，那就将数据打包成TFRecoder数据格式的数据。 Caffe-SSD数据集构造流程： 1、生成VOC个数数据集（图片、XML标注信息文件） 2、修改Caffe-SSD数据打包脚本相关路径配置 3、运行Caffe-SSD数据打包脚本 VOC格式数据集的目录下有三个文件夹： Annotation中保存的是xml格式的label信息 ImageSet中Main目录中存放不同照片列表文件 ​ train.txt：训练图片文件名列表 ​ val.txt：验证图片文件名列表 ​ trianval.txt：训练和验证的图片文件名列表 ​ test.txt：测试图片文件名列表 JPEGImages目录存放所有的图片集。Annotation和JPEGImages一一对应。 WIDERFace数据集：打开人脸检测数据集地址下载数据集。 下载数据的训练集、验证集、测试集和标注信息。 打开标注信息，可以看到文件夹中有很多txt文件，打开看一下： **** 对于VOC格式的数据集，最主要的是生成两个文件。一个是图片的标注数据，另一个是图片数据。对于图片数据，我们只需要将wider_face的图片放到对应目录下，而对于标注数据，我们需要将上面图片的文件进行解析来重新生成针对每张图片的标注信息，这个信息用XML格式存储。 对于wider_face转为voc，首先创建Annotation、ImageSets、JPEGImages三个文件夹，其中ImageSets中再创建一个Main文件夹。 将wider_face转为voc的coding： import os,cv2,sys,shutil from xml.dom.minidom import Document def writexml(filename,saveing,bboxes,xmlpath): doc=Document() annotation=doc.createElement(&quot;annotation&quot;) doc.appendChild(annotation) folder=doc.createElement(&quot;folder&quot;) folder_name=doc.createTextNode(&quot;widerface&quot;) folder.appendChild(folder_name) annotation.appendChild(folder) filenamenode=doc.createElement(&quot;filename&quot;) filename_name=doc.createTextNode(filename) filenamenode.appendChild(filename_name) annotation.appendChild(filenamenode) source=doc.createElement(&quot;source&quot;) annotation.appendChild(source) database=doc.createElement(&quot;database&quot;) database.appendChild(doc.createTextNode(&quot;wider face database&quot;)) source.appendChild(database) annotation_s=doc.createElement(&quot;annotation&quot;) annotation_s.appendChild(doc.createTextNode(&quot;PASCAL VOC2007&quot;)) source.appendChild(annotation_s) image=doc.createElement(&quot;image&quot;) image.appendChild(doc.createTextNode(&quot;flickr&quot;)) source.appendChild(image) flickrid=doc.createElement(&quot;flickrid&quot;) source.appendChild(doc.createTextNode(&quot;-1&quot;)) source.appendChild(flickrid) owner=doc.createElement(&quot;owner&quot;) annotation.appendChild(owner) flickrid_o=doc.createElement(&quot;flickrid&quot;) flickrid_o.appendChild(doc.createTextNode(&quot;yanyu&quot;)) owner.appendChild(flickrid_o) name_o=doc.createElement(&quot;name&quot;) name_o.appendChild(doc.createTextNode(&quot;yanyu&quot;)) owner.appendChild(name_o) size=doc.createElement(&quot;size&quot;) annotation.appendChild(size) width=doc.createElement(&quot;width&quot;) width.appendChild(doc.createTextNode(str(saveing.shape[1]))) height=doc.createElement(&quot;height&quot;) height.appendChild(doc.createTextNode(str(saveing.shape[0]))) depth=doc.createElement(&quot;depth&quot;) depth.appendChild(doc.createTextNode(str(saveing.shape[2]))) size.appendChild(width) size.appendChild(height) size.appendChild(depth) segmented=doc.createElement(&quot;segmented&quot;) segmented.appendChild(doc.createTextNode(&quot;0&quot;)) annotation.appendChild(segmented) for i in range(len(bboxes)): bbox=bboxes[i] objects=doc.createElement(&quot;object&quot;) annotation.appendChild(objects) object_name=doc.createElement(&quot;name&quot;) object_name.appendChild(doc.createTextNode(&quot;face&quot;)) objects.appendChild(object_name) pose=doc.createElement(&quot;pose&quot;) pose.appendChild(doc.createTextNode(&quot;Unspecified&quot;)) objects.appendChild(pose) truncated=doc.createElement(&quot;truncated&quot;) truncated.appendChild(doc.createTextNode(&quot;1&quot;)) objects.appendChild(truncated) difficult=doc.createElement(&quot;difficult&quot;) difficult.appendChild(difficult) difficult.appendChild(doc.createTextNode(&quot;0&quot;)) objects.appendChild(difficult) bndbox=doc.createElement(&quot;bndbox&quot;) objects.appendChild(bndbox) xmin=doc.createElement(&quot;xmin&quot;) xmin.appendChild(doc.createTextNode(str(bbox[0]))) bndbox.appendChild(xmin) ymin=doc.createElement(&quot;ymin&quot;) ymin.appendChild(doc.createTextNode(str(bbox[1]))) bndbox.appendChild(ymin) xmax=doc.createElement(&quot;xmax&quot;) xmax.appendChild(doc.createTextNode(str(bbox[0]+bbox[2]))) bndbox.appendChild(xmax) ymax=doc.createElement(&quot;ymax&quot;) ymax.appendChild(doc.createTextNode(str(bbox[1]+bbox[3]))) bndbox.appendChild(ymax) f=open(xmlpath,&quot;w&quot;) f.write(doc.toprettyxml(indent=&quot; &quot;)) f.close() rootdir=&quot;./wider_face&quot; def convertimgset(img_set): imgdir=rootdir+&quot;/WIDER_&quot;+img_set+&quot;/images&quot; gtfilepath=rootdir+&quot;/wider_face_split/wider_face_&quot;+img_set+&quot;_bbx_gt.txt&quot; fwrite=open(rootdir+&quot;/ImageSets/Main&quot;+img_set+&quot;.txt&quot;,&quot;w&quot;) index=0 with open(gtfilepath,&quot;r&quot;) as gtfiles: while(index&lt;1000): filename=gtfiles.readline()[:-1] if (filename==&quot;&quot;): continue imgpath=imgdir+&quot;/&quot;+filename img=cv2.imread(imgpath) if not img.data: break numbbox=int(gtfiles.readline()) bboxes=[] for i in range(numbbox): line=gtfiles.readline() lines=line.split() lines=lines[0:4] bbox=(int(lines[0]),int(lines[1]),int(lines[2]),int(lines[3])) bboxes.append(bbox) filename=filename.replace(&quot;/&quot;,&quot; &quot;) if len(bboxes)==0: print(&quot;no face&quot;) cv2.imwrite(&quot;{}/JPEGImages/{}&quot;.format(rootdir,filename),img) fwrite.write(filename.split(&quot;.&quot;)[0]+&quot;\n&quot;) xmlpath=&quot;{}/Annotations/{}&quot;.format(rootdir,filename.split(&quot;.&quot;)[0]) writexml(filename,img,bboxes,xmlpath) print(&quot;第%d张成功的图片&quot; % index) index+=1 fwrite.close() if __name__==&quot;__main__&quot;: img_sets=[&quot;train&quot;,&quot;val&quot;] for img_set in img_sets: convertimgset(img_set) shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;train.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;trainval.txt&quot;) shutil.move(rootdir+&quot;/ImageSets/Main/&quot;+&quot;val.txt&quot;,rootdir+&quot;/ImageSets/Main/&quot;+&quot;test.txt&quot;) 这篇博客废了。。。。caffe的环境太难搭了。。。。 我将框架换成tensorflow，进行另一个实战]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
        <category>SSD</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸检测综述]]></title>
    <url>%2F2019%2F08%2F28%2F%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[人脸标注方法：矩形标注和椭圆形标注 矩形标注都是用一个矩形框将画面中的人脸区域包含在内，这种标记方法很难给出一个恰好包含面部的矩形框。而人脸天然呈现椭圆形，椭圆形标注是一种较为准确的方法。椭圆形标注返回值包括：椭圆的长轴半径、椭圆的短轴半径、椭圆长轴偏转角度、椭圆圆心x坐标、椭圆圆心y坐标。 判断算法性能好坏： 每一个标记只允许有一个检测与之相对应，也就是说，我们检测出来的每一个人脸图像只能同我们在标注中人脸图像中数据中的一个相对应。如果有多个，就会视为重复检测。重复检测会被视为错误检测。接着我们就可以得出正确率和错误率，可以画出ROC曲线和PR曲线。可以根据曲线看出算法的好坏。 人脸采集常用方法： 活体检测 判断用户是否为正常操作，通过指定用户做随机动作，一搬有张嘴、摇头、点头、凝视、眨眼等等，防止照片攻击。判断用户是否真实在操作，指定用户上下移动手机，防止视频攻击和非正常动作的攻击。 3D检测 验证采集到的是否为立体人像，能够防止平面照片、不同弯曲程度的照片等。 连续检测 通过连续的检测，验证运动轨迹是否正常，防止跳过活体检测直接替换采集的照片，也能防止中途换人。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
        <category>SSD</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测常用名词]]></title>
    <url>%2F2019%2F08%2F22%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[图像分类：一张图像中是否包含某种物体 物体检测识别：若细分该任务可得到两个子任务，即目标检测，与目标识别，首先检测是视觉感知得第一步，它尽可能搜索出图像中某一块存在目标（形状、位置）。而目标识别类似于图像分类，用于判决当前找到得图像块得目标具体是什么类别。 语义分割：按对象得内容进行图像得分割，分割的依据是内容，即对象类别。 实例分割：按对象个体进行分割，分割的依据是单个目标。 滑动窗口——为什么要有候选区域？既然目标是在图像中的某一区域，那么最直接的方法就是滑窗法（sliding window approach），就是遍历图像的所有区域，用不同大小的窗口在整个图像上滑动，那么就会产生所有的矩形区域，然后再后续排查，思路很简单，但是开销巨大。 region proposal（RP）：候选区域 IOU：region proposal与Ground Truth的窗口的交集比并集的比值，相当于准确率。‘ SPP：Spatial Pyramid Pooling 空间金字塔采样 在pooing的过程中计算pooling后的结果对应的两个像素点映射到feature map上所占的范围，然后在那个范围中进行max或者average。 ROI Pooling：就是将一个个大小不同的box矩形框，都映射到大小为w*h的矩形框。 Anchor：请看Two-stage基本介绍 GT box:Ground Truth box 如上图所示，绿色的框为飞机的Ground Truth，红色的框是提取的Region Proposal。那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准(IoU&lt;0.5)，那么这张图相当于没有正确的检测出飞机。如果我们能对红色的框进行微调，使得经过微调后的窗口跟Ground Truth更接近，这样岂不是定位会更准确。 带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。 如下图所示，(a)是普通的3×33×3卷积，其视野就是3×33×3，(b)是扩张率为2，此时视野变成7×77×7，(c)扩张率为4时，视野扩大为15×1515×15，但是视野的特征更稀疏了。 后面遇见会继续完善。。。。。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD系列算法]]></title>
    <url>%2F2019%2F08%2F21%2FSSD%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[主干网络介绍 主干网络原始作者采用的VGG16，我们也可以将其他神经网络作为主干网络。例如：ResNet、MobileNets等 输入300300的image，将VGG16网络FC6、FC7换成conv6和conv7，同时将池化层变为stride=1，pool_size=3\3，这样做的目的是为了不减少feature map size，为了配合这种变化，conv6会使用扩张率为6的带孔卷积。 ​ 带孔卷积：就是不增加参数数量和model复杂度的情况下扩大卷积的感受域，用dialation_rate设置扩张率。类似于形态学操作中的膨胀。具体看目标检测常用名词 接着然后移除dropout层和fc8层，并新增conv7，conv8，conv9，conv10，conv11，在检测数据集上做finetuing。其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是38×38，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet），以保证和后面的检测层差异不是很大，这个和Batch Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma。 多尺度feature map预测多尺度feature map预测，也就是在预测的时候，在接下来预测的时候，针对接下来六个不同的尺寸进行预测。如下图的六条连线，分别是38*38、19*19、10*10、5*5、3*3、1*1。将这六个不同尺度的feature map分别作为检测、预测层的输入，最后通过NMS进行筛选和合并。 对于六种不同尺度的网络，我们通常使用pooling来降采样。对于每一层的feature map，我们输入到相应的预测网络中。而预测网络中，我们会包括Prior box的提取过程。Prior box对应Fast R-CNN中Anchor的概念，也就是说，在Prior box中，feature map上的每一个点都作为一个cell（相当于Anchor）。以这个cell为中心，按照等比的放缩，找到它在原始图片的位置。接着以这个点为中心，提取不同尺度bounding box。而这些不同尺度的bounding box就是Prior box。然后对于每一个Prior box，我们通过和真值比较，就能够拿到它的label。对于每一个Prior box，我们都会分别预测它的类别概率和坐标（x，y，w，h）。也就是说，对于每一个cell，我们会将它对应到不同的Prior box，分别来预测当前这个Prior box所对应当前这个类别的概率分布和坐标。 对Prior Box的具体定义： 这里我们假设Prior Box的输入是m*n维的feature map。 如果每一个点都作为cell，那就会有mn个cell。接着每个cell上生成固定尺寸和不同长宽比例的box。每个cell对应k个bounging box，每个bounding box预测c个类别分数和4个偏移坐标。其中c个类别分数实际上是当前bounding box所对应的不同类别的概率分布。如果输入大小为m\n，那就会输出(c+4)*k*m*n。其中尺寸(scale)和比例(ratio)是超参数。 接下来我们看看Prior box是怎么生成的： 每个feature map上的点定义了六种长宽比的default box。也就是说，最后对于每一个anchor都会获得六个不同尺寸和长宽比的default box。对于3838层，每个feature map上的点，我们都会提取4个default box作为prior box。对于19\19层、10*10层、5*5，提取6个default box也就是全部都是prior box。而3*3、1*1提取4个default作为prior box。所以最后得到8732个prior box(38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4)。prior box就是选择的default box。尺寸和比例都是可以通过SSD的配置文件进行配置，后面实战详解。 对default box进行筛选成为prior box：每一个feature map cell不是k个default box都取，prior box与GT box(Ground Truth box)做匹配，IOU&gt;阈值为正样本。IOU&lt;阈值 为负样本。介于正样本和负样本中间阈值的default box去掉。 SSD系列算法优化及扩展SSD算法对小目标不够鲁棒，原因最主要是浅层feature map的表征能力不够强。 DSSD： DSSD相当原来的SSD模型主要作了两大更新。一是替换掉VGG，而改用了Resnet-101作为特征提取网络并在对不同尺度feature maps特征进行default boxes检测时使用了更新的检测单元；二则在网络的后端使用了多个deconvolution layers以有效地扩展低维度信息的contextual information，从而有效地提高了小尺度目标的检测。 下图为DSSD模型与SSD模型的整体网络结构对比： DSOD： SSD+DenseNet=DSOD DSOD可以从0开始训练数据，不需要预训练模型。 FSSD： 借鉴了FPN的思想，重构了一组pyramid feature map（金字塔特征），使得算法的精度有了明显特征，速度也没有下降很多。具体是把网络中某些feature调整为同一size再contact（连接），得到一个像素层，以此层为base layer来生成pyramid feature map，作者称之为Feature Fusion Module。 Feature Fusion 对上面图的解读： ​ (a) image pyramid ​ (b) rcnn系列，只在最后一层feature预测 ​ (c) FPN，语义信息一层传递回去，而且有很多相加的计算 ​ (d) SSD，在各个level的feature上直接预测，每个level之间没联系 ​ (e) FSSD的做法，把各个level的feature concat，然后从fusion feature上生成feature pyramid FSSD网络结构： RSSD： rainbow concatenation方式(pooling加deconvolution)融合不同层的特征，再增加不同层之间feature map关系的同时也增加了不同层的feature map个数。这种融合方式不仅解决了传统SSD算法存在的重复框问题，同时一定程度上解决了small object的检测问题。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
        <category>SSD</category>
      </categories>
      <tags>
        <tag>SSD算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One-stage基本介绍]]></title>
    <url>%2F2019%2F08%2F20%2FOne-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ One-stage也是使用CNN卷积特征，直接回归物体的类别概率和位置坐标值（无region proposal），准确度低、速度相对于Two-stage快。 One-stage常见算法： One-stage核心组件： CNN网络 CNN网络设计原则：从简到繁到简的卷积神经网 多尺度特征融合的网络 更轻量级的CNN网络 回归网络 One-stage和Two-stage的区别在于是否存在RPN网络。]]></content>
      <categories>
        <category>目标检测</category>
        <category>One-stage</category>
      </categories>
      <tags>
        <tag>One-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Two-stage基本介绍]]></title>
    <url>%2F2019%2F08%2F18%2FTwo-stage%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[​ Two-stage相对于one-stage准确度高，但是速度相对one-stage慢。 Two-stage常见算法：RCNN、Fast RCNN、Faster RCNN、Faster RCNN变种。 Two-stage核心组件： CNN网络 CNN网络设计原则： 从简到繁再到简的卷积神经网 多尺度特征融合的网络 更轻量级的CNN网络 RPN网络 区域推荐（Anchor机制）：首先我们需要知道anchor的本质是什么，本质是SPP(spatial pyramid pooling)思想的逆向。而SPP本身是做什么的呢，就是将不同尺寸的输入resize成为相同尺寸的输出。所以SPP的逆向就是，将相同尺寸的输出，倒推得到不同尺寸的输入。当前的feature map的大小是n*c*w*h（n是张数，c是层数，w是宽，h是高），对于当前这个feature map上，也就是wh上，选择其中的每一个点作为锚点（也就是候选区域的中心点），我们以每一个点作为中心点去提取候选区域。这样的每一个点都是Anchor。而这个候选区域通常都有比例：对于Fast R-CNN三个面积尺寸（128^2，256^2，512^2），然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）。一个feature map可以提取w\h*9个候选区域。我们示意图如下： ROI Pooling 分类和回归]]></content>
      <categories>
        <category>目标检测</category>
        <category>Two-stage算法</category>
      </categories>
      <tags>
        <tag>Two-stage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NMS算法]]></title>
    <url>%2F2019%2F08%2F18%2FNMS%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ NMS全称是非极大值抑制算法。 目的：为了消除多余的框，找到最佳的物体检测的位置。 思想:选取那些邻域里分数最高的窗口，同时抑制那些分数低的窗口。 其实NMS的处理不太合理。所以有人提出了Soft-NMS。 NMS和Soft-NMS的区别： 相邻区域内的检测框的分数进行调整而非彻底抑制，从而提高了高检索率情况下的准确率。再低检索率时仍能对物体检测性能有明显提升。]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DPM算法]]></title>
    <url>%2F2019%2F08%2F18%2FDPM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ DPM算法是传统目标检测方法的巅峰。 步骤： 1、计算DPM特征图 2、计算响应图 3、Latent SVM分类器训练 4、检测识别]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
      </categories>
      <tags>
        <tag>传统目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HOG+SVM算法]]></title>
    <url>%2F2019%2F08%2F18%2FHOG-SVM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[HOG+SVM算法主要用于行人检测。 步骤： 1、提取HOG特征。如果彩色图需要用HOG特征，则需要先转化为灰度图。 2、训练SVM分类器 ３、利用滑动窗口提取目标区域，进行分类判断 4、NMS 5、输出检测结果]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
      </categories>
      <tags>
        <tag>传统目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VJ算法]]></title>
    <url>%2F2019%2F08%2F18%2FVJ%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ VJ算法全称为Viola—Jones，多用于人脸检测。 步骤： 1、Haar特征抽取 Haar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，如：眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。 2、训练人脸分类器（Adaboost算法等） 3、滑动窗口]]></content>
      <categories>
        <category>目标检测</category>
        <category>传统目标检测算法</category>
      </categories>
      <tags>
        <tag>传统目标检测算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql基础命令]]></title>
    <url>%2F2019%2F08%2F13%2Fmysql%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[sql语句最后需要以；号结尾，sql语句最后需要以；号结尾，sql语句最后需要以；号结尾。重要的事说三遍。 数据库操作select version()：显示数据库版本 select now()：显示时间 show databases：查看所有数据库 show create database 数据库名：查看创建数据库的语句 create database 数据库名字：创建数据库 create database 数据库名字 charset=utf8：创建指定编码格式的数据库 drop database 数据库名：删除数据库 use 数据库名：使用数据库 select database ()：查看当前使用的数据库 数据表操作show tables:查看当前数据库中所有表 drop table 表名：删除表 show create table 表名：查看创建表的语句 create table 表名（字段 类型 约束[，字段 类型 约束]）：创建表 参数：auto_increment表示自动增长、not null表示不能为空、primary key表示主键、default 默认值 desc 表名：查看表的状态 insert into 表名 values()：向表插入数据，按照参数类型写参数 select * from 表名：查看表中所有的数据 alter table 表名 add 列名 类型：向表中添加字段 alter table 表名 modify 列名 类型 约束：不重命列名版 alter table 表名 change 列的原名 列的新名 类型 约束 ：重命列名版 alter table 表名 drop 列名：删除字段 数据的增删改查增加全列插入：insert into 表名 values(数据) 主键字段：可以用0 null default 来占位。因为auto_increment是自动增加的 部分插入：insert into 表名(列名1) values(值1) 没有的值取默认值 多行插入：insert into 表名 values (数据1)，(数据2) update 表名 set 列名 ：整列都改 update 表名 set 列名 where 条件：根据条件改 通过其他表来更新一个表： update 其他表 as 新名 inner join 被更新的表 as 新名 on 条件 set 需要改的数据 查询：select * from 表名：查询整个表，*号代表全部 select * from 表名 where ：根据条件查询 select 查询的列名 from 表名：根据列名查询。查询多列时，列名间用,隔开 select 查询的列名 as 列的新名字 from 表名：将列查询后以新的名字显示出来 select 表名.列名 from 表名 select 表的新名字.列名 from 表名 as 表的新名字 select distinct 列名 from 表名：可以去重，只显示相同数据第一次数据出现的位置 条件查询：and、or、not都可以用，类似python的语法。判断是否为空，is NULL。 模糊查询：like ：%代表一个或多个，_代表一个。 select name from 表名 where name like “%小%”：查询名字中有”小”字的名字 select name from 表名 where name like “__”：查询两个字的名字 select name from 表名 where name like “__%”：查询两个字以上的名字 rlike：利用正则查询 范围查询：in表示在一个非连续的范围内 between 数字 and 数字表示在一个连续的范围内 排序：order by 默认从小到大排序asc：从小到大排 desc：从大到小排 如果排序字段相同，我们可以设置多个排序字段。若不设置，默认按照主键大小排。 聚合函数(count，max，min，avg，sum，round)：不能跟其他字段一起用count：计算个数，其他类似 max：计算最大的 min：计算最小的 avg：计算平均值 sum：求和 round：四舍五入 ，round(123.23,1)保留一位小数=123.2 分组：group by按照性别分组： 配合聚合函数使用计算每种性别的人数： 计算男性的人数： group_concat():查询同一组的其他字段 还可以用字符串分割 having：将达到条件的组输出，可以配合聚合函数一起使用 having和where的区别：having是在分组后进行筛选，where是在原表的基础上进行筛选 分页：limit start，count 限制查询出来的数据个数。start代表从哪开始，count 代表查询的数据个数。start默认为0 链接查询：内链接、左链接、右链接内链接：inner join ……on 在两个表中取交集，如果存在则将两表数据合并。不存在则跳过。 可以利用as化简语句： 可以根据需求修改需要显示的数据： 还可以修改数据显示的位置： 通过某个表的字段排序： 左链接：left join……on。谁在左边，谁就是左表。查询的结果为两个表匹配到的数据，左表特有的数据，对于右表不存在的数据使用null填充 将左表特有的数据提取出来： 右链接：right join……on：类似左表。可以直接将右链接的表中左表和右表的显示交换即可得右链接。自关联：补。。。。 子查询：查询里嵌套一个查询 删除删除分为物理删除和逻辑删除： 物理删除： delete from 表名:删除整个表 delete from 表名 where 条件：删除符合条件的数据 逻辑删除：（用一个字段表示，这条信息是否还能用） alter table 表名 add 字段 类型 default 默认值]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络知识散记]]></title>
    <url>%2F2019%2F08%2F13%2F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[端口端口是英文port的意译，可以认为是设备与外界通讯交流的出口。端口可分为虚拟端口和物理端口，其中虚拟端口指计算机内部或交换机路由器内的端口，不可见。例如计算机中的80端口、21端口、23端口等。物理端口又称为接口，是可见端口，计算机背板的RJ45网口，交换机路由器集线器等RJ45端口。电话使用RJ11插口也属于物理端口的范畴。（用来区分哪个进程） 同一台电脑用pid区分进程，不同电脑用端口区分进程。 端口范围是0到65535 知名端口是众所周知的端口，范围是0到1023 动态端口的范围是1024到65535 查看端口状态：用netstat -an查看 TCP/IP协议TCP/IP:这不是两个协议，这是一个协议族，包含很多协议。主要是TCP/IP协议。 四层：物理层、网络层、传输层、应用层 七层：物理层、链路层、网络层、传输层、 会话层、表示层、应用层 IP地址用来标记唯一一台电脑。每一个IP地址都包括网络地址和主机地址 网络地址相同，则处于同一个网段，主机地址用来标记网里的电脑 A类网络的IP地址范围为：1.0.0.1－126.255.255.254； B类网络的IP地址范围为：128.1.0.1－191.255.255.254； C类网络的IP地址范围为：192.0.1.1－223.255.255.254 1．A类IP地址 一个A类IP地址由1字节（每个字节是8位）的网络地址和3个字节主机地址组成,即第一段数字范围为1～126。每个A类地址可连接16387064台主机(不能用0(产生冲突)和255(广播地址)，Internet有126个A类地址。 2．B类IP地址 一个B类IP地址由2个字节的网络地址和2个字节的主机地址组成，第一段数字范围为128～191。每个B类地址可连接64516(254*254)台主机(不能用0(产生冲突)和255(广播地址))，Internet有16256个B类地址。 3．C类IP地址 一个C类地址是由3个字节的网络地址和1个字节的主机地址组成，第一段数字范围为192～223。每个C类地址可连接254台主机(不能用0(产生冲突)和255(广播地址)，Internet有2054512个C类地址。 4．D类地址用于多点播送。 第一个字节的数字范围为224～239，是多点播送地址，用于多目的地信息的传输，和作为备用。全零（“0.0.0.0”）地址对应于当前主机，全“1”的IP地址（“255.255.255.255”）是当前子网的广播地址。多播和广播的区别：广播在同一个局域网都能收到，多播是指定那些人可以收得到，其他人收不到，常用于视频会议。 5.E类地址 第一段数字范围为240～254。E类地址保留，仅作实验和开发用。 全零（“0．0．0．0”）地址对应于当前主机。全“1”的IP地址（“255．255．255．255”）是当前子网的广播地址。 在IP地址3种主要类型里，各保留了3个区域作为私有地址，常见于局域网中。 ##常用术语 网络号：网络号等于ip地址和网络掩码按位与操作 网络掩码（子网掩码）的作用：取网络号、主机号 两台电脑能通信的前提是处于同一个网络号 集线器（hub）的作用：实现多台电脑连接在一起，组成一个小型局域网，交换机也是。 集线器和交换机的区别：集线器是广播发数据，交换机不是每次都是广播，效率高。 实际地址：代表网卡地址（MAC）。由六个字节组成，前三个字节代表厂商，后三个字节代表厂商生产 arp：根据ip找mac地址 rarp：根据mac地址找ip icmp：ping的时候用 arp -a即是查看本地局域网内所有用户ip和mac地址绑定关系的一个命令。 ARP -d 就是清除缓存中的数据。也是删除ip和mac绑定的项目。 路由器：连接不同的网络，使他们之间能够通信 rip：路由解析协议 mac：标记实际转发数据时的地址 ip：标记逻辑上的地址 natmask：和ip地址一起确定网络号 默认网关：发送的ip不在同一个网段内，那么会把这个数据转发给默认网关。 为什么TCP比UDP稳定？在TCP中，如果有一方接收到对方的数据，一定会发送ack确认包给对方。而在UDP中，没有这个过程。 TCP三次握手：确定一定发送数据到对方 四次挥手：调用close时使用 TCP长连接、短连接： TTL：表示经过的路由器数目。每经过一个路由器，TTL-1。 MSL：表示一个数据包存在的最多时间 CDN：内容分发 查看域名解析的IP地址： nslookup 域名 例子：nslookup baidu.com 常见的网络攻击： DDOS攻击：拒绝服务器攻击。 DNS攻击：1.DNS服务器被劫持：篡改IP ​ 2.DNS欺骗： ARP攻击：中间人攻击]]></content>
      <categories>
        <category>网络知识</category>
      </categories>
      <tags>
        <tag>网络知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测概述]]></title>
    <url>%2F2019%2F08%2F12%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[​ 目标检测方法分为传统目标检测方法和深度学习目标检测方法。 传统目标检测方法：Viola-Jones、HOG+SVM、DPM等 Viola-Jones：采用积分图特征，进行人脸检测 HOG+SVM：行人检测。通过HOG特征结合SVM分类器进行检测。 DPM：同样通过HOG特征，并加入许多其他额外的策略进行检测。传统目标检测最好的方法。 深度学习目标检测方法：One-state、Two-stage One-stage：YOLO和SSD系列，直接回归目标位置。 Two-stage：Faster RCNN系列，利用网络对候选区进行推荐。 目标检测问题基本流程： Viola-Jones（人脸检测）步骤 1、Haar特征抽取 2、训练人脸分类器（Adaboost算法） 3、滑动窗口 HOG+SVM（行人检测）步骤 DPM（物体检测）步骤 NMS（非极大值抑制算法） 目的：为了消除多余的框，找到最佳的物体检测的位置 Soft-NMS是对NMS算法的改进]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv颜色识别]]></title>
    <url>%2F2019%2F08%2F12%2Fopencv%E9%A2%9C%E8%89%B2%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[数字图像处理中常用的采用模型是RGB（红，绿，蓝）模型和HSV（色调，饱和度，亮度），RGB广泛应用于彩色监视器和彩色视频摄像机，我们平时的图片一般都是RGB模型。而HSV模型更符合人描述和解释颜色的方式，HSV的彩色描述对人来说是自然且非常直观的。 这里的颜色识别是指根据人们的意愿提取图片中对应的颜色区域。 颜色识别步骤： 1、读取一张图片或视频. 2、用cvtcolor将它从RGB转为HSV。 3、通过inrange得出掩膜。 4、用 图像的”与”操作(bitwise_and)得出对应区域的图像。 coding： 读者可以加一些其他的操作提高效果，例如什么开运算、滤波之类的。这里就不赘述了，接下来，贴一张HSV的颜色阈值表，可以参考：]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>颜色识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魔法方法]]></title>
    <url>%2F2019%2F08%2F10%2F%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[​ 魔法方法是指Python内部已经包含的，被双下划线所包围的方法，这些方法在进行特定的操作时会自动被调用。使用Python的魔法方法可以是Python的自由度变得更高，当不需要重写魔法方法也可以在规定的默认情况下生效。在需要重写时也可以让使用者根据自己的需求来重写部分方法来达到自己的期待。 常用的魔法方法：__doc__：表示类的描述信息 __module__:表示当前操作的对象在哪个模块 __class__:表示当前操作的对象的类是什么 __call__：让对象直接调用call方法 __dict__：类或对象的所有属性 __getitem__、__setitem__、__delitem__： 魔法方法集合]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>魔法方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow（2）]]></title>
    <url>%2F2019%2F08%2F09%2Ftensorflow%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[数据读取通过线程和队列提高速度。可以边取数据边训练 模型的保存和加载 模型保存： 模型加载： 添加权重参数、损失值等的变化 首先收集变量： 而后在会话中运行： 感知机有n个输入数据，通过权重与各数据之间的计算和，比较激活函数结果，得出输出。感知机是解决分类问题。 神经网络的基本组成包括输入层、隐藏层、输出层。卷积神经网络的特点在于隐藏层分为卷积层和池化层。 神经网络的种类： 神经网络的策略是交叉熵损失，优化是通过反向传播算法（相当于梯度下降 ）。 简单神经网络：]]></content>
      <categories>
        <category>深度学习</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow（1）]]></title>
    <url>%2F2019%2F08%2F09%2Ftensorflow%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[tensorflow基本知识在tensorflow中把数据称为张量（tensor）。 张量的阶：相当于数组的维度。 张量的属性：graph、op、name、shape tensorflow中张量形状分为动态形状和静态形状，其在于有没有生成一个新的张量数据。静态形状的修改不能跨维度修改 把操作称为节点（OP），所有操作都是一个OP。 整个程序的结构称为图（graph） 运算程序的图称为会话（Session）。一次只能运行一个图 会话的作用：1、运行图的结构 2、分配资源运算 3、掌握资源 会话需要进行资源释放，需要run后进行close。否则可以使用with作为上下文管理器 可以在会话当中指定图去运行 sess.run(fetches，feed_dict=None,graph=None)启动整个图。 用来运行op和计算tensor feed_dict常与placeholder(占位符)一起使用 变量：tensorflow中的变量也是一种op，是一种特殊的张量能够进行存储持久化，它的值就是张量，默认被训练。其中有个trainable参数默认为True，如果改为False，变量将不再变化。 tf.reduce_mean()函数用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值。 如果想设置为原来向量的维度，keep_dims=True。 图的可视化（tensorboard）首先通过pip安装，而后在会话中进行写入事件。 最后在命令行启动 双引号中间填绝对路径，注意不要出现中文和空格。 变量作用域：让模型更直观的显示。 深度学习中的线性回归：]]></content>
      <categories>
        <category>深度学习</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法分类和数据分割]]></title>
    <url>%2F2019%2F08%2F09%2FML%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB%E5%92%8C%E6%95%B0%E6%8D%AE%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[机器学习算法分类 区分监督学习和非监督学习的方法：看是否有标准答案。无监督学习无标准答案，只有特征值。 区分分类问题和回归问题的方法:目标值是否是离散型。目标值是离散型，则是分类问题。目标值是连续型，则是回归问题。 数据分割模型=算法+数据 数据分为训练集和测试集 fit_transform=fit+transform：fit做的是计算平均值和标准差，transform做的是转化]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据预处理与特征工程]]></title>
    <url>%2F2019%2F08%2F09%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 前些天我把python的处理图像的库——opencv总结了一下，但这终究是传统方法处理图像。现在都是用深度学习网络处理图像。所以，在学深度学习之前，我看了些机器学习的知识。但在看机器学习的算法前，我们先来看看特征工程。 sklearn中的数据堪称完美，各大教材的数据也是一样。但是到我们现实应用中时，发现模型调用效果差，这是因为现实中的数据离完美的数据集差十万八千里。所以数据预处理和特征工程是必要的。 sklearn中六大板块有两大板块是关于数据预处理和特征工程的：也就是黄色标识的两个模块 特征抽取在机器学习中，大多数算法都只能处理数值型数据，不能处理文字。在sklearn当中，除了专用来处理文字的算法，其它的算法在fit的时候全部要求输入数组和矩阵，也不能导入文字型数据。然而在现实生活中，许多标签和特征在数据收集完毕后，都不是以数字来表现的。为了让数据适应算法，我们必须将数据进行编码，即将文字型数据转换为数值型。 preprocessing.LabelEncoder：标签专用，能够将分类转换为分类数值。这个没运行成功，好像是我的数据集有问题。我自己设了个数据 123456789101112131415from pandas import DataFrame,Seriesfrom sklearn.preprocessing import LabelEncoderimport pandas as pddata=DataFrame(data=[[-1,2],[-0.5,6],[0,18],[1,18]])print(data)y=data.iloc[:,-1]#要输入的是标签，所以容许一维数组print(y)le=LabelEncoder()le.fit(y) #导入数据，调取结果label=le.transform(y)#print(le.fit_transform(y))print("调取结果是：",label)print(le.classes_) #查看标签中有多少类型data.iloc[:,-1]=labelprint(data) preprocessing.OrdinalEncoder：特征专用，能够将分类特征转换为分类数值 字典特征抽取，把数据中的以字符串标记的数据转化为one-hot编码。 由上面代码可得下面的稀疏（sparse）矩阵： 将稀疏矩阵转为矩阵： 将矩阵转化为字典： 文本特征抽取： 和字典特征抽取不一样的是CountVectorizer没有sparse参数，只能通过矩阵的toarray转化为数组 可得： 注意：单个字母不统计。因为单个英文字母没有依据 如果文本是中文，根据需要的词频给文本添加空格。同样，单个字无法统计 利用jieba分词对数据进行one-hot编码： TF-IDF特征抽取：用以评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度 数据预处理数据归一化处理通过对原始数据进行交换把数据映射到（默认为[0,1]）之间 归一化缺点：注意在特定场景最大值最小值是变化的。另外，最大值与最小值非常容易受异常点的影响，所以这种方法的鲁棒性差，只适合传统精确小数据场景。 数据标准化处理通过对原始数据进行变换把数据变换到均值为0，方差为1的范围。 标准化总结：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 除了StandardScaler和MinMaxScaler之外，sklearn也提供了各种其他的缩放处理： 缺失值处理方法老版本用Imputer，新版本用SimpleImputer 删除：如果每列或者行数据缺失值达到一定比例时，建议放弃整行或者整列 插补：可以通过缺失值每行或者每列的平均值、中位数来填充 Imputer： SimpleImputer： 1234567891011import pandas as pdfrom sklearn.preprocessing import LabelEncoderfrom sklearn.impute import SimpleImputerdata=pd.read_csv("train.csv",index_col=0)#打印行数，列数，列索引，列非空值个数，列类型，内存占用print(data.info())Age = data.loc[:,"Age"].values.reshape(-1,1) #sklearn当中特征矩阵必须是二维imp_mean = SimpleImputer() #实例化，默认均值填补imp_mean = imp_mean.fit_transform(Age) #fit_transform一步完成调取结果data.loc[:,"Age"]=imp_mean #使用中位数填补Ageprint(data["Age"]) 处理连续型特征二值化和分段 sklearn.preprocessing.Binarizer 根据阈值将数据二值化(将特征值设置为0或1)，用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射为1。 1234from sklearn.preprocessing import BinarizerAge=data.loc[:,"Age"].values.reshape(-1,1)Bin=Binarizer(threshold=30).fit_transform(Age)print(Bin) sklearn.preprocessing.KBinsDiscretizer 这是将连续变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。三个重要参数： 12345678from sklearn.preprocessing import KBinsDiscretizerx=data.loc[:,"Age"].values.reshape(-1,1)est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')est.fit_transform(x)#查看转换后分的箱:变成了一列中的三箱 set(est.fit_transform(X).ravel()) ravel降维est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform') #查看转换后分的箱变成了哑变量result=est.fit_transform(x).toarray()print(result) 特征工程当数据预处理完成后，我们就要开始进行特征工程了 我们有四种方法可以用来选择特征：过滤法，嵌入法，包装法，和降维算法。 过滤法方差过滤这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征。VarianceThreshold有重要参数threshold，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填默认为0，即删除所有的记录都相同的特征。 12345678910from sklearn.feature_selection import VarianceThresholdimport pandas as pddata = pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]print(X.shape)selector = VarianceThreshold() #实例化，不填参数默认方差为0X_var0 = selector.fit_transform(X) #获取删除不合格特征之后的新特征矩阵#也可以直接写成 X = VairanceThreshold().fit_transform(X)print(X_var0.shape) 可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了： 123456789from sklearn.feature_selection import VarianceThresholdimport pandas as pddata = pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]import numpy as npX_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)np.median(X.var().values)print(X_fsvar.shape) 当特征是二分类时，特征的取值就是伯努利随机变量，这些变量的方差可以计算为： 其中X是特征矩阵，p是二分类特征中的一类在这个特征中所占的概率。 12345678from sklearn.feature_selection import VarianceThresholdimport pandas as pddata = pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]#若特征是伯努利随机变量，假设p=0.8，即二分类特征中某种分类占到80%以上的时候删除特征X_bvar = VarianceThreshold(.8 * (1 - .8)).fit_transform(X)print(X_bvar.shape) 如果你用knn和随机森林对没过滤的数据和过滤后的数据进行交叉验证，会发现，随机森林的准确率略低于knn，但随机森林运行的速度比knn快很多。为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？这是由于两种算法的原理中涉及到的计算量不同。最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。因此，过滤法的主要对象是：需要遍历特征或升维的算法们，而过滤法的主要目的是：在维持算法表现的前提下，帮助算法们降低计算成本。 对受影响的算法来说，我们可以将方差过滤的影响总结如下： 我们怎样知道，方差过滤掉的到底时噪音还是有效特征呢？过滤后模型到底会变好还是会变坏呢？答案是：每个数据集不一样，只能自己去尝试。这里的方差阈值，其实相当于是一个超参数，要选定最优的超参数，我们可以画学习曲线，找模型效果最好的点。 相关性过滤方差挑选完毕之后，我们就要考虑下一个问题：相关性了。我们希望选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量信息。如果特征与标签无关，那只会白白浪费我们的计算内存，可能还会给模型带来噪音。在sklearn当中，我们有三种常用的方法来评判特征与标签之间的相关性：卡方，F检验，互信息。 卡方过滤卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。再结合feature_selection.SelectKBest这个可以输入”评分标准“来选出前K个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。 123456789101112131415from sklearn.ensemble import RandomForestClassifier as RFCfrom sklearn.model_selection import cross_val_scorefrom sklearn.feature_selection import VarianceThresholdfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2import pandas as pddata=pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]import numpy as npX_fsvar = VarianceThreshold(np.median(X.var().values)).fit_transform(X)#假设在这里我一直我需要300个特征X_fschi = SelectKBest(chi2, k=300).fit_transform(X_fsvar, y)print(X_fschi.shape)print(cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean()) 可以看出，模型的效果降低了，这说明我们在设定k=300的时候删除了与模型相关且有效的特征，我们的K值设置得太小，要么我们需要调整K值，要么我们必须放弃相关性过滤。当然，如果模型的表现提升，则说明我们的相关性过滤是有效的，是过滤掉了模型的噪音的，这时候我们就保留相关性过滤的结果。 123456789#接上面的代码import matplotlib.pyplot as pltscore = []for i in range(200,390,10): X_fschi = SelectKBest(chi2, k=i).fit_transform(X_fsvar, y) once = cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean() score.append(once)plt.plot(range(200,390,10),score)plt.show() 通过这条曲线，我们可以观察到，随着K值的不断增加，模型的表现不断上升，这说明，K越大越好，数据中所有的特征都是与标签相关的。但是运行这条曲线的时间同样也是非常地长。还有一种更好的k的方法：看p值选择k 卡方检验的本质是推测两组数据之间的差异，其检验的原假设是”两组数据是相互独立的”。卡方检验返回卡方值和P值两个统计量，其中卡方值很难界定有效的范围，而p值，我们一般使用0.01或0.05作为显著性水平，即p值判断的边界，具体我们可以这样来看： 调用SelectKBest之前，我们可以直接从chi2实例化后的模型中获得各个特征所对应的卡方值和P值。 123456789#接上面的代码chivalue, pvalues_chi = chi2(X_fsvar,y)#k取多少？我们想要消除所有p值大于设定值，比如0.05或0.01的特征#pvalues_chi就是p值print(pvalues_chi)#从此看出所有特征的p值为0k = chivalue.shape[0] - (pvalues_chi &gt; 0.05).sum()print(k)X_fschi = SelectKBest(chi2, k=k).fit_transform(X_fsvar, y)print((cross_val_score(RFC(n_estimators=10,random_state=0),X_fschi,y,cv=5).mean())) 可以观察到，所有特征的p值都是0，这说明对于digit recognizor这个数据集来说，方差验证已经把所有和标签无关的特征都剔除了，或者这个数据集本身就不含与标签无关的特征。在这种情况下，舍弃任何一个特征，都会舍弃对模型有用的信息，而使模型表现下降，因此在我们对计算速度感到满意时，我们不需要使用相关性过滤来过滤我们的数据。 F检验F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的线性关系的过滤方法。它即可以做回归也可以做分类，因此包含feature_selection.f_classif（F检验分类）和feature_selection.f_regression（F检验回归）两个类。其中F检验分类用于标签是离散型变量的数据，而F检验回归用于标签是连续型变量的数据。和卡方检验一样，这两个类需要和类SelectKBest连用，并且我们也可以直接通过输出的统计量来判断我们到底要设置一个什么样的K。需要注意的是，F检验在数据服从正态分布时效果会非常稳定，因此如果使用F检验过滤，我们会先将数据转换成服从正态分布的方式。 F检验的本质是寻找两组数据之间的线性关系，其原假设是”数据不存在显著的线性关系“。它返回F值和p值两个统计量。和卡方过滤一样，我们希望选取p值小于0.05或0.01的特征，这些特征与标签时显著线性相关的，而p值大于0.05或0.01的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。 1234567#接上面的代码from sklearn.feature_selection import f_classifF, pvalues_f = f_classif(X_fsvar,y)k = F.shape[0] - (pvalues_f &gt; 0.05).sum()print(k)X_fsF = SelectKBest(f_classif, k=k).fit_transform(X_fsvar, y)print(cross_val_score(RFC(n_estimators=10,random_state=0),X_fsF,y,cv=5).mean()) 得到的结论和我们用卡方过滤得到的结论一模一样：没有任何特征的p值大于0.01，所有的特征都是和标签相关的，因此我们不需要相关性过滤。 互信息法互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。和F检验相似，它既可以做回归也可以做分类，并且包含两个类feature_selection.mutual_info_classif（互信息分类）和feature_selection.mutual_info_regression（互信息回归）。这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。互信息法不返回p值或F值类似的统计量，它返回“每个特征与目标之间的互信息量的估计”，这个估计量在[0,1]之间取值，为0则表示两个变量独立，为1则表示两个变量完全相关。 所有特征的互信息量估计都大于0，因此所有特征都与标签相关。 Embedder嵌入法嵌入法是一种让算法自己决定使用哪些特征的方法，即特征选择和算法训练同时进行。在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小选择特征。这些权值系数往往代表了特征对于模型的某种贡献或某种重要性。我们就可以基于这种贡献的评估，找出对模型建立最有用的特征。。因此相比于过滤法，嵌入法的结果会更加精确到模型的效用本身，对于提高模型效力有更好的效果。并且，由于考虑特征对模型的贡献，因此无关的特征（需要相关性过滤的特征）和无区分度的特征（需要方差过滤的特征）都会因为缺乏对模型的贡献而被删除掉，可谓是过滤法的进化版。 SelectFromModel是一个元变换器，可以与任何在拟合后具有coef_，feature_importances_属性或参数中可选惩罚项的评估器一起使用（比如随机森林和树模型就具有属性feature_importances_，逻辑回归就带有l1和l2惩罚项，线性支持向量机也支持l2惩罚项） 12345678910111213141516171819from sklearn.model_selection import cross_val_scoreimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltdata=pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import RandomForestClassifier as RFCRFC_ = RFC(n_estimators =10,random_state=0)X_embedded = SelectFromModel(RFC_,threshold=0.005).fit_transform(X,y)threshold = np.linspace(0,(RFC_.fit(X,y).feature_importances_).max(),20)score = []for i in threshold: X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y) once = cross_val_score(RFC_,X_embedded,y,cv=5).mean() score.append(once)plt.plot(threshold,score)plt.show() 从图像上来看，随着阈值越来越高，模型的效果逐渐变差，被删除的特征越来越多，信息损失也逐渐变大。 12345678910#细化学习曲线score2 = []for i in np.linspace(0,0.002,20): X_embedded = SelectFromModel(RFC_,threshold=i).fit_transform(X,y) once = cross_val_score(RFC_,X_embedded,y,cv=5).mean() score2.append(once)plt.figure(figsize=[20,5])plt.plot(np.linspace(0,0.002,20),score2)plt.xticks(np.linspace(0,0.002,20))plt.show() wrapper包装法包装法也是一个特征选择和算法训练同时进行的方法，与嵌入法十分相似，它也是依赖于算法自身的选择，比如coef_属性或feature_importances_属性来完成特征选择。但不同的是，我们往往使用一个目标函数作为黑盒来帮助我们选取特征，而不是自己输入某个评估指标或统计量的阈值。包装法在初始特征集上训练评估器，并且通过coef_属性或通过feature_importances_属性获得每个特征的重要性。 区别于过滤法和嵌入法的一次是过滤法和嵌入法训练解决所有问题，包装法要使用特征子集进行多次训练，因此它所需要的计算成本是最高的。 参数estimator是需要填写的实例化后的评估器，n_features_to_select是想要选择的特征个数，step表示每次迭代中希望移除的特征个数。除此之外，RFE类有两个很重要的属性，.support_：返回所有的特征的是否最后被选中的布尔矩阵，以及.ranking_返回特征的按数次迭代中综合重要性的排名。类feature_selection.RFECV会在交叉验证循环中执行RFE以找到最佳数量的特征，增加参数cv，其他用法都和RFE一模一样。 1234567891011121314151617181920212223from sklearn.model_selection import cross_val_scoreimport pandas as pdimport matplotlib.pyplot as pltdata=pd.read_csv("digit recognizor.csv")X = data.iloc[:,1:]y = data.iloc[:,0]from sklearn.ensemble import RandomForestClassifier as RFCfrom sklearn.feature_selection import RFERFC_ = RFC(n_estimators =10,random_state=0)selector = RFE(RFC_, n_features_to_select=340, step=50).fit(X, y)print("返回所有的特征的最后被选中的布尔矩阵个数：",selector.support_.sum())print("返回特征的按数次迭代中综合重要性的排名：",selector.ranking_)X_wrapper = selector.transform(X)print(cross_val_score(RFC_,X_wrapper,y,cv=5).mean())score = []for i in range(1,751,50): X_wrapper = RFE(RFC_,n_features_to_select=i, step=50).fit_transform(X,y) once = cross_val_score(RFC_,X_wrapper,y,cv=5).mean() score.append(once)plt.figure(figsize=[20,5])plt.plot(range(1,751,50),score)plt.xticks(range(1,751,50))plt.show() 特征选择总结至此，我们讲完了降维之外的所有特征选择的方法。这些方法的代码都不难，但是每种方法的原理都不同，并且都涉及到不同调整方法的超参数。经验来说，过滤法更快速，但更粗糙。包装法和嵌入法更精确，比较适合具体到算法去调整，但计算量比较大，运行时间长。当数据量很大的时候，优先使用方差过滤和互信息法调整，再上其他特征选择方法。使用逻辑回归时，优先使用嵌入法。使用支持向量机时，优先使用包装法。迷茫的时候，从过滤法走起，看具体数据具体分析。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(7)]]></title>
    <url>%2F2019%2F08%2F09%2Fopencv-7%2F</url>
    <content type="text"><![CDATA[直方图横坐标：图像中各个像素的灰度级 纵坐标：具有该灰度级的像素个数 归一化直方图： 横坐标：图像中各个像素的灰度级 纵坐标：出现这个灰度级的概率 掩膜：通过掩膜可以将一张图的某块区域的直方图画出来 直方图均值化： 注意:用cv2.equalizeHist进行直方图均衡化处理时，对彩色图像进行处理时，分通道进行。灰度图像直接均衡化 利用CLAHE有限对比适应性直方图均衡化，对彩色图像也是分通道进行。灰度图像直接均衡化。 背景建模：以高斯混合模型为基础的背景/前景分割算法 apply可以得到前景的掩膜 傅里叶变换补 。。。。。]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(6)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-6%2F</url>
    <content type="text"><![CDATA[特征提取图像特征：可以表达图像中对象的主要信息、并且以此为依据可以从其他未知图像中检测出相似或者相同图像。 常见的图像特征：边缘、角点、纹理 角点检测：cv2.cornerHarris() img:输入图像 blockSize：角点检测中要考虑的领域大小 ksize：Sobel求导中使用的窗口大小 k：方程参数，参数为[0.04，0.06] 亚像素级的角点检测：红色是原先的角点，绿色像素是修正后的像素 适合于跟踪的角点检测：cv2.goodFeaturesToTrack(): 输入灰度图像、检测的角点数目、设置角点的质量水平，0-1之间，低于这个数的都会被忽略、设置两个角点间的最短欧式距离。 SIFT尺度不变特征变换匹配算法： cv2.xfeatures2d.SIFT_create()：创建sift特征器 sift.detect()可以在图像找到关键点。如果想在图像的特定区域搜索，可以创建一个掩膜图像作为参数。属性如下： pt：表示图像中关键点的x坐标和y坐标 size：表示特征的直径 angle：表示特征的方向 response：表示关键点强度 octave：表示特征所在金字塔的层级 class_id：表示关键点的ID sift.compute():计算关键点的描述符，在sift.detect后使用。 sift.detectAndCompute()：直接找到关键点并计算出描述符 cv2.drawKeypoints(): image:原始图像 keypoints：特征点向量 outimage：特征点绘制的画布图像，可以是原图像 color：绘制的特征点颜色，可以是原图像。 flags：五种绘制模式： DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS:就会绘制代表关键点大小的圆圈甚至可以绘制除关键点的方向。 DEFAULT：只绘制特征点的坐标点,显示在图像上就是一个个小圆点,每个小圆点的圆心坐标都是特征点的坐标。 DRAW_OVER_OUTIMG：函数不创建输出的图像,而是直接在输出图像变量空间绘制,要求本身输出图像变量就是一个初始化好了的,size与type都是已经初始化好的变量 NOT_DRAW_SINGLE_POINTS：单点的特征点不被绘制 DRAW_RICH_KEYPOINTS：绘制特征点的时候绘制的是一个个带有方向的圆,这种方法同时显示图像的坐标,size，和方向,是最能显示特征的一种绘制方式 SURF：加速稳健特征算法，加快版的SIFT。与SIFT类似 BRIEF:这是一种特征描述符，他不提供查找特征的方法。所以我们需要使用其他的特征检测器。例如：SIFT或SURF。推荐使用STAR。 orb检测： bf暴力匹配： cv2.BFMatcher()：创建一个BFMatcher对象。参数： ​ normType：指定使用的距离测试类型。默认值为cv2.NORM_L2。cv2.NORM_L1也行。这两种适用于SIFT和SURF。对于ORB,BRIEF，应使用cv2.NORM_HAMMING。如果VTA_K==3或4，normType应设置为cv2.NORM_HAMMING2。 ​ crossCheck：默认为False。如果设置为True，匹配条件就会更加严格。 BFMatcher对象具有两个方法：match()和knnMatch()。第一种方法会返回最佳匹配，第二个方法为每个关键点返回k个最佳匹配（降序排列后取前k个）。 cv2.drawMatches cv2.drawMatchsKnn：就像使用 cv2.drawKeypoints() 绘制关点一样我们可以使用 cv2.drawMatches()来绘制匹配的点。它会将两幅图像先水平排列然后在最佳匹配的点之间绘制直线从原图像到目标图像。如果前面使用的是BFMatcher.knnMatch()现在我们可以使用函数 cv2.drawMatchsKnn 为每个关键点和它的 k 个最佳匹配点绘制匹配线。如果 k 等于 2就会为每个关键点绘制两条最佳匹配直线。如果我们选择性绘制就给函数传入一个掩模。 对ORB描述符进行暴力匹配： match=bf.match(des1，des2)：返回值是一个DMatch对象列表。具有以下属性： DMatch.distance - 描述符之间的距离。越小越好。 • DMatch.trainIdx - 目标图像中描述符的索引。 • DMatch.queryIdx - 查询图像中描述符的索引。 • DMatch.imgIdx - 目标图像的索引 对SIFT描述符进行暴力匹配： FLANN匹配器： cv2.FlannBasedMatcher( [, indexParams[, searchParams]] ) indexParams： searchParams：]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(5)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-5%2F</url>
    <content type="text"><![CDATA[视频处理Videocapture：VideoCapture(args)：如果args=0，则打开摄像头。如果args=路径， 则打开视频源 摄像头设置和查询： 调用摄像头读取图像数据，以及使用 cap.set( propId ， value ) cap.get( propId ) 图片合成视频： 视频分解图片： normalize：归一化函数为了消除指标之间的影响，需要对数据进行标准化处理，以解决数据指标之间的可比性 cv2.normalize(src[, dst[, alpha[, beta[, norm_type[, dtype[, mask]]]]]]) src:输入数组 dst:与src大小相同的输出数组 alpha:下限边界 beta:上限边界 norm_type:NORM_MINMAX NORM_INF NORM_L1 NORM_L2 dType:当输出为负时，输出数组具有与src相同的类型。否则，具有与src相同的信道数和深度 mask:掩膜 画图cv2.rectangle（）：若将五改为-1，则填充整个矩形 cv2.circle：要画圆的话，只需要指定圆形的中心点坐标和半径大小，颜色和粗细 cv2.ellispe:一个参数是中心点的位置坐标。下一个参数是长轴和短轴的长度。椭圆沿逆时针方向旋转的角度。椭圆弧沿顺时针方向起始的角度和结束角度，如果是 0 到 360，就是整个椭圆。 图像金字塔我们对同一图像的不同分辨率的子图像处理。比如我们在一幅图像中查找某个目标比如脸，我们不知目标在图像中的尺寸大小。这种情况下我们创建一组图，这些图像是具有不同分率的原始图像。我们把组图像叫做图像字塔简单来就是同一图像的不同分率的子图集合。如果我们把大的图像放在底部最小的放在顶部。看来像一座金字塔，故而得名图像金字塔。 图像金字塔：高斯金字塔和拉普拉斯金字塔 向下采样：将图像缩小，图像信息丢失 向上采样：将图像放大，图像会变模糊 向下采样和向上采样不是可逆的，无法将图像变为原始图像。 向下采样函数：cv2.pyrDown(原始图像) 向上采样函数：cv2.pyrUP 拉普拉斯金字塔： opencv删除窗口：删除所有窗口 ：cv2.destroyAllWindow() 删除指定窗口：cv2.destroyWindow(“original”) waitKey(x):参数为等待键盘触发的时间。单位为毫秒。如不输入参数，则输入任意键退出。 setMouseCallback():event为事件 x，y代表鼠标位于窗口的（x，y）位置 flags：代表鼠标的拖曳事件以及鼠标和键盘联合的事件。共有32种 param：函数指针，标识所相应的事件函数。 namewindow WINDOW_NORMAL 用户可以改变窗口的大小； WINDOW_AUTOSIZE 窗口大小会根据显示图像自动调整，用户必能手动改变窗口大小； WINDOW_OPENGL 支持OpenGL。]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(4)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-4%2F</url>
    <content type="text"><![CDATA[图像类型转换 cvtColor:颜色空间转换函数 cv2.cvtColor()支持多种颜色空间之间的转换，其 需要注意的是cvtColor()函数不能直接将RGB图像转换为二值图像(Binary Image)，需要借助threshold()函数 HSV中：H为色彩，取值范围是[0,179]，S为饱和度，取值范围是[0,255],V是亮度，取值范围是[0,255]。 支持的转换类型和转换码如下： 1、RGB和BGR（opencv默认的彩色图像的颜色空间是BGR）颜色空间的转换 COLOR_BGR2RGB COLOR_RGB2BGR COLOR_RGBA2BGRA COLOR_BGRA2RGBA 2、向RGB和BGR图像中增添alpha通道 COLOR_RGB2RGBA COLOR_BGR2BGRA 3、从RGB和BGR图像中去除alpha通道 COLOR_RGBA2RGB COLOR_BGRA2BGR 4、从RBG和BGR颜色空间转换到灰度空间 COLOR_RGB2GRAY COLOR_BGR2GRAY COLOR_RGBA2GRAY COLOR_BGRA2GRAY 5、从灰度空间转换到RGB和BGR颜色空间 COLOR_GRAY2RGB COLOR_GRAY2BGR COLOR_GRAY2RGBA COLOR_GRAY2BGRA 6、RGB和BGR颜色空间与BGR565颜色空间之间的转换 COLOR_RGB2BGR565 COLOR_BGR2BGR565 COLOR_BGR5652RGB COLOR_BGR5652BGR COLOR_RGBA2BGR565 COLOR_BGRA2BGR565 COLOR_BGR5652RGBA COLOR_BGR5652BGRA 7、灰度空间域BGR565之间的转换 COLOR_GRAY2BGR555 COLOR_BGR5552GRAY 8、RGB和BGR颜色空间与CIE XYZ之间的转换 COLOR_RGB2XYZ COLOR_BGR2XYZ COLOR_XYZ2RGB COLOR_XYZ2BGR 9、RGB和BGR颜色空间与uma色度（YCrCb空间）之间的转换 COLOR_RGB2YCrCb COLOR_BGR2YCrCb COLOR_YCrCb2RGB COLOR_YCrCb2BGR 10、RGB和BGR颜色空间与HSV颜色空间之间的相互转换 COLOR_RGB2HSV COLOR_BGR2HSV COLOR_HSV2RGB COLOR_HSV2BGR 11、RGB和BGR颜色空间与HLS颜色空间之间的相互转换 COLOR_RGB2HLS COLOR_BGR2HLS COLOR_HLS2RGB COLOR_HLS2BGR 12、RGB和BGR颜色空间与CIE Lab颜色空间之间的相互转换 COLOR_RGB2Lab COLOR_BGR2Lab COLOR_Lab2RGB COLOR_Lab2BGR 13、RGB和BGR颜色空间与CIE Luv颜色空间之间的相互转换 COLOR_RGB2Luv COLOR_BGR2Luv COLOR_Luv2RGB COLOR_Luv2BGR 14、Bayer格式（raw data）向RGB或BGR颜色空间的转换 COLOR_BayerBG2RGB COLOR_BayerGB2RGB COLOR_BayerRG2RGB COLOR_BayerGR2RGB COLOR_BayerBG2BGR COLOR_BayerGB2BGR COLOR_BayerRG2BGR COLOR_BayerGR2BGR inrange：实现二值化功能 image=cv2.inrance(hsv,lower_red,upper_red) 第一个参数：hsv指的是原图 第二个参数：lower_red指的是图像中低于这个lower_red的值，图像值变为0 第三个参数：upper_red指的是图像中高于这个upper_red的值，图像值变为0 而在lower_red～upper_red之间的值变成255 图像中的与、或、异或、非操作opencv中的bitwise_not，bitwise_xor，bitwise_or，bitwise_and的使用方法与效果。 bitwise_and是对二进制数据进行“与”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“与”操作，1&amp;1=1，1&amp;0=0，0&amp;1=0，0&amp;0=0。 bitwise_or是对二进制数据进行“或”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“或”操作，1|1=1，1|0=0，0|1=0，0|0=0。 bitwise_xor是对二进制数据进行“异或”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“异或”操作，1^1=0,1^0=1,0^1=1,0^0=0。 bitwise_not是对二进制数据进行“非”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“非”操作，1=0，0=1。 Hough直线变换cv2.HoughLines()，返回值就是距离和角度。这个函数的第一个参数是一个二值化图像。第二个和第三个值代表距离和角度的精确度。第四个参数是阈值，只有累加其中的值高于阈值时才被认为是一条直线。 cv2.HoughLinesP():比上面多两个参数，minLineLength和MaxLineGap。比较简单 minLineLength：线的最短长度，比这个短的线都忽略 MaxLineGap：两条线段的最大间断，如果小于此值，这两条直线被看成一条直线 Hough圆环变换：HoughCircles(image, method, dp, minDist[, circles[, param1[, param2[, minRadius[, maxRadius]]]]]) image参数表示8位单通道灰度输入图像矩阵。 method参数表示圆检测方法，目前唯一实现的方法是HOUGH_GRADIENT。 dp参数表示累加器与原始图像相比的分辨率的反比参数。例如，如果dp = 1，则累加器具有与输入图像相同的分辨率。如果dp=2，累加器分辨率是元素图像的一半，宽度和高度也缩减为原来的一半。 minDist参数表示检测到的两个圆心之间的最小距离。如果参数太小，除了真实的一个圆圈之外，可能错误地检测到多个相邻的圆圈。如果太大，可能会遗漏一些圆圈。 circles参数表示检测到的圆的输出向量，向量内第一个元素是圆的横坐标，第二个是纵坐标，第三个是半径大小。 param1参数表示Canny边缘检测的高阈值，低阈值会被自动置为高阈值的一半。 param2参数表示圆心检测的累加阈值，参数值越小，可以检测越多的假圆圈，但返回的是与较大累加器值对应的圆圈。 minRadius参数表示检测到的圆的最小半径。 maxRadius参数表示检测到的圆的最大半径]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(3)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-3%2F</url>
    <content type="text"><![CDATA[图像轮廓和图像边缘不一样，边缘不连续。将边缘连接成一个整体构成轮廓。 提取图像轮廓的方法：先调用cv2.findContours()，后调用cv2.drawCont() cv2.findCount函数使用方法: 参数mode的几种方式： 参数method的几种方法： 轮廓特征：cv2.moments()会将计算得到的矩以一个字典的形式返回。 轮廓面积可以使用函数 cv2.contourArea() 计算得到 轮廓周长：cv2.arcLength(cnt，True) 函数的第二个参数用来指定对象的形状是闭合的(True)，还是打开的。 轮廓近似：cv2.approxPolyDP(cnt，epsilon，True) 凸包：函数cv2.convexHull() 可以用来检测一个曲线是否具有凸性缺陷并能纠正凸性缺陷。凸性曲线总是凸出来的，至少是平的。如果有地方凹去了就叫做凸性缺陷。 凸性检测：cv2.isContourConvex()可以用来检测一个曲线是不是凸的，返回True和False。 边界矩形：直边界矩形和旋转的边界矩形。 直边界矩形：面积不是最小的。x,y,w,h=cv2.boundingRect()。x,y,为矩形左上角的坐标，w,h为矩形的宽和高 旋转的边界矩形：面积最小。考虑了对象的旋转。cv2.minAreaRect())。返回的是一个 Box2D 结构，其中包含矩形左上点的坐标x，y矩形的宽和高w，h以及旋度。但是绘制个矩形矩形的 4 个点可以函数cv2.boxPoints() 获 得。 最小外接圆：cv2.minEnlosingCircle() 椭圆拟合：旋转边界矩形的内切圆 cv2.fitEllipse(cnt) 形状匹配：cv2.matchShape()可以帮我们比较两个形状或轮廓的相似度。返回值越小，匹配越好。 getStructuringElement构建一个核。前面腐蚀膨胀的numpy构建的结构化元素是正方形的 getStructuringElement 与 Numpy 定义的元素结构是完全一样的这个函数的第一个参数表示内核的形状，有三种形状可以选择。 矩形：MORPH_RECT; 交叉形：MORPH_CROSS; 椭圆形：MORPH_ELLIPSE; 第二和第三个参数分别是内核的尺寸以及锚点的位置。对于锚点的位置，有默认值（-1,-1），表示锚点位于中心点。element形状唯一依赖锚点位置，其他情况下，锚点只是影响了形态学运算结果的偏移。 透视变换在不同的视觉拍摄同一个物体，会有不同的图像。透视变换就是类似于改变拍摄物体的角度 仿射变换：由平移、错切、缩放、反转、旋转复合而成，是透视变换的特殊形式 OpenCV提供了两个变换函数cv2.warpAﬃne(仿射变换)和cv2.warpPerspective(透视变换) ，cv2.warpAﬃne 接收的参数是 2×3 的变换矩阵，而cv2.warpPerspective 接收的参数是 3×3 的变换矩阵。 函数cv2.warpAﬃne() 的第三个参数的是输出图像的大小。它的格式应是图像的(宽,高)。注意的是图像的宽对应的是列数，高对应的是行数。可以实现图片平移。可以和cv2.getAffineTransform配合使用 其中两个参数是变换前后的位置关系 函数cv2.warpPerspective配合cv2.getPerspectiveTransform()使用。同时可以用findHomography返回的单应性矩阵。 getPersonspectiveTransform得出变换矩阵： 得出变换矩阵以后用warpPerspective()：第一个参数是输入图像，M是变换矩阵，第三个参数是输出图像的大小 findHomography：提供正确估计的好的匹配被叫做inliers，而其他的叫做outliers。cv2.findHomography()返回一个掩图来指定inline和outline。第一个和第二个参数分别是原图像和目的图像，第三个参数可选为cv2.RANSAC、cv2.LMEDS.第二个参数取值范围在1到10。 getPerspectiveTransform和findHomography的区别： 旋转：cv2.getRotationMatrix2D()]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(2)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-2%2F</url>
    <content type="text"><![CDATA[图像滤波：即在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像预处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。 图像滤波图像滤波的目的 a、消除图像中混入的噪声 b、为图像识别抽取出图像特征 均值滤波，用blur函数 方框滤波：进行归一化处理后，和均值滤波相同。当normalize为零时，不进行归一化处理。当normalize为一时，进行归一化处理（默认）。目标图像深度通常于原始图像一样，值为-1。 高斯滤波： 注意：ksize：核大小必须为单数 sigmaX、sigmaY：控制权重 中值滤波： 两种边缘保留滤波： biateraFilter：能在保持边界清晰的情况下有效的去除噪音 filter2D:对2D图像实施低通滤波。去除噪音，模糊图像 膨胀和腐蚀膨胀、腐蚀时用到的kernel的形状一般有下面三种： 矩形: MORPH_RECT 交叉形: MORPH_CROSS 椭圆形: MORPH_ELLIPSE 图像腐蚀：调用erode函数 图像膨胀：调用dialate 开运算：先通过图像腐蚀，后经过图像膨胀可以图像去噪。iteration表示次数 闭运算：先是先通过图像膨胀，后经过图像腐蚀 梯度运算：图像膨胀-图像腐蚀 图像礼帽（图像顶帽）：原始图像-开运算图像，得到噪声图像 图像黑帽：闭运算-原始图像 求梯度梯度简单来说就是求导。Sobel，Scharr是求一阶导数或二阶导数。Scharr是对Sobel的优化。Laplacian是求二阶导数。 Sobel算子：当一个像素右边的值减去左边的值不为零，该像素为边界。 计算梯度的函数： ddepth通常取cv2.CV_64F。 dx=0,dy=1计算y轴的边界,dx=1,dy=0计算x轴的边界。满足条件dx&gt;=0&amp;&amp;dy&gt;=0&amp;&amp;dx+dy=1 将图像中的负值转为正： 将一个图像的边缘提取出来：如果没有converScaleAbs，所有的负值都会被截断为0.换句话就是把边界丢掉。 Scharr算子：比sobel算子精准，使用方法基本一样 计算梯度调用函数：cv2.Scharr(src,ddpetch,dx,dy) des=cv2.Scharr(src,ddpetch,dx,dy) 等价于des=cv2.Sobel(src,ddpetch,dx,dy,-1) 为图像扩边：cv2.copyMakeBorder()src:原图图像 top,bottom,left,right分别表示在原图四周扩充边缘的大小 borderType：扩充边缘的类型，就是外插的类型，OpenCV中给出以下几种方式 * BORDER_REPLICATE * BORDER_REFLECT * BORDER_REFLECT_101 * BORDER_WRAP * BORDER_CONSTANT canny函数提取图片边缘cv2.canny(img,threshold1,threshold2) img代表原始图像，threshold1、threshold2为阈值。两个阈值越小，得出图像边缘越详细。反之，边框越边缘。 提取原理步骤：1、高斯模糊 - GaussianBlur 2、灰度转换 - cvtColor 3、计算梯度 – Sobel/Scharr 4、非最大信号抑制5、高低阈值输出二值图像]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[opencv(1)]]></title>
    <url>%2F2019%2F08%2F08%2Fopencv-1%2F</url>
    <content type="text"><![CDATA[​ 因为我是对图像处理的方面比较感兴趣的。所以我也是对python的视觉处理模块opencv进行了学习。这个模块虽然是传统方法，但还是蛮有用的。而且它有上千个API，我总结了以下我所学到的。开始吧。 cv2是opencv的扩展模块。 imread第二个参数类型: cv2.imwrite:保存图片 下文中：绝对路径代表绝对路径也行 改变图片某行某列的像素： 批量改变图片像素： 获取图像属性： 拆分通道： 合并通道： 图片移位： 将两张图片加在一起：1、取模加法 2、饱和运算 注意：两张图片的大小和类型相等 减法(subtract)、乘法(multiply)和除法(divide)和加法(add)类似 图像融合：将两张或两张以上的图片融合到一张图片上 图像缩放（参数必须为整数）： 图像翻转：调用cv2.flip(原始图像，flipcode) 三种情况： filpcode=0：以x轴为对称轴的上下翻转 flipcode&gt;0:以Y轴为对称轴的左右翻转 flipcode&lt;0:X、Y轴各翻转一次 图像颜色反转： 图片打上马赛克： 图片上写字： 图片修补： 图片亮度增强： 图像阈值分割：调用了threshold函数 五种分割方法： 二进制阈值化（cv2.THRESH_BINARY）：选定一个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素阈点值设为最大，若小于该阈值，则将该像素点阈值设为零。 反二进制阈值化（cv.THRESH_BINARY_INV）：选定一个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素点的阈值设为零，若小于该阈值，则将该像素点阈值设为最大。 截断阈值化（cv.THRESH_TRUNC）：选定个阈值，用每一个像素的阈值和该阈值比较，大于该阈值则将该像素点的阈值设为该阈值，小于该阈值的像素点的阈值不变。 阈值化为0（THRESH_TOZERO）：先选定一个阈值，像素点的灰度值大于该阈值的不进行任何改变；像素点的灰度值小于该阈值的，其灰度值全部变为0。 反阈值化为0(THRESH_TOZERO_INV)：先选定一个阈值，像素点的灰度值小于该阈值的不进行任何改变；像素点的灰度值大于该阈值的，其灰度值全部变为0。 还有另一种阈值分割函数：自适应阈值函数 cv2.adaptiveThreshold()]]></content>
      <categories>
        <category>python</category>
        <category>opencv</category>
      </categories>
      <tags>
        <tag>视觉处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间模块]]></title>
    <url>%2F2019%2F08%2F08%2F%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python里面的时间模块。time 模块提供各种时间相关的功能。在 Python 中，与时间处理有关的模块包括：time，datetime 以及 calendar。 首先先来一波术语解释： 时间戳（timestamp）的方式：通常来说，时间戳表示的是从 1970 年 1 月 1 日 00:00:00 开始按秒计算的偏移量（time.gmtime(0)）此模块中的函数无法处理 1970 纪元年以前的日期和时间或太遥远的未来（处理极限取决于 C 函数库，对于 32 位系统来说，是 2038 年） UTC（Coordinated Universal Time，世界协调时）也叫格林威治天文时间，是世界标准时间。在中国为 UTC+8 DST（Daylight Saving Time）即夏令时的意思。一些实时函数的计算精度可能低于它们建议的值或参数，例如在大部分 Unix 系统，时钟一秒钟“滴答”50~100 次 gmtime()，localtime()和 strptime() 以时间元祖（struct_time）的形式返回。 注一：范围真的是0-61.这是基于历史原因。 ##time time.altzone：返回格林威治西部的夏令时地区的偏移秒数；如果该地区在格林威治东部会返回负值（如西欧，包括英国）；对夏令时启用地区才能使用。 time.asctime([t])：接受时间元组并返回一个可读的形式为”Tue Dec 11 18:07:14 2015”（2015年12月11日 周二 18时07分14秒）的 24 个字符的字符串。 time.clock()：用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） time.ctime([secs]) ：作用相当于 asctime(localtime(secs))，未给参数相当于 asctime() time.gmtime([secs])：接收时间辍（1970 纪元年后经过的浮点秒数）并返回格林威治天文时间下的时间元组 t（注：t.tm_isdst 始终为 0） time.daylight：如果夏令时被定义，则该值为非零。 time.localtime([secs])：接收时间辍（1970 纪元年后经过的浮点秒数）并返回当地时间下的时间元组 t（t.tm_isdst 可取 0 或 1，取决于当地当时是不是夏令时） time.mktime(t)：接受时间元组并返回时间辍（1970纪元后经过的浮点秒数） time.perf_counter()：返回计时器的精准时间（系统的运行时间），包含整个系统的睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 time.process_time() ：返回当前进程执行 CPU 的时间总和，不包含睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 time.sleep(secs)：推迟调用线程的运行，secs 的单位是秒。 time.clock()：用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。 Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） time.time()：返回当前时间的时间戳（1970 纪元年后经过的浮点秒数） time.timezone：time.timezone 属性是当地时区（未启动夏令时）距离格林威治的偏移秒数（美洲 &gt;0；大部分欧洲，亚洲，非洲 &lt;= 0） time.tzname：time.tzname 属性是包含两个字符串的元组：第一是当地非夏令时区的名称，第二个是当地的 DST 时区的名称。 time.strftime(format[, t]) ：把一个代表时间的元组或者 struct_time（如由 time.localtime() 和 time.gmtime() 返回）转化为格式化的时间字符串。如果 t 未指定，将传入 time.localtime()。如果元组中任何一个元素越界，将会抛出 ValueError 异常。 time.strptime(string[, format])：把一个格式化时间字符串转化为 struct_time。实际上它和 strftime() 是逆操作 format 格式如下： 注1：“%p”只有与“%I”配合使用才有效果。 注2：范围真的是*0 ~ 61（你没有看错哦）；60代表闰秒，61是基于历史原因保留。 注3：当使用 strptime() 函数时，只有当在这年中的周数和天数被确定的时候%U 和 %W 才会被计算。 datetimedatetime.today()：返回一个表示当前本地时间的 datetime 对象，等同于 datetime.fromtimestamp(time.time()) datetime.now(tz=None)：返回一个表示当前本地时间的 datetime 对象；如果提供了参数 tz，则获取 tz 参数所指时区的本地时间 datetime.utcnow():返回一个当前 UTC 时间的 datetime 对象 datetime.fromtimestamp(timestamp, tz=None): 根据时间戮创建一个 datetime 对象，参数 tz 指定时区信息 datetime.utcfromtimestamp(timestamp): 根据时间戮创建一个 UTC 时间的 datetime 对象 datetime.fromordinal(ordinal): 返回对应 Gregorian 日历时间对应的 datetime 对象 datetime.combine(date, time): 根据参数 date 和 time，创建一个 datetime 对象 datetime.strptime(date_string, format): 将格式化字符串转换为 datetime 对象 datetime.timedelta对象代表两个时间之间的时间差，两个date或datetime对象相减就可以返回一个timedelta对象。 *datetime.timedelta([days[, seconds[, microseconds[, milliseconds[, minutes[, hours[, weeks]]]]]]]) * 往前算： 往后算： datetime.date()：返回一个 date 对象datetime.time() - 返回一个 time 对象（tzinfo 属性为 None） datetime.timetz()： 返回一个 time() 对象（带有 tzinfo 属性） datetime.replace([year[, month[, day[, hour[, minute[, second[, microsecond[, tzinfo]]]]]]]])： 生成一个新的日期对象，用参数指定日期和时间代替原有对象中的属性 datetime.astimezone(tz=None)： 传入一个新的 tzinfo 属性，返回根据新时区调整好的 datetime 对象 datetime.utcoffset()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.utcoffset(self) datetime.dst()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.dst(self) datetime.tzname()：如果 tzinfo 属性是 None，则返回 None；否则返回 self.tzinfo.tzname(self) datetime.timetuple()：返回日期对应的 time.struct_time 对象（类似于 time模块的 time.localtime()） datetime.utctimetuple()：返回 UTC 日期对应的 time.struct_time 对象 datetime.toordinal()：返回日期对应的 Gregorian Calendar 日期（类似于 self.date().toordinal()） datetime.timestamp()：返回当前时间的时间戳（类似于 time 模块的 time.time()） datetime.weekday()：返回 0 ~ 6 表示星期几（星期一是 0，依此类推） datetime.isoweekday()： 返回 1 ~ 7 表示星期几（星期一是1， 依此类推） datetime.isocalendar() ：返回一个三元组格式 (year, month, day) datetime.isoformat(sep=’T’)：返回一个 ISO 8601 格式的日期字符串，如 “YYYY-MM-DD” 的字符串 datetime.__str__()：对于 date 对象 d 来说，str(d) 相当于 d.isoformat() datetime.ctime()：返回一个表示日期的字符串，相当于 time模块的 time.ctime(time.mktime(d.timetuple())) datetime.strftime(format):返回自定义格式化字符串表示日期。 datetime.__format__(format)跟 datetime.strftime(format) 一样，这使得调用 str.format() 时可以指定 data 对象的字符串 calendarcalendar.calendar(year,w,l,c)(year为年份，w是每日宽度，c为间隔距离，l为每星期行数) 日历 *calendar.isleap(year) *：如果是闰年就返回True，否则返回False calendar.leapdays(y1,y2)：返回y1与y2两年间的闰年总数 *calendar.month(year,month,w,l) *：year代表年份，month代表月日历，每行宽度间隔为w，l是每星期的行数 *calendar.prcal *：相当于print(calendar.calendar(year,w,l,c)) calendar.prmonth(year,month,w,l)：相当于print(calendar.calendar(year,w,l,c)) calendar.weekday(year,month,day)： 返回指定日期的日期码，0-6（星期），1-12（月份） calendar.firstweekday()返回当前起始日期的设置 calendar.setfirstweekday()：设置每周的起始日期码 calendar.timegm(时间元祖) 和time.gmtime相反 calendar.monthrange(year,month)：返回两个整数。第一个是该月第一天是星期几的日期码，第二个是该月的日期码。日从0（星期一）到6（星期日）;月从1到12。 timeit测试一个函数的执行时间：timeit.timeit timeit.repeat:返回一个包含了每次实验的执行时间的列表 三个时间模块的函数特别多，我也没有一一去试。用到的时候才会深究，各位看官根据自己的情况来试着使用起来吧。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python时间管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（3）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[textwrap用来重新格式化文本的输出 fill()调整换行符,每行显示给定宽度 dedent()去除缩进 indent()给定前缀 首行缩进： 多余的省略号shorter() wrap():wrap(s,width) 以单词为单位(包括字符)最大长度不超过width个字符 itertoolsPython的内建模块itertools提供了非常有用的用于操作迭代对象的函数 count()count()会创建一个无限的迭代器，所以下述代码会打印出自然数序列，根本停不下来，只能按Ctrl+C退出。 cycle()cycle()会把传入的一个序列无限重复下去： repeat()repeat()负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数： chainchain()可以把一组迭代对象串联起来，形成一个更大的迭代器： groupby()groupby()把迭代器中相邻的重复元素挑出来放在一起： 实际上挑选规则是通过函数完成的，只要作用于函数的两个元素返回的值相等，这两个元素就被认为是在一组的，而函数返回值作为组的key。如果我们要忽略大小写分组，就可以让元素’A’和’a’都返回相同的key： permutations()输出输入序列的全排列,考虑顺序 combinations()同上，不过不考虑顺序 product()输出输入序列的笛卡儿积 compress可以对一个序列的筛选结果施加到另一个相关的序列上 dropwhile()筛选满足条件的元素 islice() zip_longeszip可产生元祖。当其中摸个输入序列中没有元素可以继续迭代时，迭代过程结束。所以整个迭代的长度和最短的输入序列相同。 如果不想这样就用zip_longest：]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（2）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[collection：collections是Python内建的一个集合模块，提供了许多有用的集合类。 nametuplenamedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。 利用namedtuple定义和使用具名元祖：第一个参数为类名，第二个参数为类的各个字段的名字 具名元祖有些专用的属性：类属性_fields,类方法_make(),实例方法_asdict() 如果需要修改任何属性，可以通过使用nametupled实例的_replace方法来实现。该方法会创建按一个新的命名元祖，并对相应的值进行替换。 dequedeque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈。deque(maxlen=N)创建一个固定长度的队列，当有新记录加入而队列已满时会自动移除最老的那条记录。 defaultdictdefaultdict:使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict。defaultdict的一个特点就是会自动初始化第一个值。 用defaultdict的效率比不用高: OrderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用OrderedDict。 OrderDict的大小是普通字典的2倍多，这是由于它额外创建的链表所导致。 注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Key Counter一个简单的计数器 计数器的更新包括增加和减少两种，增加使用update，减少用subtract。 most_common(x):根据x返回频率前x的项。 itemgetteritemgetter可以通过公共键对字典列表排序。 ChainMapChainMap可接受多个映射然后再逻辑上是它们表现为一个单独的映射结构。如果有重复的键，那么会采用第一个映射中所对应的值。修改映射的操作总是会作用在列出的第一个映射结构上。 randomrandom.random() 产生0-1的随机浮点数 random.uniform(a, b) 产生指定范围内的随机浮点数 random.randint(a, b) 产生指定范围内的随机整数 random.randrange([start], stop[, step]) 从一个指定步长的集合中产生随机数 random.choice(sequence) 从序列中产生一个随机数 random.shuffle(x[, random]) 将一个列表中的元素打乱 random.sample(sequence, k) 从序列中随机获取指定长度的片断 functoolsreduce(function, sequence, value)对sequence中的item顺序迭代调用function，如果有value，还可以作为初始值调用。function接收的参数个数只能为2，先把sequence中第一个值和第二个值当参数传给function，再把function的返回值和第三个值当参数传给function，然后只返回一个结果。 partial基于一个函数创建一个新的可调用对象，把原函数的某些参数固定。 偏函数：只需要传一次值，后面想传就传]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用模块（1）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 这几年，python大火。其中的一个原因是python的库特别多，而且封装非常好。接下来我来总结以下我用过的一些库，虽然都不是很大的库，但还是有用的。 jsonjson主要执行序列化和反序列化的功能，通过Python的json模块，可以将字符串形式的json数据转化为字典，也可以将Python中的字典数据转化为字符串形式的json数据。 通过json字符串转为字典 json.loads 字典转换为json：json.dumps json.loads()、dumps解码python json格式 json.load、dump加载json格式文件 pickle使用方法与json一样 区别： json是可以在不同语言之间交换数据的，而pickle只在python之间使用。 json只能序列化最基本的数据类型，而pickle可以序列化所有的数据类型，包括类，函数都可以序列化。 hashlib该模块提供了常见的摘要算法。如MD5，SHA1……摘要算法又称哈希算法、散列算法，通过一个函数，把任意长度的数据转换为一个长度固定的数据串。 摘要算法是一个单向函数，通过摘要函数f()对任意长度的数据data计算出固定长度的摘要digest，目的是发现原始数据是否被改动过。计算f(data)很容易，但通过digest反推data却非常困难，对原始数据做一个字节的修改，都会导致计算出来的摘要不同。 stringstring.digits:包含数字0-9的字符串 string.letters:包含所有字母（大写或小写）的字符串 string.lowercase:包含所有小写字母的字符串 string.printable:包含所有可打印字符的字符串 string.punctuation:包含所有标点的字符串 string.uppercase:包含所有大写字母的字符串 string.ascii_letters和string.digits方法，其中ascii_letters是生成所有字母，从a-z和A-Z,digits是生成所有数字0-9. 词云 decimal如果期望获得更高的精度（并且愿意牺牲掉一些性能），可以使用decimal模块。 fractions模块可以用来处理涉及分数的数学计算问题 copy完成深拷贝和浅拷贝 is和==的区别：==是看值，is看是否指向同一个 浅拷贝：拷贝内容的地址 深拷贝：开发另一片空间存放要拷贝的内容 copy会判断数据类型是否为可变类型，如元祖为不可变类型，则只会完成浅拷贝。 fileinput可以快速对一个或多个文件进行循环遍历 fileinput.input([files[, inplace[, backup[, mode[, openhook]]]]]])功能:生成FileInput模块类的实例。能够返回用于for循环遍历的对象。注意:文件名可以提供多个 inplace：是否返回输出结果到源文件中，默认为零不返回。设置为1时返回。 backup：备份文件的扩展名 mode：读写模式。只能时读、写、读写、二进制四种模式。默认是读 openhook：必须是一个函数，有两个参数，文件名和模式。返回相应的打开文件对象 fileinput.filename()：返回当前正在读取的文件的名称。在读取第一行之前，返回None。 fileinput.fileno()：返回当前文件的整数“文件描述符”。如果没有打开文件（在第一行之前和文件之间），则返回-1。 fileinput.lineno()：返回刚读过的行的累计行号。在读取第一行之前，返回0。读取完最后一个文件的最后一行后，返回该行的行号。 fileinput.filelineno()：返回当前文件中的行号。在读取第一行之前，返回0。读取完最后一个文件的最后一行后，返回该文件中该行的行号。 fileinput.isfirstline()：如果刚刚读取的行是其文件的第一行，则返回true，否则返回false。 fileinput.isstdin()：如果读取了最后一行sys.stdin，则返回true，否则返回false。 fileinput.nextfile()：关闭当前文件，以便下一次迭代将读取下一个文件的第一行（如果有的话）; 未从文件中读取的行将不计入累计行数。直到读取下一个文件的第一行之后才会更改文件名。在读取第一行之前，此功能无效; 它不能用于跳过第一个文件。读取完最后一个文件的最后一行后，此功能无效。 fileinput.close()关闭序列 subprocess：subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用subprocess.PIPE将多个子进程的输入和输出连接在一起，构成管道(pipe) subprocess.call():父进程等待子进程完成，返回退出信息(returncode，相当于Linux exit code) subprocess.check_call():父进程等待子进程完成，返回0，检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性，可用try…except…来检查 subprocess.check_output():父进程等待子进程完成，返回子进程向标准输出的输出结果，检查退出信息，如果returncode不为0，则举出错误subprocess.CalledProcessError，该对象包含有returncode属性和output属性，output属性为标准输出的输出结果，可用try…except…来检查。 subprocess.Popen()，以下为参数： args：shell命令，可以是字符串，或者序列类型，如list,tuple。 bufsize：缓冲区大小，可不用关心 stdin,stdout,stderr：分别表示程序的标准输入，标准输出及标准错误 shell：与上面方法中用法相同 cwd：用于设置子进程的当前目录 env：用于指定子进程的环境变量。如果env=None，则默认从父进程继承环境变量 universal_newlines：不同系统的的换行符不同，当该参数设定为true时，则表示使用\n作为换行符 常用方法： poll() ： 检查子进程状态 kill() ： 终止子进程 send_signal() :向子进程发送信号 terminate() ： 终止子进程 communicate:从PIPE中读取PIPE的文本，该方法会阻塞父进程，直到子进程完成 常用属性：pid：子进程的pid，returncode：子进程的退出码。]]></content>
      <categories>
        <category>python</category>
        <category>常用模块</category>
      </categories>
      <tags>
        <tag>python常用模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[元类]]></title>
    <url>%2F2019%2F08%2F07%2F%E5%85%83%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[​ 今天来讲一下python里面较难的一个东西。我学到的python内容里面有两个东西是比较难的，一个是描述符，另一个就是今天讲的元类。 通过python知识散记我们知道global的功能是将局部变量转为全局变量，接下来要说的是globals()。这个globals()函数会以字典类型返回当前位置的全部全局变量。 __builtins__模块是默认加载的，在ipython中打开。 python通过类创建对象，通过元类创建类。（类也是对象） 动态创建类： 函数choose_class根据name的不同而动态的创建不同的类，但这种做法效率相当low。因为如果类多了就会要很多if-else来判断创建哪个类。 通过type动态创建类。命名规则： type(类名，由父类名称构成的元祖（针对继承的情况，可以为空），包含属性的字典（名称和值）) 继承： 添加实例方法： 添加类方法： 添加静态方法： 元类应用： metaclass用来指定按照upper_attr来创建，如果不指定，则默认使用type创建。Foo传到class_name，父类(object)传到class_name，新的字典传到class_attr。 用类完成以上代码：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的元类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas用法总结]]></title>
    <url>%2F2019%2F08%2F07%2Fpandas%2F</url>
    <content type="text"><![CDATA[​ pandas也是一个非常强大的库，所以我也只是总结了我用到的方法。 pandas常用的数据类型：1、Series 一维 带标签的数组（标签就是索引）2、DataFrame 二维 Series的容器 Series：通过列表创建Series： 索引可以指定，默认从0开始： 通过字典创建Series：可以通过astype修改类型 取值： 可以将条件和value、index配合使用： pandas读取外部数据： read_csv:读取CSV文件 read_excel:读取excel文件 其他文件类似 DataFrame通过列表创建： 通过数组创建： 通过字典创建： DataFrame的基础属性： shape ：行数 列数 dtypes：列数据类型 ndim：数据维度 index：行索引 columns：列索引 values：对象值 DataFrame的方法： head(n):显示头n行。默认是前5行 tail(n)：显示尾n行。 info()：行数，列数，列索引，列非空值个数，列类型，内存占用 describe()：计数 均值 标准差 最大值 四分位数 最小值 DataFrame排序：sort_values()。通过设置by来确定排序的key。设置ascending确定升序or降序。 DataFrame的取值： 方括号写数字，表示取行。对行进行操作。根据实际情况写 对列进行操作： 配合使用： loc：DataFrame通过标签索引获取行数据 根据多个索引取多个对应的值： iloc：DataFrame通过位置获取列数据。与ioc类似，只是将索引换成数字。 数组合并： 1、join 按行索引合并 2、merge按列索引进行合并 on指定按哪一列合并 how：合并方式 inner(交集，默认) outer(并集) left(左边为准，NaN补全) right(右边为准，NaN补全) 如果列索引不同。可以left_on和right_on指定左边、右边DataFrame的合并列。 另一种写法： 分组:groupby(by) by:通过什么分组，可以设置多个条件分组 聚合：count 计算数量 sum 求和 mean 求平均值 median 求中位数 std、var 求标准差和方差 min、max 求最大和最小值 DataFrame的索引和复合索引：简单的索引操作： 获取index：df.index 指定index：df.index=[“”,””] 同理可得指定columns：df.columns=[“ “,” “] 重新设置index：df.reindex() 指定某一列成为index：df.set_index()。drop决定是否保留设定的列 可以设定多个列成为index： 返回index的唯一值：df.set_index().index.unique() 时间序列：date_range(start,end,period,freq) 生成一段时间范围。start和end表示范围，periods表示个数，freq表示频率(年、月、天) 频率类型： 时间段：PeriodIndex 重采样resample：指的是将时间序列从一个频率转化为另一个频率进行处理的过程。将高频率数据转化为低频率数据为降采样。低频率数据转化为高频率为升采样。 判断数据是否为NaN：pd.isnull() pd.notfull() 在DataFrame中对缺失数据（NaN）的处理： 方式1：删除NaN所在的行列dropna(axis,how,inplace):how=”any”时一行(列)里有一个为nan就删。how=”all”时，一行全部为nan时才删。inplace为True，原地修改。False为False，不修改。 方式2：填充数据，fillna() 处理为0的数据：t[t==0]=np.nan 计算平均值时：nan不参与计算，0参与]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（3）]]></title>
    <url>%2F2019%2F08%2F07%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 今天的知识散记讲三个器，哪三个器呢？装饰器、迭代器、生成器。还有列表生成式、字典生成式。 装饰器：python装饰器就是用于拓展原来函数功能的一种函数，这个函数的特殊之处在于它的返回值也是一个函数，使用python装饰器的好处就是在不用更改原函数的代码前提下给函数增加新的功能。 使用闭包完成的装饰器原理： 当执行test(test)时，func已经指向test()函数。所以func()相当于test()。这就是装饰器的原理。但是实际不是这样写的，请看下图： 使用装饰器对有参数的函数装饰： 对不定长参数的装饰： 装饰器对有返回值的函数装饰： 带参数的装饰器 用类做装饰器： 装饰器的一个关键特性：函数装饰器在导入模块时立即执行，而被装饰的函数只在明确调用时运行。 装饰器完了，自己慢慢悟吧。 迭代器迭代器只能前进不能后退。使用迭代器不要求事先准备好整个迭代过程中的所有元素。迭代器仅仅在迭代到某个元素时才计算该元素，而在这之前或之后元素可以不存在或者被销毁。因此迭代器适合遍历一些数量巨大甚至无限的序列。 Python中迭代器的本质上每次调用__next__()方法都返回下一个元素或抛出StopIteration的容器对象 由于Python中没有“迭代器”这个类，因此具有以下两个特性的类都可以称为“迭代器”类： 1、有__next__()方法，返回容器的下一个元素或抛出StopIteration异常 2、有__iter__()方法，返回迭代器本身 all(iterable)：如果迭代器里面的所有元素都为True时,返回True;否则返回False any(iterable）：如果迭代器里面的所有元素为False,返回False;否则返回True. 生成器一个个的生成数据，但占用内存更少，生成器是特殊的迭代器。 当yield存在函数时，函数就变成一个生成器。yield不像return那样返回值，而是每次产生多个值。每次使用yield产生一个值，函数就会被冻结。 用aim.__next__()得出结果： aim.send()得出结果： 列表生成式和字典推导式：]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（2）]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 接着上次的知识散记，我们接着看。 闭包在函数内部定义一个函数，并且这个函数用到了外面的变量。将这个函数以及用到的一些变量称之为闭包。 当某个函数被当成对象返回时，夹带了外部 变量，就形成了一个闭包。 global：一般多用在函数内，声明变量的作用域为全局作用域。箭头变化的时候加，箭头不变的时候可以不加。（箭头类似指针） nonlocal：nonlocal关键字用来在函数或其他作用域中使用外层(非全局)变量 lambda匿名函数： 不要看匿名函数蛮简单的，其实还是有点注意事项的。对默认参数的赋值只会在函数定义的时候绑定一次。x是一个自由变量。在运行的时候绑定，而不是在定义的时候绑定。执行时，x的值是多少就是多少。如果希望匿名函数可以在定义的时候绑定，并保持值不变，则采用下面方法。 assert:断言用来直接让程序崩溃，在程序中置入检查点 条件后可以添加字符串，用来解释断言 format: 对*和**的解释：星号(*)和(**)作为形参的时候是起到“打包”的作用，相反，作为实参的时候是起到“解包”的作用。 星号(*)或(**)作为形参，表示调用可变参数函数：通过在形参前加一个星号(*)或两个星号(**)来指定函数可以接收任意数量的实参。 从两个示例的输出可以看出：当参数形如 *args 时，传递给函数的任意个实参会按位置打包成一个元 组（tuple）；当参数形如 **args 时，传递给函数的任意个 key = value 实参会被包装进一个字典（dict）。 星号(**)和(*)作为实参时，表示通过解包参数调用函数： 常用内置函数补充：复数可以通过complex（real，imag）来指定。conjugete提取共轭复数 hasattr：hasattr() 函数用于判断对象是否包含对应的属性。如果对象有该属性返回 True，否则返回 False。 上下文管理器（context manager）：任何实现了__enter__和__exit__方法的对象都可称为上下文管理器 __enter__():方法返回资源对象，这里就是你将要打开的那个文件对象，__exit__()处理一些清除工作。因为File类实现上下文管理器，现在就可以使用with语句了。 实现上下文管理器的其他方法：使用contextmanager装饰器 python的三种修饰符：staticmethod、classmethod 和 property，作用分别是把类中定义的实例方法变成静态方法、类方法和类属性。staticmethod、classmethod具体看python的类和对象。 注意： 函数先定义，再修饰它；反之会编译器不认识； 修饰符“@”后面必须是之前定义的某一个函数； 每个函数只能有一个修饰符，大于等于两个则不可以。 property用法:它的作用把方法当作属性来访问（注意getnum和setter的顺序，一定getnum在第一个）]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python知识散记（1）]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E7%9F%A5%E8%AF%86%E6%95%A3%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​ 今天来讲一下python的一些散装知识，还是蛮多的。所以分了几个部分。今天的部分是最简单的，废话少说。开始吧 导入模块方法：1、最常见的方式，直接将要导入的模块名称写在后面导入。import xxxx 2、from .. import .. 与import类似，只是更明确的要导入的方法或变量。 3、from modname import *，导入所有的类和公有方法。 if __name__==”__main__“:让你写的脚本模块既可以导入到别的模块中用，另外该模块自己也可执行。 常用的一些内置函数callable(object)：检查对象object是否可调用。如果返回True，object仍然可能调用失败；但如果返回False，调用对象ojbect绝对不会成功。 divmod(a,b)：以元祖的方式放回a//b以及a%b。 ord(str)：把对应的字符转成整数. chr(integer)：把整数转化成对应的字母. bool(x)：把一个值转化为布尔值,如果该值为假或者省略返回False,否则返回True abs(x) ：返回一个数的绝对值.该参数可以是整数或浮点数.如果参数是一个复数,则返回其大小 round(number[, ndigits])：返回浮点数number保留ndigits位小数后四舍五入的值。 dir([object]): 没有参数,返回当前局部范围的名单列表。有参数，试图返回该对象的有效的属性列表 issubclass(class, classinfo):返回True如果参数class是classinfo的一个子类，否则返回False。 isinstance(object, classinfo):返回True如果参数object是classinfo的一个实例，否则返回False(适用于继承)。 zip(*iterables):生成一个迭代器，迭代器聚合了从每个可迭代数集里的元素。它的内容只能被消费一次 map：第一个参数 function 以参数序列中的每一个元素调用 function函数，返回包含每次 function 函数返回值的新列表。 filter：filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。应该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 enumerate是一个非常有用的函数，直接看效果。 eval(str [,globals [,locals ]])：用来计算存储在字符串中的有效python表达式。 exec(object[, globals[, locals]])， 用来执行存储在字符串或文件中的python语句 格式化输出格式： python语句中一些基本规则和特殊字符： python调试：python调试两种方法都有用到pdb模块 第一种：在代码的目录下，打开cmd，输入python -m 文件名 h：帮助命令 第二种：可以在交互界面进行调试]]></content>
      <categories>
        <category>python</category>
        <category>知识散记</category>
      </categories>
      <tags>
        <tag>python知识散记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一些协程。协程是python中独有的，在其他语言中是没有这个概念的。协程是利用线程在等待的时候做事情。 使用yield完成协程： 使用greenlet完成协程： 使用gevent完成协程： 要想用时间模块延迟，则必须打补丁： 我还没看完，后面补。未完待续……]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python协程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的进程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多进程：multiprocessing模块 os.getpid()获取当前进程的id os.getppid()获取父进程的id 大量启动子进程，可以用进程池pool批量创建子进程。可以通过processes改变创建的进程数目。 apply_async(func[, args=()[, kwds={}[, callback=None]]])该函数用于传递不定参数，非阻塞且支持结果返回进行回调。 将函数添加到进程池： map(func, iterable[, chunksize=None])：Pool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到返回结果。 注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。 close()：关闭进程池（pool），使其不在接受新的任务。 terminate()：结束工作进程，不在处理未处理的任务。 join([timeout])：主进程阻塞等待子进程的退出，join方法必须在close或terminate之后使用。timeout表示等待最多时间。若超出，则会直接执行下列代码 Value、Array是通过共享内存的方式共享数据 Value：将一个值存放在内存中， Array：将多个数据存放在内存中，但要求数据类型一致 Value: Array:两种情况 若为数字，表示开辟的共享内存中的空间大小，（Value表示为该空间绑定一个数值） 若为数组，表示在共享内存中存入数组 说明：三个0表示开辟的共享内存容量为3，当再超过3时就会报错。 Manager（Value、Array、dict、list、Lock、Semaphore等）是通过共享进程的方式共享数据。 进程间通信：Queue ，Pipe 使用方法和队列差不多 q.get_nowait()：和get()差不多，不用等。 Pipe:Pipe可以是单向(half-duplex)，也可以是双向(duplex)。我们通过mutiprocessing.Pipe(duplex=False)创建单向管道 (默认为双向)。一个进程从PIPE一端输入对象，然后被PIPE另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。 这里的Pipe是双向的。 Pipe对象建立的时候，返回一个含有两个元素的表，每个元素代表Pipe的一端(Connection对象)。我们对Pipe的某一端调用send()方法来传送对象，在另一端使用recv()来接收。 生产者与消费者模式：在两者中找一个缓冲的东西（队列，缓冲池），解决数据生产方和数据处理方数据不分配的问题。 耦合：谁和谁的关系越强，耦合性就越强。耦合性越强，程序维护越难。 解耦的好处：哪块不合适，就改那块。 接上面：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的线程]]></title>
    <url>%2F2019%2F08%2F06%2Fpython%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[重要的话写在前面：进程间不共享全局变量，线程间共享全局变量。 同步：按预定的先后次序进行运行 异步：不确定的次序 对于操作系统来说，一个任务就是一个进程。进程内的子任务成为线程 ，一个进程至少有一个线程 多任务执行的方式： 多进程 多线程 多进程+多线程 多线程：Python的标准库提供了两个模块：_thread和threading，_thread是低级模块，threading是高级模块，对_thread进行了封装。绝大多数情况下，我们只需要使用threading这个高级模块。 传入参数为元祖。也就是即使只有一个参数，也要写逗号。 join():将线程加入到当前线程，并等待其终止 判断线程是否在运行： 守护线程：将daemon属性设为True，则该线程无法被连接 daemon属性可以保证主线程结束时可以同时结束子线程或者使主线程等待子线程结束后在结束。故称为守护线程。daemon默认为False，如需修改，必须在调用start()方法启动线程之前进行设置。不适用与idle的交互模式或脚本模式 当daemon属性为False时，主线程会检测子线程是否结束，如果子线程还在运行，则主线程会等待他完成后在退出。当daemon属性为True时，子线程没执行的不再执行，主线程直接退出。 通过轮询终止线程： threading的常用方法： ​ active_count() 当前活动的 Thread 对象个数 ​ current_thread() 返回当前 Thread 对象 ​ current_thread().name返回当前的Thread对象的名字 ​ get_ident() 返回当前线程 ​ enumerater() 返回当前活动 Thread 对象列表 ​ main_thread() 返回主 Thread 对象 ​ settrace(func) 为所有线程设置一个 trace 函数 ​ setprofile(func) 为所有线程设置一个 profile 函数 ​ stack_size([size]) 返回新创建线程栈大小；或为后续创建的线程设定栈大小为 size ​ TIMEOUT_MAX Lock.acquire(), RLock.acquire(), Condition.wait() 允许的最大值 threading 可用对象列表： ​ Thread 表示执行线程的对象 ​ Lock 锁原语对象 ​ RLock 可重入锁对象，使单一进程再次获得已持有的锁(递归锁) Condition： 条件变量对象，使得一个线程等待另一个线程满足特定条件，比如改变状态或某个值 ​ wait(timeout): 线程挂起，直到收到一个notify通知或者超时（可选的，浮点数，单位是秒s）才会被唤醒继续运行。wait()必须在已获得Lock前提下才能调用，否则会触发RuntimeError。 ​ condition = threading.Condition(lock=None) # 创建Condition对象 参数可以不传 ​ condition.acquire() # 加锁 ​ condition.release() # 解锁 ​ condition.wait(timeout=None) # 阻塞，直到有调用notify(),或者notify_all()时再触发 ​ condition.wait_for(predicate, timeout=None) # 阻塞，等待predicate条件为真时执行 ​ condition.notify(n=1) # 通知n个wait()的线程执行, n默认为1 ​ condition.notify_all() # 通知所有wait着的线程执行 ​ with condition: # 支持with语法，不必每次手动调用acquire()/release() Semaphore 为线程间共享的有限资源提供一个”计数器”，如果没有可用资源会被阻塞 Events：它是由线程设置的信号标志，如果信号标志为真，则其他线程等待直到信号接触。 Event对象实现了简单的线程通信机制，它提供了设置信号，清除信号，等待等用于实现线程间的通信。 event = threading.Event() 创建一个event 1、设置信号 event.set() 使用Event的set（）方法可以设置Event对象内部的信号标志为真。Event对象提供了isSet（）方法来判断其内部信号标志的状态。 当使用event对象的set（）方法后，isSet（）方法返回真 2、清除信号 event.clear() 使用Event对象的clear（）方法可以清除Event对象内部的信号标志，即将其设为假，当使用Event的clear方法后，isSet()方法返回假 3 、等待 event.wait() Event对象wait的方法只有在内部信号为真的时候才会很快的执行并完成返回。当Event对象的内部信号标志为假时，则wait方法一直等待到其为真时才返回。也就是说必须set新号标志为真 主线程在等事件设置后才继续执行 event使用示范： Barrier :创建一个”阻碍”，必须达到指定数量的线程后才可以继续 每个线程中都调用了wait()方法，在所有（此处设置为3）线程调用wait方法之前是阻塞的。也就是说，只有等到3个线程都执行到了wait方法这句时，所有线程才继续执行。 计算处于alive的Thread对象数量： 多线程避免全局变量的改变：上锁。上锁后执行的代码越少越好。 互斥锁：Lock是比较低级的同步原语，当被锁定后不属于特定的线程。一个锁有两个状态:Locked和unLocked.刚创建的的Locked处于unlocked状态。如果锁处于unlocked状态，acquire()方法将其修改为Locked并立即返回。如果锁处于locked状态，则阻塞当前线程并等待其他线程释放锁，然后将其修改为locked并立即返回。release()方法用来将锁的状态从locked修改为unlocked并立即返回。如果锁的状态本来就是unlocked，则会抛出异常 可重入锁Rlock对象也是一种常用的线程同步原语，可被同一个线程acquire()多次。当locked状态时，某现场拥有该锁，当处于unlocked状态时，该锁不属于任何线程。Rlock对象的acquire()/release()调用对可以嵌套，仅当最后一个或者最外层的release执行结束后，锁才会被设置为unlocked状态 死锁：双方都在等待对方的条件满足 避免死锁的方法：1、添加超时事件 2、 尽量避免（银行家算法） Threadlocal：保存当前线程的专有状态，这个状态对其他线程不可见。 全局变量local就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local看成全局变量，但每个属性如local.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 线程池：threadpool模块或concerrent.futures模块 threadpool模块比较老旧，不是主流。 threadpool.ThreadPool(poolsize)：定义一个线程池，创建了poolsize个线程。 threadpool.makeRequest(开启多线程的函数，函数相关参数，[回调函数]) putRequest:将所有要运行多线程的请求扔进线程池。 concerrent.futures：这个模块轻松绕开GIL ThreadPoolExecutor构造实例的时候，传入max_workers参数来设置线程池中最多能同时运行的线程数目。 使用submit函数来提交线程需要执行的任务（函数名和参数）到线程池中，并返回该任务的句柄（类似于文件、画图），注意submit()不是阻塞的，而是立即返回。 通过submit函数返回的任务句柄，能够使用done()方法判断该任务是否结束。上面的例子可以看出，由于任务有2s的延时，在task1提交后立刻判断，task1还未完成，而在延时4s之后判断，task1就完成了。 使用cancel()方法可以取消提交的任务，如果任务已经在线程池中运行了，就取消不了。这个例子中，线程池的大小设置为2，任务已经在运行了，所以取消失败。如果改变线程池的大小为1，那么先提交的是task1，task2还在排队等候，这是时候就可以成功取消。 使用result()方法可以获取任务的返回值。查看内部代码，发现这个方法是阻塞的 as_completed:一次取出所有任务的结果。as_completed()方法是一个生成器，在没有任务完成的时候，会阻塞，在有某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞住，循环到所有的任务结束。从结果也可以看出，先完成的任务会先通知主线程。 map的作用和submit一样，但略有不同。输出顺序和参数列表的顺序相同 wait方法接受三个参数，等待的任务序列，超时时间，以及等待条件。等待条件return_when默认为ALL_COMPLTED，表明要等待所有的任务都结束。还可以设为FIRST_COMPLETED，表示第一个任务完成就结束等待。FITST_EXCEPTION(注意要导入) 通过类创建线程： t.start后一定调用run函数，不定义run函数该线程不执行。对其他函数的调用只能在run函数里执行。多线程可以共享全局变量，但当数据量大时，数据会出错（产生资源竞争）。 线程是真的多，看到最后。迷迷糊糊，有错一定要提醒我。而且很多我还没有用过。后面用到的话，会补充上去的。接下来说最后一个：GIL。何为GIL？ GIL：全局解释器锁 单CPU的系统中运行多个进程那样，内存中可以存放多个程序，但任意时刻，只有一个程序在CPU中运行。同样地，虽然Python解释器中可以“运行”多个线程，但在任意时刻，只有一个线程在解释器中运行。 GIL保证了多线程时只有一个线程被调用。 所以多进程效率比多线程高，但是进程间通信比线程难。 解决方法：用C语言写关键部分。模块（ctypes）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F08%2F06%2Fnumpy%2F</url>
    <content type="text"><![CDATA[​ 最近这几年，机器学习和深度学习大火。而这其中的数据计算是非常多，而这得益于python的numpy模块。多提一句：numpy是没有GIL（多线程解释器锁）的。所以，numpy中的计算是非常快的。那什么是GIL呢？请看python的多线程。话不多说，让我们开启今天的数学之旅。PS：numpy很多API，我还没有学完。我只是总结了一部分，后面会补充的。 通过array生成矩阵： 可以通过dtype设置矩阵数据的类型。astype可以修改数据类型。 还有其他生成矩阵的方式： numpy生成随机数： seed的使用方法： 使用RandomState获得随机数生成器 &gt;&gt;&gt;rdm=np.random.RandomState(2) # 2是随机数种子&gt;&gt;&gt;a=np.random.randint(0,20,(3,4))&gt;&gt;&gt;aarray([[ 8, 11, 1, 5], [ 3, 8, 0, 18], [ 5, 15, 8, 14]]) reshape&amp;resize可以通过reshape修改列表的行数和列数，resize改变数组的尺寸大小。根据reshape传入的参数判断转为哪种数组。 numpy运算加、减、乘、除类似。如果两个矩阵形状相同，两个矩阵对应的元素做操作。若两个矩阵形状不相同，其中一个矩阵的维度与另一个矩阵的维度相同，可以在该维度上做操作。 通过axis求每行（列）的元素和或最大、最小值。0代表列，1代表行。 还可以获得最大、最小值： 其他一些计算:累加、累差、中位数、平均值 对clip函数的解释：小于5的元素都设为5，大于9的元素都设为9 flatten对数组展开为一维数组 numpy的合并： numpy的分割： 转置矩阵： numpy.linspace(start,stop,num,endpoint,retstep,dtype)该方法的作用是子啊start和stop之间产生num个线性向量。每个向量之间的距离是相同的。创建等差数列 start：起始点 stop：终止点 num : int, optional 默认50，生成start和stop之间50个等差间隔的元素 endpoint : bool, optional。If True, stop is the last sample. Otherwise, it is not included. Default is True。生成等差间隔的元素，但是不包含stop，即间隔为 （stop - start）/num retstep : bool, optional。If True, return (samples, step), where step is the spacing between samples.返回一个（array，num）元组，array是结果数组，num是间隔大小 dtype : dtype, optional。The type of the output array. If dtype is not given, infer the data type from the other input arguments。输出数组的类型。如果没有给出dtype，则从其他输入参数推断数据类型。 numpy.newaxi：插入新的维度 &gt;&gt;&gt; import numpy as np &gt;&gt;&gt;b=np.array([1,2,3,4,5,6])&gt;&gt;&gt;b[np.newaxis]&gt;&gt;&gt;array([[1, 2, 3, 4, 5, 6]]) numpy.meshgrid：生成网格点坐标矩阵 由上图，得知meshgrid的作用是：根据传入的两个一维数组参数生成两个数组元素的列表。如果第一个参数是xarray，维度是xdimesion，第二个参数是yarray，维度是ydimesion。那么生成的第一个二维数组是以xarray为行，共ydimesion行的向量；而第二个二维数组是以yarray的转置为列，共xdimesion列的向量 可能你看到这还不明白，看下图： A,B,C,D,E,F是6个网格点，坐标如图，如何用矩阵形式（坐标矩阵）来批量描述这些点的坐标呢？答案如下： 这就是坐标矩阵——横坐标矩阵X中的每个元素，与纵坐标矩阵Y中对应位置元素，共同构成一个点的完整坐标。 numpy.logspace创建等比数列，参数和logspace差不多，第四个参数代表基数。默认为10，如果想要修改其默认基数，输入第四个参数base，改变就可以了。 未完待续……]]></content>
      <categories>
        <category>python</category>
        <category>数学计算</category>
      </categories>
      <tags>
        <tag>python的数学计算模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一下网络编程。这里面用到一个库：socket。网络通信其实就是两个进程之间在编程。先说两个重要的协议：TCP协议 和 UDP协议。TCP协议是传输控制协议，UDP协议是数据传输协议。TCP和UDP的区别：TCP慢但是稳定，因为它经过了三次握手和四次挥手，不会丢失数据。UDP快。 socket:注意参数是一个tuple，包含地址和端口号。 在同一个os中，端口不允许相同，即如果某个端口已经被使用了，那么在这个进程释放之前，其他进程都不能使用这个端口。（端口用来区分进程，若相同，不能把数据发送到准确的进程） 创建Socket时，AF_INET指定使用IPv4协议，如果要用更先进的IPv6，就指定为AF_INET6。SOCK_STREAM指定使用面向流的TCP协议，这样，一个Socket对象就创建成功，但是还没有建立连接。 coding： 主机名可以通过调用socket.gethostname()获得 接收数据时，调用recv(max)方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到recv()返回空数据，表示接收完毕，退出循环。当我们接收完数据后，调用close()方法关闭Socket，这样，一次完整的网络通信就结束了 创建TCP连接时，主动发起连接的叫客户端，被动响应连接的叫服务器。 客户端要主动发起TCP连接，必须知道服务器的IP地址和端口号。 TCP服务端建立步骤： 一般是服务器（接受方）绑定端口，客户端（发送方）不绑定 UDP不需要调用listen（）方法。可以直接接收数据 TCP调用listen()方法开始监听端口，将主动套接字（默认）变为被动套接字，传入的参数指定等待连接的最大数量 TCP服务端： TCP客户端： 一般send()和recv()用于TCP，sendto()及recvfrom()用于UDP。sendto和recvfrom一般用于UDP协议中,但是如果在TCP中connect函数调用后也可以用。 服务器编程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立Socket连接，随后的通信就靠这个Socket连接了。 由于服务器会有大量来自客户端的连接，所以，服务器要能够区分一个Socket连接是和哪个客户端绑定的。一个Socket依赖4项：服务器地址、服务器端口、客户端地址、客户端端口来唯一确定一个Socket。但是服务器还需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。 然后，我们要绑定监听的地址和端口()。服务器可能有多块网卡，可以绑定到某一块网卡的IP地址上，也可以用0.0.0.0绑定到所有的网络地址，还可以用127.0.0.1绑定到本机地址。127.0.0.1是一个特殊的IP地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。 端口号需要预先指定。请注意，小于1024的端口号必须要有管理员权限才能绑定： 每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接 利用多线程和socket进行聊天室的创建（UDP）： 下面这段代码是单进程服务器，配合进程或线程也可以建立多任务服务器（TCP）： serverSocket：当这个套接字被关闭时，代表不再接收新的客户端连接 clientSocket：当这个套接字被关闭时，代表不能使用send和recv发收数据。 当利用线程建立多任务服务器时，clientSocket不能关闭。因为子线程共用数据 当利用进程建立时，clientSocket能关闭。子进程和父进程完全”一样“（实时拷贝） 单进程实现多任务： 最后，讲一下单播，多播和广播。 单播：一对一 多播：一对多 广播：一对所有 UDP有广播，TCP没有广播 UDP发送广播数据的条件：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2019%2F08%2F05%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[正则表达式（不能随意添加空格，不然会改变原来含义）： 元字符(不能匹配自身): . $ ^ ( ) { } [ ] $ + \ | *， |：A | B 会匹配 A 或 B 中出现的任何字符。为了能够更加合理的工作，| 的优先级非常低。例如 Fish|C 应该匹配 Fish 或 C，而不是匹配 Fis，然后一个 ‘h’ 或 ‘C’。同样，我们使用 | 来匹配 ‘|’ 字符本身；或者包含在一个字符类中，像这样 [|]。 ^:匹配字符串的起始位置。如果设置了 MULTILINE 标志，就会变成匹配每一行的起始位置。在 MULTILINE 中，每当遇到换行符就会立刻进行匹配。 $:匹配字符串的结束位置，每当遇到换行符也会离开进行匹配。 +：用于指定前一个字符匹配一次或多次 ：匹配的是零次或多次 ？：指定前一个字符匹配零次或者一次。 {m，n}(m和n都是十进制整数)：它的含义是前一个字符必须匹配m次到n（包括n次）次之间 原始字符串来表示正则表达式（就是在字符串前边加上 r） \A:只匹配字符串的起始位置。如果没有设置 MULTILINE 标志的时候，\A 和 ^ 的功能是一样的；但如果设置了 MULTILINE 标志，则会有一些不同：\A 还是匹配字符串的起始位置，但 ^ 会对字符串中的每一行都进行匹配。 \Z:只匹配字符串的结束位置 \B:另一个零宽断言，与 \b 的含义相反，\B 表示非单词边界的位置。 零宽断言： 有些元字符它们不匹配任何字符，只是简单地表示成功或失败，因此这些字符也称之为零宽断言 前向断言： （1）：前向肯定断言：如果当前包含的正则表达式（这里以 … 表示）在当前位置成功匹配，则代表成功，否则失败。一旦该部分正则表达式被匹配引擎尝试过，就不会继续进行匹配了；剩下的模式在此断言开始的地方继续尝试。 （2）：前向否定断言：这跟前向肯定断言相反（不匹配则表示成功，匹配表示失败） 假定我们要处理一段html，我们要替换掉相对url，例如text 这个a标签我们要替换成text，而对于代码 这样的a标签则要保留不做替换。这个应用场景下 就需要判断A标签的href属性如果不是以http://开头则匹配，即要做前向否定的断言. 脱字符：^ ,例如[^ 5 ]会匹配任何字符 “5”之外的任何字符 [ ]他们指定一个字符类用于存放你需要的字符集合。可以单独列出需要匹配字符，也可以两个字符和一个横杆-指定匹配的范围。元字符在方括号中不会触发“特殊功能”，在字符类中，它们只匹配自身。 反斜杠 \：如果在反斜杠后边紧跟着一个元字符，那么元字符的“特殊功能”也不会被触发。例如你需要匹配符号[ 或 \，你可以在他们前面加上一个反斜杠，以消除他们的特殊功能：\[ ,\\ 注意用小括号括住要重复的内容： 匹配ip（万能版）： 非捕获组和命名组： 非捕获组的语法是 (?:…)，这个 … 你可以替换为任何正则表达式。 “捕获”就是匹配的意思啦，普通的子组都是捕获组，因为它们能从字符串中匹配到数据 命名组：：(?P)。很明显，&lt; &gt; 里边的 name 就是命名组的名字啦，除了使用名字访问， 命名组仍然可以使用数字序号进行访问 正则表达式使用以下方法修改字符串： split(***string***[, maxsplit=0**])*：通过正则表达式匹配来分割字符串。如果在 RE 中，你使用了捕获组，那么它们的内容会作为一个列表返回。你可以通过传入一个 *maxsplit 参数来设置分割的数量。如果 maxsplit 的值是非 0，表示至多有 maxsplit 个分割会被处理，剩下的内容作为列表的最后一个元素返回。 .sub(***replacement***, string**[,* count=0**])*返回一个字符串，这个字符串从最左边开始，所有 RE 匹配的地方都替换成 *replacement。如果没有找到任何匹配，那么返回原字符串。可选参数 *count 指定最多替换的次数，必须是一个非负值。默认值是 0，意思是替换所有找到的匹配。下边是使用 sub() 方法的例子，它会将所有的颜色替换成 color： subn:subn() 方法跟 sub() 方法干同样的事情，但区别是返回值为一个包含有两个元素的元组：一个是替换后的字符串，一个是替换的数目。 匹配方法： 匹配的方法和属性： group(N) 返回第N组括号匹配的字符，groups() 返回所有括号匹配的字符，以tuple格式 match匹配的m： findall() 需要在返回前先创建一个列表，而 finditer() 则是将匹配对象作为一个迭代器返回 利用compile来先编译的方法是模式级别的方法（适用于多次使用该正则表达式），可以针对同一种模式做多次匹配，如下图：另一种是模式对象方法 import re import re p=re.compile() re.search(“”,””) p.search() 贪婪模式和非贪婪模式： 贪婪模式是让正则表达式尽可能的匹配符合的内容 在匹配的字符后面加一个问号，启动非贪婪模式 编译标志：编译标志让你可以修改正则表达式的工作方式。在 re模块下，编译标志均有两个名字：完整名和简写 ASCII(re.A) 使得 \w，\W，\b，\B，\s 和 \S 只匹配 ASCII 字符，而不匹配完整的 Unicode 字符。这个标志仅对 Unicode 模式有意义，并忽略字节模式。 DOTALL(re.S) 使得 . 可以匹配任何字符，包括换行符。如果不使用这个标志，. 将匹配除了换行符的所有字符。 IGNORECASE(re.I) 字符类和文本字符串在匹配的时候不区分大小写。举个例子，正则表达式 [A-Z] 也将会匹配对应的小写字母，像 FishC 可以匹配 FishC，fishc 或 FISHC 等。如果你不设置 LOCALE，则不会考虑语言（区域）设置这方面的大小写问题。 LOCALE(re.L) 使得 \w，\W，\b 和 \B 依赖当前的语言（区域）环境，而不是 Unicode 数据库。区域设置是 C 语言的一个功能，主要作用是消除不同语言之间的差异。例如你正在处理的是法文文本，你想使用 \w+ 来匹配单词，但是 \w 只是匹配 [A-Za-z] 中的单词，并不会匹配 ‘é’ 或 ‘&#231;’。如果你的系统正确的设置了法语区域环境，那么 C 语言的函数就会告诉程序 ‘é’ 或 ‘&#231;’ 也应该被认为是一个字符。当编译正则表达式的时候设置了 LOCALE 的标志，\w+ 就可以识别法文了，但速度多少会受到影响。 MULTILINE(re.M) 通常 ^ 只匹配字符串的开头，而 $ 则匹配字符串的结尾。当这个标志被设置的时候，^ 不仅匹配字符串的开头，还匹配每一行的行首；&amp; 不仅匹配字符串的结尾，还匹配每一行的行尾。 VERBOSE(re.X) 这个标志使你的正则表达式可以写得更好看和更有条理，因为使用了这个标志，空格会被忽略（除了出现在字符类中和使用反斜杠转义的空格）；这个标志同时允许你在正则表达式字符串中使用注释， 符号后边的内容是注释，不会递交给匹配引擎（除了出现在字符类中和使用反斜杠转义的 ） 正则表达式特殊符号及用法：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读取、写入excel]]></title>
    <url>%2F2019%2F08%2F05%2F%E8%AF%BB%E5%8F%96%E3%80%81%E5%86%99%E5%85%A5excel%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python如何对excel进行 读取和写入操作。 ##xlrd只支持对excel文件个是为xls文件的读取。 table = data.sheets()[0] #通过索引顺序获取 table = data.sheet_by_index(sheet_index)) #通过索引顺序获取 table = data.sheet_by_name(sheet_name)#通过名称获取 name=workbook_r.sheet_names() #获取文件的所有工作表名字 对行的操作： nrows = table.nrows #获取该sheet中的有效行数 table.row(rowx) #返回由该行中所有的单元格对象组成的列表 table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表 table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表 table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表 table.row_len(rowx) #返回该列的有效单元格长度 对列的操作： ncols = table.ncols #获取列表的有效列数 table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表 table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表 table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表 table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 获取单元格内容： 单元格 A1= table.cell(0,0).value C4=able.cell(2,3).value 使用行列索引 cell_A1 = table.row(0)[0].value cell_A2 = table.col(1)[0].value ##xlwt只支持对Excel文件格式为xls文件的写入 add_sheet(sheet_name): 添加sheet get_sheet(Sheet_name): 选择sheet save(file_name): 保存]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>xlrd、xlwt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收GC]]></title>
    <url>%2F2019%2F08%2F05%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6GC%2F</url>
    <content type="text"><![CDATA[​ 每种语言都有自己的垃圾回收机制。接下来我们来讲一下python的垃圾回收机制。 小整数对象池：python对小整数的定义为[-5，257)，这些整数对象是提前建立好的，不会被垃圾回收。单个字母也一样，但是当定义两个相同的字符串（没有空格等特殊符号），触发intern机制，引用计数为零，触发垃圾回收。 引用计数机制的优点：简单、实时性（一旦没有引用，内存就直接释放了）。 缺点：维护引用计数消耗资源、循环引用 python以引用计数为主，隔代回收为辅进行垃圾回收 GC模块（不能重写del方法）： 1、gc.set_debug(flags) 设置gc的debug日志，一般设置为gc.DEBUG_LEAK 2、gc.collect([generation]) 显式进行垃圾回收，可以输入参数，0代表只检查第一代的对象，1代表检查一，二代的对象，2代表检查一，二，三代的对象，如果不传参数，执行一个full collection，也就是等于传2。 返回不可达（unreachable objects）对象的数目 3、gc.get_threshold() 获取的gc模块中自动执行垃圾回收的频率 4、gc.set_threshold(threshold0[,threshold1[, threshold2]) 设置自动执行垃圾回收的频率。 5、gc.get_count() 获取当前自动执行垃圾回收的计数器，返回一个长度为3的列表 6、gc.disable() 把gc关闭,gc.enable()打开gc（默认打开） 7.gc.garbage 存储垃圾 导致引用计数+1的情况： 导致引用计数-1的情况： 查看一个对象的引用计数： 因为调用函数的时候传入a，所以是2.真正的引用计数=sys.getrefcount()-1]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂生产模式]]></title>
    <url>%2F2019%2F08%2F05%2F%E5%B7%A5%E5%8E%82%E7%94%9F%E4%BA%A7%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[​ 今天，我们来讲一下什么是工厂方法模式。工厂方法模式是在基类完成基本框架的搭建，在子类中具体实现方法的实现。工厂模式是一种典型的解耦模式。 函数或者类之间的关系越强，耦合性越强。代码就越难更新。 使用函数进行解耦： 使用类进行解耦：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python生产模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类和对象]]></title>
    <url>%2F2019%2F08%2F05%2F%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[​ 众所周知，面向过程是根据业务逻辑从上到下写代码，面向过程：根据业务逻辑从上到下写代码面向对象：将数据与函数绑定到一起，进行封装，这样能更快速的开发程序，减少了重复代码的重写过程。面向对象语言三个基本要素：封装 继承 多态把函数和全局变量和在一起就是封装。而python就属于面向对象的语言。 类与对象的关系和区别：类是抽象的概念，仅仅代表事物的模板。对象是一个能够看得到，摸得着的具体的实体比如：飞机是对象，飞机图纸是类。 类由三部分构成: 类的名称:类名 类的属性：一组数据 一个特殊的对象：能够知道这个对象的class. 类的方法：允许进行的操作 类的抽象：拥有相同或者类似属性和行为的对象都可以抽象出一个类 python调用__init__方法的作用：初始化对象 python调用__str__方法： 私有方法：外界不能直接调用 私有属性：可以添加，可以添加后取值，不可以取值。（以双下划线开头为私有变量，单下划线开头的为保护变量） 通过内部方法取值： 私有属性无法取值的原因是因为名字重整技术，其实可以通过_类名_属性可以访问： 在这里讲一下私有的原理：这是通过名字重整机制改变的。命名规则：_类名__num（尽量不要用） python可以自己调用__del__方法，__del__是该对象被删除后调用的方法，注意：这里的删除是没有引用对象 完全删除后（没有引用对象）： 测量对象的引用对象：用sys模块的sys.getrefcount，得出结果后减一。 __new__方法：创建对象 实例化对象相当于做三件事： 1.通过__init__来创建对象，然后找了一个变量来接收___init__的返回值。这个返回值表示创建出来的对象引用。 2.__init__（刚刚创建出来的对象的引用） 3.返回对象的引用。 __new__方法 和 \init__方法的区别: 1.\init__ 通常用于初始化一个新实例，控制这个初始化的过程，比如添加一些属性， 做一些额外的操作，发生在类实例被创建完以后。它是实例级别的方法。没有返回值。负责初始化 2._new_ 通常用于控制生成一个新实例的过程。它是类级别的方法，参数是cls。负责创建。__init__和\new__方法合起来相当于C++的构造函数。 下面讲一个新的概念：单例 单例是创建了多少个对象都是指向同一片内存。 只初始化一次对象： 继承：可以少写代码。继承父类的方法和属性 继承可以多类继承。当子类和父类有什么不同时，可以进行重写（在子类中写一个和父类方法名相同的方法进行不同操作） 私有方法，私有属性不能被继承，可以被间接调用，如果在子类中实现了一个公有方法调用的方法或属性，那么这个方法是不能调用继承的父类的私有方法和私有属性 多继承： 当子类中的方法和父类方法名一样时会按照以下顺序去进行执行： 子类调用父类的三种方法： 父类.方法 super().init(不用传self，传参数) super(父类，self).init（不传self，参数）可以根据父类指定调用哪一个父类 多态：在写完方法的时候并不知道调用的是什么方法。真正执行的时候才知道 接下来的内容也很重要哦： 类在程序中也属于一个对象，称之为类对象。同过类创建出来的对象称之为实例对象。 类属性(classmethod)：类对象里的属性 实例属性：实例对象里的属性 实例属性和类属性的区别：实例属性和具体的某个实例对象有关系，且一个实例对象和另外一个实例对象是不共享属性的。类属性属于类对象，并且多个实例对象共享同一个类属性。 实例方法和类方法： 静态方法： 实例方法 类方法和静态方法的区别：实例方法和类方法必须传一个参数（实例方法self用来接收对象，类方法cls用于接收类），静态方法不需要参数（可以有）。 动态添加属性和方法： slots可以限制添加属性：这可以告诉解释器这个类的所有实例属性都在这了。可以节省大量内存。每个子类都要定义slots属性，因为解释器会忽略继承的slots属性。 内建属性： getattribute:属性拦截器 使用类名调用类属性时，不会经过__getattribute__方法，其他均要调用。（可以用来做日志）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python的类和对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python异常]]></title>
    <url>%2F2019%2F08%2F05%2Fpython%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[​ 今天我们来讲一下python的异常。何为异常？即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。异常时python对象，表示一个错误。当python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。 ##try/except可以用来捕获异常 ##一个try捕获多个异常： ##一个try对多个except：根据不同的except做不同的操作 ##try-finally语句无论是否发生异常都将执行最后的代码。 ##异常传递 ##raise自定义异常： ##最后是python的标准异常：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python数据类型]]></title>
    <url>%2F2019%2F08%2F04%2Fpython%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[​ 每种编程语言都有属于自己的数据类型。今天，我们就来讲一下python的数据类型——列表，元祖，字典，字符串，堆，集合，队列。 列表​ 同时还有一些比较常用的方法，类似cmp(比较大小，python3已经找不到。如要使用，可以利用operator模块)，len(list)计算列表元素个数，max(list)求列表中的最大值，min(list)求最小值。下面对上面列表的排序方法(list.sort)进行讲解： 下张图片是对以上方法的coding： ##元祖（tuple） 元祖不像列表可以改变。元祖是不可变的。但可以利用切片灵活使用。 字典（dict）####注意：字典支持常见的集合操作（&amp;等操作） 字典有多个键与其对应的值构成的键-值对组成。键-值对称为项。每个键和它的值之间用:(冒号)隔开，项之间用,(逗号)隔开。 len(d)返回d中项(键-值对)的数量 d[k]返回关联到键k的值 d[k]=v将值关联到键k上 del d[k] 删除键为k的项 k in d 检查d中是否有含有键为k的值 clear()：清除字典中所有的项 fromkeys()：使用给定的键建立新的字典，每个键都对应一个默认的值None，也可以设立默认值 get():访问字典项，访问一个不存在的键时，没有异常。且可以定义默认值为None。 items和iteritems： items将所有的项以列表方式返回，列表中的每一项都表示为（键，值）对的形式。 iteritems方法的作用大致相同，但是会返回一个迭代器对象而不是列表。 keys和iterkeys： keys方法将字典中的键以列表形式返回，而iterkeys则返回针对键的迭代器 values和itervalues： values方法以列表的形式返回字典中的值，itervalues返回值的迭代器。与返回键的列表不同的是，返回值的列表中可以包含重复的元素。 pop：用来获得对应于给定键的值，然后将这个键-值对从字典中移除。 popitem：弹出随机的项 setdefault：获得与给定键相关联的值，能在字典中不含有给定键的情况下设立相应的键值。 update：可以利用一个字典项更新另外一个字典 堆（heapq）使用该数据类型前，我们先导入一个新的模块heapq heappush(heap,x):将x入对 heapop(heap)：将堆中最小的元素弹出 heapify(heap)：将heap属性强制应用到任意一个列表 heapreplace(heap,x)：将堆中最小的元素弹出，同时将x入堆 nlargest(n,iter)：返回iter中前n大的元素 nsmallest(n,iter)：返回iter中前n小的元素 集合（set）集合可取交集、取并集、取差集、对称差集： 利用set做去重操作： 集合分为可变集合和不可变集合： 队列（Queue）在python中，队列时线程间最常用的交换数据形式，Queue模块时提供队列操作的模块。 先进先出（FIFO）： 先进后出（LIFO）： 优先级队列：优先级队列put进去一个元祖，（优先级，数据），优先级数字越小，优先级越高。 注意：如果有两个元素优先级是一样的，那么在出队的时候是按照先进先出的顺序。 双端队列： 队列的方法： 使用put方法往队列中添加元素，需要考虑是否能放下的问题如果放不下了，默认会阻塞(block=True)，阻塞时可以定义超时时间timeout。可以使用block=False设置阻塞时立即报错 使用get()从队列里取数据。如果为空的话，blocking= False 直接报 empty异常。如果blocking = True，就是等一会，timeout必须为 0或正数。None为一直等下去，0为不等，正数n为等待n秒还不能读取，报empty异常。 字符串（string）用+拼接字符串： 将值转换为字符串的机制： 1、通过str函数，把值转换为合理形式的字符串，以便用户可以理解 2、通过repr函数创建一个字符串，以合法的python表达式的形式表示值 join和split： 字符串格式化方式： capitalize() 把字符串的第一个字符改为大写 casefold() 把整个字符串的所有字符改为小写 center(width) 将字符串居中，并使用空格填充至长度 width 的新字符串 count(sub[, start[, end]]) 返回 sub 在字符串里边出现的次数，start 和 end 参数表示范围，可选。 encode(encoding=’utf-8’, errors=’strict’) 以 encoding 指定的编码格式对字符串进行编码。 startswith(prefix[, start[, end]]) 检查字符串是否以 prefix 开头，是则返回 True，否则返回 False。start 和 end 参数可以指定范围检查，可选。 endswith(sub[, start[, end]])检查字符串是否以 sub 子字符串结束，如果是返回 True，否则返回 False。start 和 end 参数表示范围，可选。 startswith和endswith如果需要同时针对多个选项做检查，只需要给startswith和endswith提供包含可能选项的元祖。 expandtabs([tabsize=8]) 把字符串中的 tab 符号（\t）转换为空格，如不指定参数，默认的空格数是 tabsize=8。 find(sub[, start[, end]])检测 sub 是否包含在字符串中，如果有则返回索引值，否则返回 -1，start 和 end 参数表示范围，可选。 maketrans() 方法用于创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。 注：两个字符串的长度必须相同，为一一对应的关系。 translate(table)根据 table 的规则（可以由 str.maketrans(‘a’, ‘b’) 定制）转换字符串中的字符 index(sub[, start[, end]]) 跟 find 方法一样，不过如果 sub 不在 string 中会产生一个异常。 isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返回 True，否则返回 False。 isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True，否则返回 False。 isdecimal() 如果字符串只包含十进制数字则返回 True，否则返回 False。 isdigit() 如果字符串只包含数字则返回 True，否则返回 False。 islower() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是小写，则返回 True，否则返回 False。 isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False。 isspace() 如果字符串中只包含空格，则返回 True，否则返回 False。 istitle() 如果字符串是标题化（所有的单词都是以大写开始，其余字母均小写），则返回 True，否则返回 False。 isupper() 如果字符串中至少包含一个区分大小写的字符，并且这些字符都是大写，则返回 True，否则返回 False。 join(sub) 以字符串作为分隔符，插入到 sub 中所有的字符之间。 ljust(width) 返回一个左对齐的字符串，并使用空格填充至长度为 width 的新字符串。 rjust(width) 返回一个右对齐的字符串，并使用空格填充至长度为 width 的新字符串。 format也可以完成对齐的任务。“&lt;”：左对齐 “&gt;”：右对齐 “^”：居中对齐 lower() 转换字符串中所有大写字符为小写。 lstrip() 去掉字符串左边的所有空格无法去除中间的字符 rstrip() 删除字符串末尾的空格。无法去除中间的字符 partition(sub) 找到子字符串 sub，把字符串分成一个 3 元组 (pre_sub, sub, fol_sub)，如果字符串中不包含 sub 则返回 (‘原字符串’, ‘’, ‘’) replace(old, new[, count]) 把字符串中的 old 子字符串替换成 new 子字符串，如果 count 指定，则替换不超过 count 次。 rfind(sub[, start[, end]]) 类似于 find() 方法，不过是从右边开始查找。返回值是下标 rindex(sub[, start[, end]]) 类似于 index() 方法，不过是从右边开始。 rpartition(sub) 类似于 partition() 方法，不过是从右边开始查找。 split(sep=None, maxsplit=-1) 不带参数默认是以空格为分隔符切片字符串，如果 maxsplit 参数有设置，则仅分隔 maxsplit 个子字符串，返回切片后的子字符串拼接的列表。 splitlines(([keepends])) 在输出结果里是否去掉换行符，默认为 False，不包含换行符；如果为 True，则保留换行符。。 strip([chars]) 删除字符串前边和后边所有的空格，chars 参数可以定制删除的字符，可选。 swapcase() 翻转字符串中的大小写。 title() 返回标题化（所有的单词都是以大写开始，其余字母均小写）的字符串。 upper() 转换字符串中的所有小写字符为大写。 zfill(width) 返回长度为 width 的字符串，原字符串右对齐，前边用 0 填充。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2019%2F08%2F03%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[​ 你好陌生人，欢迎来到湛蓝星空的博客。这是我的第一篇博客，纠结了很久要不要写，因为我通常都是用一些笔记软件来记录学的一些知识。在这里，希望你能学到你希望学到的东西。 联系方式 qq：1171708687 邮箱：1171708687@qq.com 微信：18269658106 加我的时候备注博客就好 PS：本人实在太菜。如果有错误，请谅解。]]></content>
      <categories>
        <category>你好</category>
      </categories>
      <tags>
        <tag>你好</tag>
      </tags>
  </entry>
</search>
